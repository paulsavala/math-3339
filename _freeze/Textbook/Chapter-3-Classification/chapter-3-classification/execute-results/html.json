{
  "hash": "c7c51a97c055f210e6ece416ca5b9ef5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Chapter 3: Classification Models\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Chapter Resources {#ch3-resources}\n\n**Related Assignments:**\n\n- [Chapter 3 Homework](../../Assignments/Chapter%203%20-%20Classification/chapter-3-homework.qmd)\n\n---\n\n## Introduction {#ch3-intro}\n\nYou've spent weeks learning regression—predicting continuous values like house prices or income. But what happens when you need to predict categories instead? Will this customer churn or stay? Is this email spam or legitimate? Does this patient have the disease or not?\n\nThis is classification, and it's everywhere. Classification powers the spam filter in your email, the fraud detection on your credit card, the recommendation systems suggesting what you should watch next, and the medical diagnostics helping doctors identify diseases. If you've ever wondered \"will this happen or not?\" or \"which category does this belong to?\"—that's a classification problem.\n\nJust like with regression, there isn't one \"best\" classification algorithm. Logistic regression is fast and interpretable. Decision trees are easy to explain to non-technical stakeholders. Random forests are robust and powerful. Support vector machines handle high-dimensional data elegantly. k-Nearest neighbors is beautifully simple but computationally expensive. Each has strengths, weaknesses, and situations where it shines.\n\nAnd the evaluation is completely different from regression. You can't use R² or MSE. Instead, you'll need to navigate confusion matrices, ROC curves, precision-recall tradeoffs, and decide whether false positives or false negatives are more costly for your specific problem. A model that's 99% accurate might be completely useless if you're trying to detect a rare disease.\n\nThis chapter will teach you not just how to fit classification models, but how to think like a data scientist choosing between them. You'll learn:\n\n- How five major classification algorithms work and when to use each\n- How to interpret confusion matrices and choose the right metrics\n- The precision-recall tradeoff and why it matters\n- How to handle imbalanced datasets (the most common real-world scenario)\n- How to visualize decision boundaries to understand what your model is actually doing\n- How to use ROC curves to compare model performance\n\nLet's jump in.\n\n---\n\n## 1. Machine Learning Paradigms: Supervised vs Unsupervised Learning {#ch3-1}\n\nBefore we dive into specific classification algorithms, let's step back and understand a fundamental distinction in machine learning: supervised versus unsupervised learning. Understanding this paradigm will help you recognize when classification is the right approach and when other techniques might be more appropriate.\n\n### 1.1 Supervised Learning: Learning from Labels {#ch3-1-1}\n\n**Supervised learning** is what we've been doing throughout this course: you give the model input data (features) and the correct answers (labels/targets), and it learns to predict the right answer for new inputs.\n\n::: {#8fc602d6 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load Spotify data (supervised classification example)\nspotify_df = pd.read_csv('../data/spotify.csv')\n\n# We have features (X) and a target (y)\n# Let's predict whether a song will be popular (binary classification)\nX = spotify_df[['danceability', 'energy', 'valence', 'tempo']]\ny = (spotify_df['popularity'] > 50).astype(int)  # 1 if popular, 0 if not\n\nprint(\"Supervised learning setup:\")\nprint(f\"Features (X) shape: {X.shape}\")\nprint(f\"Target (y) shape: {y.shape}\")\nprint(f\"Class distribution: {y.value_counts().to_dict()}\")\nprint(\"\\nFirst few rows of features:\")\n\nX.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSupervised learning setup:\nFeatures (X) shape: (42663, 4)\nTarget (y) shape: (42663,)\nClass distribution: {0: 33578, 1: 9085}\n\nFirst few rows of features:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>valence</th>\n      <th>tempo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.792</td>\n      <td>0.7310</td>\n      <td>0.8380</td>\n      <td>113.007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.418</td>\n      <td>0.3280</td>\n      <td>0.6800</td>\n      <td>164.315</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.199</td>\n      <td>0.0957</td>\n      <td>0.0391</td>\n      <td>77.722</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.862</td>\n      <td>0.5210</td>\n      <td>0.3730</td>\n      <td>154.983</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.168</td>\n      <td>0.1960</td>\n      <td>0.0554</td>\n      <td>83.898</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cc392c63 .cell execution_count=2}\n``` {.python .cell-code}\nprint(\"\\nCorresponding targets (1 = popular, 0 = not popular):\")\ny.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCorresponding targets (1 = popular, 0 = not popular):\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n0    0\n1    0\n2    0\n3    1\n4    0\nName: popularity, dtype: int64\n```\n:::\n:::\n\n\nThe key to supervised learning is that we have labels. We know what the correct answer is for each training example. The model learns by comparing its predictions to the true labels and adjusting to get closer.\n\n::: {.callout-note}\nBoth regression (from Chapter 2) and classification are supervised learning tasks. The difference is that regression predicts continuous values while classification predicts discrete categories. But both require labeled training data.\n:::\n\n### 1.2 Unsupervised Learning: Finding Patterns Without Labels {#ch3-1-2}\n\n**Unsupervised learning** is different: you only have input data (X), no labels (y). The model's job is to find patterns, structure, or groupings in the data on its own.\n\nFor example, we can use unsupervised learning to find natural groupings of flowers based on their measurements. We don't tell the model what the groupings should be—it discovers them on its own. This is what clustering algorithms like KMeans do.\n\n::: {#dc69cfaf .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Using Iris dataset, but ignoring the labels (species)\n# Can we find natural groupings of flowers based on their measurements?\niris = load_iris(as_frame=True)\niris_df = iris.frame\n\nX_unlabeled = iris_df[['petal length (cm)', 'petal width (cm)']]\n\n# K-Means clustering: find groups in the data\nkmeans = KMeans(n_clusters=3, random_state=1, n_init=10)\nclusters = kmeans.fit_predict(X_unlabeled)\n\n# Visualize the clusters\nplt.figure(figsize=(10, 6))\nplt.scatter(X_unlabeled['petal length (cm)'], X_unlabeled['petal width (cm)'],\n            c=clusters, cmap='viridis', alpha=0.6)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c='red', marker='X', s=200, edgecolors='black', label='Centroids')\nplt.xlabel('Petal Length (cm)')\nplt.ylabel('Petal Width (cm)')\nplt.title('K-Means Clustering: Discovering Flower Groups (Unsupervised)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-4-output-1.png){width=812 height=523}\n:::\n:::\n\n\nNotice what happened: we didn't tell the model which flowers were which species. We just said \"find 3 groups\" and it discovered natural clusters based on petal measurements alone.\n\nCommon unsupervised learning tasks include:\n\n- **Clustering:** Grouping similar items together (customer segmentation, document clustering)\n- **Dimensionality Reduction:** Reducing many features to a few key ones (PCA, t-SNE for visualization)\n- **Anomaly Detection:** Finding unusual examples (fraud detection, manufacturing defects)\n\n### 1.3 When to Use Each Paradigm {#ch3-1-3}\n\nUse **supervised learning** when:\n\n- You have labeled data (you know the correct answers)\n- You want to predict something specific\n- You can define success (accuracy, error rate, etc.)\n\nUse **unsupervised learning** when:\n\n- You don't have labels (or getting labels is too expensive)\n- You want to explore data structure\n- You're looking for patterns you don't know exist yet\n\n::: {.callout-note}\nMost real-world applications use supervised learning, because prediction is usually the goal. Unsupervised learning is powerful for exploration and preprocessing, but harder to evaluate (how do you know if clusters are \"good\"?). Unsupervised learning is often used as an intermediate step to generate new features, which are then used in a supervised learning model.\n:::\n\nFor the rest of this chapter, we'll focus on supervised classification, since that's where you'll spend most of your time as a data scientist.\n\n---\n\n## 2. Classification vs Regression: What's Different? {#ch3-2}\n\n### 2.1 The Fundamental Difference {#ch3-2-1}\n\nIn regression, we predict a continuous value. In classification, we predict a category. Seems simple, but this fundamental difference changes everything about how we build, train, and evaluate models.\n\n::: {#1e24da60 .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Let's load a classification dataset - Titanic survival\n# This is a classic binary classification problem: survived or didn't survive\ntitanic = pd.read_csv('../data/titanic.csv')\n\n# Look at the target variable\nprint(\"Target variable (Survived) value counts:\")\nprint(titanic['Survived'].value_counts())\nprint(f\"\\nProportion survived: {titanic['Survived'].mean():.2%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTarget variable (Survived) value counts:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nProportion survived: 38.38%\n```\n:::\n:::\n\n\nInstead of predicting a number on a continuous scale, we're predicting one of two discrete outcomes: 0 (didn't survive) or 1 (survived). This is **binary classification**—the most common type.\n\n### 2.2 Types of Classification Problems {#ch3-1-2}\n\n**Binary Classification:** Two possible outcomes (yes/no, spam/ham, fraud/legitimate)\n\n- Titanic survival\n- Email spam detection\n- Loan default prediction\n- Disease diagnosis\n\n**Multi-class Classification:** More than two categories\n\n- Iris flower species (setosa, versicolor, virginica)\n- Handwritten digit recognition (0-9)\n- Customer segment classification\n- Image classification (cat, dog, bird, etc.)\n\nMost of this chapter focuses on binary classification since it's simpler to visualize and understand. But the techniques extend naturally to multi-class problems.\n\n### 2.3 Why We Can't Use Linear Regression {#ch3-1-3}\n\nYou might be tempted to use linear regression for classification. Just predict the category as a number, right? Let's see why that breaks:\n\n::: {#df458e31 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nimport seaborn as sns\n\n# Prepare simple features\ntitanic_clean = titanic[['Age', 'Survived', 'Pclass', 'Fare']].dropna()\nX = titanic_clean[['Age', 'Pclass', 'Fare']].values\ny = titanic_clean['Survived'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Try linear regression (WRONG!)\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\nlinear_pred = linear_model.predict(X_test)\n\nprint('Predictions:')\nprint(linear_pred[:5])\n\nprint('Classes:')\nprint(y_test[:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredictions:\n[0.33588326 0.70372944 0.4655869  0.77416664 0.0881573 ]\nClasses:\n[0 1 1 1 0]\n```\n:::\n:::\n\n\nSee the problem? Linear regression gives us predictions like 0.73 or -0.15 or 1.42. But we need 0 or 1! We could threshold at 0.5, but linear regression makes no guarantee that predictions will be between 0 and 1. It's using the wrong tool for the job.\n\nClassification models are designed to output probabilities (values between 0 and 1) or direct class predictions. That's why we need specialized algorithms.\n\n---\n\n## 3. Logistic Regression: The Foundation {#ch3-3}\n\n### 3.1 From Linear to Logistic {#ch3-2-1}\n\nLogistic regression might sound like a regression technique, but don't be fooled—it's pure classification. The name comes from its history: it takes linear regression and transforms it to work for classification.\n\nHere's the key insight: instead of predicting the outcome directly, logistic regression predicts the **probability** of the positive class. It does this by taking a linear combination of features (just like linear regression) and passing it through the **sigmoid function**:\n\n$$\n\\text{probability} = \\frac{1}{1 + e^{-z}}\n$$\n\nwhere $z$ is the linear combination: $z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...$\n\nThe sigmoid function has a beautiful property: it squashes any real number into the range (0, 1), making it perfect for probabilities.\n\n::: {#8487486b .cell execution_count=6}\n``` {.python .cell-code}\n# Visualize the sigmoid function\nz = np.linspace(-10, 10, 100)\nsigmoid = 1 / (1 + np.exp(-z))\n\nplt.figure(figsize=(10, 6))\nplt.plot(z, sigmoid, 'b-', linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Decision threshold (0.5)')\nplt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\nplt.xlabel('z (linear combination of features)', fontsize=12)\nplt.ylabel('Probability of positive class', fontsize=12)\nplt.title('The Sigmoid Function: Turning Linear into Probability', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-7-output-1.png){width=815 height=528}\n:::\n:::\n\n\nWhen $z = 0$, the probability is exactly 0.5. As $z$ increases, the probability approaches 1. As $z$ decreases, the probability approaches 0. The sigmoid smoothly transitions between these extremes.\n\n### 3.2 Fitting Logistic Regression {#ch3-2-2}\n\nLet's fit a logistic regression model to predict Titanic survival:\n\n::: {#67b67802 .cell execution_count=7}\n``` {.python .cell-code}\n# Fit logistic regression\nlog_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_model.fit(X_train, y_train)\n\n# Get predictions - both probabilities and classes\ny_pred_proba = log_model.predict_proba(X_test)[:, 1]  # Probability of class 1\ny_pred_class = log_model.predict(X_test)  # Predicted class (0 or 1)\n\n# Show the difference\ncomparison = pd.DataFrame({\n    'True': y_test,\n    'Prob_Survived': y_pred_proba,\n    'Predicted': y_pred_class\n})\n\ncomparison.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>True</th>\n      <th>Prob_Survived</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.302603</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.741984</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.459317</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.791706</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.118771</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0.707625</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0.183988</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>0.202158</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0.217621</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0.261196</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice the two types of predictions:\n\n- **Probabilities** (from `predict_proba`): Values between 0 and 1 representing confidence\n- **Classes** (from `predict`): Hard 0/1 decisions using a threshold (default 0.5)\n\nIf the probability is above 0.5, we predict class 1 (survived). Otherwise, class 0 (didn't survive). But you can adjust this threshold based on your problem—more on that later.\n\nHow can we evaluate this model's performance? One simple way is to ask about the average probability for each different true class (i.e. average probabibility of survival for those who actually survived vs. those who didn't).\n\n::: {#17a9d497 .cell execution_count=8}\n``` {.python .cell-code}\n# Calculate average probability for each true class\navg_prob_survived = y_pred_proba[y_test == 1].mean()\navg_prob_not_survived = y_pred_proba[y_test == 0].mean()\n\nprint(f\"Average probability for those who survived: {avg_prob_survived:.3f}\")\nprint(f\"Average probability for those who didn't survive: {avg_prob_not_survived:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage probability for those who survived: 0.505\nAverage probability for those who didn't survive: 0.339\n```\n:::\n:::\n\n\nThis is good, but it fails to capture the variability in predictions. A better approach would be to look at the distribution of probabilities for each class.\n\nLet's visualize this:\n\n::: {#562f7289 .cell execution_count=9}\n``` {.python .cell-code}\nimport seaborn as sns\nimport pandas as pd\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=pd.DataFrame({'prob': y_pred_proba[y_test == 1], 'class': 'Survived'}), x='prob', alpha=0.7, label='Survived', bins=20, color='blue', stat='density')\nsns.histplot(data=pd.DataFrame({'prob': y_pred_proba[y_test == 0], 'class': 'Did not survive'}), x='prob', alpha=0.7, label='Did not survive', bins=20, color='red', stat='density')\nplt.xlabel('Predicted Probability of Survival')\nplt.ylabel('Density')\nplt.title('Distribution of Predicted Probabilities by True Outcome')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-10-output-1.png){width=812 height=523}\n:::\n:::\n\n\nWe see that there are more people who survived with higher predicted probability of survival, and more people who died with lower predicted probability. This shows that our model is somewhat calibrated - it tends to give higher probabilities to those who actually survived and lower probabilities to those who didn't.\n\nWe'll learn more advanced techniques later, but this is a good starting point.\n\n### 3.3 Interpreting Coefficients {#ch3-3-3}\n\nJust like linear regression, logistic regression has coefficients. But the interpretation is different. In particular, we're primarily concerned with the **odds ratio** of a coefficient, which is calculated as $e^{\\beta}$ where $\\beta$ is the coefficient. This tells us how the odds of the outcome change for a one-unit increase in the predictor.\n\n::: {#a98bcca7 .cell execution_count=10}\n``` {.python .cell-code}\n# Let's use more features to make interpretation interesting\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ntitanic_features = titanic[features + ['Survived']].dropna()\n\nX_full = titanic_features[features].values\ny_full = titanic_features['Survived'].values\n\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n    X_full, y_full, test_size=0.2, random_state=42\n)\n\n# Fit the model\nlog_model_full = LogisticRegression(random_state=42, max_iter=1000)\nlog_model_full.fit(X_train_full, y_train_full)\n\n# Display coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': log_model_full.coef_[0],\n    'Odds_Ratio': np.exp(log_model_full.coef_[0])\n})\n\ncoef_df\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Coefficient</th>\n      <th>Odds_Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pclass</td>\n      <td>-1.129795</td>\n      <td>0.323100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Age</td>\n      <td>-0.051874</td>\n      <td>0.949448</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SibSp</td>\n      <td>-0.295579</td>\n      <td>0.744101</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Parch</td>\n      <td>0.241379</td>\n      <td>1.273004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fare</td>\n      <td>0.003802</td>\n      <td>1.003810</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Odds ratio** measures how much the odds of the outcome change for a one-unit increase in the predictor. An odds ratio greater than 1 indicates a positive association with the outcome, while an odds ratio less than 1 indicates a negative association.\n\nFor example, if `Fare` has an odds ratio of 1.5, it means that for every one-unit increase in fare, the odds of survival increase by 50%. If `Pclass` has an odds ratio of 0.7, it means that for every one-unit increase in class (higher class number), the odds of survival decrease by 30%. Here we see that:\n\n- Increasing passenger class by 1 (e.g. 3rd class -> 2nd class) _decreased_ the predicted probability of survival by about 68% $(1-0.323 = 0.677)$.\n- Increasing the age by 1 year slightly decreased (~5%) the predicted probability of survival.\n- Increasing the number of parents/children in the family (`Parch`) increased the predicted probability of survival by a whopping 27%!\n\n::: {.callout-warning}\nBe careful about collinearity! Remember that last chapter we discussed how collinearity can lead to non-interpretable coefficients. Since logistic regression is simply performing linear regression under the hood, all those same caveats apply here. For example, it seems weird to say that people in higher passenger classes are _less likely_ to survive. What's probably going on is collinearity between `Pclass` and `Fare`, since both are essentially measuring the same thing (how much the ticket cost).\n:::\n\n---\n\n## 4. The Confusion Matrix: Understanding Errors {#ch3-4}\n\n### 4.1 What Is a Confusion Matrix? {#ch3-3-1}\n\nWhen you make predictions, you'll make mistakes. The confusion matrix breaks down exactly what kinds of mistakes you're making. It's a 2×2 table for binary classification:\n\n::: {#fcad9e7a .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Get predictions\ny_pred = log_model_full.predict(X_test_full)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test_full, y_pred)\n\n# Visualize it\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Not Survived', 'Survived'],\n            yticklabels=['Not Survived', 'Survived'])\nplt.title('Confusion Matrix', fontsize=14)\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.show()\n\nprint(f\"\\nTrue Negatives (TN): {cm[0, 0]} people died and we predicted they would die\")\nprint(f\"False Positives (FP): {cm[0, 1]} people survived but we predicted they would die\")\nprint(f\"False Negatives (FN): {cm[1, 0]} people died but we predicted they would survive\")\nprint(f\"True Positives (TP): {cm[1, 1]} people survived and we predicted they would survive\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-12-output-1.png){width=658 height=528}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTrue Negatives (TN): 74 people died and we predicted they would die\nFalse Positives (FP): 13 people survived but we predicted they would die\nFalse Negatives (FN): 30 people died but we predicted they would survive\nTrue Positives (TP): 26 people survived and we predicted they would survive\n```\n:::\n:::\n\n\n**The four quadrants:**\n\n- **True Negatives (TN)**: Correctly predicted \"didn't survive\"\n- **False Positives (FP)**: Predicted \"survived\" but actually didn't (Type I error)\n- **False Negatives (FN)**: Predicted \"didn't survive\" but actually did (Type II error)\n- **True Positives (TP)**: Correctly predicted \"survived\"\n\nHere we have shown the confusion matrix with the actual counts from our logistic regression model, such as 74 people, 13 people, etc. However, we're often more interested in understanding how well our model predicted the correct values. For example, of the people who actually survived, what percentage did we predict will and won't survive? In other words, we're normalizing along the rows so that each rows adds up to 100%.\n\n::: {#80154ddf .cell execution_count=12}\n``` {.python .cell-code}\n# Calculate row-wise percentages (normalize by actual positives)\nrow_sums = cm.sum(axis=1)\nrecall_precision = cm.astype(float) / row_sums.reshape(-1, 1)\n\n# Display as a confusion matrix, with numbers formatted as percentages\nplt.figure(figsize=(8, 6))\nsns.heatmap(recall_precision, annot=True, fmt='.0%', cmap='Blues', cbar=False,\n            xticklabels=['Not Survived', 'Survived'],\n            yticklabels=['Not Survived', 'Survived'])\nplt.title('Normalized Confusion Matrix (Row-wise)', fontsize=14)\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-13-output-1.png){width=658 height=528}\n:::\n:::\n\n\nHere we see that, of the people who actually died, 87% were correctly predicted as having died (True Negative rate), while 13% were incorrectly predicted as having survived (False Positive rate). Similarly, of the people who actually survived, 92% were correctly predicted as having survived (True Positive rate), while 8% were incorrectly predicted as having died (False Negative rate).\n\n::: {.callout-tip}\nDisplaying the data normalized relative to the true values is typically more useful than looking at the raw numbers. When converting to percentages we see that our model did quite well predicting people who will die, but very poorly predicting people who will survive. This suggests our model may be biased towards predicting death, which could be important to consider for real-world applications.\n:::\n\n### 4.2 Computing Metrics from the Confusion Matrix {#ch3-3-2}\n\nAll the important classification metrics come directly from these four numbers:\n\n**Accuracy**: What percentage of all predictions were correct?\n$$\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n$$\n\n::: {#708247e2 .cell execution_count=13}\n``` {.python .cell-code}\naccuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\nprint(f\"Accuracy: {accuracy:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.699\n```\n:::\n:::\n\n\n**Precision**: When you predict positive, how often are you right?\n$$\n\\text{Precision} = \\frac{TP}{TP + FP}\n$$\n\n::: {#cfb84e9e .cell execution_count=14}\n``` {.python .cell-code}\nprecision = cm[1, 1] / (cm[1, 1] + cm[0, 1])\nprint(f\"Precision: {precision:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 0.667\n```\n:::\n:::\n\n\n**Recall (Sensitivity)**: Of all actual positives, how many did you find?\n$$\n\\text{Recall} = \\frac{TP}{TP + FN}\n$$\n\n::: {#e9375945 .cell execution_count=15}\n``` {.python .cell-code}\nrecall = cm[1, 1] / (cm[1, 1] + cm[1, 0])\nprint(f\"Recall: {recall:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall: 0.464\n```\n:::\n:::\n\n\n**F1 Score**: Harmonic mean of precision and recall\n$$\n\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\n\n::: {#f4405294 .cell execution_count=16}\n``` {.python .cell-code}\nf1 = 2 * (precision * recall) / (precision + recall)\nprint(f\"F1 Score: {f1:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1 Score: 0.547\n```\n:::\n:::\n\n\n### 4.3 When Each Metric Matters {#ch3-3-3}\n\nDifferent problems care about different metrics:\n\n**Use Accuracy when:**\n\n- Classes are balanced\n- False positives and false negatives are equally costly\n- Example: Predicting coin flips (50/50 balanced, no asymmetric cost)\n\n**Use Precision when:**\n\n- False positives are very costly\n- You want to be confident when you predict positive\n- Example: Spam detection (marking legitimate email as spam is very annoying, but one or two spam emails slipping through is acceptable)\n\n**Use Recall when:**\n\n- False negatives are very costly\n- You want to catch all positive cases, even if it means some false alarms\n- Example: Disease screening (missing a sick patient with a life-threatening condition is much worse than telling a healthy patient that they are sick)\n\n**Use F1 Score when:**\n\n- You want a balance between precision and recall\n- Classes are imbalanced\n- Example: Fraud detection (imbalanced, and both FP and FN have significant consequences)\n\n::: {.callout-note}\nThere's almost always a tradeoff between precision and recall. Increase one, and the other goes down. You need to decide which matters more for your specific problem.\n:::\n\nLet's see the full classification report:\n\n::: {#4cfc50cc .cell execution_count=17}\n``` {.python .cell-code}\nprint(classification_report(y_test_full, y_pred,\n                          target_names=['Not Survived', 'Survived']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\nNot Survived       0.71      0.85      0.77        87\n    Survived       0.67      0.46      0.55        56\n\n    accuracy                           0.70       143\n   macro avg       0.69      0.66      0.66       143\nweighted avg       0.69      0.70      0.69       143\n\n```\n:::\n:::\n\n\nThis report shows precision, recall, and F1 for both classes, plus overall accuracy.\n\n---\n\n## 5. Decision Trees: Interpretable Non-Linear Classifiers {#ch3-5}\n\n### 5.1 How Decision Trees Work {#ch3-4-1}\n\nDecision trees make predictions by asking a series of yes/no questions about the features. They split the data recursively based on feature values, creating a tree structure.\n\nHere's the beautiful part: decision trees are incredibly interpretable. You can literally draw out the decision-making process and explain it to anyone.\n\n::: {#ea5fa65c .cell execution_count=18}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n# Fit a simple decision tree\ntree_model = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_model.fit(X_train_full, y_train_full)\n\n# Visualize the tree\nplt.figure(figsize=(20, 10))\nplot_tree(tree_model, feature_names=features, class_names=['Not Survived', 'Survived'],\n          filled=True, fontsize=10, rounded=True)\nplt.title('Decision Tree for Titanic Survival', fontsize=16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-19-output-1.png){width=1507 height=783}\n:::\n:::\n\n\nEach box shows:\n\n- The question being asked (e.g., \"Fare <= 26.27?\")\n- The Gini impurity (measure of how mixed the classes are, discussed in the next section)\n- The number of samples reaching this node\n- The class distribution (number of zeros and ones)\n- The predicted class\n\n### 5.2 Splitting Criteria: Gini vs Entropy {#ch3-4-2}\n\nDecision trees decide where to split by maximizing information gain. The most common criteria is the Gini impurity.\n\n**Gini Impurity**: Measures how often a randomly chosen element would be incorrectly classified\n$$\n\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2\n$$\n\nwhere $p_i$ is the proportion of samples in class $i$.\n\n::: {.callout-note}\nAnother criteria you'll sometimes see is the Entropy, which is a measure of the amount of \"disorder\" or uncertainty. It is defined as\n$$\n\\text{Entropy} = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n$$\nThe actual values of Gini and entropy are often extremely similar, and are measuring similar things. Therefore, we'll focus on Gini impurity in this textbook.\n:::\n\nBut what exactly is Gini computing? Let's look at the tree we just created. In the first node, we see that there are 308 samples with class 0 (survived) and 154 samples with class 1 (did not survive). The Gini impurity is computed as follows:\n$$\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=1}^2 p_i^2 = 1 - \\left(\\frac{308}{462}\\right)^2 - \\left(\\frac{154}{462}\\right)^2 = \\frac{4}{9} = 0.444\n$$\n\nNotice that this is exactly the Gini imprutiy stated in that node in the tree. \n\nIn general, how should we think about the value for Gini impurity?\n\nImagine that, in one of the nodes, there were 100 samples, and all of the samples were people who survived. Then the Gini impurity would be\n\n$$\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{100}{100}\\right)^2 - \\left(\\frac{0}{100}\\right)^2 = 0\n$$\n\nSimilarly, if all of the samples were people who did not survive, the Gini impurity would be\n\n$$\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{0}{100}\\right)^2 - \\left(\\frac{100}{100}\\right)^2 = 0\n$$\n\nOn the other hand, suppose there were a 50-50 split of the samples, with 50 samples of each class. Then the Gini impurity would be\n\n$$\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{50}{100}\\right)^2 - \\left(\\frac{50}{100}\\right)^2 = \\frac{1}{2} = 0.5\n$$\n\nNote how Gini impurity is small at the extremes (all samples have the same class), and bigger when the classes are more balanced. We can see this in the graph below, where the x-axis represents the proportion of class 1, and the y-axis represents the Gini impurity.\n\n::: {#d57f773f .cell execution_count=19}\n``` {.python .cell-code}\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0, 1, 100, endpoint=True)\ngini = 1 - (p**2 + (1-p)**2)\n\ngini_df = pd.DataFrame({\n    'p': p,\n    'gini': gini\n})\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='p', y='gini', data=gini_df)\nplt.xlabel('Proportion of Class 1')\nplt.ylabel('Gini Impurity')\nplt.title('Gini Impurity in a Binary Classification Problem')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-20-output-1.png){width=812 height=523}\n:::\n:::\n\n\nSo, how is Gini impurity actually used to determine the splitting in decision trees? Our goal is to choose a split (i.e. a question) that best splits the data. For example, if we wanted to determine how well students will do in a class, asking whether they own a dog is a valid question, but won't give me any additional information. On the other hand, if we ask if they regularly attend tutoring, we will get a much better idea of their performance.\n\nIn decision trees we use Gini impurity by calculating the Gini impurity for each possible split, and then choosing the split that has the smallest Gini impurity. This is because a smaller Gini impurity means that the split is better at separating the classes.\n\n### 5.3 Controlling Tree Depth: The Overfitting Problem {#ch3-4-3}\n\nTrees have a dangerous tendency: if you let them grow without limits, they'll memorize the training data.\n\n::: {#40df4136 .cell execution_count=20}\n``` {.python .cell-code}\n# Compare different tree depths\ndepths = [1, 3, 5, 10, 20, None]  # None means no limit\ntrain_scores = []\ntest_scores = []\n\nfor depth in depths:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train_full, y_train_full)\n    train_scores.append(tree.score(X_train_full, y_train_full))\n    test_scores.append(tree.score(X_test_full, y_test_full))\n\n# Plot\ndepth_labels = [str(d) if d is not None else 'Unlimited' for d in depths]\nplt.figure(figsize=(10, 6))\nplt.plot(depth_labels, train_scores, 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(depth_labels, test_scores, 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('Maximum Tree Depth', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('Tree Depth vs Performance: The Overfitting Story', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-21-output-1.png){width=823 height=528}\n:::\n:::\n\n\nClassic overfitting! Training accuracy keeps increasing with depth, but test accuracy peaks and then plateaus or even decreases. The tree is memorizing noise in the training data.\n\nWe can see this concretely by printing out one of the decision paths in the deep trees:\n\n::: {#2ac6915f .cell execution_count=21}\n``` {.python .cell-code}\n# Print one of the decision paths in the deep trees\ntree = DecisionTreeClassifier(max_depth=20, random_state=42)\ntree.fit(X_train_full, y_train_full)\n\nsample_id = 0\nsingle_sample = X_test_full[sample_id].reshape(1, -1) # Reshape for single sample input\n\n# Get the decision path for the single sample\nnode_indicator = tree.decision_path(single_sample)\nnode_indices = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n\n# Get tree structure information for interpreting the path\nchildren_left = tree.tree_.children_left\nchildren_right = tree.tree_.children_right\nfeature = tree.tree_.feature\nthreshold = tree.tree_.threshold\nfeature_names = features\n\nprint(f\"Decision path for sample {sample_id}:\")\nfor node_id in node_indices:\n    if children_left[node_id] != children_right[node_id]:  # Check if it's a split node\n        feature_index = feature[node_id]\n        threshold_value = threshold[node_id]\n        sample_feature_value = single_sample[0, feature_index]\n\n        if sample_feature_value <= threshold_value:\n            decision = f\"{feature_names[feature_index]} ({sample_feature_value:.2f}) <= {threshold_value:.2f}\"\n        else:\n            decision = f\"{feature_names[feature_index]} ({sample_feature_value:.2f}) > {threshold_value:.2f}\"\n        print(f\"  Node {node_id}: Split on {decision}\")\n    else:  # It's a leaf node\n        print(f\"  Node {node_id}: Leaf node reached (prediction: {tree.predict(single_sample)[0]})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision path for sample 0:\n  Node 0: Split on Fare (13.00) <= 52.28\n  Node 1: Split on Age (42.00) > 5.50\n  Node 11: Split on Pclass (2.00) <= 2.50\n  Node 12: Split on Age (42.00) <= 42.50\n  Node 13: Split on Fare (13.00) > 12.31\n  Node 23: Split on Pclass (2.00) > 1.50\n  Node 43: Split on Age (42.00) > 15.00\n  Node 45: Split on Age (42.00) > 39.50\n  Node 103: Split on Fare (13.00) <= 26.50\n  Node 104: Leaf node reached (prediction: 1)\n```\n:::\n:::\n\n\nWe start by asking reasonable seeming questions, but then asking more and more specific questions that seem unnecessary. For example, we ask if the persons age is:\n\n1. Over 5.5\n2. Less than 42.5\n3. Over 15\n4. Over 39.5\n\nPutting these together, we've asked if the person is between the ages of 39.5 and 42.5. That's unnecessarily specific; do we really believe that a person being exactly 40 or 41 years old is so important? Instead, what's happening is that our model is overfitting to the training data, and the results don't generalize to the test data.\n\n**Common hyperparameters to control overfitting:**\n\n- `max_depth`: Maximum depth of the tree\n- `min_samples_split`: Minimum samples required to split a node\n- `min_samples_leaf`: Minimum samples required at a leaf node\n- `max_features`: Number of features to consider for each split\n\n### 5.4 Feature Importance {#ch3-5-4}\n\nTrees can tell you which features are most important for making predictions:\n\n::: {#d0ee3822 .cell execution_count=22}\n``` {.python .cell-code}\n# Get feature importances\ntree_final = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_final.fit(X_train_full, y_train_full)\n\nimportance_df = pd.DataFrame({\n    'Feature': features,\n    'Importance': tree_final.feature_importances_\n}).sort_values('Importance', ascending=False)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance from Decision Tree', fontsize=14)\nplt.gca().invert_yaxis()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-23-output-1.png){width=834 height=528}\n:::\n:::\n\n\nFeature importance represents how much each feature contributes to reducing impurity across all splits. Higher values mean more important features.\n\n::: {.callout-note}\nFeature importance can be calculated a number of different ways. One way is to take a column, randomly shuffle the values, and see how the impurity changes. If the impurity decreases, then the feature is important. This is called **permutation feature importance**.\n\nFor `DecisionTreeClassifier`, feature importance is calculated using **impurity decrease**. The impurity decrease is the difference in impurity before and after a split. The higher the impurity decrease, the more important the feature. For example, if a feature can be used to end up with nodes with small impurity, that means that feature can split the data into groups where one class is highly represented. Recall from above that impurity is a measure of how mixed up the classes are in a node. So if a feature can be used to end up with nodes with small impurity, that means that feature can split the data into groups where one class is highly represented. This is why the feature is important.\n:::\n\n::: {.callout-tip}\nDifferent models calculate feature importance differently. However, they're all trying to answer the same question: how much does this feature matter in terms of making predictions?\n\nDon't treat feature importance like a gold standard. It's a tool for understanding your model, but it's not a substitute for domain knowledge. Instead, you can often use feature importance to double-check your own understanding of the problem. If you believe a feature should be important, but it's not showing up in the feature importance, then why not? Is it a problem with the model/data, or a problem with your understanding?\n:::\n\n---\n\n## 6. Random Forests: Ensemble Power {#ch3-6}\n\n### 6.1 Why Ensembles Work {#ch3-5-1}\n\nHere's a powerful idea: what if instead of training one tree, we trained many trees and let them vote?\n\nRandom Forests are one of the most successful examples. The key insight: many **weak learners** can combine to create a **strong learner**.\n\n::: {.callout-note}\n**Ensemble learning** is the idea of combining many weak learners to create a strong learner. The key insight is that many weak learners can combine to create a strong learner.\n\nBy a **weak learner** we typically mean a very simple model, such as a decision tree with a depth of two. By a **strong learner** we typically mean a more complex model, such as a decision tree with a depth of ten.\n\nThe idea is that one large _complex_ model may overfit to the training data, but many small _simple_ models can combine to create a strong learner that generalizes well to the test data.\n\nRandom forests are one example of ensemble models, but we'll learn more. Ensemble models are typically the gold standard in classical machine learning.\n:::\n\n**How Random Forests work:**\n\n1. Create many decision trees (e.g., 100 trees)\n2. For each tree:\n   - Sample a random subset of the data (bootstrapping)\n   - At each split, only consider a random subset of features\n3. Make predictions by majority vote (classification), or by averaging probabilities (regression)\n\nThe randomness in both samples and features ensures that trees are different from each other. After all, if we took all rows and all columns, each weak learner would likely be exactly the same. By sampling a random subset of the data and features, we ensure that each tree is different from the others. \n\nWhen weak learners disagree, it's often because they're focusing on different aspects of the data. When they agree, you can be more confident.\n\n### 6.2 Fitting a Random Forest {#ch3-5-2}\n\n::: {#cd7905db .cell execution_count=23}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Fit random forest\nrf_model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\nrf_model.fit(X_train_full, y_train_full)\n\n# Compare to single tree\ntree_comparison = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_comparison.fit(X_train_full, y_train_full)\n\nprint(\"Single Decision Tree:\")\nprint(f\"  Training Accuracy: {tree_comparison.score(X_train_full, y_train_full):.3f}\")\nprint(f\"  Test Accuracy: {tree_comparison.score(X_test_full, y_test_full):.3f}\")\n\nprint(\"\\nRandom Forest (50 trees):\")\nprint(f\"  Training Accuracy: {rf_model.score(X_train_full, y_train_full):.3f}\")\nprint(f\"  Test Accuracy: {rf_model.score(X_test_full, y_test_full):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle Decision Tree:\n  Training Accuracy: 0.765\n  Test Accuracy: 0.678\n\nRandom Forest (50 trees):\n  Training Accuracy: 0.788\n  Test Accuracy: 0.699\n```\n:::\n:::\n\n\nRandom forests typically achieve better generalization than single trees. The ensemble reduces overfitting through diversity.\n\n::: {.callout-tip}\nRandom forests are most useful for complex data with many columns, and with complex relationships between columns. For simple data with few columns, a single decision tree is often sufficient.\n:::\n\n::: {.callout-warning}\nYou may feel like \"if one tree is good, then more trees are better!\" This is true to an extent, but it comes with a tradeoff. When you train an ensemble model such as a random forest, you're training many models. Each model takes time to train, and each model uses memory and compute time. Imagine training a single tree which takes ten seconds to train. \n\nNow imagine training 50 trees, which would take 500 seconds to train, or nearly ten minutes! Combine this with hyperparameter tuning such as through a grid search, and you could easily be looking at an hour or more of training time. If the improvement in performance is large this may be worth it. But if your data is simple enough to get by with a simpler model, you can save hours of compute by going with a simpler model.\n:::\n\n### 6.3 Feature Importance in Random Forests {#ch3-5-3}\n\nRandom forests also provide feature importances, but they're generally more reliable than single trees because they average across many trees:\n\n::: {#603e05e5 .cell execution_count=24}\n``` {.python .cell-code}\n# Get feature importances from both models\ntree_importance = pd.DataFrame({\n    'Feature': features,\n    'Decision Tree': tree_comparison.feature_importances_,\n    'Random Forest': rf_model.feature_importances_\n})\n\n# Melt the dataframe for plotting\nimportance_melted = tree_importance.melt(\n    id_vars='Feature',\n    var_name='Model',\n    value_name='Importance'\n)\n\n# Sort by average importance across both models\navg_importance = tree_importance.set_index('Feature').mean(axis=1).sort_values(ascending=False)\nimportance_melted['Feature'] = pd.Categorical(\n    importance_melted['Feature'],\n    categories=avg_importance.index,\n    ordered=True\n)\n\n# Create dodged bar chart\nplt.figure(figsize=(10, 6))\nsns.barplot(data=importance_melted, x='Importance', y='Feature', hue='Model', palette='Set2')\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance: Decision Tree vs Random Forest', fontsize=14)\nplt.legend(title='Model', fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-25-output-1.png){width=951 height=566}\n:::\n:::\n\n\nRandom forest importances tend to be more stable because they're averaged across many trees with different random subsets of data and features.\n\n### 6.4 Hyperparameter Tuning {#ch3-6-4}\n\nRandom forests have several important hyperparameters:\n\n::: {#a1feb1f6 .cell execution_count=25}\n``` {.python .cell-code}\n# Test different numbers of trees\nn_trees_list = [25, 50, 75, 100, 150, 200]\nscores = []\n\nfor n_trees in n_trees_list:\n    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n    rf.fit(X_train_full, y_train_full)\n    scores.append(rf.score(X_test_full, y_test_full))\n\nplt.figure(figsize=(10, 6))\nplt.plot(n_trees_list, scores, 'o-', linewidth=2, markersize=8)\nplt.xlabel('Number of Trees', fontsize=12)\nplt.ylabel('Test Accuracy', fontsize=12)\nplt.title('Random Forest Performance vs Number of Trees', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-26-output-1.png){width=831 height=528}\n:::\n:::\n\n\nPerformance typically improves with more trees, but you get diminishing returns. After a certain point (often 100-500 trees), adding more trees barely helps (or even leads to overfitting)but makes training slower. Use hyperparameter tuning to find the right number of trees. A typically starting point is 50 to 100 trees.\n\n::: {.callout-tip}\nRandom forests are often a great default choice for classification. They're robust, handle non-linear relationships, require minimal hyperparameter tuning, and rarely overfit badly. When in doubt, try a random forest!\n:::\n\n---\n\n## 7. Support Vector Machines: Maximum Margin Classifiers {#ch3-7}\n\n### 7.1 The Margin Concept {#ch3-7-1}\n\nSupport Vector Machines (SVMs) have a beautiful geometric intuition: find the decision boundary that maximizes the distance to the nearest points from each class.\n\nThink of it like this: if you're drawing a line to separate two groups of points, you want it to be as far as possible from both groups. This gives you more confidence that future points will be classified correctly.\n\nThe \"support vectors\" are the points closest to the decision boundary—these are the critical points that define where the boundary goes.\n\n::: {#89088db2 .cell execution_count=26}\n``` {.python .cell-code}\nfrom sklearn.svm import SVC\n\n# Fit SVM with linear kernel\nsvm_linear = SVC(kernel='linear', random_state=42)\nsvm_linear.fit(X_train_full, y_train_full)\n\nprint(f\"SVM Linear Kernel Accuracy: {svm_linear.score(X_test_full, y_test_full):.3f}\")\nprint(f\"Number of support vectors: {len(svm_linear.support_)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSVM Linear Kernel Accuracy: 0.678\nNumber of support vectors: 374\n```\n:::\n:::\n\n\n### 7.2 The Kernel Trick {#ch3-7-2}\n\nHere's where SVMs get really powerful: the **kernel trick**. By using different kernel functions, SVMs can create non-linear decision boundaries while still solving a linear problem in a higher-dimensional space.\n\n**Common kernels:**\n\n- **Linear**: Creates straight boundaries (like logistic regression)\n- **RBF (Radial Basis Function)**: Creates circular/curved boundaries\n- **Polynomial**: Creates polynomial curves as boundaries\n\n::: {#3b710429 .cell execution_count=27}\n``` {.python .cell-code}\n# Compare different kernels\nkernels = ['linear', 'rbf', 'poly']\nsvm_models = {}\n\nfor kernel in kernels:\n    svm = SVC(kernel=kernel, random_state=42)\n    svm.fit(X_train_full, y_train_full)\n    svm_models[kernel] = svm\n    print(f\"{kernel:8s} kernel - Test Accuracy: {svm.score(X_test_full, y_test_full):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlinear   kernel - Test Accuracy: 0.678\nrbf      kernel - Test Accuracy: 0.615\npoly     kernel - Test Accuracy: 0.629\n```\n:::\n:::\n\n\n### 6.3 Visualizing SVM Decision Boundaries {#ch3-6-3}\n\nLet's see how different kernels create different boundaries. We'll use California housing data, where different regions of the state have vastly different housing prices—a perfect example of non-linear geographic clustering:\n\n::: {#53182662 .cell execution_count=28}\n``` {.python .cell-code}\n# Load California housing data\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\ncalifornia = fetch_california_housing(as_frame=True)\nca_df = california.frame\n\n# Create binary classification: expensive (>$3) vs affordable (<=$3) houses\n# Median house value is in $100,000s, so 3 = $300,000\nca_df['expensive'] = (ca_df['MedHouseVal'] > 3.0).astype(int)\n\n# Use longitude and median house value for visualization\n# Different parts of California have very different price patterns\nX_viz = ca_df[['Latitude', 'MedHouseVal']].values\ny_viz = ca_df['expensive'].values\n\n# Sample for faster visualization (full dataset is 20k+ points)\nnp.random.seed(42)\nsample_idx = np.random.choice(len(X_viz), size=2000, replace=False)\nX_viz_raw = X_viz[sample_idx]\ny_viz = y_viz[sample_idx]\n\n# Scale features for SVM (IMPORTANT: SVMs are sensitive to feature scales)\nscaler = StandardScaler()\nX_viz = scaler.fit_transform(X_viz_raw)\n\nexpensive = y_viz == 1\n\n# Create a mesh for plotting decision boundaries (in scaled space)\nlon_range = np.linspace(X_viz[:, 0].min(), X_viz[:, 0].max(), 100)\nprice_range = np.linspace(X_viz[:, 1].min(), X_viz[:, 1].max(), 100)\nlon_mesh, price_mesh = np.meshgrid(lon_range, price_range)\nmesh_points = np.c_[lon_mesh.ravel(), price_mesh.ravel()]\n\n# Fit SVMs with different kernels on scaled data\nsvm_linear_viz = SVC(kernel='linear', random_state=42)\nsvm_rbf_viz = SVC(kernel='rbf', gamma='auto', random_state=42)\n\nsvm_linear_viz.fit(X_viz, y_viz)\nsvm_rbf_viz.fit(X_viz, y_viz)\n\n# Create predictions on mesh\nmesh_linear = svm_linear_viz.predict(mesh_points).reshape(lon_mesh.shape)\nmesh_rbf = svm_rbf_viz.predict(mesh_points).reshape(lon_mesh.shape)\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor ax, mesh_pred, title in zip(axes, [mesh_linear, mesh_rbf],\n                                 ['SVM - Linear Kernel', 'SVM - RBF Kernel']):\n    ax.contourf(lon_mesh, price_mesh, mesh_pred, levels=1, cmap='RdYlGn', alpha=0.4)\n    ax.scatter(X_viz[expensive, 0], X_viz[expensive, 1], c='darkgreen', marker='o',\n               s=20, edgecolors='black', alpha=0.5, label='Expensive (>$300k)')\n    ax.scatter(X_viz[~expensive, 0], X_viz[~expensive, 1], c='darkred', marker='x',\n               s=20, alpha=0.5, label='Affordable (≤$300k)')\n    ax.set_xlabel('Latitude', fontsize=12)\n    ax.set_ylabel('Median House Value ($100k)', fontsize=12)\n    ax.set_title(title, fontsize=14)\n    ax.legend()\n    ax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-29-output-1.png){width=1527 height=566}\n:::\n:::\n\n\nSee the difference? The linear kernel creates a straight boundary—it tries to separate expensive from affordable homes with a single line. But the RBF kernel creates smooth, curved boundaries that adapt to the geographic clustering of housing prices.\n\nNotice how the RBF kernel captures the reality that certain geographic regions (coastal areas, Bay Area) command higher prices regardless of other factors. The curved decision boundary wraps around these high-value clusters much more naturally than a straight line ever could.\n\n### 7.4 When to Use SVMs {#ch3-7-4}\n\n**Strengths:**\n\n- Effective in high-dimensional spaces\n- Memory efficient (only stores support vectors)\n- Flexible with different kernels\n- Works well with clear margin of separation\n\n**Weaknesses:**\n\n- Slow to train on large datasets (doesn't scale well beyond ~10,000 samples)\n- Requires feature scaling (sensitive to feature magnitudes)\n- Choosing the right kernel and hyperparameters can be tricky\n- Less interpretable than trees or logistic regression\n\n::: {.callout-warning}\nAlways scale your features before using SVMs! They're very sensitive to feature magnitudes. Use `StandardScaler` or `MinMaxScaler` from scikit-learn.\n:::\n\n---\n\n## 8. k-Nearest Neighbors: Simple but Powerful {#ch3-8}\n\n### 8.1 The k-NN Algorithm {#ch3-7-1}\n\nk-Nearest Neighbors might be the simplest classification algorithm: to classify a new point, find the k closest training points and let them vote.\n\nThat's it. No training phase. No learning parameters. Just store the data and compute distances when you need to make predictions.\n\n::: {#ae1e83bf .cell execution_count=29}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Fit k-NN with k=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_full, y_train_full)\n\nprint(f\"k-NN (k=5) Test Accuracy: {knn.score(X_test_full, y_test_full):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-NN (k=5) Test Accuracy: 0.650\n```\n:::\n:::\n\n\n::: {.callout-warning}\nKNN models sound like they should be simple. You simply find the k closest training points and let them vote. But in practice, they can be devilishly complex. For example, how do you measure \"closeness\"? In some data that may be simple, such as closeness in position or time. But what about data on students? What do we mean by the \"closest students\"? Closest in age? Same/similar major? Same/similar year in college? Are all of these equally important? Is one more important than another? How about closeness in courses taken? When should we consider two courses \"close\"?\n\nAll of these questions are enormously important in building effective KNN models, but they don't have easy answers. KNN models, more than many others, require extensive testing to determine what works best.\n:::\n\n### 8.2 Choosing k: The Bias-Variance Tradeoff Again {#ch3-7-2}\n\nThe value of k controls the bias-variance tradeoff:\n\n- **Small k (e.g., k=1)**: Very flexible, low bias, high variance (overfitting)\n- **Large k (e.g., k=100)**: Smoother boundaries, high bias, low variance (underfitting)\n\n::: {#a3cb1c92 .cell execution_count=30}\n``` {.python .cell-code}\n# Test different values of k\nk_values = [1, 3, 5, 10, 20, 50, 100]\ntrain_scores_knn = []\ntest_scores_knn = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_full, y_train_full)\n    train_scores_knn.append(knn.score(X_train_full, y_train_full))\n    test_scores_knn.append(knn.score(X_test_full, y_test_full))\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, train_scores_knn, 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(k_values, test_scores_knn, 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('k (number of neighbors)', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('k-NN: Choosing k', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-31-output-1.png){width=823 height=528}\n:::\n:::\n\n\nNotice the pattern: small k gives high training accuracy but may overfit. Moderate k (often 3-10) tends to work best, but hyperparameter tuning is needed to determine the best choice.\n\n### 8.3 Distance Metrics: How Do We Measure \"Closeness\"? {#ch3-8-3}\n\nThe entire k-NN algorithm hinges on one question: how do you measure which points are \"closest\"? This isn't just a technical detail—it fundamentally changes how your model behaves. Scikit-learn supports several distance metrics, and choosing the right one can dramatically affect performance.\n\n**The most common distance metrics:**\n\n1. **Euclidean distance** (default): The straight-line distance between two points\n   $$d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n\n2. **Manhattan distance**: Sum of absolute differences (think of it as the number of city blocks between two locations, you can only walk horizontally or vertically)\n   $$d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$$\n\n3. **Minkowski distance**: A mixture between Euclidean and Manhattan (p=1 is Manhattan, p=2 is Euclidean, but you can set p to any positive real number)\n   $$d(x, y) = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}$$\n\n4. **Cosine distance**: Measures angle between vectors (ignores the length of the vectors, and only cares about how far apart the directions they point are)\n   $$d(x, y) = 1 - \\frac{x \\cdot y}{||x|| \\cdot ||y||}$$\n\n5. **Hamming distance**: Checks whether two values are equal or not, and computes the average number of features where two samples are equal\n   $$d(x, y) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{1}(x_i \\neq y_i)$$\n   where $\\mathbb{1}(x_i \\neq y_i)$ equals 1 if $x_i \\neq y_i$ and 0 otherwise.\n\nLet's see how different metrics perform on our Titanic data:\n\n::: {#bd50153b .cell execution_count=31}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Test different distance metrics\nmetrics = ['euclidean', 'manhattan', 'minkowski', 'cosine', 'hamming']\nmetric_scores = {}\n\nfor metric in metrics:\n    # Some metrics need additional parameters\n    if metric == 'minkowski':\n        knn = KNeighborsClassifier(n_neighbors=5, metric=metric, p=3)\n    else:\n        knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n\n    knn.fit(X_train_full, y_train_full)\n    train_score = knn.score(X_train_full, y_train_full)\n    test_score = knn.score(X_test_full, y_test_full)\n    metric_scores[metric] = {'train': train_score, 'test': test_score}\n\n    # print(f\"{metric:12s} - Train: {train_score:.3f}, Test: {test_score:.3f}\")\n\n# Plot the metric and train/test score\nplt.figure(figsize=(10, 6))\nplt.plot(metrics, [scores['train'] for scores in metric_scores.values()], 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(metrics, [scores['test'] for scores in metric_scores.values()], 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('Distance Metric', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('k-NN: Choosing Distance Metric', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-32-output-1.png){width=823 height=528}\n:::\n:::\n\n\nDifferent metrics can give meaningfully different results! But which should you use?\n\n**When to use each metric:**\n\n**Euclidean distance** (the default) works well when:\n\n- All features have similar scales and units\n- You care about the actual geometric distance\n- Features are continuous numeric values\n- Example: Geographic coordinates (latitude/longitude), physical measurements\n\n**Manhattan distance** works well when:\n\n- Features represent different units that shouldn't be combined quadratically\n- You have grid-like data (think city blocks, not \"as the crow flies\")\n- You want to reduce the influence of outliers (no squaring!)\n- Example: Recommender systems, routing/navigation problems\n\n**Cosine distance** works well when:\n\n- You care about direction/orientation, not magnitude\n- Data is high-dimensional and sparse\n- Feature scales vary wildly\n- Example: Text data (word counts), recommendation systems with user ratings\n\n**Hamming distance** works well when:\n\n- You have categorical or binary features\n- All features are equally important (no scaling needed)\n- You want to count how many features differ, not by how much\n- Example: DNA sequences, binary feature vectors, categorical data (after one-hot encoding)\n\n::: {.callout-note}\nHamming distance treats all feature differences equally. If Feature A differs by 0.1 and Feature B differs by 10, Hamming sees both as \"different.\" It's perfect for categorical data where \"different is different\" regardless of magnitude, but not ideal for continuous numeric features where the size of the difference matters.\n:::\n\nLet's visualize how these different metrics create different neighborhoods. We'll use a simple 2D example:\n\n::: {#126f2ab3 .cell execution_count=32}\n``` {.python .cell-code}\nfrom sklearn.neighbors import NearestNeighbors\n\n# Create a simple 2D point to query\nquery_point = np.array([[2.0, 3.0]])\n\n# Create some sample points\nnp.random.seed(42)\nsample_points = np.random.rand(20, 2) * 5\n\n# Find 5 nearest neighbors using different metrics\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor ax, metric in zip(axes, ['euclidean', 'manhattan', 'cosine']):\n    # Find neighbors\n    nbrs = NearestNeighbors(n_neighbors=5, metric=metric)\n    nbrs.fit(sample_points)\n    distances, indices = nbrs.kneighbors(query_point)\n\n    # Plot\n    ax.scatter(sample_points[:, 0], sample_points[:, 1], c='lightgray',\n               s=100, alpha=0.6, label='Other points')\n    ax.scatter(sample_points[indices[0], 0], sample_points[indices[0], 1],\n               c='blue', s=100, edgecolors='black', linewidth=2, label='5 nearest neighbors')\n    ax.scatter(query_point[0, 0], query_point[0, 1], c='red', s=200,\n               marker='*', edgecolors='black', linewidth=2, label='Query point')\n\n    # Draw lines to nearest neighbors\n    for idx in indices[0]:\n        ax.plot([query_point[0, 0], sample_points[idx, 0]],\n                [query_point[0, 1], sample_points[idx, 1]],\n                'b--', alpha=0.3, linewidth=1)\n\n    ax.set_title(f'{metric.capitalize()} Distance', fontsize=14)\n    ax.set_xlabel('Feature 1', fontsize=12)\n    ax.set_ylabel('Feature 2', fontsize=12)\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-33-output-1.png){width=1719 height=470}\n:::\n:::\n\n\nSee how the same query point has different nearest neighbors depending on the metric? Euclidean forms circular neighborhoods, Manhattan forms diamond-shaped neighborhoods, and cosine focuses on angular similarity.\n\n### 8.4 Mixing Metrics: Different Features Need Different Distances {#ch3-7-4}\n\nHere's a critical insight that's often overlooked: **real datasets have different types of features, and each type needs its own distance metric**.\n\nThink about internet service provider (ISP) customer data and predicting churn:\n\n- **Internet Service**: Categorical (DSL, Fiber optic, No internet). We want to know if two customers have the same service type—not treat \"DSL\" as somehow numerically between \"No internet\" and \"Fiber optic\"\n- **Contract type**: Categorical (Month-to-month, One year, Two year). Either the same or different.\n- **Gender**: Categorical (Male, Female). Same or different.\n- **Monthly Charges**: Continuous numeric variable. A customer paying $50/month is more similar to one paying $55 than to one paying $100.\n- **Tenure**: Continuous numeric variable. The actual difference in months matters.\n\nThe problem? When you call `KNeighborsClassifier(metric='euclidean')`, it treats ALL features the same way! It computes Euclidean distance on internet service type and contract (treating categorical values as if they were numbers) just like it does on monthly charges and tenure.\n\n**The solution: Create a custom distance metric that treats different feature types appropriately.**\n\nFor example, you could define a custom distance function that:\n\n- Uses **Hamming distance** (equality check) for categorical features (Internet Service, Contract, Gender)\n- Uses **Euclidean distance** for continuous features (Monthly Charges, Tenure)\n\nThese can be difficult to implement by hand, so working together with an AI coding assistant is the way to go.\n\n::: {.callout-note}\n**Why does this matter?**\n\nImagine comparing two ISP customers:\n- Customer A: DSL, Month-to-month contract, Male, $50/month, 12 months tenure\n- Customer B: Fiber optic, Month-to-month contract, Male, $52/month, 14 months tenure\n\nWith pure Euclidean distance, if internet service is encoded as DSL=0 and Fiber optic=1, the difference in service type contributes \"1\" to the distance calculation, just like a $1/month price difference. But internet service type is categorical! Having DSL vs Fiber optic is a fundamental categorical difference—not a numeric one.\n\nWith the mixed metric, we recognize that internet service differs (Hamming distance = 1), contract and gender are the same (Hamming distance = 0 for each), and then we properly compute Euclidean distance for the continuous features (monthly charges, tenure) where the magnitude of difference actually matters.\n:::\n\n::: {.callout-warning}\n**When building custom distance metrics:**\n\n1. **Identify feature types first**: Which are categorical? Which are continuous?\n2. **Scale continuous features**: Use StandardScaler before computing distances\n3. **Don't scale categorical features**: They represent discrete categories, not magnitudes\n4. **Test your metric**: Does it give sensible distances for sample pairs?\n5. **Weight carefully**: You might want to weight categorical and continuous distances differently\n\nThe custom metric approach requires more work, but it's often worth it for datasets with mixed feature types!\n:::\n\n::: {.callout-tip}\n**How to choose a distance metric:**\n\n1. **Start with Euclidean** (the default) - it works well in most cases\n2. **Try Hamming** if you have categorical features that have no obvious ordering\n3. **Try Manhattan** if you have outliers or features on very different scales\n4. **Try Cosine** if your data is high-dimensional or sparse (like text data)\n5. **Use cross-validation** to compare metrics on your specific dataset\n6. **Always scale your features** before using distance-based methods!\n\nThe \"best\" metric depends on your data and problem. Don't just accept the default—experiment and use validation performance to guide your choice.\n:::\n\n::: {.callout-warning}\n**Feature scaling is critical for k-NN!** If one feature ranges from 0-1 and another ranges from 0-1000, the second feature will dominate the distance calculation. Always use `StandardScaler` or `MinMaxScaler` before fitting k-NN models.\n\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n```\n:::\n\n### 8.5 When to Use k-NN {#ch3-7-5}\n\n**Strengths:**\n\n- Simple to understand and implement\n- No training phase (though this is also a weakness)\n- Naturally handles multi-class problems\n- Can capture complex patterns\n\n**Weaknesses:**\n\n- Slow prediction (has to compute distances to all training points)\n- Memory intensive (stores all training data)\n- Requires feature scaling\n- Choosing k can be tricky\n- Determining appropriate distance metric can be complex\n\n---\n\n## 9. ROC Curves and AUC: Comparing Models {#ch3-9}\n\n### 9.1 The ROC Curve {#ch3-9-1}\n\nSo far we've been using a fixed threshold (0.5) to convert probabilities to class predictions. But what if we tried different thresholds?\n\nThe **ROC curve** (Receiver Operating Characteristic) shows model performance across all possible thresholds. It plots:\n\n- **True Positive Rate (TPR)** = Recall = TP / (TP + FN)\n- **False Positive Rate (FPR)** = FP / (FP + TN)\n\n::: {#361ccac7 .cell execution_count=33}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Get probability predictions from logistic regression\ny_proba_log = log_model_full.predict_proba(X_test_full)[:, 1]\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_full, y_proba_log)\n\n# Compute AUC (Area Under the Curve)\nauc = roc_auc_score(y_test_full, y_proba_log)\n\n# Plot\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Logistic Regression (AUC = {auc:.3f})')\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier (AUC = 0.5)')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate (Recall)', fontsize=12)\nplt.title('ROC Curve', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-34-output-1.png){width=666 height=676}\n:::\n:::\n\n\n**Interpreting the ROC curve:**\n\n- The diagonal line represents a random classifier (flip a coin)\n- The closer your curve sticks to the top-left corner, the better\n- AUC (Area Under the Curve) summarizes performance in one number\n- AUC = 1.0: Perfect classifier\n- AUC = 0.5: Random guessing\n- AUC < 0.5: Worse than random (you're predicting backwards!)\n\n::: {.callout-note}\nWhy is an ROC curve \"sticking to the top-left corner\" a good thing? The top-left corner means we have essentially zero false positives, and high true positives. \n:::\n\n### 9.2 Comparing Multiple Models with ROC {#ch3-8-2}\n\nLet's compare all our models on the same ROC plot:\n\n::: {#90d75fcc .cell execution_count=34}\n``` {.python .cell-code}\n# Get probabilities from all models\nmodels_for_roc = {\n    'Logistic Regression': log_model_full,\n    'Decision Tree': tree_final,\n    'Random Forest': rf_model,\n    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42).fit(X_train_full, y_train_full),\n    'k-NN': knn\n}\n\nplt.figure(figsize=(10, 8))\n\nfor name, model in models_for_roc.items():\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test_full)[:, 1]\n    else:\n        # SVM without probability=True would fail here\n        y_proba = model.predict_proba(X_test_full)[:, 1]\n\n    fpr, tpr, _ = roc_curve(y_test_full, y_proba)\n    auc = roc_auc_score(y_test_full, y_proba)\n    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves: Model Comparison', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-35-output-1.png){width=815 height=676}\n:::\n:::\n\n\nThis visualization makes it easy to compare models at a glance. The model with the highest AUC is typically performing best across all thresholds.\n\n### 9.3 When to Use ROC/AUC {#ch3-8-3}\n\n**Use ROC/AUC when:**\n\n- You want threshold-independent evaluation\n- Classes are relatively balanced\n- You care about ranking (who's more likely to be positive?)\n\n**Don't use ROC/AUC when:**\n\n- Classes are severely imbalanced\n- You have a specific threshold constraint\n- You care more about absolute performance at one threshold\n\n---\n\n## 10. Class Imbalance: The Real-World Problem {#ch3-10}\n\n### 10.1 Why Class Imbalance Matters {#ch3-9-1}\n\nThe truth is, most real-world classification problems have imbalanced classes. Fraud detection? Maybe 0.1% of transactions are fraud. Disease diagnosis? Most patients who show up a a hospital don't have a specific disease. Email spam? While we all get spam, that's not the majority of our email.\n\nClass imbalance breaks naive approaches. For example, suppose we had data where 1% of the people had a given rare disease. If we built a model that predicted everyone as not having the disease, we would be 99% accurate. But that's not a very useful model!\n\n### 10.2 Detecting Class Imbalance {#ch3-9-2}\n\nAlways check class balance before building models. Let's look at a real example: predicting whether a song on Spotify will become a \"viral hit\" (popularity score above 80). Most songs don't go viral, so this is naturally imbalanced:\n\n::: {#d99ced96 .cell execution_count=35}\n``` {.python .cell-code}\n# Load Spotify data and create an imbalanced classification problem\nspotify_df = pd.read_csv('../data/spotify.csv')\n\n# Create binary target: viral hit (popularity > 80) vs not\nspotify_df['viral_hit'] = (spotify_df['popularity'] > 80).astype(int)\n\n# Select numeric features for our model\nspotify_features = ['danceability', 'energy', 'loudness', 'speechiness',\n                    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n\n# Drop rows with missing values\nspotify_clean = spotify_df[spotify_features + ['viral_hit']].dropna()\n\nX_imbalanced = spotify_clean[spotify_features].values\ny_imbalanced = spotify_clean['viral_hit'].values\n\n# Split imbalanced data\nX_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n    X_imbalanced, y_imbalanced, test_size=0.2, random_state=42\n)\n\nprint(f\"Loaded Spotify dataset with {len(y_imbalanced)} songs\")\nprint(f\"Viral hits (popularity > 90): {y_imbalanced.sum()}\")\nprint(f\"Regular songs: {(y_imbalanced == 0).sum()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded Spotify dataset with 42663 songs\nViral hits (popularity > 90): 125\nRegular songs: 42538\n```\n:::\n:::\n\n\nNow let's check the class balance:\n\n::: {#886eefb1 .cell execution_count=36}\n``` {.python .cell-code}\n# Check balance with seaborn\ndef check_class_balance(y, name=\"Dataset\"):\n    counts = pd.Series(y).value_counts()\n    df = pd.DataFrame({'Class': ['Class 0', 'Class 1'], 'Count': counts})\n    sns.barplot(x='Class', y='Count', data=df)\n    plt.title(name)\n    plt.show()\n\ncheck_class_balance(y_imbalanced, \"Spotify Viral Hit Dataset - Highly imbalanced\")\ncheck_class_balance(y_full, \"Titanic Dataset - Fairly balanced\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-37-output-1.png){width=610 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-37-output-2.png){width=593 height=449}\n:::\n:::\n\n\n**Rules of thumb:**\n\n- Ratio < 2:1 → Probably not a problem\n- Ratio 2:1 to 10:1 → Moderate imbalance, be careful with metrics\n- Ratio > 10:1 → Severe imbalance, definitely needs special handling\n\n### 10.3 Handling Class Imbalance: Class Weights {#ch3-9-3}\n\nOne simple approach: tell the model to weight the minority class more heavily during training. Many models have the ability to assign weights to different classes, using the `class_weight` parameter with a value of `'balanced'`.\n\nFirst, let's fit a \"naive\" model that ignores the imbalance:\n\n::: {#19cc9ac8 .cell execution_count=37}\n``` {.python .cell-code}\n# Fit a naive model (ignores imbalance)\nnaive_model = LogisticRegression(random_state=42, max_iter=1000)\nnaive_model.fit(X_train_imb, y_train_imb)\n\ny_pred_naive = naive_model.predict(X_test_imb)\nconf_mat = confusion_matrix(y_test_imb, y_pred_naive)\n\n# Plot normalized confusion matrix (normalize by row)\nplt.figure(figsize=(5, 5))\nconf_mat_norm = conf_mat.astype('float') / conf_mat.sum(axis=1, keepdims=True)\nsns.heatmap(conf_mat_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=['Not Viral', 'Viral'],\n            yticklabels=['Not Viral', 'Viral'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Naive Model Confusion Matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-38-output-1.png){width=425 height=449}\n:::\n:::\n\n\nNotice how every single song is predicted as being not popular. That's because the model gets \"better\" predictions by predicting the majority class (not viral) for every song.\n\nNow let's compare with a model that uses class weights:\n\n::: {#f0acec14 .cell execution_count=38}\n``` {.python .cell-code}\n# Fit with class weights\nweighted_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\nweighted_model.fit(X_train_imb, y_train_imb)\n\ny_pred_weighted = weighted_model.predict(X_test_imb)\n\n# Plot normalized confusion matrix (normalize by row)\nplt.figure(figsize=(5, 5))\nconf_mat_weighted = confusion_matrix(y_test_imb, y_pred_weighted)\nconf_mat_weighted_norm = conf_mat_weighted.astype('float') / conf_mat_weighted.sum(axis=1, keepdims=True)\nsns.heatmap(conf_mat_weighted_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=['Not Viral', 'Viral'],\n            yticklabels=['Not Viral', 'Viral'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Model with balanced class weights Confusion Matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-39-output-1.png){width=444 height=449}\n:::\n:::\n\n\nThe `class_weight='balanced'` parameter automatically weights classes inversely proportional to their frequency. This forces the model to pay more attention to minority class errors, and results in a fairly strong model with good precision and recall.\n\n### 10.4 Handling Class Imbalance: Resampling {#ch3-9-4}\n\nAnother approach: change the data itself so that it's balanced.\n\n**Undersampling**: Remove examples from majority class\n\n**Oversampling**: Duplicate examples from minority class\n\n**SMOTE** (Synthetic Minority Over-sampling Technique): Create synthetic minority class examples\n\n::: {#212418aa .cell execution_count=39}\n``` {.python .cell-code}\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Apply SMOTE\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_imb, y_train_imb)\n\nprint(\"Original training set:\")\ncheck_class_balance(y_train_imb, \"Before SMOTE\")\n\nprint(\"\\nAfter SMOTE:\")\ncheck_class_balance(y_train_smote, \"After SMOTE\")\n\n# Train on balanced data\nsmote_model = LogisticRegression(random_state=42, max_iter=1000)\nsmote_model.fit(X_train_smote, y_train_smote)\n\n# Evaluate on original imbalanced test set\ny_pred_smote = smote_model.predict(X_test_imb)\n\nprint(\"\\nModel trained on SMOTE data:\")\nconf_mat_smote = confusion_matrix(y_test_imb, y_pred_smote)\nconf_mat_smote_norm = conf_mat_smote.astype('float') / conf_mat_smote.sum(axis=1, keepdims=True)\nplt.figure(figsize=(5, 5))\nsns.heatmap(conf_mat_smote_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=['Not Viral', 'Viral'],\n            yticklabels=['Not Viral', 'Viral'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('SMOTE Model Confusion Matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal training set:\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-40-output-2.png){width=610 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAfter SMOTE:\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-40-output-4.png){width=610 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel trained on SMOTE data:\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-40-output-6.png){width=425 height=449}\n:::\n:::\n\n\n::: {.callout-warning}\nWhen using resampling techniques like SMOTE, only resample the training data! Never resample the test set—you want to evaluate on the natural class distribution.\n:::\n\n### 10.5 Choosing Metrics for Imbalanced Data {#ch3-9-5}\n\nWith imbalanced data, accuracy is almost always misleading. Use:\n\n- **Precision**: When false positives are costly\n- **Recall**: When false negatives are costly\n- **F1 Score**: When you want a balance\n- **AUC-ROC**: Threshold-independent, but can be optimistic with severe imbalance\n\n---\n\n## 11. Comparing All Models: A Practical Guide {#ch3-10}\n\n### 11.1 Model Selection Framework {#ch3-10-1}\n\nWith so many classification algorithms, how do you choose? Here's a practical framework:\n\n**Start with logistic regression if:**\n\n- You need interpretability (coefficients matter)\n- You want fast training and prediction\n- You suspect linear decision boundaries\n- You have limited data\n\n**Use decision trees if:**\n\n- You need maximum interpretability (show the tree to stakeholders)\n- Features are on different scales (trees don't need scaling)\n- You have non-linear relationships\n- You're okay with potential overfitting\n\n**Use random forests if:**\n\n- You want robust performance without much tuning\n- You have enough data (hundreds or thousands of samples)\n- You don't need interpretability\n- You want feature importance estimates\n\n**Use SVMs if:**\n\n- You have high-dimensional data (many features)\n- You have clear margin of separation\n- You're willing to spend time tuning hyperparameters\n- Dataset is not too large (< 10,000 samples)\n\n**Use k-NN if:**\n\n- You have small datasets\n- You don't need fast predictions\n- You have low-to-moderate dimensions\n- Decision boundaries are very irregular\n\n### 11.2 Complete Model Comparison {#ch3-10-2}\n\nLet's do a comprehensive comparison:\n\n::: {#ed08a0db .cell execution_count=40}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\n# Define models\ncomparison_models = {\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Compare on Titanic data using cross-validation\nresults = []\n\nfor name, model in comparison_models.items():\n    # Cross-validation scores\n    cv_scores = cross_val_score(model, X_full, y_full, cv=5, scoring='accuracy')\n\n    # Fit and evaluate\n    model.fit(X_train_full, y_train_full)\n    y_pred_comp = model.predict(X_test_full)\n\n    # Compute metrics\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n    results.append({\n        'Model': name,\n        'CV Accuracy (mean)': cv_scores.mean(),\n        'CV Accuracy (std)': cv_scores.std(),\n        'Test Accuracy': accuracy_score(y_test_full, y_pred_comp),\n        'Precision': precision_score(y_test_full, y_pred_comp),\n        'Recall': recall_score(y_test_full, y_pred_comp),\n        'F1 Score': f1_score(y_test_full, y_pred_comp)\n    })\n\nresults_df = pd.DataFrame(results).set_index('Model')\nresults_df\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CV Accuracy (mean)</th>\n      <th>CV Accuracy (std)</th>\n      <th>Test Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.689077</td>\n      <td>0.042540</td>\n      <td>0.699301</td>\n      <td>0.666667</td>\n      <td>0.464286</td>\n      <td>0.547368</td>\n    </tr>\n    <tr>\n      <th>Decision Tree</th>\n      <td>0.715719</td>\n      <td>0.041416</td>\n      <td>0.678322</td>\n      <td>0.619048</td>\n      <td>0.464286</td>\n      <td>0.530612</td>\n    </tr>\n    <tr>\n      <th>Random Forest</th>\n      <td>0.703103</td>\n      <td>0.037517</td>\n      <td>0.664336</td>\n      <td>0.600000</td>\n      <td>0.428571</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>SVM (RBF)</th>\n      <td>0.666699</td>\n      <td>0.072339</td>\n      <td>0.615385</td>\n      <td>0.513514</td>\n      <td>0.339286</td>\n      <td>0.408602</td>\n    </tr>\n    <tr>\n      <th>k-NN (k=5)</th>\n      <td>0.659795</td>\n      <td>0.059218</td>\n      <td>0.650350</td>\n      <td>0.565217</td>\n      <td>0.464286</td>\n      <td>0.509804</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### 11.3 Visualizing Model Performance {#ch3-10-3}\n\n::: {#49503d12 .cell execution_count=41}\n``` {.python .cell-code}\n# Plot comparison\nmetrics = ['Test Accuracy', 'Precision', 'Recall', 'F1 Score']\nresults_df[metrics].plot(kind='bar', figsize=(12, 6), rot=45)\nplt.ylabel('Score', fontsize=12)\nplt.title('Model Comparison Across Metrics', fontsize=14)\nplt.legend(loc='lower right')\nplt.ylim(0.5, 1.0)\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-3-classification_files/figure-html/cell-42-output-1.png){width=1143 height=565}\n:::\n:::\n\n\n### 11.4 The Importance of Context {#ch3-10-4}\n\nThere's no single \"best\" model. The right choice depends on:\n\n1. **Problem requirements**: Speed? Interpretability? Accuracy?\n2. **Data characteristics**: Size? Dimensionality? Imbalance?\n3. **Computational resources**: Training time? Prediction time? Memory?\n4. **Business context**: Cost of errors? Regulatory requirements?\n\nA model with 90% accuracy might be useless if it misses the 10% that actually matters. A model with 75% accuracy might be perfect if it catches the critical cases.\n\n---\n\n## Summary {#ch3-summary}\n\nYou've learned the fundamentals of classification and explored five major approaches. Let's recap the key insights.\n\n**Classification is fundamentally different from regression.** You're predicting categories, not continuous values. This changes everything: the algorithms, the evaluation metrics, the challenges you'll face. Linear regression is the wrong tool. You need classifiers designed to output probabilities or discrete predictions.\n\n**Each algorithm has a sweet spot.** Logistic regression for speed and interpretability with linear boundaries. Decision trees for maximum explainability. Random forests for robust performance without much tuning. SVMs for high-dimensional data with clear margins. k-NN for small datasets with complex local patterns. There's no universal best—context matters.\n\n**The confusion matrix is your diagnostic tool.** True positives, false positives, true negatives, false negatives—these four numbers tell you exactly where your model succeeds and fails. Every metric (accuracy, precision, recall, F1) derives from them. Master confusion matrices and you can navigate any classification problem.\n\n**Accuracy alone is almost always insufficient.** Especially with imbalanced data, accuracy can be completely misleading. You need to understand precision (when I predict positive, am I usually right?) and recall (of all actual positives, how many do I catch?). The tradeoff between them depends on your specific problem's costs. Medical diagnosis? Maximize recall. Spam detection? Maybe maximize precision. There's no one-size-fits-all answer.\n\n**ROC curves let you compare models across all thresholds.** Instead of committing to 0.5 as your decision threshold, ROC curves show performance across all possible thresholds. AUC summarizes this in one number. Higher is better.\n\n**Class imbalance is the norm, not the exception.** Fraud detection, disease diagnosis, rare event prediction—most interesting real-world problems have imbalanced classes. Naive models will just predict the majority class and claim victory with high accuracy. You need to detect imbalance (check value counts!), use appropriate metrics (forget accuracy, use F1 or AUC), and handle it properly (class weights, SMOTE, or other resampling techniques).\n\n**Visualization helps build intuition.** Decision boundaries, ROC curves, confusion matrix heatmaps—these aren't just pretty pictures. They help you understand what your model is actually doing. A model might have great accuracy but terrible decision boundaries. Visualization helps you see problems that metrics alone might hide.\n\nClassification is a core data science skill. You'll use it constantly: predicting customer churn, detecting fraud, diagnosing diseases, filtering spam, recommending products, identifying images. The algorithms you've learned here are the foundation. Master them, understand their tradeoffs, and you'll be equipped to tackle real classification problems.\n\nUse your brain. That's what it's there for.\n\n---\n\n## Practice Exercises {#ch3-practice}\n\n1. **Build and Compare Classifiers**: Using the Titanic dataset (or another binary classification dataset), fit all five classifier types (Logistic Regression, Decision Tree, Random Forest, SVM, k-NN). Create confusion matrices for each and compare their precision, recall, and F1 scores. Which performs best? Why do you think that is?\n\n2. **ROC Curve Comparison**: Using the same dataset from Exercise 1, plot ROC curves for all five models on the same figure. Which model has the highest AUC? Does this match the model with the best accuracy? Why or why not?\n\n3. **Hyperparameter Tuning**: Take a Decision Tree classifier and experiment with different values of `max_depth` (try 1, 3, 5, 10, 20, None). Plot training and test accuracy vs depth. At what depth does overfitting become apparent? How can you tell?\n\n4. **Feature Importance Analysis**: Fit a Random Forest on a classification dataset with multiple features. Extract and visualize feature importances. Which features are most predictive? Now remove the top feature and retrain. How much does performance drop?\n\n5. **Class Imbalance Challenge**: Create an imbalanced dataset (90% class 0, 10% class 1) using `make_classification`. Fit a naive logistic regression and check its confusion matrix. Then try three approaches to handle the imbalance: class weights, random oversampling, and SMOTE. Which works best? Use F1 score to compare.\n\n6. **Threshold Tuning**: Using logistic regression with `predict_proba()`, manually try different classification thresholds (0.3, 0.5, 0.7, 0.9). For each threshold, compute precision and recall. Plot precision vs recall as you vary the threshold. Explain the tradeoff you observe.\n\n---\n\n## Additional Resources {#ch3-additional-resources}\n\n- [Scikit-learn Classification Documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) - Official docs for all classifiers\n- [Understanding the ROC Curve](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) - Google's ML crash course on ROC/AUC\n- [Imbalanced-learn Documentation](https://imbalanced-learn.org/) - Handling imbalanced datasets with Python\n- [Confusion Matrix Guide](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) - Clear explanation of TP, FP, TN, FN\n- [Precision vs Recall](https://towardsdatascience.com/precision-vs-recall-386cf9f89488) - When to optimize for which metric\n- [Random Forests Explained](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) - Original paper by Leo Breiman (creator of random forests)\n- [SVM Visualization](https://www.youtube.com/watch?v=efR1C6CvhmE) - StatQuest video explaining SVMs visually\n\n",
    "supporting": [
      "chapter-3-classification_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}