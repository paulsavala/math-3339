{
  "hash": "48c3dfd0d0d88df7bae9a7c1a666d9a6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Chapter 1: Exploratory Data Analysis and AI-Assisted Coding\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Chapter Resources {#ch1-resources}\n\n**Related Assignments:**\n\n- [Chapter 1 Homework](../../Assignments/Chapter%201%20-%20EDA/chapter-1-homework.qmd)\n\n---\n\n## Introduction {#ch1-intro}\n\n### What is EDA?\nYou can't build effective models if you don't understand your data. Exploratory Data Analysis (EDA) is the foundation of data science - it's where you explore and understand your data before building models. In this first chapter we'll learn the tools and techniques needed to do effective EDA.\n\n### AI Coding Assistants\nThroughout this course we'll learn both how to do work by hand, and how to use AI tools to scale our work. In today's landscape, LLMs are exceptionally powerful, and can act as a \"calculator for words\". However, it's important to remember that LLMs are tools, not replacements for understanding. \n\nGiven the stochastic (unpredictable, non-deterministic) nature of LLMs, it's important to learn how to work with them effectively as partners, rather than human replacements. It's also important to learn how to do certain things by hand, such as calculating averages, making quick graphs, and selecting subsets of your data to analyze. If you're reliant on LLMs to do every time step for you, you'll take significantly longer to build models, and you'll also be less likely to catch errors in your work. By learning to work _with_ LLMs, instead of treating LLMs as a replacement for your own work, you'll be able to build models more quickly and with greater accuracy.\n\n---\n\n## 1. Getting Started with AI Coding Assistants {#ch1-1}\n\n### 1.1 Why Use AI for Coding?\n\nImagine the following situation: You have a data set with five columns of data, one of which is a target variable you want to predict. For example, you might be trying to predict the price of a house based on its size, number of bedrooms, number of bathrooms, and location. You want to build a model to predict the price of a house based on these features. This data is small enough that you could do the EDA by hand, including:\n- Making graphs for each column\n- Calculating summary statistics\n- Checking for missing values\n- Checking for outliers\n- Checking for data quality issues\n\nHowever, what if your data had three hundred columns? Now you move well beyond what you could do by hand, and what could be analyzed by a human. This is where AI coding assistants come in. \n\nThis is exactly how we'll use LLMs in this class. We'll first learn the topics by hand at a foundational level and a small scale. Then, we'll learn how to effectively use AI to scale our work. \n\n::: {.callout-note}\nAI won't take your job. Someone who knows how to use AI more effectively than you do will take your job. Be that person.\n:::\n\n### 1.2 Writing Effective Prompts\n\nLearning to write good prompts is like learning to communicate clearly with a colleague. You wouldn't walk up to a coworker and say \"make graph\" and expect them to know exactly what you want. The same goes for AI coding assistants.\n\nHere are the key principles for effective prompting:\n\n**1. Be specific about what you want**\n\nDon't just ask for \"a visualization\" when you know you want a histogram. Tell the AI exactly what type of output you're looking for.\n\n**2. Provide context**\n\nLet the AI know what data you're working with, what columns exist, and what you're trying to accomplish. The more context you provide, the better the results.\n\n**3. Start simple, then iterate**\n\nDon't try to get everything perfect in one prompt. Start with a basic request, see what you get, and refine from there. This is much faster than trying to write the \"perfect\" prompt.\n\n**4. Specify the details that matter**\n\nIf you care about axis labels, titles, colors, or specific parameter values, say so. If you don't specify, the AI will make assumptions that might not match what you need.\n\nLet's look at some examples. Here's a poor prompt:\n\n> \"Make a graph of the data\"\n\nWhat's wrong with this? There's no context about what data we're working with, what type of graph we want, or what we're trying to show. The AI has to guess everything.\n\nHere's a better prompt:\n\n> \"I'm working with a housing dataset that has columns for price, square_feet, bedrooms, and location. Create a histogram showing the distribution of house prices. Use 30 bins, add a title 'Distribution of House Prices', and label the x-axis as 'Price ($)' and y-axis as 'Frequency'. I'm working in Python and using matplotlib for graphing.\"\n\nSee the difference? This prompt tells the AI:\n\n- What data we have\n- What visualization we want (histogram)\n- What variable to plot (price)\n- Specific parameters (30 bins)\n- Formatting details (title, axis labels)\n\nLet's see this in action. Here's what you might get from that better prompt:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming you have a DataFrame called 'housing_df'\nplt.figure(figsize=(10, 6))\nplt.hist(housing_df['price'], bins=30, edgecolor='black')\nplt.title('Distribution of House Prices')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.show()\n```\n\nNotice that even with a good prompt, you might need to iterate. Maybe you look at this histogram and realize you want different bin sizes, or you want to add grid lines. That's fine! Just refine your prompt: \"Great, now add grid lines to make it easier to read the values.\"\n\nThe key is that you're building on what works, not starting from scratch each time. This iterative approach is exactly how you should work with AI assistants throughout this course.\n\n### 1.3 When to Use AI vs. Coding By Hand\n\nHow do you decide when to use AI, and when to do things by hand? You should start by asking yourself the following question: \"How well do I understand my data and the task I'm trying to accomplish?\" \n\nIf you understand it deeply and know exactly what you want to accomplish, then AI can be a great tool. However, if you're just starting to understand your data, and/or the task isn't clear, you should be working by hand.\n\nBecause LLMs take time to generate code, and because they often don't do quite what you had in mind, at the early stages you can spend more time modifying/re-prompting an LLM than you would if you were doing things by hand. You want a quick feedback loop between your brain and your screen. As soon as a question pops into your head (\"I wonder what the relationship is between...\") you should be able to answer it quickly. Changing your focus to work with an LLM will likely take too long, potentially result in more errors, and take you away from the task at hand.\n\n::: {.callout-note}\nAs you become more comfortable with your data and the task at hand, you can start to use AI to scale your work. You can use AI to generate code for you, and then modify it to fit your needs. You can also use AI to generate code for you, and then modify it to fit your needs. When you look at your work and say \"I know I'm on the right track, now I just need to do more\", then you should be using AI to scale your work.\n:::\n\n### Learning outcomes:\n**_By hand_ you should be able to:**\n\n- Write prompts that effectively describe what you want the LLM to accomplish.\n- Critique prompts as to whether or not they follow the suggestions outlined above.\n- Know when to use an LLM, and when to write code by hand.\n\n---\n\n## 2. Data Manipulation with Pandas {#ch1-2}\n\nPandas is the workhorse library for data manipulation in Python. If you're going to be a data scientist, you need to know Pandas inside and out. The good news is that once you learn the basics, everything else follows a similar pattern.\n\n### 2.1 Loading Data\n\nLet's start with the most basic task: loading data into Python. Most of the time, your data will be stored in a CSV (comma-separated values) file. Pandas makes this incredibly easy.\n\n::: {#2db17f9b .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load the California housing dataset\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Take a look at the first few rows\nhousing_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWhat's happening here? We're importing the pandas library (always abbreviated as `pd`), then using the `read_csv()` function to load our data. The result is a DataFrame, which you can think of as a table with rows and columns, similar to an Excel spreadsheet.\n\nThe `.head()` method shows us the first 5 rows. This is always a good first step when you load data—you want to see what you're working with.\n\nNow we can see our columns and get a sense of what the data looks like. This is California housing data with information about different districts. Notice that each row has an index (0, 1, 2, 3, 4) on the left side. Pandas automatically creates this for us.\n\nLet's explore our data a bit more:\n\n::: {#99725878 .cell execution_count=2}\n``` {.python .cell-code}\n# How many rows and columns do we have?\nprint(f'df shape: {housing_df.shape}\\n')\n\n# What are the column names?\nprint(f'df columns: {housing_df.columns}\\n')\n\n# What data types are in each column?\nprint('df dtypes:')\nprint(housing_df.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndf shape: (20640, 10)\n\ndf columns: Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income',\n       'median_house_value', 'ocean_proximity'],\n      dtype='object')\n\ndf dtypes:\nlongitude             float64\nlatitude              float64\nhousing_median_age    float64\ntotal_rooms           float64\ntotal_bedrooms        float64\npopulation            float64\nhouseholds            float64\nmedian_income         float64\nmedian_house_value    float64\nocean_proximity        object\ndtype: object\n```\n:::\n:::\n\n\nThe `.shape` attribute tells us the dimensions of our DataFrame. The `.columns` attribute gives us the column names. And `.dtypes` tells us what type of data is in each column (integers, floating point numbers, strings, etc.).\n\nWhy do we care about data types? Because you can't calculate the average of text data, and you can't use numbers as categories. Making sure your data types are correct is a crucial early step in any analysis. Even things that _seem_ like numbers may not be. For example, suppose that we had a row in our housing data which looked like this:\n\n```\nmedian_house_value  total_rooms  ...\nUnknown             1500.0       ...\n```\n\nIf we try to calculate the average house value, we'll get an error:\n\n```python\nhousing_df['median_house_value'].mean()\n```\n\nbecause the `median_house_value` column isn't numerical (it has a string value in it).\n\n::: {.callout-note}\nYou should always start by looking at your data using `df.head()`! This will help you catch any data type issues early. Following up with basic data checks (column names, data types, etc.) is also best practices.\n:::\n\n### 2.2 Selecting and Filtering Data\n\nNow that we have data loaded, we need to know how to slice it up and look at specific parts. This is where Pandas really shines.\n\n**Selecting columns:**\n\n::: {#b8d36e63 .cell execution_count=3}\n``` {.python .cell-code}\n# Select a single column (returns a Series)\nhouse_values = housing_df['median_house_value']\n\n# Select multiple columns\nmultiple_cols = ['median_house_value', 'ocean_proximity']\n\n# Select multiple columns (returns a DataFrame)\nvalue_and_location = housing_df[multiple_cols]\n```\n:::\n\n\nNotice that, when selecting multiple columns, you should enclose them in a list, like `['median_house_value', 'ocean_proximity']`. Here we did this in two steps: 1) Write down the columns we want, 2) Extract them from the data. However, there's no reason you can't do them in a single step:\n\n```python\nvalue_and_location = housing_df[\n    ['median_house_value', \n    'ocean_proximity']]\n```\n\nSometimes this can be confusing, because people think that double brackets `[['median_house_value', 'ocean_proximity']]` are some kind of special syntax. They're not. They're just a list of column names.\n\n**Filtering rows:**\n\nHere's where things get really useful. Let's say we only want to look at districts near the bay. We'll break this out into multiple steps, and then show how to combine it in a single step.\n\n::: {#81e46042 .cell execution_count=4}\n``` {.python .cell-code}\n# Check if each row is near the bay by creating a \"mask\"\nnear_bay_mask = housing_df['ocean_proximity'] == 'NEAR BAY'\n# Output: [True, True, True, True, True, False, False, ...]\n\n# Use this \"mask\" to filter the DataFrame\nnear_bay_districts = housing_df[near_bay_mask]\n\nnear_bay_districts.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWhat's going on here? The expression `housing_df['ocean_proximity'] == 'NEAR BAY'` creates a True/False (boolean) value for each row—True if the district is near the bay, False otherwise. Then we use that boolean mask to filter the DataFrame.\n\nHere we split it out into two steps, where we first created the mask `near_bay_mask = housing_df['ocean_proximity'] == 'NEAR BAY'`, and then used that mask to filter the DataFrame `housing_df[near_bay_mask]`. This is a common pattern when filtering data. However, you can also write this in just one step:\n\n```python\nnear_bay_districts = housing_df[housing_df['ocean_proximity'] == 'NEAR BAY']\n```\n\nOnce again, people sometimes find this confusing because of the `housing_df` inside the outer `housing_df`. However, as we've seen, all that's going on is two steps: 1) Create a mask to select what you want (e.g. districts near the bay), 2) Use that mask to filter the DataFrame.\n\nLet's try some more complex filters:\n\n::: {#71319941 .cell execution_count=5}\n``` {.python .cell-code}\n# Districts with more than 1000 total rooms\nlarge_districts = housing_df[housing_df['total_rooms'] > 1000]\n\n# Districts that are both near the bay AND have expensive houses (> $400,000)\nexpensive_bay_area = housing_df[(housing_df['ocean_proximity'] == 'NEAR BAY') &\n                                 (housing_df['median_house_value'] > 400000)]\n\n# Districts that are either very cheap OR very expensive\nextreme_values = housing_df[(housing_df['median_house_value'] < 150000) |\n                             (housing_df['median_house_value'] > 400000)]\n\nexpensive_bay_area.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>-122.27</td>\n      <td>37.80</td>\n      <td>52.0</td>\n      <td>249.0</td>\n      <td>78.0</td>\n      <td>396.0</td>\n      <td>85.0</td>\n      <td>1.2434</td>\n      <td>500001.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>-122.21</td>\n      <td>37.83</td>\n      <td>40.0</td>\n      <td>4991.0</td>\n      <td>674.0</td>\n      <td>1616.0</td>\n      <td>654.0</td>\n      <td>7.5544</td>\n      <td>411500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>-122.18</td>\n      <td>37.81</td>\n      <td>30.0</td>\n      <td>292.0</td>\n      <td>38.0</td>\n      <td>126.0</td>\n      <td>52.0</td>\n      <td>6.3624</td>\n      <td>483300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>-122.23</td>\n      <td>37.81</td>\n      <td>52.0</td>\n      <td>2315.0</td>\n      <td>292.0</td>\n      <td>861.0</td>\n      <td>258.0</td>\n      <td>8.8793</td>\n      <td>410300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice a few things:\n- We use `&` for \"and\" and `|` for \"or\"\n- We need parentheses around each condition when combining them\n- The comparison operators (`>`, `<`, `==`) work just like you'd expect\n\nHere's a practical example. Let's say you're analyzing housing affordability and you want to find all inland districts with median incomes above 5 and median house values under $300,000:\n\n::: {#6f68a577 .cell execution_count=6}\n``` {.python .cell-code}\n# Your target districts\naffordable_inland = housing_df[(housing_df['ocean_proximity'] == 'INLAND') &\n                               (housing_df['median_income'] > 5) &\n                               (housing_df['median_house_value'] < 300000)]\n\nprint(f\"Found {len(affordable_inland)} districts matching criteria\")\n\naffordable_inland.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 534 districts matching criteria\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>968</th>\n      <td>-121.88</td>\n      <td>37.67</td>\n      <td>25.0</td>\n      <td>2244.0</td>\n      <td>301.0</td>\n      <td>937.0</td>\n      <td>324.0</td>\n      <td>6.4524</td>\n      <td>296900.0</td>\n      <td>INLAND</td>\n    </tr>\n    <tr>\n      <th>969</th>\n      <td>-121.89</td>\n      <td>37.67</td>\n      <td>20.0</td>\n      <td>2948.0</td>\n      <td>471.0</td>\n      <td>1181.0</td>\n      <td>474.0</td>\n      <td>6.0604</td>\n      <td>247900.0</td>\n      <td>INLAND</td>\n    </tr>\n    <tr>\n      <th>975</th>\n      <td>-121.87</td>\n      <td>37.66</td>\n      <td>52.0</td>\n      <td>775.0</td>\n      <td>134.0</td>\n      <td>315.0</td>\n      <td>123.0</td>\n      <td>5.0677</td>\n      <td>233300.0</td>\n      <td>INLAND</td>\n    </tr>\n    <tr>\n      <th>979</th>\n      <td>-121.87</td>\n      <td>37.67</td>\n      <td>10.0</td>\n      <td>4337.0</td>\n      <td>800.0</td>\n      <td>1813.0</td>\n      <td>743.0</td>\n      <td>5.5000</td>\n      <td>247200.0</td>\n      <td>INLAND</td>\n    </tr>\n    <tr>\n      <th>981</th>\n      <td>-121.85</td>\n      <td>37.68</td>\n      <td>4.0</td>\n      <td>4719.0</td>\n      <td>741.0</td>\n      <td>1895.0</td>\n      <td>742.0</td>\n      <td>6.8132</td>\n      <td>282500.0</td>\n      <td>INLAND</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis kind of filtering is something you'll do constantly in data science. You'll often want to analyze specific subsets of your data to understand patterns.\n\n::: {.callout-note}\nBreaking complex queries, such as the affordable inland districts example above, into smaller steps can make it easier to understand and debug. Compare the query above with the same query written on a single line:\n```python\naffordable_inland = housing_df[(housing_df['ocean_proximity'] == 'INLAND') & (housing_df['median_income'] > 5) & (housing_df['median_house_value'] < 300000)]\n```\nThis is much more difficult to read and debug!\n:::\n\n### 2.3 Data Cleaning Basics\n\nReal-world data is messy. You'll have missing values, duplicates, wrong data types—all sorts of problems. Let's learn how to spot and fix them.\n\n**Checking for missing values:**\n\n::: {#34c94ea8 .cell execution_count=7}\n``` {.python .cell-code}\n# How many missing values in each column?\nhousing_df.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nlongitude               0\nlatitude                0\nhousing_median_age      0\ntotal_rooms             0\ntotal_bedrooms        207\npopulation              0\nhouseholds              0\nmedian_income           0\nmedian_house_value      0\nocean_proximity         0\ndtype: int64\n```\n:::\n:::\n\n\nIn this command, `housing_df.isnull()` is returning `True`/`False` for each value in the DataFrame. Then we use `.sum()` to count up the number of `True` values in each column.\n\nSo we have 207 missing values in the `total_bedrooms` column. What do we do about this?\n\n**Option 1: Drop rows with missing values**\n\n```python\n# Drop any row that has at least one missing value\nhousing_clean = housing_df.dropna()\n\n# Drop only rows where a specific column (total_bedrooms) is missing a value\n# This is useful if this column is extremely important, and your analysis wouldn't make sense without it\nhousing_clean = housing_df.dropna(subset=['total_bedrooms'])\n```\n\n::: {.callout-warning}\nBe careful with this approach! If you have many columns and missing values are scattered throughout, you might end up dropping most of your data. This is especially true if you're working with data with lots of columns and/or columns which aren't especially important. For example, a store may have a rewards number column. However, not all customers are reward customers. If we dropped all rows with any missing value, then all non-rewards customers would disappear from our data!\n:::\n\n\n**Option 2: Fill missing values**\n\n```python\n# Fill with a specific value (e.g. zero)\nhousing_df['total_bedrooms'] = housing_df['total_bedrooms'].fillna(0)\n\n# Fill missing values with the median (for numeric columns)\nmedian_bedrooms = housing_df['total_bedrooms'].median()\nhousing_df['total_bedrooms'] = housing_df['total_bedrooms'].fillna(median_bedrooms)\n```\n\nWhich approach should you use? It depends on your data and your analysis. If you have lots of data and relatively few missing values, dropping might be fine. If missing values are common, you'll need to fill them thoughtfully.\n\n::: {callout-warning}\nBe careful with filling missing values! Sometimes a missing value is a signal that something is wrong. For example, if a house is missing a price, it might be because it's not for sale. Or it may indicate something, such as a lack of sale price meaning the home wasn't sold. When you fill missing values with other values, you are making assumptions about the data that may not be true and run the risk of corrupting your data.\n:::\n\n**Checking for duplicates:**\n\nWe can check for duplicates using the `.duplicated()` method. As with other methods we've seen today, it returns `True`/`False` values according to whether the row is a duplicate or not. By using `.sum()` we add up (i.e. count) the number of duplicate rows.\n\n::: {#8015bacc .cell execution_count=8}\n``` {.python .cell-code}\n# Are there any duplicate rows?\nprint(f'Duplicates: {housing_df.duplicated().sum()}')\n\n# Remove duplicates\nhousing_clean = housing_df.drop_duplicates()\n\n# Remove duplicates based on specific columns\n# (e.g., if you only care about unique combinations of location and ocean proximity)\nhousing_unique = housing_df.drop_duplicates(subset=['longitude', 'latitude', 'ocean_proximity'])\n\nprint(f'Duplicates after removing: {housing_unique.duplicated().sum()}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDuplicates: 0\nDuplicates after removing: 0\n```\n:::\n:::\n\n\n::: {callout-warning}\nDuplicates aren't necessarily bad! If I'm a customer at a store and I return multiple times in the same day, depending on the info the store saves above me (e.g. name, date of visit), this may look like a duplicate, when it was really just me shopping multiple times.\n:::\n\n**Fixing data types:**\n\nSometimes Pandas doesn't guess the right data type when loading your data. For example, a column of numbers might be loaded as strings:\n\n::: {#614be9ea .cell execution_count=9}\n``` {.python .cell-code}\n# Check current data type\nprint(housing_df['median_house_value'].dtype)  # Shows float64\n\n# Convert a column to categorical (useful for columns with few unique values)\nhousing_df['ocean_proximity'] = housing_df['ocean_proximity'].astype('category')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfloat64\n```\n:::\n:::\n\n\n**Sample data cleaning workflow:**\n\nHere's a complete example of a basic data cleaning workflow:\n\n::: {#ced80e23 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load the data\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Check for issues\nprint(\"Shape:\", housing_df.shape)\nprint(\"\\nMissing values:\")\nprint(housing_df.isnull().sum())\nprint(\"\\nDuplicates:\", housing_df.duplicated().sum())\nprint(\"\\nData types:\")\nprint(housing_df.dtypes)\n\n# Clean the data\nhousing_clean = housing_df.copy()  # Make a copy so we don't modify the original\n\n# Remove duplicates (if any)\nhousing_clean = housing_clean.drop_duplicates()\n\n# Fill missing total_bedrooms with the median\nmedian_bedrooms = housing_clean['total_bedrooms'].median()\nhousing_clean['total_bedrooms'] = housing_clean['total_bedrooms'].fillna(median_bedrooms)\n\n# Convert ocean_proximity to categorical\nhousing_clean['ocean_proximity'] = housing_clean['ocean_proximity'].astype('category')\n\n# Verify the cleaning worked\nprint(\"\\nAfter cleaning:\")\nprint(\"Shape:\", housing_clean.shape)\nprint(\"Missing values:\", housing_clean.isnull().sum().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape: (20640, 10)\n\nMissing values:\nlongitude               0\nlatitude                0\nhousing_median_age      0\ntotal_rooms             0\ntotal_bedrooms        207\npopulation              0\nhouseholds              0\nmedian_income           0\nmedian_house_value      0\nocean_proximity         0\ndtype: int64\n\nDuplicates: 0\n\nData types:\nlongitude             float64\nlatitude              float64\nhousing_median_age    float64\ntotal_rooms           float64\ntotal_bedrooms        float64\npopulation            float64\nhouseholds            float64\nmedian_income         float64\nmedian_house_value    float64\nocean_proximity        object\ndtype: object\n\nAfter cleaning:\nShape: (20640, 10)\nMissing values: 0\n```\n:::\n:::\n\n\nThis workflow checks for issues, fixes them, and verifies the fixes worked. You'll use patterns like this constantly in your EDA work.\n\nThe key takeaway: always inspect your data before you analyze it. You need to know what you're working with, spot problems early, and fix them before they cause issues down the line. Don't just assume your data is clean—check!\n\n### Learning outcomes:\n**_By hand_ you should be able to:**\n\n- Load data using `df = pd.read_csv(...)`\n- Use `.head()` and `.tail()` to inspect your data.\n- List the columns in the data\n- List the data types and understand common types (e.g. int, object, float, bool)\n- Select one or more columns\n- Filter rows using criteria (e.g. city == \"Houston\", age > 25, etc.)\n- Filter rows use \"and\" (`&`) and \"or\" (`|`)\n- Count the number of missing values using `.isnull()` and `.sum()`\n- Drop rows with missing values, including dropping rows where only a certain column is missing values\n- Fill missing values with a fixed number (e.g. zero) or a calculated value (e.g. the mean value from the column)\n- Change data types\n\n---\n\n## 3. Statistical Summaries and Data Profiling {#ch1-3}\n\nOnce your data is loaded and cleaned, the next step is understanding what it contains. You need to know the typical values, the spread of the data, and whether there are any weird patterns. This is where statistical summaries come in.\n\n### 3.1 Descriptive Statistics\n\nPandas makes it incredibly easy to get summary statistics for your data. Let's start with the simplest approach:\n\n::: {#841b717c .cell execution_count=11}\n``` {.python .cell-code}\n# Get summary statistics for all numeric columns\nhousing_df.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20433.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-119.569704</td>\n      <td>35.631861</td>\n      <td>28.639486</td>\n      <td>2635.763081</td>\n      <td>537.870553</td>\n      <td>1425.476744</td>\n      <td>499.539680</td>\n      <td>3.870671</td>\n      <td>206855.816909</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.003532</td>\n      <td>2.135952</td>\n      <td>12.585558</td>\n      <td>2181.615252</td>\n      <td>421.385070</td>\n      <td>1132.462122</td>\n      <td>382.329753</td>\n      <td>1.899822</td>\n      <td>115395.615874</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-124.350000</td>\n      <td>32.540000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.499900</td>\n      <td>14999.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-121.800000</td>\n      <td>33.930000</td>\n      <td>18.000000</td>\n      <td>1447.750000</td>\n      <td>296.000000</td>\n      <td>787.000000</td>\n      <td>280.000000</td>\n      <td>2.563400</td>\n      <td>119600.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-118.490000</td>\n      <td>34.260000</td>\n      <td>29.000000</td>\n      <td>2127.000000</td>\n      <td>435.000000</td>\n      <td>1166.000000</td>\n      <td>409.000000</td>\n      <td>3.534800</td>\n      <td>179700.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-118.010000</td>\n      <td>37.710000</td>\n      <td>37.000000</td>\n      <td>3148.000000</td>\n      <td>647.000000</td>\n      <td>1725.000000</td>\n      <td>605.000000</td>\n      <td>4.743250</td>\n      <td>264725.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-114.310000</td>\n      <td>41.950000</td>\n      <td>52.000000</td>\n      <td>39320.000000</td>\n      <td>6445.000000</td>\n      <td>35682.000000</td>\n      <td>6082.000000</td>\n      <td>15.000100</td>\n      <td>500001.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWhat is all this telling us? Let's break it down row by row:\n\n- **count**: How many non-missing values exist\n- **mean**: The average value\n- **std**: Standard deviation (how spread out the values are)\n- **min**: The smallest value\n- **25%**: The 25th percentile (25% of values are below this)\n- **50%**: The median (middle value when sorted)\n- **75%**: The 75th percentile (75% of values are below this)\n- **max**: The largest value\n\nBut what do these numbers actually mean for our housing data? Let's interpret:\n\nThe median house value is about $206,856 on average, with a standard deviation of $115,396. That's a pretty big spread—prices vary a lot across California! The median (50th percentile) is $179,700, which is lower than the mean. What does that tell us? It suggests the distribution might be right-skewed, meaning there are some very expensive districts pulling the average up.\n\nFor total rooms, the typical district has around 2,636 rooms, with most districts falling between 1,448 and 3,148 rooms (the 25th to 75th percentile range). But look at that max value: 39,320 rooms! That's a huge outlier that we might want to investigate.\n\nSometimes you want statistics for a single column:\n\n::: {#9540dd55 .cell execution_count=12}\n``` {.python .cell-code}\n# Mean of a specific column\navg_value = housing_df['median_house_value'].mean()\nprint(f\"Average house value: ${avg_value:,.0f}\")\n\n# Median\nmedian_value = housing_df['median_house_value'].median()\nprint(f\"Median house value: ${median_value:,.0f}\")\n\n# Standard deviation\nvalue_std = housing_df['median_house_value'].std()\nprint(f\"House value standard deviation: ${value_std:,.0f}\")\n\n# Specific percentiles\npercentile_90 = housing_df['median_house_value'].quantile(0.90)\nprint(f\"90th percentile: ${percentile_90:,.0f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage house value: $206,856\nMedian house value: $179,700\nHouse value standard deviation: $115,396\n90th percentile: $376,600\n```\n:::\n:::\n\n\nWhy would you care about the median vs. the mean? The median is more robust to outliers. If you have one very expensive district in an area of typical districts, the mean will be pulled way up, but the median will stay reasonable. When you're trying to understand \"typical\" values, the median is often more useful.\n\nHere's a practical example. Let's say you want to understand house values in different proximity to the ocean:\n\n::: {#7a337dec .cell execution_count=13}\n``` {.python .cell-code}\n# Average value by ocean proximity\nvalue_by_proximity = housing_df.groupby('ocean_proximity')['median_house_value'].agg(['mean', 'median', 'count'])\nprint(value_by_proximity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          mean    median  count\nocean_proximity                                \n<1H OCEAN        240084.285464  214850.0   9136\nINLAND           124805.392001  108500.0   6551\nISLAND           380440.000000  414700.0      5\nNEAR BAY         259212.311790  233800.0   2290\nNEAR OCEAN       249433.977427  229450.0   2658\n```\n:::\n:::\n\n\nNow we're getting somewhere! Districts near the ocean (especially islands!) are significantly more expensive on average. This kind of breakdown is crucial for understanding your data—overall statistics can hide important patterns in subgroups.\n\n::: {.callout-tip}\nPandas can automatically do this (and more) for you with the `.describe()` method. Try running `housing_df.describe()` on your data and see what it gives you.\n:::\n\n\n### 3.2 Outliers\n Outliers are another important aspect of understanding your data. These are values that are far from the typical range. The typical way to identify outliers is to find data that is in the top and bottom 1% of the data.\n\n::: {#f4189c7a .cell execution_count=14}\n``` {.python .cell-code}\n# Find the 1st and 99th percentiles\nbottom_1_percent = housing_df['median_house_value'].quantile(0.01)\ntop_1_percent = housing_df['median_house_value'].quantile(0.99)\n\nprint(f\"Bottom 1%: ${bottom_1_percent:,.0f}\")\nprint(f\"Top 1%: ${top_1_percent:,.0f}\")\n\n# Find potential outliers\noutliers = housing_df[(housing_df['median_house_value'] < bottom_1_percent) |\n                       (housing_df['median_house_value'] > top_1_percent)]\n\nprint(f\"\\nFound {len(outliers)} potential outliers\\n\")\n\nprint(outliers[['median_house_value', 'ocean_proximity', 'median_income']].head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBottom 1%: $50,000\nTop 1%: $500,001\n\nFound 199 potential outliers\n\n      median_house_value ocean_proximity  median_income\n1175             42500.0          INLAND         0.8252\n1176             45100.0          INLAND         1.0585\n1177             39400.0          INLAND         1.3289\n1181             41800.0          INLAND         2.2045\n1188             49000.0          INLAND         1.7727\n```\n:::\n:::\n\n\nThere's nothing particular about the top and bottom 1%. You could certainly look at the top and bottom 1%, or 2%, or 5%, or 10%, or any other percentage you want. The key is to look at the data and see if it makes sense. Primarily what you're looking for is data that doesn't make sense. If a district has very high house values, that could be totally reasonable, and of course there will always be _some_ data in the top/bottom 1%. The problem is when those values don't make sense. For example, if house values are shown as negative, that's not real. You need to investigate before deciding what to do. Similarly, if values are impossibly high, that also doesn't make sense.\n\n::: {.callout-tip}\nShould you remove outliers? Not automatically! They might be legitimate data (yes, some houses really are that expensive), or they might be errors (someone entered $5,000,000 instead of $500,000). You need to investigate before deciding what to do. Often this is a judgement call, more than a clear-cut decision.\n:::\n\n### Learning outcomes: {#ch1-3-outcomes}\n**_By hand_ you should be able to:**\n\n- Use `.describe()`\n- Calculate the mean, median, standard deviation and percentiles from a column\n- Use `.quantile()` to find potential outliers on the high and low end\n\n---\n\n## 4. Data Visualization Principles {#ch1-4}\n\nNumbers and summary statistics are useful, but humans are visual creatures. A good visualization can reveal patterns that would take hours to find in tables of numbers. But here's the thing: not all visualizations are created equal. You need to match the right type of plot to the question you're asking.\n\n### 4.1 Choosing the Right Visualization {#ch1-4-1}\n\nThe type of visualization you choose depends on what you're trying to show. Here are the most common scenarios:\n\n**Want to see the distribution of a single variable?** Use a histogram or box plot.\n\n**Want to see the relationship between two numeric variables?** Use a scatter plot.\n\n**Want to compare values across categories?** Use a bar plot.\n\n**Want to see how something changes over time?** Use a line plot.\n\nLet's break these down with examples.\n\n#### 4.1.1 Histograms {#ch1-4-1-1}\nHistograms show you how data is distributed. They're perfect for answering questions like \"Are house values normally distributed?\" or \"How many districts fall into different value ranges?\"\n\n::: {#150a55ac .cell execution_count=15}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of median house values\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value', bins=30)\nplt.title('Distribution of Median House Values')\nplt.xlabel('Median House Value ($)')\nplt.ylabel('Count')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-16-output-1.png){width=825 height=523}\n:::\n:::\n\n\nThis shows you the shape of your data. Is it symmetric? Skewed? Are there outliers? All of this becomes immediately visible.\n\n#### 4.1.2 Box plots {#ch1-4-1-2}\n\nBox plots are another way to visualize distributions, especially useful for comparing across groups:\n\n::: {#8138fc84 .cell execution_count=16}\n``` {.python .cell-code}\n# Compare house value distributions across ocean proximity categories\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value')\nplt.title('House Values by Ocean Proximity')\nplt.xlabel('Ocean Proximity')\nplt.ylabel('Median House Value ($)')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-17-output-1.png){width=842 height=582}\n:::\n:::\n\n\nThe box shows the 25th to 75th percentile (the middle 50% of data), the line in the middle is the median, and the \"whiskers\" extend to show the range. Points beyond the whiskers are potential outliers.\n\n::: {.callout-tip}\nBox plots are excellent for quickly comparing distributions across multiple groups. You can instantly see which group has higher medians, more variability, or more outliers.\n:::\n\n#### 4.1.3 Scatter plots {#ch1-4-1-3}\n\nScatter plots reveal relationships between two numeric variables:\n\n::: {#c2f49dbf .cell execution_count=17}\n``` {.python .cell-code}\n# Relationship between median income and house value\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-18-output-1.png){width=842 height=523}\n:::\n:::\n\n\nIf you see points trending upward from left to right, that's a positive relationship—higher incomes are associated with more expensive houses. If points are scattered randomly, there's no clear relationship.\n\nYou can add a third variable using color:\n\n::: {#7b91ef23 .cell execution_count=18}\n``` {.python .cell-code}\n# Add ocean proximity as color\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', alpha=0.6)\nplt.title('House Value vs. Median Income by Ocean Proximity')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-19-output-1.png){width=842 height=523}\n:::\n:::\n\n\nNow you can see if the relationship between income and house values differs by ocean proximity. Maybe coastal districts are consistently more expensive for the same income level.\n\n#### 4.1.4 Bar plots {#ch1-4-1-4}\n\nBar plots compare values across categories:\n\n::: {#eca444cb .cell execution_count=19}\n``` {.python .cell-code}\n# Average house value by ocean proximity\nplt.figure(figsize=(10, 6))\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', estimator='mean')\nplt.title('Average House Value by Ocean Proximity')\nplt.xlabel('Ocean Proximity')\nplt.ylabel('Average House Value ($)')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-20-output-1.png){width=842 height=582}\n:::\n:::\n\n\nThis makes comparisons obvious at a glance. You can immediately see which proximity category has the highest average house values.\n\n::: {.callout-note}\nThe key to choosing visualizations: think about what question you're asking. \"How is this variable distributed?\" → histogram. \"Is there a relationship between these two things?\" → scatter plot. \"Which group is highest?\" → bar plot. Match the visualization to the question.\n:::\n\n### 4.2 Creating Visualizations with Seaborn {#ch1-4-2}\n\nSeaborn is built on top of matplotlib and makes creating statistical visualizations much easier. It has sensible defaults, nice-looking styles, and works seamlessly with pandas DataFrames.\n\nLet's walk through the basic visualizations you'll use constantly:\n\n#### 4.2.1 Histograms {#ch1-4-2-1}\n\n::: {#63ff129c .cell execution_count=20}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Basic histogram\nsns.histplot(data=housing_df, x='median_house_value')\nplt.show()\n\n# With less bins\nsns.histplot(data=housing_df, x='median_house_value', bins=10)\nplt.show()\n\n# With KDE (smooth density curve) overlay\nsns.histplot(data=housing_df, x='median_house_value', kde=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-21-output-1.png){width=603 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-21-output-2.png){width=603 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-21-output-3.png){width=603 height=429}\n:::\n:::\n\n\nThe `bins` parameter controls how many bars you see. Too few bins and you lose detail. Too many and it gets noisy. Usually 20-50 bins is a good starting point.\n\n#### 4.2.2 Scatter plots {#ch1-4-2-2}\n\n::: {#541c6e77 .cell execution_count=21}\n``` {.python .cell-code}\n# Basic scatter plot\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n# With color by category\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-22-output-1.png){width=618 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-22-output-2.png){width=618 height=429}\n:::\n:::\n\n\nScatter plots are your go-to tool for finding relationships. If you're doing EDA and wondering whether two variables are related, make a scatter plot. It takes 2 seconds and can reveal patterns that summary statistics miss.\n\n::: {.callout-tip}\nWhen exploring relationships, always make a scatter plot first. You might have the same correlation coefficient but completely different patterns. The classic example is Anscombe's quartet—four datasets with identical statistics but totally different patterns when plotted.\n:::\n\n#### 4.2.3 Bar plots {#ch1-4-2-3}\n\n::: {#caa11b5c .cell execution_count=22}\n``` {.python .cell-code}\n# Count of observations by category\nsns.countplot(data=housing_df, x='ocean_proximity')\nplt.xticks(rotation=45)\nplt.show()\n\n# Average value by category\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', estimator='mean')\nplt.xticks(rotation=45)\nplt.show()\n\n# Grouped bar plot - create age groups first\nhousing_df['age_group'] = pd.cut(housing_df['housing_median_age'],\n                                  bins=[0, 20, 35, 100],\n                                  labels=['New', 'Mid', 'Old'])\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', hue='age_group')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-23-output-1.png){width=601 height=488}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-23-output-2.png){width=618 height=488}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-23-output-3.png){width=618 height=488}\n:::\n:::\n\n\nBar plots are perfect for comparing across categories. The `estimator` parameter lets you choose what to show—mean, median, sum, etc.\n\n#### 4.2.4 Box plots {#ch1-4-2-4}\n\n::: {#f6d9634f .cell execution_count=23}\n``` {.python .cell-code}\n# Distribution by category\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value')\nplt.xticks(rotation=45)\nplt.show()\n\n# Horizontal (sometimes easier to read with long labels)\nsns.boxplot(data=housing_df, y='ocean_proximity', x='median_house_value')\nplt.show()\n\n# Multiple categories\nhousing_df['income_bracket'] = pd.cut(housing_df['median_income'],\n                                       bins=[0, 2.5, 4.5, 20],\n                                       labels=['Low', 'Medium', 'High'])\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value', hue='income_bracket')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-24-output-1.png){width=618 height=488}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-24-output-2.png){width=658 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-24-output-3.png){width=618 height=488}\n:::\n:::\n\n\nBox plots pack a lot of information into a small space. You see the median, the spread (IQR), and outliers all at once.\n\n#### 4.2.5 Pair plots (bonus) {#ch1-4-2-5}\n\nWhen you want to see relationships between multiple variables at once:\n\n::: {#434c577b .cell execution_count=24}\n``` {.python .cell-code}\n# Scatter plots for all numeric variables\nsns.pairplot(housing_df[['median_house_value', 'median_income', 'housing_median_age', 'total_rooms']])\nplt.show()\n\n# Color by category\nsns.pairplot(housing_df[['median_house_value', 'median_income', 'housing_median_age',\n                         'total_rooms', 'ocean_proximity']],\n             hue='ocean_proximity')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-25-output-1.png){width=946 height=947}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-25-output-2.png){width=1091 height=947}\n:::\n:::\n\n\nThis creates a grid of scatter plots showing every pair of variables. It's incredibly useful for initial exploration—you can spot all the relationships at once.\n\n### 4.3 Best Practices for Effective Visualizations {#ch1-4-3}\n\nA visualization without labels is just decorative art. You need to make your plots readable and informative. Here are the key principles:\n\n**1. Always add titles and axis labels**\n\n::: {#fbf8bde5 .cell execution_count=25}\n``` {.python .cell-code}\nprint('Bad: no labels')\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\nprint('Good: clear labels')\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income', fontsize=14, fontweight='bold')\nplt.xlabel('Median Income (in $10k)', fontsize=12)\nplt.ylabel('Median House Value ($)', fontsize=12)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBad: no labels\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-26-output-2.png){width=618 height=429}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nGood: clear labels\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-26-output-4.png){width=621 height=454}\n:::\n:::\n\n\nYour title should answer \"What am I looking at?\" Your axis labels should include units where relevant (dollars, income brackets, etc.).\n\n**2. Make the plot big enough to read**\n\n::: {#d78cca83 .cell execution_count=26}\n``` {.python .cell-code}\n# Too small (default size is often cramped)\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n# Better: specify figure size\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-27-output-1.png){width=544 height=356}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-27-output-2.png){width=842 height=503}\n:::\n:::\n\n\nThe `figsize` parameter takes (width, height) in inches. A good starting point is (10, 6) for most plots.\n\n**3. Use color meaningfully**\n\nColor should convey information, not just look pretty:\n\n::: {#ec27109c .cell execution_count=27}\n``` {.python .cell-code}\n# Color by category to show groups\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', palette='Set2')  # Use a colorblind-friendly palette\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-28-output-1.png){width=618 height=429}\n:::\n:::\n\n\nSeaborn has many built-in palettes: 'Set2', 'colorblind', 'husl', etc. Choose one that's easy to distinguish and colorblind-safe.\n\n::: {.callout-warning}\nAvoid using red and green together as your only color distinction. About 8% of men have some form of color blindness that makes red-green distinctions difficult. Use colorblind-friendly palettes like 'colorblind' or 'Set2'.\n:::\n\n**4. Add grid lines for easier reading**\n\n::: {#dd57d832 .cell execution_count=28}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.grid(True, alpha=0.3)  # alpha makes it subtle\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-29-output-1.png){width=842 height=523}\n:::\n:::\n\n\nSubtle grid lines make it easier to read values from your plot.\n\n**5. Format numbers appropriately**\n\nLarge numbers benefit from formatting:\n\n::: {#c75abbf4 .cell execution_count=29}\n``` {.python .cell-code}\nimport matplotlib.ticker as mtick\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value')\n\n# Format y-axis as integers\nplt.gca().yaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))\n\n# Format x-axis as dollars\nplt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))\n\nplt.title('Distribution of Median House Values')\nplt.xlabel('Median House Value')\nplt.ylabel('Count')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-30-output-1.png){width=829 height=523}\n:::\n:::\n\n\nThis adds dollar signs and comma separators, making the plot much easier to read.\n\n**6. Adjust plot limits when needed**\n\nSometimes outliers make it hard to see the main pattern:\n\n::: {#a24c3eb9 .cell execution_count=30}\n``` {.python .cell-code}\n# If you have extreme outliers\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.xlim(0, 10)  # Focus on median incomes under 10\nplt.ylim(0, 500000)  # Focus on house values under $500k\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-31-output-1.png){width=850 height=508}\n:::\n:::\n\n\nUse this carefully—you're choosing to hide data. But sometimes it's necessary to see the pattern in the majority of your data.\n\n**Complete example with all best practices:**\n\n::: {#189c5387 .cell execution_count=31}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Set the style for all plots\nsns.set_style('whitegrid')\n\n# Create the plot\nplt.figure(figsize=(12, 7))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', palette='Set2', s=100, alpha=0.6)\n\n# Add labels and title\nplt.title('House Value vs. Median Income by Ocean Proximity',\n          fontsize=16, fontweight='bold', pad=20)\nplt.xlabel('Median Income (in $10k)', fontsize=13)\nplt.ylabel('Median House Value ($)', fontsize=13)\n\n# Format the y-axis as currency\nplt.gca().yaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))\n\n# Adjust legend\nplt.legend(title='Ocean Proximity', fontsize=11, title_fontsize=12)\n\n# Add subtle grid\nplt.grid(True, alpha=0.3)\n\n# Tight layout to prevent label cutoff\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-1-eda_files/figure-html/cell-32-output-1.png){width=1144 height=664}\n:::\n:::\n\n\nThis creates a professional-looking plot that's easy to read and interpret.\n\n::: {.callout-tip}\nSave your well-formatted plotting code as a template. When you find styling that works well, reuse it. You don't want to be tweaking font sizes every time you make a plot.\n:::\n\nThe key takeaway: visualizations are tools for understanding, not just decoration. Choose the right type of plot for your question, label everything clearly, and make it easy to read. When someone looks at your visualization, they should immediately understand what they're seeing and why it matters.\n\n### Learning outcomes: {#ch1-4-outcomes}\n**_By hand_ you should be able to:**\n\n- Pick an appropriate visualization for your data\n- Compare and contrast the visualizations discussed (e.g. their use cases, what data they work with, etc.)\n- Create a basic histogram, scatter plot, bar chart and box plot using Seaborn\n- Assign a title to your graph\n\n---\n\n## 5. Testing Your Data Analysis Code {#ch1-5}\n\n### 5.1 Why Testing Matters {#ch1-5-1}\n\nHere's a scenario that happens all the time: You write some code to filter your data, run your analysis, get results, and present them to your team. Then someone asks \"wait, why are there only 5 data points?\". Your entire analysis is now useless.\n\nTesting catches these errors before they become embarrassing mistakes. And in data science, testing doesn't have to be complicated. Simple checks can save you from major headaches.\n\nThe goal is simple: make sure your code is doing what you think it's doing. If you filter data, did you actually get the rows you expected? If you drop missing values, did the count of missing values actually drop? These are the kinds of things you should be checking.\n\n### 5.2 Using assert Statements {#ch1-5-2}\n\nThe simplest way to test your data analysis code is with `assert` statements. An assert is like a sanity check: you state what should be true, and if it's not, Python stops and tells you something is wrong.\n\nHere's the basic idea:\n\n::: {#d164aae9 .cell execution_count=32}\n``` {.python .cell-code}\n# This will pass (no error)\nassert 5 > 3\n\n# This will fail and raise an error\nassert 2 > 3\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-cyan-fg\">Cell</span><span class=\"ansi-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[32]</span><span class=\"ansi-green-fg\">, line 5</span>\n<span class=\"ansi-green-fg\">      2</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> <span class=\"ansi-green-fg\">5</span> &gt; <span class=\"ansi-green-fg\">3</span>\n<span class=\"ansi-green-fg\">      4</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># This will fail and raise an error</span>\n<span class=\"ansi-green-fg\">----&gt; </span><span class=\"ansi-green-fg\">5</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> <span class=\"ansi-green-fg\">2</span> &gt; <span class=\"ansi-green-fg\">3</span>\n\n<span class=\"ansi-red-fg\">AssertionError</span>: </pre>\n```\n:::\n\n:::\n:::\n\n\nNotice that when an assertion fails, your code stops. That's the point! You want to know immediately when something is wrong.\n\n::: {.callout-tip}\nYou can (and should!) add a message to the assert statement to help you understand what went wrong.\n\n::: {#7f4f0c9d .cell execution_count=33}\n``` {.python .cell-code}\nassert 2 > 3, \"Two is not greater than three!\"\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-cyan-fg\">Cell</span><span class=\"ansi-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[33]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-green-fg\">----&gt; </span><span class=\"ansi-green-fg\">1</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> <span class=\"ansi-green-fg\">2</span> &gt; <span class=\"ansi-green-fg\">3</span>, <span class=\"ansi-yellow-fg\">\"</span><span class=\"ansi-yellow-fg\">Two is not greater than three!</span><span class=\"ansi-yellow-fg\">\"</span>\n\n<span class=\"ansi-red-fg\">AssertionError</span>: Two is not greater than three!</pre>\n```\n:::\n\n:::\n:::\n\n\n:::\n\n**Common mistake: Case sensitivity**\n\nLet's say you want to filter for districts near the bay. The code below looks good at first glance:\n\n::: {#aade9c00 .cell execution_count=34}\n``` {.python .cell-code}\n# Intentional typo: \"near bay\" instead of \"NEAR BAY\"\nnear_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n```\n:::\n\n\nHowever, later you decide to print out how many districts are near the bay.\n\n::: {#c64ff0fa .cell execution_count=35}\n``` {.python .cell-code}\nprint(f\"Found {len(near_bay)} districts near the bay\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 0 districts near the bay\n```\n:::\n:::\n\n\nZero districts! But your code didn't crash, so you might not notice. This is where asserts help:\n\n::: {#bfab5277 .cell execution_count=36}\n``` {.python .cell-code}\n# Filter the data\nnear_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n\n# Assert that we got some results\nassert len(near_bay) > 0, \"No districts found! Check your filter condition\"\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-cyan-fg\">Cell</span><span class=\"ansi-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[36]</span><span class=\"ansi-green-fg\">, line 5</span>\n<span class=\"ansi-green-fg\">      2</span> near_bay = housing_df[housing_df[<span class=\"ansi-yellow-fg\">'</span><span class=\"ansi-yellow-fg\">ocean_proximity</span><span class=\"ansi-yellow-fg\">'</span>] == <span class=\"ansi-yellow-fg\">'</span><span class=\"ansi-yellow-fg\">near bay</span><span class=\"ansi-yellow-fg\">'</span>]\n<span class=\"ansi-green-fg\">      4</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Assert that we got some results</span>\n<span class=\"ansi-green-fg\">----&gt; </span><span class=\"ansi-green-fg\">5</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> <span style=\"color:rgb(0,135,0)\">len</span>(near_bay) &gt; <span class=\"ansi-green-fg\">0</span>, <span class=\"ansi-yellow-fg\">\"</span><span class=\"ansi-yellow-fg\">No districts found! Check your filter condition</span><span class=\"ansi-yellow-fg\">\"</span>\n\n<span class=\"ansi-red-fg\">AssertionError</span>: No districts found! Check your filter condition</pre>\n```\n:::\n\n:::\n:::\n\n\nNow the code fails loudly, and the error message tells you exactly what went wrong. The problem is that the values in that column are capitalized, and you forgot to capitalize them. By using an `assert` you immediately catch the problem and can investigate it, before it becomes a bigger issue.\n\n**Checking data after operations**\n\nUse asserts after every major operation to verify it did what you expected:\n\n::: {#a08f7d30 .cell execution_count=37}\n``` {.python .cell-code}\n# Start with the full dataset\nprint(f\"Starting with {len(housing_df)} rows\")\n\n# Drop missing values\nhousing_clean = housing_df.dropna()\n\n# Assert that we actually dropped some rows (since we know there were missing values)\nassert len(housing_clean) < len(housing_df), \"Expected to drop some rows with missing values\"\n\nprint(f\"After dropping missing values: {len(housing_clean)} rows\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting with 20640 rows\nAfter dropping missing values: 20433 rows\n```\n:::\n:::\n\n\n**Checking ranges and values**\n\nAfter computing statistics, verify they make sense:\n\n::: {#218cae10 .cell execution_count=38}\n``` {.python .cell-code}\n# Calculate median house value\nmedian_value = housing_df['median_house_value'].median()\nprint(f\"Median house value: ${median_value:,.0f}\")\n\n# Sanity checks\nassert median_value > 0, \"House values should be positive\"\nassert median_value < 10000000, \"Median seems unrealistically high - check your data\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedian house value: $179,700\n```\n:::\n:::\n\n\n**Checking for duplicates**\n\nAfter removing duplicates, verify they're actually gone:\n\n::: {#fe27e5f4 .cell execution_count=39}\n``` {.python .cell-code}\n# Remove duplicates\nhousing_no_dupes = housing_df.drop_duplicates()\n\n# Verify no duplicates remain\nassert housing_no_dupes.duplicated().sum() == 0, \"Duplicates still exist after dropping\"\n\nprint(f\"Successfully removed duplicates. {len(housing_df) - len(housing_no_dupes)} rows removed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSuccessfully removed duplicates. 0 rows removed\n```\n:::\n:::\n\n\n**A practical example: Complete data cleaning with asserts**\n\nHere's how you might use asserts throughout a real data cleaning workflow:\n\n::: {#619b9e9d .cell execution_count=40}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('../data/housing.csv')\n\n# Assert basic structure\nassert len(df) > 0, \"DataFrame is empty\"\nassert 'median_house_value' in df.columns, \"Missing expected column\"\n\nprint(f\"Starting with {len(df)} rows\")\n\n# Check for missing values before cleaning\nmissing_before = df.isnull().sum().sum()\nprint(f\"Missing values before cleaning: {missing_before}\")\n\n# Fill missing values\ndf_clean = df.copy()\nmedian_bedrooms = df_clean['total_bedrooms'].median()\ndf_clean['total_bedrooms'] = df_clean['total_bedrooms'].fillna(median_bedrooms)\n\n# Assert that we reduced missing values\nmissing_after = df_clean.isnull().sum().sum()\nassert missing_after < missing_before, \"Should have fewer missing values after cleaning\"\n\nprint(f\"Missing values after cleaning: {missing_after}\")\n\n# Filter for expensive houses\nexpensive = df_clean[df_clean['median_house_value'] > 400000]\n\n# Assert we got some results\nassert len(expensive) > 0, \"No expensive houses found - check threshold\"\nassert len(expensive) < len(df_clean), \"All houses are expensive? Something is wrong\"\n\nprint(f\"Found {len(expensive)} expensive districts\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting with 20640 rows\nMissing values before cleaning: 207\nMissing values after cleaning: 0\nFound 1744 expensive districts\n```\n:::\n:::\n\n\n::: {.callout-tip}\nUse assert statements liberally during development. They're like guardrails that keep your analysis on track. Once your code is working correctly, you can leave them in—they serve as documentation of what your code expects to be true.\n:::\n\n**What about \"real\" testing?**\n\nAssert statements are great for quick checks during data exploration. In later chapters, we'll learn about proper unit tests, which are essential when you're building production data pipelines or reusable analysis code. But for now, assert statements will catch 90% of your mistakes with 10% of the effort.\n\nThe key principle: **Don't trust your code blindly**. Check that it's doing what you think it's doing. Your future self (and your team) will thank you.\n\n### Learning outcomes: {#ch1-5-outcomes}\n**_By hand_ you should be able to:**\n\n- Understand what `assert` statements do\n- Write `assert` statements to check conditions\n- Write appropriate responses that will be displayed if the `assert` fails\n- Understand when `assert` statements are and are not appropriate to use\n\n---\n\n## 6. Scaling EDA with AI {#ch1-6}\n\nYou now know how to do EDA by hand: load data, clean it, compute statistics, make visualizations, test your assumptions. This is crucial foundational knowledge. But here's the thing: what if you have 50 columns instead of 10? What if you want to compare patterns across 20 different categories? What if you want to check 100 different hypotheses?\n\nThis is where AI coding assistants shine. Once you understand what you're doing and why, you can use AI to scale your work massively. The key is that you already know what to look for, so you can verify the AI's output and catch mistakes.\n\n### 6.1 Automating Exploratory Visualizations {#ch1-6-1}\n\nLet's say you want to create histograms for every numeric column in your dataset. Doing this by hand would be tedious:\n\n```python\n# By hand: repetitive and error-prone\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value')\nplt.title('Distribution of Median House Value')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_income')\nplt.title('Distribution of Median Income')\nplt.show()\n\n# ... repeat for every column ...\n```\n\nInstead, you can use AI to generate this systematically. Here's an effective prompt:\n\n> \"I have a pandas DataFrame called `housing_df` with these numeric columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value.\n>\n> Write Python code using Pandas and Seaborn that creates a histogram for each numeric column. Use seaborn's histplot, make each figure 10x6 inches, add appropriate titles (formatted nicely with spaces instead of underscores), and save each plot as a PNG file named after the column.\n>\n> Use a loop to avoid repetitive code.\"\n\nThe AI will generate something like this:\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# List of numeric columns\nnumeric_cols = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n                'total_bedrooms', 'population', 'households', 'median_income',\n                'median_house_value']\n\nfor col in numeric_cols:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=housing_df, x=col, bins=30)\n\n    # Format title nicely\n    title = col.replace('_', ' ').title()\n    plt.title(f'Distribution of {title}')\n    plt.xlabel(title)\n    plt.ylabel('Count')\n\n    # Save the plot\n    plt.savefig(f'{col}_histogram.png', dpi=300, bbox_inches='tight')\n    plt.close()  # Close to avoid memory issues\n\nprint(f\"Created {len(numeric_cols)} histograms\")\n```\n\n::: {.callout-note}\nNotice how the prompt was specific about what you wanted. You told the AI the column names, the figure size, the formatting requirements, and the output format. Good prompts get good results.\n:::\n\n**Comparing distributions across groups**\n\nWant to see how house values differ across all ocean proximity categories? That's a perfect job for AI:\n\n> \"Using my `housing_df` DataFrame with columns 'ocean_proximity' and 'median_house_value', create box plots comparing house values across all ocean proximity categories. Make the plot readable with rotated x-axis labels. Also calculate summary statistics (mean, median, count) for each category and save them all in a dataframe.\"\n\n**Creating a comprehensive visualization suite**\n\nFor a complete EDA report, you might prompt:\n\n> \"Create a comprehensive visualization suite for the California housing dataset. For each numeric variable, create:\n> 1. A histogram showing the distribution\n> 2. A box plot showing the distribution by ocean_proximity\n> 3. A scatter plot against median_house_value (the target variable)\n>\n> Organize these into subplots so each variable gets a row with 3 plots. Save the output as a single multi-page PDF.\"\n\nThe AI will generate code with subplots, proper layout management, and file handling—saving you hours of work.\n\n::: {.callout-note}\nNotice how, in all of these cases, you told the AI precisely what to do. In addition, you used the things you have learned this chapter to prompt it precisely. We **do not** want to give AI overly vague instructions like \"investigate my data\" or \"find patterns\". It's our job as the data scientist to _guide_ the AI to help us, not to turn over the entire thought process to it.\n:::\n\n### 6.2 Testing Across Data Subsets {#ch1-6-2}\n\nImagine you want to analyze how the relationship between income and house values differs across different regions. By hand, you'd need to:\n\n1. Filter for each ocean proximity category\n2. Compute correlations for each\n3. Create scatter plots for each\n4. Compare the results\n\nThat's tedious. Instead, prompt the AI:\n\n> \"For the `housing_df` DataFrame, I want to analyze how the relationship between 'median_income' and 'median_house_value' varies by 'ocean_proximity'.\n>\n> For each ocean proximity category:\n>\n> 1. Calculate the correlation coefficient\n> 2. Create a scatter plot with a regression line\n> 3. Print summary statistics\n>\n> Store the summary statistics and correlation coefficients in a dataframe. Save the scatter plots with a filename that includes the ocean proximity category.\"\n\n::: {.callout-note}\nOnce again, we give the AI precise instructions. Think of it as an assistant who is eager to help, but needs careful direction at every step.\n:::\n\nThe AI will generate code that loops through each category, performs the analysis, and presents it clearly. You then review the results with your understanding of the data, catching any issues.\n\n**Systematic subset analysis**\n\nYou can scale this to any grouping:\n\n```python\n# AI-generated code for systematic analysis\nfor proximity in housing_df['ocean_proximity'].unique():\n    subset = housing_df[housing_df['ocean_proximity'] == proximity]\n\n    print(f\"\\n=== Analysis for {proximity} ===\")\n    print(f\"Number of districts: {len(subset)}\")\n    print(f\"Median house value: ${subset['median_house_value'].median():,.0f}\")\n    print(f\"Median income: ${subset['median_income'].median():.2f}\")\n\n    # Correlation\n    corr = subset['median_income'].corr(subset['median_house_value'])\n    print(f\"Income-Value correlation: {corr:.3f}\")\n\n    # Visualization\n    plt.figure(figsize=(8, 5))\n    sns.regplot(data=subset, x='median_income', y='median_house_value',\n                scatter_kws={'alpha':0.5})\n    plt.title(f'Income vs Value: {proximity}')\n    plt.show()\n```\n\nThe point isn't that you couldn't write this code—you absolutely can now! The point is that AI lets you analyze 10 subsets as easily as 1 subset. You're scaling your analysis, not replacing your understanding.\n\n::: {.callout-warning}\nAI is excellent at generating repetitive analysis code, but it cannot make judgment calls about your data. You need to understand what the analyses mean, whether the results make sense, and what actions to take. AI scales your work; it doesn't replace your thinking.\n:::\n\nThe key pattern: **Do it by hand first, scale with AI second**. Learn to check one column carefully, then use AI to check all 50 columns. Learn to analyze one subset, then scale to all subsets. Your understanding guides the AI; the AI amplifies your productivity.\n\n### Learning outcomes: {#ch1-6-outcomes}\n**_By hand_ you should be able to:**\n\n- Write effective prompts for doing extensive EDA\n\n---\n\n## 7. The Iterative Nature of EDA {#ch1-7}\n\n### 7.1 EDA as a Process, Not a Checklist {#ch1-7-1}\n\nHere's a mistake beginners make: they treat EDA like a todo list. Load data ✓, check missing values ✓, make histogram ✓, done!\n\nReal EDA doesn't work like that. EDA is a conversation with your data. You ask a question, look at the answer, and that answer suggests new questions. You follow the thread wherever it leads.\n\nLet's see this in action with our housing data:\n\n**Starting question:** \"What affects house values?\"\n\n::: {#436f984a .cell execution_count=41}\n``` {.python .cell-code}\n# Initial exploration: which variables correlate with house value?\nnumeric_cols = ['housing_median_age', 'total_rooms', 'total_bedrooms',\n                'population', 'households', 'median_income']\n\nfor col in numeric_cols:\n    corr = housing_df[col].corr(housing_df['median_house_value'])\n    print(f\"{col:20s}: {corr:6.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhousing_median_age  :  0.106\ntotal_rooms         :  0.134\ntotal_bedrooms      :  0.050\npopulation          : -0.025\nhouseholds          :  0.066\nmedian_income       :  0.688\n```\n:::\n:::\n\n\n**Insight:** Median income has the strongest correlation (0.688). That makes sense—wealthier areas have more expensive houses.\n\n**New question:** \"Does this relationship hold across all regions?\"\n\n::: {#e1f4f642 .cell execution_count=42}\n``` {.python .cell-code}\n# Check if income-value relationship varies by ocean proximity\nfor proximity in housing_df['ocean_proximity'].unique():\n    subset = housing_df[housing_df['ocean_proximity'] == proximity]\n    corr = subset['median_income'].corr(subset['median_house_value'])\n    print(f\"{proximity:15s}: {corr:.3f} ({len(subset):5d} districts)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNEAR BAY       : 0.633 ( 2290 districts)\n<1H OCEAN      : 0.679 ( 9136 districts)\nINLAND         : 0.699 ( 6551 districts)\nNEAR OCEAN     : 0.704 ( 2658 districts)\nISLAND         : -0.540 (    5 districts)\n```\n:::\n:::\n\n\n**Insight:** The correlation is weakest for islands (only 5 districts though—small sample!). Strongest for inland areas.\n\n**New question:** \"Why is the island correlation so different? Let's look at those islands.\"\n\n::: {#56203ecf .cell execution_count=43}\n``` {.python .cell-code}\n# Investigate the island districts\nislands = housing_df[housing_df['ocean_proximity'] == 'ISLAND']\nprint(islands[['median_income', 'median_house_value', 'housing_median_age']].to_string())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      median_income  median_house_value  housing_median_age\n8314         2.1579            450000.0                27.0\n8315         2.8333            414700.0                52.0\n8316         3.3906            300000.0                52.0\n8317         2.7361            450000.0                52.0\n8318         2.6042            287500.0                29.0\n```\n:::\n:::\n\n\n**Insight:** Only 5 island districts total, with a narrow range of incomes. The low correlation might just be due to small sample size and limited variation.\n\nSee what happened? One question led to another, which led to another. We started with \"what affects house values?\" and ended up investigating the peculiarities of island properties. This is normal. This is good.\n\n**EDA is not linear.** You don't march through a predetermined set of steps. You explore, find something interesting, investigate it, find something else interesting, investigate that. Some paths lead nowhere. Some paths lead to important discoveries. You won't know until you follow them.\n\n::: {.callout-note}\nThe best data scientists are curious. When they see something unusual in the data, they don't just note it and move on—they dig deeper. Why is this unusual? What does it mean? What else should I check?\n:::\n\n### 7.2 Documenting Your Findings {#ch1-7-2}\n\nHere's a problem: you do all this exploration, find interesting patterns, make decisions about the data, and then... a week later you can't remember what you found or why you made those decisions.\n\nDocumentation isn't just for other people. It's for future you.\n\n**What to document:**\n\n1. **Interesting patterns you discovered**\n   - \"House values show clear geographic clustering—coastal areas are consistently more expensive\"\n   - \"Total_bedrooms has 207 missing values (1%), appears random, not systematically related to other variables\"\n\n2. **Decisions you made and why**\n   - \"Filled missing bedrooms with median rather than dropping rows—only 1% missing, no pattern suggesting systematic missingness\"\n   - \"Kept all outliers in house values—verified these are real (e.g., expensive areas like San Francisco), not data errors\"\n\n3. **Questions that need follow-up**\n   - \"Island category has only 5 districts—may need to group with NEAR_OCEAN for modeling\"\n   - \"Longitude/latitude show clear clustering—should we create region categories?\"\n\n4. **Hypotheses for modeling**\n   - \"Median_income will likely be the strongest predictor\"\n   - \"May need to include interactions between location and other features\"\n\n**How to document:**\n\nThe simplest approach is to use markdown cells in your Jupyter notebook or Quarto document:\n\n```python\n# Bad: No context\nhousing_df = housing_df.dropna()\n\n# Better: Explain your reasoning\n# Dropping 207 rows with missing total_bedrooms (1% of data)\n# Missing values appear random (no correlation with other variables)\n# Median imputation would be valid alternative, but with 20k+ rows,\n# dropping 1% has minimal impact\nhousing_df = housing_df.dropna()\n```\n\nEven better, use markdown cells to write full explanations:\n\n```markdown\n## Data Cleaning Decisions\n\n### Missing Values\n- `total_bedrooms`: 207 missing (1%)\n  - Dropped these rows rather than imputing\n  - Rationale: Small percentage, appears random, large dataset can afford the loss\n  - Alternative considered: median imputation (would change results minimally)\n\n### Outliers\n- Kept all outliers in `median_house_value`\n  - Verified these are real (checked against known expensive areas)\n  - Removing would bias model toward typical houses\n  - Will monitor model performance on these points\n```\n\n**Why this matters:**\n\nThree months from now, someone (maybe you!) will ask \"why did we drop those missing values instead of imputing them?\" If you documented your reasoning, you can answer immediately. If you didn't, you'll have to re-do the analysis to remember.\n\n::: {.callout-tip}\nDocument as you go, not at the end. When you make a decision, write down why while it's fresh in your mind. Your future self will thank you.\n:::\n\n### Learning outcomes: {#ch1-7-outcomes}\n**_By hand_ you should be able to:**\n\n- Describe how EDA is a process, as opposed to a checklist\n- Give examples of how to go about EDA\n- Give examples of things that should be done by hand, and things that should be done with the help of an AI\n- Write `assert` statements that check if AI generated code is properly performing EDA\n\n---\n\n## Summary {#ch1-summary}\n\nYou've now learned the foundations of exploratory data analysis. Let's recap the key points:\n\n**The Fundamentals:**\n\n- **Data manipulation** with Pandas: loading, selecting, filtering, and cleaning data\n- **Statistical summaries**: understanding distributions, detecting outliers, and computing meaningful statistics\n- **Visualization**: choosing the right plots, creating them with Seaborn, and following best practices for readability\n- **Testing**: using assert statements to catch errors early and verify your assumptions\n\n**Working with AI:**\n\n- Write specific, detailed prompts that include context and requirements\n- Start simple and iterate—don't try to get everything perfect in one prompt\n- Use AI to scale your work after you understand the fundamentals by hand\n- Always verify AI-generated code and results with your own understanding\n\n**The EDA Mindset:**\n\n- EDA is iterative, not linear—follow insights wherever they lead\n- Document your findings and decisions as you go\n- Let EDA guide your modeling choices—every decision should be informed by what you learned about the data\n- Stay curious and ask \"why?\" when you see something interesting\n\n**The Core Philosophy:**\n\nEDA is foundational to everything else in data science. You can't build good models without understanding your data. AI is a powerful tool for scaling your work, but it's not a replacement for understanding. Learn to do things by hand first, then use AI to do them at scale.\n\nUse your brain. That's what it's there for.\n\n---\n\n## Practice Exercises {#ch1-practice}\n\nComplete these exercises using the California housing dataset to reinforce what you've learned:\n\n**1. Prompt Engineering Practice**\n\nWrite three prompts for an AI coding assistant to perform the following tasks with the housing dataset. Make your prompts specific and detailed:\n\na. Create a comprehensive correlation analysis between all numeric features and median_house_value, sorted by correlation strength, with a visualization\n\nb. Analyze how housing age affects house values across different ocean proximity categories, including statistical tests and visualizations\n\nc. Generate a data quality report that identifies potential issues (outliers, missing values, suspicious patterns) and suggests remediation strategies\n\n**2. Visualization Design**\n\nFor each scenario below, choose the appropriate visualization type and explain why:\n\na. You want to show how the distribution of house ages differs between coastal and inland areas\n\nb. You want to investigate whether there's a relationship between population density (population/households) and median income\n\nc. You want to compare the median house values across all five ocean proximity categories\n\nd. You want to show how house values have changed across different housing ages, looking for trends\n\n**3. Data Quality Investigation**\n\nUsing assert statements and pandas methods:\n\na. Write code to verify that after cleaning, no column has more than 5% missing values\n\nb. Create assertions to check that all house values are positive and less than $10 million\n\nc. Write a test to ensure that total_bedrooms is always less than or equal to total_rooms\n\nd. Verify that each ocean_proximity category has at least 100 samples\n\n**4. Complete EDA Workflow**\n\nPerform a complete exploratory data analysis on a subset of the housing data:\n\na. Filter for districts with median_income between 3 and 6\n\nb. Document three interesting patterns you discover in this subset\n\nc. Create at least three visualizations that reveal insights\n\nd. Write down two hypotheses about what would make a good predictive model for this subset\n\ne. Use assert statements to verify your filtering worked correctly\n\n**5. Scaling with AI**\n\nWithout actually running the code, write detailed prompts that would generate:\n\na. Code to create scatter plots of median_house_value vs. each numeric feature, all on a single figure with subplots\n\nb. A function that calculates and reports summary statistics (mean, median, std, min, max) for any numeric column, grouped by any categorical column\n\nc. Code that identifies all pairs of numeric features with correlation > 0.7 and creates scatter plots for each pair\n\n---\n\n## Additional Resources {#ch1-additional-resources}\n\n**Pandas Documentation:**\n\n- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html) - Comprehensive guide to Pandas functionality\n- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) - Quick-start tutorial\n- [Pandas Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html) - Common recipes for data manipulation\n\n**Visualization:**\n\n- [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html) - Examples of every Seaborn plot type\n- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html) - Deep dive into matplotlib\n- [From Data to Viz](https://www.data-to-viz.com/) - Guide to choosing the right visualization\n\n**Data Quality and Cleaning:**\n\n- [Data Cleaning with Python](https://realpython.com/python-data-cleaning-numpy-pandas/) - Comprehensive guide\n- [Pandas Missing Data](https://pandas.pydata.org/docs/user_guide/missing_data.html) - Official guide to handling missing values\n\n**Working with AI:**\n\n- [Effective Prompting Guide](https://www.promptingguide.ai/) - Principles for writing good prompts\n- [Gemini for Developers](https://ai.google.dev/) - Google's AI documentation for developers\n\n**Statistics and EDA:**\n\n- [Think Stats (Free Book)](https://greenteapress.com/thinkstats2/html/index.html) - Statistical thinking for programmers\n- [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers) - Practical statistical approaches\n\n**Practice Datasets:**\n\n- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) - Hundreds of datasets for practice\n- [Kaggle Datasets](https://www.kaggle.com/datasets) - Real-world datasets with community analyses\n- [Data.gov](https://data.gov/) - US government public datasets\n\n",
    "supporting": [
      "chapter-1-eda_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}