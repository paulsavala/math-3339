{
  "hash": "bdde10f4b79494f444f8583eaf9281f0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Chapter 7: Neural Networks and Deep Learning\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Module Resources\n\n**Related Assignments:**\n\n- [Module 7 Homework](../../Assignments/Module%207%20-%20Neural%20Networks/module-7-homework.qmd)\n- [Module 7 Quiz](../../Assignments/Module%207%20-%20Neural%20Networks/module-7-quiz.qmd)\n\n---\n\n## Introduction\n\nYou've learned linear regression, logistic regression, decision trees, random forests, SVMs, and k-nearest neighbors. Each of these models has strengths and weaknesses. Each makes specific assumptions about your data. And each has a ceiling—a limit to the complexity of patterns it can learn.\n\nNeural networks shatter that ceiling.\n\nThink about it this way: linear regression draws a straight line through your data. Polynomial regression adds some curves. Decision trees create rectangular decision boundaries. Random forests combine many rectangles. But what if your data has patterns that can't be captured by lines, curves, or rectangles? What if the relationship between your features and target is deeply complex, involving intricate interactions between dozens or hundreds of variables?\n\nThat's where neural networks come in. They're universal function approximators—given enough data and the right architecture, they can learn virtually any pattern. This is why neural networks power image recognition, natural language processing, speech recognition, game playing, and countless other applications that seemed impossible just a decade ago.\n\nBut here's the thing: with great power comes great complexity. Neural networks have more hyperparameters, require more data, take longer to train, and are harder to interpret than the models you've worked with so far. You can't just throw a neural network at every problem and expect it to work. You need to understand when to use them, how to build them, and most importantly—how to diagnose and fix the inevitable problems that arise during training.\n\nThis chapter will teach you not just how to build neural networks in PyTorch, but how to think about them. You'll learn what makes them different from traditional machine learning models, when they're worth the added complexity, and how to recognize and fix common training problems. By the end, you'll understand why neural networks are revolutionary—and why sometimes a simple logistic regression is still the better choice.\n\nLet's jump in.\n\n---\n\n## 1. What Are Neural Networks? The Building Blocks\n\n### 1.1 The Perceptron: Where It All Begins\n\nHere's a secret: you already understand the building block of neural networks. Remember linear regression? A perceptron is basically the same thing.\n\nA perceptron takes multiple inputs, multiplies each by a weight, sums everything up, adds a bias term, and passes the result through an activation function. Sound familiar? It should—this is exactly what logistic regression does.\n\n::: {#ad7a2faf .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# A simple perceptron in action\ndef perceptron(inputs, weights, bias):\n    \"\"\"\n    A basic perceptron: weighted sum + bias + activation\n    \"\"\"\n    # Weighted sum\n    z = np.dot(inputs, weights) + bias\n\n    # Activation function (sigmoid for this example)\n    activation = 1 / (1 + np.exp(-z))\n\n    return activation\n\n# Example with 3 inputs\ninputs = np.array([0.5, -0.3, 0.8])\nweights = np.array([0.4, 0.6, -0.2])\nbias = 0.1\n\noutput = perceptron(inputs, weights, bias)\nprint(f\"Inputs: {inputs}\")\nprint(f\"Weights: {weights}\")\nprint(f\"Bias: {bias}\")\nprint(f\"Output: {output:.4f}\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInputs: [ 0.5 -0.3  0.8]\nWeights: [ 0.4  0.6 -0.2]\nBias: 0.1\nOutput: 0.4900\n```\n:::\n:::\n\n\nThe perceptron computes: output = activation(w₁x₁ + w₂x₂ + w₃x₃ + b). This is exactly the same structure as logistic regression. So what makes neural networks different?\n\n### 1.2 Stacking Layers: The Magic of Depth\n\nThe magic happens when you stack perceptrons into layers. Take the outputs from one layer and feed them as inputs to the next layer. Do this multiple times. Suddenly, you have a neural network.\n\nHere's why this matters: each layer learns increasingly complex representations of your data. The first layer might learn simple patterns (edges in images, common words in text). The second layer combines those patterns into more complex features (shapes from edges, phrases from words). Deeper layers learn even more abstract representations (objects from shapes, concepts from phrases).\n\n::: {#9fc6abbf .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\n# A simple multi-layer perceptron (MLP)\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNN, self).__init__()\n\n        # First layer: input to hidden\n        self.layer1 = nn.Linear(input_size, hidden_size)\n\n        # Second layer: hidden to output\n        self.layer2 = nn.Linear(hidden_size, output_size)\n\n        # Activation function\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Pass through first layer\n        x = self.layer1(x)\n\n        # Apply activation\n        x = self.relu(x)\n\n        # Pass through second layer\n        x = self.layer2(x)\n\n        return x\n\n# Create a network: 10 inputs -> 5 hidden neurons -> 1 output\nmodel = SimpleNN(input_size=10, hidden_size=5, output_size=1)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimpleNN(\n  (layer1): Linear(in_features=10, out_features=5, bias=True)\n  (layer2): Linear(in_features=5, out_features=1, bias=True)\n  (relu): ReLU()\n)\n```\n:::\n:::\n\n\nThis network has two layers of weights (called \"layers\") and one hidden layer. Don't let the terminology confuse you: \"hidden layer\" just means it's between the input and output. The network takes 10 input features, transforms them into 5 hidden representations, then produces 1 output.\n\n### 1.3 What's Actually Happening Inside?\n\nLet's trace what happens when data flows through this network:\n\n1. **Input layer:** You feed in your features (e.g., 10 numbers representing house characteristics)\n2. **First transformation:** The network computes 5 different weighted sums of those 10 inputs—each hidden neuron sees all inputs but weights them differently\n3. **Activation:** Apply ReLU to each of those 5 values (we'll explain ReLU soon)\n4. **Second transformation:** Combine those 5 hidden values into a final output\n5. **Output:** A single prediction\n\nHere's the key insight: by learning the right weights, the network can create useful intermediate representations in that hidden layer. Those 5 hidden neurons might learn to represent things like \"overall house quality,\" \"neighborhood desirability,\" \"age-related factors,\" etc.—concepts that help predict house price but aren't explicitly in your data.\n\n::: {#6d2baa12 .cell execution_count=3}\n``` {.python .cell-code}\n# Let's see it in action with random data\nrandom_house_features = torch.randn(1, 10)  # 1 house, 10 features\nprediction = model(random_house_features)\n\nprint(f\"Input shape: {random_house_features.shape}\")\nprint(f\"Output shape: {prediction.shape}\")\nprint(f\"Predicted value: {prediction.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInput shape: torch.Size([1, 10])\nOutput shape: torch.Size([1, 1])\nPredicted value: 0.1822\n```\n:::\n:::\n\n\nThe network takes in our features and produces a prediction. Right now it's random nonsense because we haven't trained it—but the structure is there.\n\n![Placeholder: Multi-layer perceptron architecture diagram](images/mlp-architecture.png)\n\n---\n\n## 2. Activation Functions: Adding Non-Linearity\n\n### 2.1 Why We Need Activation Functions\n\nHere's a problem: if you stack linear transformations without activation functions, you just get another linear transformation. No matter how many layers you add, the network can only learn linear patterns. It's like stacking multiple straight rulers end-to-end—you still just get a straight line.\n\nActivation functions add **non-linearity**. They allow the network to learn curves, corners, complex decision boundaries—patterns that can't be captured by straight lines.\n\n### 2.2 Common Activation Functions\n\nLet's look at the most important activation functions you'll encounter:\n\n::: {#1cb8e4f5 .cell execution_count=4}\n``` {.python .cell-code}\n# Visualize common activation functions\nx = np.linspace(-5, 5, 100)\n\n# ReLU: Rectified Linear Unit\nrelu = np.maximum(0, x)\n\n# Sigmoid: Squashes to (0, 1)\nsigmoid = 1 / (1 + np.exp(-x))\n\n# Tanh: Squashes to (-1, 1)\ntanh = np.tanh(x)\n\n# Plot them\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].plot(x, relu, linewidth=2, color='#e74c3c')\naxes[0].set_title('ReLU', fontsize=14)\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].axvline(x=0, color='black', linewidth=0.5)\n\naxes[1].plot(x, sigmoid, linewidth=2, color='#3498db')\naxes[1].set_title('Sigmoid', fontsize=14)\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(y=0, color='black', linewidth=0.5)\naxes[1].axvline(x=0, color='black', linewidth=0.5)\n\naxes[2].plot(x, tanh, linewidth=2, color='#2ecc71')\naxes[2].set_title('Tanh', fontsize=14)\naxes[2].grid(True, alpha=0.3)\naxes[2].axhline(y=0, color='black', linewidth=0.5)\naxes[2].axvline(x=0, color='black', linewidth=0.5)\n\nfor ax in axes:\n    ax.set_xlabel('Input', fontsize=12)\n    ax.set_ylabel('Output', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-7-neural-networks_files/figure-html/cell-5-output-1.png){width=1431 height=374}\n:::\n:::\n\n\n**ReLU (Rectified Linear Unit):** The most commonly used activation function. It's dead simple: `f(x) = max(0, x)`. If the input is positive, pass it through. If negative, output zero. Why is this so popular? It's computationally cheap, doesn't suffer from vanishing gradients, and works remarkably well in practice.\n\n**Sigmoid:** Squashes any input to a value between 0 and 1. This is what logistic regression uses. It's still useful for the output layer when you need probabilities, but rarely used in hidden layers anymore because gradients vanish when inputs are far from zero.\n\n**Tanh:** Like sigmoid but squashes to (-1, 1) instead. Centers the data around zero, which can help with training. Still suffers from vanishing gradients.\n\n### 2.3 Choosing the Right Activation\n\nHere's a practical guide:\n\n- **Hidden layers:** Use ReLU. It's simple, fast, and effective. Start here.\n- **Binary classification output:** Use Sigmoid to get probabilities between 0 and 1\n- **Multi-class classification output:** Use Softmax (we'll see this later)\n- **Regression output:** No activation (or use linear activation)—you want the network to output any real number\n\n::: {#fcb24f0f .cell execution_count=5}\n``` {.python .cell-code}\n# Building a network with proper activations\nclass BetterNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(BetterNN, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()  # ReLU for hidden layer\n        self.layer2 = nn.Linear(hidden_size, output_size)\n        # No activation on output for regression\n        # Would add Sigmoid for binary classification\n        # Would add Softmax for multi-class classification\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)  # Apply ReLU after first layer\n        x = self.layer2(x)  # No activation after output layer\n        return x\n\nmodel = BetterNN(input_size=10, hidden_size=20, output_size=1)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBetterNN(\n  (layer1): Linear(in_features=10, out_features=20, bias=True)\n  (relu): ReLU()\n  (layer2): Linear(in_features=20, out_features=1, bias=True)\n)\n```\n:::\n:::\n\n\nSee how we only apply ReLU after the hidden layer, not the output? That's because we're doing regression—we want any real number as output. For binary classification, we'd add `torch.sigmoid()` to the output.\n\n---\n\n## 3. The Training Process: Forward and Backward\n\n### 3.1 Forward Pass: Making Predictions\n\nThe forward pass is straightforward: data flows from input through each layer to produce a prediction. You've already seen this in the `forward()` method. Let's trace it explicitly:\n\n::: {#f452fde1 .cell execution_count=6}\n``` {.python .cell-code}\n# Create some fake data\nX = torch.randn(5, 10)  # 5 samples, 10 features each\ny = torch.randn(5, 1)   # 5 target values\n\nmodel = BetterNN(input_size=10, hidden_size=20, output_size=1)\n\n# Forward pass: get predictions\npredictions = model(X)\n\nprint(f\"Input shape: {X.shape}\")\nprint(f\"Predictions shape: {predictions.shape}\")\nprint(f\"Predictions:\\n{predictions.detach().numpy()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInput shape: torch.Size([5, 10])\nPredictions shape: torch.Size([5, 1])\nPredictions:\n[[-0.17862587]\n [-0.27817434]\n [-0.23991197]\n [ 0.01879379]\n [ 0.12353444]]\n```\n:::\n:::\n\n\nThe model takes 5 examples with 10 features each and produces 5 predictions. Right now they're garbage because the weights are random—but the machinery works.\n\n### 3.2 Loss Calculation: Measuring Error\n\nJust like in Module 3, we need a loss function to measure how wrong our predictions are. For regression, we typically use Mean Squared Error (MSE):\n\n::: {#af0e71e6 .cell execution_count=7}\n``` {.python .cell-code}\n# Calculate MSE loss\ncriterion = nn.MSELoss()\nloss = criterion(predictions, y)\n\nprint(f\"Loss (MSE): {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss (MSE): 0.2008\n```\n:::\n:::\n\n\nThe loss is a single number telling us how bad our predictions are on average. During training, we want to minimize this number.\n\n### 3.3 Backward Pass: Computing Gradients\n\nHere's where neural networks get interesting. Remember gradient descent from Module 3? We need the gradient of the loss with respect to every weight in the network. For a network with thousands or millions of weights, computing these gradients by hand would be impossible.\n\nThis is where **backpropagation** comes in. The beautiful thing is: you don't need to understand the details. PyTorch handles it automatically. But conceptually, here's what's happening:\n\n1. Start at the loss (the final output)\n2. Use the chain rule to compute how much each weight contributed to that loss\n3. \"Propagate\" this information backward through the network\n4. Store the gradient for each weight\n\nIn PyTorch, this is one line:\n\n::: {#e7289819 .cell execution_count=8}\n``` {.python .cell-code}\n# Backward pass: compute gradients\nloss.backward()\n\n# Now every parameter has a gradient stored\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name}: gradient shape = {param.grad.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlayer1.weight: gradient shape = torch.Size([20, 10])\nlayer1.bias: gradient shape = torch.Size([20])\nlayer2.weight: gradient shape = torch.Size([1, 20])\nlayer2.bias: gradient shape = torch.Size([1])\n```\n:::\n:::\n\n\nSee? PyTorch computed gradients for every weight in the network. The `layer1.weight` has shape (20, 10) because there are 20 hidden neurons, each connected to 10 inputs. The gradients have the same shape—one gradient value per weight.\n\n### 3.4 Optimization: Updating Weights\n\nNow we use those gradients to update the weights. Remember gradient descent? Same idea:\n\n```\nnew_weight = old_weight - learning_rate * gradient\n```\n\nPyTorch optimizers handle this for us:\n\n::: {#886d052f .cell execution_count=9}\n``` {.python .cell-code}\n# Create an optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Zero out old gradients (important!)\noptimizer.zero_grad()\n\n# Forward pass\npredictions = model(X)\nloss = criterion(predictions, y)\n\n# Backward pass\nloss.backward()\n\n# Update weights\noptimizer.step()\n\nprint(f\"Loss after one update: {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss after one update: 0.2008\n```\n:::\n:::\n\n\nThat's one complete training step: forward pass → compute loss → backward pass → update weights.\n\n### 3.5 The Training Loop Pattern\n\nIn practice, we repeat this process many times over the entire dataset. Here's the standard pattern:\n\n::: {#263b5f9e .cell execution_count=10}\n``` {.python .cell-code}\n# Training loop pattern (won't actually train anything useful with random data)\nnum_epochs = 3\n\nfor epoch in range(num_epochs):\n    # Forward pass\n    predictions = model(X)\n    loss = criterion(predictions, y)\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear old gradients\n    loss.backward()        # Compute new gradients\n    optimizer.step()       # Update weights\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/3, Loss: 0.1926\nEpoch 2/3, Loss: 0.1849\nEpoch 3/3, Loss: 0.1775\n```\n:::\n:::\n\n\nSee the pattern? This is the foundation of training every neural network. The specifics might change (different optimizers, different loss functions, validation loops, etc.) but this basic structure remains the same.\n\n---\n\n## 4. PyTorch Fundamentals: Building Your First Real Network\n\n### 4.1 Tensors: The Fundamental Data Structure\n\nPyTorch uses **tensors** instead of NumPy arrays. Tensors are like arrays but can run on GPUs and support automatic differentiation (the backward pass we just saw).\n\n::: {#934606b5 .cell execution_count=11}\n``` {.python .cell-code}\n# Creating tensors\na = torch.tensor([1, 2, 3])\nb = torch.randn(3, 4)  # 3x4 random tensor\nc = torch.zeros(2, 2)  # 2x2 tensor of zeros\n\nprint(f\"a: {a}\")\nprint(f\"b shape: {b.shape}\")\nprint(f\"c:\\n{c}\")\n\n# Tensors work like NumPy arrays\nd = a * 2\ne = b + 1\nprint(f\"a * 2: {d}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na: tensor([1, 2, 3])\nb shape: torch.Size([3, 4])\nc:\ntensor([[0., 0.],\n        [0., 0.]])\na * 2: tensor([2, 4, 6])\n```\n:::\n:::\n\n\nFor most purposes, tensors behave exactly like NumPy arrays. The main difference is that tensors track gradients automatically when you tell them to.\n\n### 4.2 Loading Real Data: California Housing\n\nLet's work with the California housing dataset we've used in previous modules. We'll build a neural network to predict house prices—a regression problem.\n\n::: {#c8fb0f7d .cell execution_count=12}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\nhousing = pd.read_csv('../data/housing.csv')\n\n# Convert ocean_proximity categorical variable to dummy variables\nhousing = pd.get_dummies(housing, columns=['ocean_proximity'], drop_first=True)\n\n# Prepare features and target\nX = housing.drop('median_house_value', axis=1)\ny = housing['median_house_value'].values\n\nprint(f\"Features shape after encoding: {X.shape}\")\nprint(f\"Feature names: {list(X.columns)}\")\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# IMPORTANT: Scale features for neural networks\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Training set: {X_train_scaled.shape}\")\nprint(f\"Test set: {X_test_scaled.shape}\")\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train_scaled)\ny_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\nX_test_tensor = torch.FloatTensor(X_test_scaled)\ny_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n\nprint(f\"Tensor shapes: X_train={X_train_tensor.shape}, y_train={y_train_tensor.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFeatures shape after encoding: (20640, 12)\nFeature names: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity_INLAND', 'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY', 'ocean_proximity_NEAR OCEAN']\nTraining set: (16512, 12)\nTest set: (4128, 12)\nTensor shapes: X_train=torch.Size([16512, 12]), y_train=torch.Size([16512, 1])\n```\n:::\n:::\n\n\nTwo important preprocessing steps here. First, we convert `ocean_proximity` from categorical text values (like \"NEAR BAY\", \"INLAND\") to dummy variables using `pd.get_dummies()`. Neural networks need numeric inputs, so we convert each category into a binary (0/1) column. Using `drop_first=True` prevents multicollinearity by dropping one category as a reference.\n\nSecond, notice the scaling step—this is crucial for neural networks. Features with different scales can cause training problems. Neural networks work best when input features are normalized to similar ranges.\n\n### 4.3 Creating DataLoaders for Efficient Training\n\nFor large datasets, we don't pass the entire dataset through the network at once. Instead, we use mini-batches. PyTorch's `DataLoader` handles this automatically:\n\n::: {#2172dbe7 .cell execution_count=13}\n``` {.python .cell-code}\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Create dataset\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# Create DataLoader with batch size of 64\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True  # Shuffle data each epoch\n)\n\n# See how it works\nfor batch_X, batch_y in train_loader:\n    print(f\"Batch X shape: {batch_X.shape}\")\n    print(f\"Batch y shape: {batch_y.shape}\")\n    break  # Just show one batch\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBatch X shape: torch.Size([64, 12])\nBatch y shape: torch.Size([64, 1])\n```\n:::\n:::\n\n\nThe DataLoader automatically splits our data into batches of 64 examples each. During training, we'll process one batch at a time, computing gradients and updating weights after each batch.\n\n### 4.4 Building a Housing Price Predictor\n\nLet's build a proper neural network for this task:\n\n::: {#4e626852 .cell execution_count=14}\n``` {.python .cell-code}\nclass HousingNN(nn.Module):\n    def __init__(self, input_size):\n        super(HousingNN, self).__init__()\n\n        # Network architecture\n        self.layer1 = nn.Linear(input_size, 64)  # Input to 64 hidden neurons\n        self.layer2 = nn.Linear(64, 32)          # 64 to 32 hidden neurons\n        self.layer3 = nn.Linear(32, 1)           # 32 to 1 output\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Forward pass through three layers\n        x = self.layer1(x)\n        x = self.relu(x)\n\n        x = self.layer2(x)\n        x = self.relu(x)\n\n        x = self.layer3(x)\n        # No activation on output for regression\n\n        return x\n\n# Create the model\ninput_size = X_train_scaled.shape[1]  # Number of features\nmodel = HousingNN(input_size)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model architecture:\")\nprint(model)\nprint(f\"\\nTotal parameters: {total_params:,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel architecture:\nHousingNN(\n  (layer1): Linear(in_features=12, out_features=64, bias=True)\n  (layer2): Linear(in_features=64, out_features=32, bias=True)\n  (layer3): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n)\n\nTotal parameters: 2,945\n```\n:::\n:::\n\n\nThis network has three layers with 64, 32, and 1 neurons respectively. Why these numbers? There's no perfect formula—architecture design is part science, part art, part experimentation. These are reasonable starting points for a dataset this size.\n\n### 4.5 Training the Network\n\nNow let's actually train it:\n\n::: {#ffbd0dbe .cell execution_count=15}\n``` {.python .cell-code}\n# Setup\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer, not SGD\n\n# Training loop\nnum_epochs = 50\ntrain_losses = []\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    epoch_loss = 0\n\n    for batch_X, batch_y in train_loader:\n        # Forward pass\n        predictions = model(batch_X)\n        loss = criterion(predictions, batch_y)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    # Average loss for this epoch\n    avg_loss = epoch_loss / len(train_loader)\n    train_losses.append(avg_loss)\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.2f}\")\n\n# Plot training loss\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Training Loss (MSE)', fontsize=12)\nplt.title('Training Loss Over Time', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 10/50, Loss: 13785384616.68\nEpoch 20/50, Loss: 7671714087.69\nEpoch 30/50, Loss: 5662778961.36\nEpoch 40/50, Loss: 4936042822.45\nEpoch 50/50, Loss: 4676608069.46\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-7-neural-networks_files/figure-html/cell-16-output-2.png){width=802 height=528}\n:::\n:::\n\n\nLook at that loss curve—it starts high and drops quickly, then gradually levels off. This is what healthy training looks like. The model is learning!\n\n### 4.6 Evaluating the Model\n\nLet's see how well it predicts on the test set:\n\n::: {#a7e4181f .cell execution_count=16}\n``` {.python .cell-code}\n# Evaluation mode\nmodel.eval()\n\nwith torch.no_grad():  # Don't compute gradients during evaluation\n    test_predictions = model(X_test_tensor)\n    test_loss = criterion(test_predictions, y_test_tensor)\n\nprint(f\"Test Loss (MSE): {test_loss.item():.2f}\")\n\n# Look at some predictions\ncomparison = pd.DataFrame({\n    'Actual': y_test_tensor.squeeze().numpy()[:10],\n    'Predicted': test_predictions.squeeze().numpy()[:10]\n})\ncomparison['Error'] = comparison['Actual'] - comparison['Predicted']\ncomparison.head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Loss (MSE): nan\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Actual</th>\n      <th>Predicted</th>\n      <th>Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>47700.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>45800.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500001.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>218600.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>278000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>158700.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>198200.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>157500.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>340000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>446600.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe predictions aren't perfect, but they're in the right ballpark. This is a working neural network!\n\n---\n\n## 5. Loss Functions for Neural Networks\n\n### 5.1 Connecting to What You Know\n\nRemember loss functions from Module 3? Neural networks use the exact same ones. The only difference is we're computing them on neural network predictions instead of linear model predictions.\n\nFor **regression problems** (predicting continuous values):\n- **MSE (Mean Squared Error):** Penalizes large errors heavily\n- **MAE (Mean Absolute Error):** Treats all errors equally\n- **Huber Loss:** Compromise between MSE and MAE\n\nFor **classification problems** (predicting categories):\n- **Binary Cross-Entropy:** For two-class problems (spam/not spam)\n- **Cross-Entropy:** For multi-class problems (digit recognition)\n\n### 5.2 Regression Loss Functions\n\nWe just used MSE. Let's compare it to MAE:\n\n::: {#a164b66c .cell execution_count=17}\n``` {.python .cell-code}\n# MSE vs MAE\ncriterion_mse = nn.MSELoss()\ncriterion_mae = nn.L1Loss()  # L1 loss is MAE\n\n# Make some predictions\nwith torch.no_grad():\n    predictions = model(X_test_tensor[:100])  # First 100 test examples\n    actual = y_test_tensor[:100]\n\n    mse_loss = criterion_mse(predictions, actual)\n    mae_loss = criterion_mae(predictions, actual)\n\nprint(f\"MSE Loss: {mse_loss.item():.2f}\")\nprint(f\"MAE Loss: {mae_loss.item():.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE Loss: nan\nMAE Loss: nan\n```\n:::\n:::\n\n\nMSE is larger because it squares errors—a prediction that's off by 10,000 contributes 100,000,000 to MSE but only 10,000 to MAE. Choose MSE when large errors are especially bad. Choose MAE when you want to treat all errors equally.\n\n### 5.3 Classification Loss Functions\n\nFor classification, we use cross-entropy. Let's build a quick binary classifier to see it in action:\n\n::: {#e9a18651 .cell execution_count=18}\n``` {.python .cell-code}\n# Binary classification example: high-value homes (>300k) vs. low-value\ny_binary = (y_train > 300000).astype(int)\ny_binary_tensor = torch.FloatTensor(y_binary).reshape(-1, 1)\n\n# Binary classification network\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.layer1 = nn.Linear(input_size, 32)\n        self.layer2 = nn.Linear(32, 16)\n        self.layer3 = nn.Linear(16, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.sigmoid(self.layer3(x))  # Output probability\n        return x\n\nbinary_model = BinaryClassifier(input_size)\n\n# Binary cross-entropy loss\ncriterion_bce = nn.BCELoss()\n\n# Quick prediction\nwith torch.no_grad():\n    probs = binary_model(X_train_tensor[:5])\n    print(f\"Predicted probabilities: {probs.squeeze().numpy()}\")\n    print(f\"Actual labels: {y_binary[:5]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted probabilities: [0.42973122 0.4555295  0.45162523 0.4445805  0.46844554]\nActual labels: [0 1 0 0 0]\n```\n:::\n:::\n\n\nThe model outputs probabilities between 0 and 1 (thanks to sigmoid). Binary cross-entropy measures how confident the model is when it's wrong. A confident wrong prediction gets penalized heavily.\n\n### 5.4 Understanding the Loss Landscape\n\nHere's a helpful mental model: imagine a landscape where height represents loss. Training is like rolling a ball downhill—you're trying to reach the lowest point. Gradient descent tells you which direction is downhill.\n\nFor simple models like linear regression, the landscape is bowl-shaped with one clear minimum. For neural networks, the landscape is complex with many hills and valleys. But typically, many of those valleys are \"good enough\"—your model doesn't need to find the global best to work well.\n\n::: {.callout-note}\n**Why the loss goes down:** Each gradient descent step pushes the weights slightly downhill. Over many steps, you reach a valley. You might not reach the lowest possible valley in the entire landscape, but you'll reach a good-enough valley where the loss is low.\n:::\n\n---\n\n## 6. Diagnosing Training: Reading the Curves\n\n### 6.1 Training vs. Validation Loss\n\nThe most important skill in deep learning: reading loss curves. Here's what you need to plot:\n\n- **Training loss:** Loss computed on the training data\n- **Validation loss:** Loss computed on held-out validation data\n\nThese two curves tell you everything about how training is going.\n\n::: {#f6f2162e .cell execution_count=19}\n``` {.python .cell-code}\n# Let's train a new model and track both losses\nfrom sklearn.model_selection import train_test_split\n\n# Split training data into train and validation\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_train_scaled, y_train, test_size=0.2, random_state=42\n)\n\n# Convert to tensors\nX_tr_tensor = torch.FloatTensor(X_tr)\ny_tr_tensor = torch.FloatTensor(y_tr).reshape(-1, 1)\nX_val_tensor = torch.FloatTensor(X_val)\ny_val_tensor = torch.FloatTensor(y_val).reshape(-1, 1)\n\n# Create new model\nmodel_diag = HousingNN(input_size)\noptimizer = torch.optim.Adam(model_diag.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# Training with validation tracking\ntrain_dataset = TensorDataset(X_tr_tensor, y_tr_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nnum_epochs = 100\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model_diag.train()\n    epoch_train_loss = 0\n\n    for batch_X, batch_y in train_loader:\n        predictions = model_diag(batch_X)\n        loss = criterion(predictions, batch_y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_train_loss += loss.item()\n\n    avg_train_loss = epoch_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # Validation phase\n    model_diag.eval()\n    with torch.no_grad():\n        val_predictions = model_diag(X_val_tensor)\n        val_loss = criterion(val_predictions, y_val_tensor)\n        val_losses.append(val_loss.item())\n\n# Plot both losses\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label='Training Loss', linewidth=2)\nplt.plot(val_losses, label='Validation Loss', linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Training vs. Validation Loss', fontsize=14)\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-7-neural-networks_files/figure-html/cell-20-output-1.png){width=802 height=528}\n:::\n:::\n\n\nSee how both curves decrease? That's good—the model is learning patterns that generalize to new data. The validation loss is slightly higher than training loss, which is normal and expected.\n\n### 6.2 Overfitting: When Your Model Memorizes\n\nHere's what overfitting looks like:\n\n![Placeholder: Training curves showing overfitting](images/training-curves-overfitting.png)\n\n**Signs of overfitting:**\n- Training loss keeps decreasing\n- Validation loss starts increasing (or stops decreasing)\n- Large gap between training and validation loss\n\nWhat's happening? The model is learning patterns specific to the training data—including noise—that don't generalize to new data. It's memorizing instead of learning.\n\n**How to fix it:**\n1. Get more training data\n2. Use regularization (dropout, L2 penalty)\n3. Simplify the model (fewer layers, fewer neurons)\n4. Stop training earlier (early stopping)\n\n### 6.3 Underfitting: When Your Model Is Too Simple\n\nUnderfitting looks different:\n\n![Placeholder: Training curves showing underfitting](images/training-curves-underfitting.png)\n\n**Signs of underfitting:**\n- Both training and validation loss are high\n- Both losses plateau quickly\n- Small gap between training and validation loss\n\nThe model isn't complex enough to learn the patterns in your data. It's like trying to fit a curve with a straight line.\n\n**How to fix it:**\n1. Make the model more complex (more layers, more neurons)\n2. Train longer\n3. Decrease regularization\n4. Add better features\n\n### 6.4 The Goldilocks Zone: Just Right\n\nThis is what you want:\n\n![Placeholder: Healthy training curves](images/training-curves-good.png)\n\n**Signs of good fit:**\n- Both losses decrease steadily\n- Validation loss tracks training loss closely\n- Losses plateau together at a low value\n- Small but acceptable gap between them\n\nThis model has learned generalizable patterns without memorizing noise. This is your target.\n\n### 6.5 Using Training Curves to Debug\n\nWhen your model isn't working, look at the curves first:\n\n**Problem:** Validation loss is much higher than training loss\n- **Diagnosis:** Overfitting\n- **Solution:** Add regularization, get more data, or simplify model\n\n**Problem:** Both losses are high and not decreasing\n- **Diagnosis:** Underfitting or learning rate too low\n- **Solution:** Make model more complex or increase learning rate\n\n**Problem:** Loss is erratic, jumping up and down\n- **Diagnosis:** Learning rate too high\n- **Solution:** Decrease learning rate\n\n**Problem:** Loss starts decreasing then suddenly explodes\n- **Diagnosis:** Learning rate way too high\n- **Solution:** Significantly decrease learning rate\n\n---\n\n## 7. Regularization in Deep Learning\n\n### 7.1 Dropout: Randomly Forgetting\n\nDropout is beautifully simple: during training, randomly turn off some neurons. This forces the network to learn redundant representations—it can't rely on any single neuron.\n\n::: {#66b1ebe2 .cell execution_count=20}\n``` {.python .cell-code}\nclass RegularizedNN(nn.Module):\n    def __init__(self, input_size):\n        super(RegularizedNN, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 64)\n        self.dropout1 = nn.Dropout(p=0.3)  # Drop 30% of neurons\n\n        self.layer2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(p=0.3)\n\n        self.layer3 = nn.Linear(32, 1)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.dropout1(x)  # Apply dropout after activation\n\n        x = self.relu(self.layer2(x))\n        x = self.dropout2(x)\n\n        x = self.layer3(x)\n        return x\n\nmodel_dropout = RegularizedNN(input_size)\nprint(model_dropout)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegularizedNN(\n  (layer1): Linear(in_features=12, out_features=64, bias=True)\n  (dropout1): Dropout(p=0.3, inplace=False)\n  (layer2): Linear(in_features=64, out_features=32, bias=True)\n  (dropout2): Dropout(p=0.3, inplace=False)\n  (layer3): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n)\n```\n:::\n:::\n\n\nDuring training, dropout randomly zeros out 30% of activations. During evaluation (when you set `model.eval()`), dropout is turned off—all neurons are active.\n\nWhy does this help? By randomly disabling neurons, you prevent any single neuron from becoming too specialized. The network learns more robust representations.\n\n![Placeholder: Dropout visualization showing neurons being randomly disabled](images/dropout-visualization.png)\n\n### 7.2 Batch Normalization: Stabilizing Training\n\nBatch normalization normalizes the activations within each layer. This helps gradients flow better and allows higher learning rates.\n\n::: {#e08f9b55 .cell execution_count=21}\n``` {.python .cell-code}\nclass BatchNormNN(nn.Module):\n    def __init__(self, input_size):\n        super(BatchNormNN, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 64)\n        self.bn1 = nn.BatchNorm1d(64)  # Batch norm after first layer\n\n        self.layer2 = nn.Linear(64, 32)\n        self.bn2 = nn.BatchNorm1d(32)\n\n        self.layer3 = nn.Linear(32, 1)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.bn1(x)  # Normalize before activation\n        x = self.relu(x)\n\n        x = self.layer2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.layer3(x)\n        return x\n\nmodel_bn = BatchNormNN(input_size)\nprint(model_bn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBatchNormNN(\n  (layer1): Linear(in_features=12, out_features=64, bias=True)\n  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer2): Linear(in_features=64, out_features=32, bias=True)\n  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer3): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n)\n```\n:::\n:::\n\n\nBatch normalization is a bit like standardizing your input features, but applied within the network. It often speeds up training and can improve final performance.\n\n### 7.3 L2 Regularization: Penalizing Large Weights\n\nRemember L2 regularization from Module 4? We can use it in neural networks too:\n\n::: {#7834bff0 .cell execution_count=22}\n``` {.python .cell-code}\n# L2 regularization via weight decay in optimizer\noptimizer_l2 = torch.optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    weight_decay=0.01  # L2 regularization strength\n)\n```\n:::\n\n\nThe `weight_decay` parameter adds a penalty for large weights, encouraging the model to keep weights small. This discourages overfitting.\n\n### 7.4 Early Stopping: Knowing When to Quit\n\nThe simplest regularization technique: stop training when validation loss stops improving.\n\n::: {#56fb27c7 .cell execution_count=23}\n``` {.python .cell-code}\n# Early stopping example\nbest_val_loss = float('inf')\npatience = 10  # Stop if no improvement for 10 epochs\npatience_counter = 0\n\nfor epoch in range(200):  # Max 200 epochs\n    # ... training code ...\n\n    # Check validation loss (using our previous val_loss)\n    if epoch > 0:  # Skip first epoch\n        current_val_loss = val_losses[-1]\n\n        if current_val_loss < best_val_loss:\n            best_val_loss = current_val_loss\n            patience_counter = 0\n            # Save model checkpoint here\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEarly stopping at epoch 11\n```\n:::\n:::\n\n\nEarly stopping prevents the model from overfitting by stopping before validation performance degrades.\n\n### 7.5 Comparing Regularization Techniques\n\nLet's see the impact:\n\n::: {.callout-tip}\n**Regularization Quick Guide:**\n- **Dropout:** Use when you have overfitting. Start with p=0.3-0.5\n- **Batch Normalization:** Almost always helps. Use it.\n- **L2 Regularization:** Use with weight_decay=0.001-0.01. Helps with overfitting.\n- **Early Stopping:** Always monitor validation loss and stop when it plateaus.\n:::\n\nYou can combine multiple regularization techniques. In fact, that's common—use batch normalization + dropout + early stopping together.\n\n---\n\n## 8. Architecture Design: How Many Layers? How Many Neurons?\n\n### 8.1 The Capacity Question\n\nNetwork architecture determines the model's **capacity**—its ability to learn complex patterns. More layers and more neurons = more capacity. But more capacity also means more risk of overfitting.\n\nThe fundamental question: Does your model have enough capacity to learn the patterns in your data, but not so much that it memorizes noise?\n\n### 8.2 Going Deeper vs. Going Wider\n\nTwo ways to increase capacity:\n\n**Deeper networks** (more layers):\n- Can learn more abstract, hierarchical representations\n- Better for very complex patterns\n- Harder to train (vanishing gradients)\n\n**Wider networks** (more neurons per layer):\n- More capacity without as much depth\n- Easier to train\n- May not learn hierarchical features as effectively\n\n### 8.3 Rules of Thumb for Architecture Design\n\nHere are practical starting points:\n\n**For tabular data (like our housing dataset):**\n- Start with 2-3 hidden layers\n- Each layer has 32-128 neurons\n- Make layers smaller as you go deeper (e.g., 128 → 64 → 32)\n\n**For image data:**\n- Convolutional networks (not covered here)\n- Many layers (10-100+)\n- Specialized architectures (ResNet, VGG, etc.)\n\n**For text data:**\n- Recurrent or transformer networks (not covered here)\n- Architecture depends heavily on task\n\n**General principle:** Start simple, add complexity only if needed.\n\n### 8.4 Experimenting with Architecture\n\nLet's try different architectures on our housing data:\n\n::: {#44c348f3 .cell execution_count=24}\n``` {.python .cell-code}\n# Three different architectures\nclass SmallNN(nn.Module):\n    def __init__(self, input_size):\n        super(SmallNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 16)\n        self.fc2 = nn.Linear(16, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\nclass MediumNN(nn.Module):\n    def __init__(self, input_size):\n        super(MediumNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass LargeNN(nn.Module):\n    def __init__(self, input_size):\n        super(LargeNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 32)\n        self.fc4 = nn.Linear(32, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        return self.fc4(x)\n\n# Count parameters in each\nfor name, ModelClass in [('Small', SmallNN), ('Medium', MediumNN), ('Large', LargeNN)]:\n    model_temp = ModelClass(input_size)\n    params = sum(p.numel() for p in model_temp.parameters())\n    print(f\"{name} model: {params:,} parameters\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSmall model: 225 parameters\nMedium model: 2,945 parameters\nLarge model: 12,033 parameters\n```\n:::\n:::\n\n\nFor this dataset, the medium model is probably best. The small model might underfit (not enough capacity). The large model might overfit (too much capacity for the amount of data we have).\n\n### 8.5 How to Choose\n\nHere's a decision process:\n\n1. **Start with a simple architecture** (2 layers, 32-64 neurons)\n2. **Train and evaluate** - Look at training curves\n3. **Diagnose:**\n   - If underfitting (high training loss) → Increase capacity\n   - If overfitting (training loss << validation loss) → Decrease capacity or add regularization\n4. **Iterate** - Adjust and retrain\n\nDon't try to guess the perfect architecture upfront. Experiment systematically.\n\n---\n\n## 9. Hyperparameter Tuning: The Art of Training Neural Networks\n\n### 9.1 Learning Rate: The Most Important Hyperparameter\n\nLearning rate controls how big a step we take during gradient descent. Too small = slow training. Too large = unstable training or divergence.\n\n::: {#ab80f333 .cell execution_count=25}\n``` {.python .cell-code}\n# Learning rate experiment\nlearning_rates = [0.0001, 0.001, 0.01, 0.1]\nresults = {}\n\nfor lr in learning_rates:\n    model_lr = HousingNN(input_size)\n    optimizer = torch.optim.Adam(model_lr.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    # Train for 20 epochs\n    losses = []\n    for epoch in range(20):\n        model_lr.train()\n        epoch_loss = 0\n\n        for batch_X, batch_y in train_loader:\n            predictions = model_lr(batch_X)\n            loss = criterion(predictions, batch_y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        losses.append(epoch_loss / len(train_loader))\n\n    results[lr] = losses\n\n# Plot learning curves for different learning rates\nplt.figure(figsize=(10, 6))\nfor lr, losses in results.items():\n    plt.plot(losses, label=f'LR = {lr}', linewidth=2)\n\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Training Loss', fontsize=12)\nplt.title('Effect of Learning Rate', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.yscale('log')  # Log scale to see all curves\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-7-neural-networks_files/figure-html/cell-26-output-1.png){width=823 height=528}\n:::\n:::\n\n\nSee how learning rate affects training? Too small (0.0001) and loss decreases slowly. Too large (0.1) and training is unstable. The sweet spot is around 0.001-0.01 for this problem.\n\n### 9.2 Batch Size and Its Effects\n\nBatch size determines how many examples we use to compute each gradient:\n\n- **Small batches (8-32):** Noisy gradients, slower per epoch, but can help generalization\n- **Large batches (128-512):** Smooth gradients, faster per epoch, but might overfit\n\nCommon practice: start with 32 or 64, adjust based on your GPU memory and dataset size.\n\n### 9.3 Number of Epochs and Early Stopping\n\nHow many epochs to train? There's no fixed answer. Use early stopping:\n\n1. Monitor validation loss\n2. If it hasn't improved for N epochs (patience), stop\n3. Save the best model (lowest validation loss)\n\nThis prevents overfitting and saves time.\n\n### 9.4 Learning Rate Scheduling\n\nSometimes it helps to decrease learning rate during training:\n\n::: {#c42b637e .cell execution_count=26}\n``` {.python .cell-code}\n# Learning rate scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',     # Minimize loss\n    factor=0.5,     # Multiply LR by 0.5\n    patience=5      # Wait 5 epochs\n)\n\n# During training:\n# val_loss = ... (compute validation loss)\n# scheduler.step(val_loss)  # Adjust LR based on validation loss\n```\n:::\n\n\nThis automatically reduces learning rate when validation loss plateaus, helping the model fine-tune.\n\n### 9.5 Systematic Experimentation\n\nDon't guess-and-check randomly. Be systematic:\n\n1. **Fix all but one hyperparameter**\n2. **Try several values** for that hyperparameter\n3. **Pick the best value**\n4. **Move to next hyperparameter**\n\nThis is where AI assistants shine—they can help you run many experiments in parallel and organize results.\n\n::: {.callout-tip}\n**Hyperparameter Priority:**\n1. Learning rate (most important)\n2. Architecture (number of layers/neurons)\n3. Batch size\n4. Regularization strength\n5. Everything else\n\nTune in this order. Learning rate has the biggest impact.\n:::\n\n---\n\n## 10. When to Use Neural Networks (and When Not To)\n\n### 10.1 Problems Where Neural Networks Excel\n\nNeural networks are worth the complexity when:\n\n**You have lots of data:** Neural networks need data to shine. With only 1000 examples, logistic regression might work better. With 100,000+ examples, neural networks can learn patterns that simpler models miss.\n\n**The patterns are complex:** Images, text, speech, video—these have complex, hierarchical patterns that neural networks capture well. Tabular data with simple relationships? Maybe not.\n\n**You need flexibility:** Neural networks are universal function approximators. If you don't know what mathematical form your relationship takes, neural networks can learn it.\n\n**Examples:**\n- Image classification (faces, medical images, objects)\n- Natural language processing (translation, sentiment, generation)\n- Speech recognition\n- Game playing (Go, Chess, video games)\n- Recommendation systems at scale\n\n### 10.2 Problems Where Traditional ML Is Better\n\nUse simpler models when:\n\n**You have limited data:** With 500 examples, logistic regression or random forests will likely outperform neural networks. Neural networks need more data to tune their many parameters.\n\n**Interpretability matters:** Need to explain your model to stakeholders or regulators? Linear models and decision trees are far more interpretable than neural networks.\n\n**The patterns are simple:** If your relationship is roughly linear, why use a neural network? Linear regression is faster, simpler, and easier to interpret.\n\n**You need fast predictions:** Neural networks are slower at inference than simpler models. For real-time applications with tight latency requirements, simpler models might be better.\n\n**Examples:**\n- Small tabular datasets (housing prices with 5000 examples)\n- Problems requiring explanation (loan decisions, medical diagnosis)\n- Linear relationships (basic price prediction)\n- Real-time prediction with tight latency constraints\n\n### 10.3 A Decision Framework\n\n::: {#5c8985c8 .cell execution_count=27}\n\n::: {.cell-output .cell-output-display execution_count=27}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Consideration</th>\n      <th>Use Neural Networks When...</th>\n      <th>Use Traditional ML When...</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dataset size</td>\n      <td>10,000+ examples</td>\n      <td>&lt; 5,000 examples</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Pattern complexity</td>\n      <td>Highly complex (images, text, etc.)</td>\n      <td>Moderate (tabular with clear features)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Interpretability</td>\n      <td>Not critical</td>\n      <td>Must explain decisions</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Training time</td>\n      <td>Can afford hours/days</td>\n      <td>Need results in minutes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Inference speed</td>\n      <td>Can afford milliseconds</td>\n      <td>Need microsecond response</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Hyperparameter tuning</td>\n      <td>Can experiment extensively</td>\n      <td>Need simple, quick tuning</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### 10.4 The Pragmatic Approach\n\nIn practice, you should try both:\n\n1. Start with a simple baseline (linear regression, logistic regression, random forest)\n2. If performance is insufficient, try a neural network\n3. Compare results and complexity\n4. Choose based on the tradeoff between performance and complexity\n\nSometimes a random forest gives you 92% accuracy and a neural network gives 93%—but the neural network takes 10x longer to train and tune. Is 1% worth it? It depends on your problem.\n\n### 10.5 Neural Networks as Part of Your Toolkit\n\nDon't think of neural networks as replacing traditional ML. Think of them as a powerful addition to your toolkit. For some problems, they're the best tool. For others, they're overkill.\n\nThe key is knowing which tool to reach for—and that comes from understanding what each tool does well.\n\n---\n\n## Summary\n\nNeural networks are revolutionary, but they're not magic. They're universal function approximators built from simple building blocks: perceptrons stacked into layers with activation functions providing non-linearity. Training happens through the forward pass (make predictions), loss calculation (measure error), backward pass (compute gradients via backpropagation), and optimization (update weights).\n\nThe fundamental workflow is always the same: load data → build model → define loss → create optimizer → train loop → evaluate. PyTorch handles the complex parts (automatic differentiation, gradient computation) while you focus on architecture, hyperparameters, and diagnosis.\n\nTraining curves tell you everything. Validation loss tracking training loss? Good. Validation loss increasing while training decreases? Overfitting—add regularization. Both losses high? Underfitting—increase capacity. Losses erratic? Learning rate too high. Reading these curves is your most valuable skill.\n\nRegularization prevents overfitting: dropout randomly disables neurons, batch normalization stabilizes training, L2 regularization penalizes large weights, and early stopping quits when validation performance plateaus. Use them together.\n\nArchitecture design is part science, part art. Start simple—2-3 layers with 32-64 neurons. If underfitting, add capacity (more layers or neurons). If overfitting, reduce capacity or add regularization. Don't guess the perfect architecture—experiment systematically.\n\nLearning rate is your most important hyperparameter. Too low and training is slow. Too high and training explodes. Find the sweet spot through experimentation. Batch size, number of epochs, and regularization strength matter too, but learning rate matters most.\n\nFinally, know when to use neural networks and when not to. They excel with large datasets, complex patterns, and problems requiring flexibility. They struggle with small data, when interpretability matters, and for simple patterns. Try both neural networks and traditional ML, then choose based on the performance-complexity tradeoff.\n\nNeural networks are a powerful tool—but they're just one tool. The art is knowing when to use them and when to reach for something simpler. Use your brain. That's what it's there for.\n\n---\n\n## Practice Exercises\n\n1. **Build and train an MLP:** Create a 3-layer neural network for the Titanic survival dataset. Use 64 neurons in the first hidden layer, 32 in the second. Train for 50 epochs and plot training/validation loss curves. Identify whether your model is overfitting, underfitting, or well-fit.\n\n2. **Activation function experiment:** Build three identical networks but use different activation functions (ReLU, Sigmoid, Tanh) in the hidden layers. Train all three on the same data and compare final validation performance. Which works best? Why?\n\n3. **Regularization comparison:** Take a network that's overfitting and fix it three different ways: (a) add dropout, (b) add batch normalization, (c) use early stopping. Compare the validation loss curves. Which technique helps most?\n\n4. **Learning rate search:** Train the same model with learning rates [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]. Plot all training curves on one graph. Identify the optimal learning rate range.\n\n5. **Architecture design:** You have a dataset with 50 features and 20,000 examples for binary classification. Design three different architectures (small, medium, large) and justify your choices. Implement and compare them.\n\n6. **When to use neural networks:** For each scenario, decide whether you'd use a neural network or traditional ML, and explain why:\n   - Predicting apartment rent from 8 features with 2,000 examples\n   - Classifying dog breeds from images with 50,000 examples\n   - Predicting customer churn from 30 features with 5,000 examples\n   - Detecting faces in photos\n\n---\n\n## Additional Resources\n\n- [PyTorch Official Tutorial](https://pytorch.org/tutorials/) - Comprehensive tutorials from basics to advanced topics\n- [Neural Networks and Deep Learning (Michael Nielsen)](http://neuralnetworksanddeeplearning.com/) - Free online book with excellent intuitive explanations\n- [CS231n: Convolutional Neural Networks](http://cs231n.github.io/) - Stanford course notes with fantastic explanations\n- [Distill.pub](https://distill.pub/) - Beautiful interactive articles explaining neural network concepts\n- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - Official API documentation\n- [Deep Learning Book (Goodfellow et al.)](https://www.deeplearningbook.org/) - Comprehensive but more mathematical treatment\n- [fast.ai Practical Deep Learning](https://course.fast.ai/) - Practical course focused on getting results quickly\n\n",
    "supporting": [
      "chapter-7-neural-networks_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}