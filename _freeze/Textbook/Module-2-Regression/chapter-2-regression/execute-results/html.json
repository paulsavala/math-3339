{
  "hash": "6dda28d793f0d57845beda1438fa8b51",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Chapter 2: Regression Models\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Chapter Resources {#ch2-resources}\n\n**Related Assignments:**\n\n- [Chapter 2 Homework](../../Assignments/Module%202%20-%20Regression/module-2-homework.qmd)\n\n---\n\n## Introduction {#ch2-intro}\n\nLinear regression is perhaps the most important algorithm in machine learning. Not because it's the most powerful—it's not. Not because it always works—it doesn't. Linear regression matters because understanding it deeply gives you the foundation to understand almost everything else in machine learning.\n\nHere's the thing: most machine learning is just sophisticated regression. Neural networks? Stacked regressions with non-linear activation functions. Logistic regression? Linear regression passed through a special function. Even tree-based methods are essentially breaking the data into regions and fitting constants (which are just simple regressions) in each region.\n\nBut linear regression has a dirty secret: it only works well when certain assumptions are met. Violate those assumptions, and your beautiful model with a high R² might be making terrible predictions. This is where many beginners get burned. They fit a model, see a nice R² value, and think they're done. Then they deploy it to production and watch it fail spectacularly.\n\nThis chapter is about becoming a regression expert. Not just someone who can call `LinearRegression().fit()`, but someone who knows when regression will work, when it won't, and how to fix it when it breaks. We'll cover the assumptions behind linear regression, how to check if they're violated, what to do when they are, and how regularization techniques can save you when things get messy.\n\n::: {.callout-tip}\nYou may assume that linear regression is only useful when your data has a linear relationship. But this is not true! Even data with complex relationships can be modeled using linear regression with the right features, transformations, and regularization techniques.\n:::\n\nBy the end of this chapter, you'll understand:\n\n- What assumptions linear regression makes and why they matter\n- How to diagnose problems using residual plots\n- When and how to use polynomial features for non-linear relationships\n- How multicollinearity breaks coefficient interpretation\n- How Ridge, Lasso, and Elastic Net regularization fix overfitting and multicollinearity\n- How to choose between different regression approaches\n\nLet's jump in.\n\n---\n\n## 1. Train/Validation/Test Methodology {#ch2-1}\n\nBefore we dive into specific regression models, we need to establish a crucial foundation: how to properly split and evaluate our data. Understanding train/validation/test methodology is essential for building models that actually generalize to new data.\n\n### 1.1 Why Three Sets? {#ch2-1-1}\n\nYou've seen train/test splits. But proper ML methodology requires **three** sets. Here's why:\n\n- **Training set:** Used to fit the model (learn parameters)\n- **Validation set:** Used to tune hyperparameters and select models\n- **Test set:** Used ONCE at the very end to get an unbiased estimate of performance\n\n::: {.callout-note}\nBefore you freak out at the huge block of code below, slow down! You're not expected to write code like this from scratch. Instead, what you should do is go through, look at the code comments (everything after the hashtag `#`), and get a rough feeling for what it's doing. In practice, an LLM would write this code for you based on your prompt.\n:::\n\n::: {#4e2b7b3b .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Load California housing data for demonstration\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Select features and target\nX = housing_df[['median_income', 'housing_median_age', 'total_rooms', 'population']]\ny = housing_df['median_house_value']\n\n# Remove missing values\nX_clean = X.dropna()\ny_clean = y[X_clean.index]\n\n# Proper three-way split\ndef train_val_test_split(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=1):\n    \"\"\"Split data into train, validation, and test sets\"\"\"\n\n    # First split: separate test set\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    # Second split: separate train and validation\n    # val_size needs to be recalculated relative to remaining data\n    val_size_adjusted = val_size / (train_size + val_size)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n    )\n\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\n# Split the data\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X_clean, y_clean)\n\nprint(\"Dataset sizes (California housing):\")\nprint(f\"Training: {len(X_train)} samples ({len(X_train)/len(X_clean)*100:.0f}%)\")\nprint(f\"Validation: {len(X_val)} samples ({len(X_val)/len(X_clean)*100:.0f}%)\")\nprint(f\"Test: {len(X_test)} samples ({len(X_test)/len(X_clean)*100:.0f}%)\")\nprint(f\"Total: {len(X_clean)} samples\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset sizes (California housing):\nTraining: 12384 samples (60%)\nValidation: 4128 samples (20%)\nTest: 4128 samples (20%)\nTotal: 20640 samples\n```\n:::\n:::\n\n\n::: {.callout-warning}\n**Never use the test set for model selection or hyperparameter tuning.** If you evaluate multiple models on the test set and pick the best, you've contaminated it. The test set should be used exactly once, at the very end, to get an honest estimate of how your chosen model will perform on new data.\n:::\n\n### 1.2 Cross-Validation: Getting More Reliable Estimates {#ch2-1-2}\n\nA single train/validation split can be unlucky—maybe the validation set happens to be easy or hard. **Cross-validation** solves this by using multiple validation sets.\n\nIn cross-validation, we split the training data into multiple \"folds\". The model is fit on some folds and validated on the remaining fold. This process is repeated multiple times, with each fold serving as the validation set once. The final cross-validation score is the average of all validation scores.\n\nThe benefits of this are that:\n\n1. We can better understand the variability of our predictions, because we can see how much the model's performance changes when we use different validation sets.\n2. We can keep our test set \"clean\", and only use it once at the very end.\n\nLet's implement this in code. Here we'll use cross-validation to find the optimal number of neighbors for a KNN regression model. We'll choose between 2, 5, 10, 20, 30, 40, and 50 neighbors. Rather than doing a single fit on the training set and then validating on the validation set, we'll use cross-validation. Doing so allows us to only use the test set once at the very end, and also to better understand the variability of our predictions, which we'll show as error bars.\n\n::: {#b49c1828 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\n# Use cross-validation to find the optimal number of neighbors\n# Test different values of n_neighbors\nn_neighbors_options = [2, 5, 10, 20, 30, 40, 50]\n\nresults = []\nfor n in n_neighbors_options:\n    # Create model with this value of n_neighbors\n    model = KNeighborsRegressor(n_neighbors=n)\n\n    # Perform 5-fold cross-validation\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5,\n                                 scoring='neg_mean_squared_error')\n\n    # Convert to MSE (scores are negative)\n    mse_scores = -cv_scores\n    mean_mse = mse_scores.mean()\n    std_mse = mse_scores.std()\n\n    results.append({\n        'n_neighbors': n,\n        'mean_mse': mean_mse,\n        'std_mse': std_mse\n    })\n\n# Find the best n_neighbors\nbest_result = min(results, key=lambda x: x['mean_mse'])\n\n# Visualize the results\nplt.figure(figsize=(10, 6))\nn_values = [r['n_neighbors'] for r in results]\nmean_mses = [r['mean_mse'] for r in results]\nstd_mses = [r['std_mse'] for r in results]\n\nplt.errorbar(n_values, mean_mses, yerr=std_mses, marker='o', linewidth=2,\n             capsize=5, capthick=2, markersize=8, alpha=0.75)\nplt.xlabel('Number of Neighbors', fontsize=12)\nplt.ylabel('Mean Squared Error', fontsize=12)\nplt.title('Cross-Validation: Finding Optimal n_neighbors', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-3-output-1.png){width=815 height=528}\n:::\n:::\n\n\nWe see that 10 neighbors appears optimal, as it has the lowest mean squared error with reasonably low variation. This means that it generalizes well to new data.\n\nCross-validation gives you:\n\n1. **More reliable estimates:** Averages over multiple validation sets\n2. **Uncertainty quantification:** Standard deviation tells you how variable performance is\n3. **Better use of data:** Every example gets to be in the validation set once\n\n::: {.callout-tip}\nUse cross-validation for model selection when you have limited data. For very large datasets, a single train/val/test split is often sufficient and much faster.\n:::\n\n### Learning outcomes: {#ch2-2-outcomes}\n**_By hand_ you should be able to:**\n\n- Explain why a train-test-validation split is key for producing reliable findings, and the role of each of the three sets (training, testing, validation)\n- Explain what cross validation is, and when it's beneficial\n\n---\n\n## 2. Linear Regression {#ch2-2}\n\n### 2.1 The Line of Best Fit {#ch2-2-1}\n\nYou've probably heard linear regression described as \"finding the line of best fit.\" But what does \"best\" actually mean? Best according to what criteria?\n\nThe answer: **best means the line that minimizes the sum of squared errors**. For each data point, we calculate how far the prediction is from the actual value (the error), square it, and add up all these squared errors. We can either minimize this sum, or minimize the average, they're equivalent. The line that makes this value as small as possible is our \"best fit\" line. The equation is\n\n$$\n\\frac{1}{n}\\displaystyle\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n$$\n\nThis is called mean squared error (MSE), which is the loss function for linear regression. The goal of linear regression is to find the line that minimizes this loss.\n\nLet's see this in action with the NYC census data:\n\n::: {#451457ee .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Load NYC census data\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Filter to complete cases for income prediction\nnyc_clean = nyc_census[['IncomePerCap', 'Income']].dropna()\n\n# Let's predict median household income from per-capita income\n# Start with just one feature to visualize easily\nX = nyc_clean[['IncomePerCap']].values  # Income per capita\ny = nyc_clean['Income'].values  # Median household income\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit linear regression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\n# Visualize the fit\nplt.figure(figsize=(10, 6))\nplt.scatter(X_train, y_train, alpha=0.5, s=20, label='Training Data')\nplt.plot(sorted(X_train.flatten()), model.predict(sorted(X_train.reshape(-1, 1))),\n         'r-', linewidth=2, label='Best Fit Line')\nplt.xlabel('Income Per Capita ($)', fontsize=12)\nplt.ylabel('Median Household Income ($)', fontsize=12)\nplt.title('Linear Regression: Per Capita Income vs Household Income', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Slope (coefficient): {model.coef_[0]:.4f}\")\nprint(f\"Intercept: {model.intercept_:.4f}\")\nprint(f\"Training R²: {model.score(X_train, y_train):.4f}\")\nprint(f\"Test R²: {model.score(X_test, y_test):.4f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-4-output-1.png){width=851 height=528}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSlope (coefficient): 0.9894\nIntercept: 27540.5059\nTraining R²: 0.6878\nTest R²: 0.7202\n```\n:::\n:::\n\n\nThe regression line tries to find the balance that minimizes total squared error across all points. The slope and y-intercept are learned (machine learning) such that the sum of squared residuals, or MSE, is as small as possible.\n\n::: {.callout-note}\nYou _are not_ expected to write code like this from hand. However, you _are_ expected to _understand_ it! You will be given quiz questions with blocks of code like the code above, and asked questions about it, which you must answer without the help of an LLM.\n:::\n\n### 2.2 The Assumptions Behind Linear Regression {#ch2-2-2}\n\nLinear regression makes four key assumptions. Violate them, and your model might give you terrible predictions even if the R² looks good.\n\n**The Four Assumptions:**\n\n1. **Linearity:** The relationship between X and y is actually linear. If it's curved, a straight line won't fit well (however, we'll handle those cases later in this chapter).\n\n2. **Independence:** Each observation is independent. One house's price doesn't directly affect another's. (This can be violated with time series or spatial data.)\n\n3. **Homoscedasticity:** The \"spread\" of points around the line should be roughly the same everywhere. If errors are tiny for low X values but huge for high X values, that's a problem. (The fancy term means \"constant variance.\")\n\n4. **Normality:** The residuals (errors) are normally distributed. This matters more for statistical inference than prediction.\n\n**What happens when you violate these?**\n\n- Violate **linearity**: Your predictions will be systematically wrong in certain ranges\n- Violate **independence**: Your confidence intervals and p-values become unreliable\n- Violate **homoscedasticity**: Some predictions are much more uncertain than others (but you won't know which ones!)\n- Violate **normality**: Your confidence intervals might be wrong, but predictions can still be okay\n\nWe'll learn how to check these assumptions using diagnostic plots in Section 3.\n\n### 2.3 Interpreting Coefficients {#ch2-2-3}\n\nCoefficients tell you the relationship between features and the target. Let's extract and interpret them:\n\n::: {#2958ff0f .cell execution_count=4}\n``` {.python .cell-code}\n# Fit a model with multiple features\nfeatures = ['TotalPop', 'Professional', 'Poverty', 'Unemployment']\nnyc_multi = nyc_census[features + ['Income']].dropna()\nX_multi = nyc_multi[features].values\ny_multi = nyc_multi['Income'].values\n\nX_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n    X_multi, y_multi, test_size=0.2, random_state=42\n)\n\n# Fit model\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_train_multi, y_train_multi)\n\n# Display coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': model_multi.coef_\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"Regression Coefficients:\")\nprint(coef_df)\nprint(f\"\\nIntercept: {model_multi.intercept_:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegression Coefficients:\n        Feature  Coefficient\n1  Professional   777.977464\n0      TotalPop    -0.184356\n3  Unemployment  -191.478521\n2       Poverty -1036.991744\n\nIntercept: 53635.7278\n```\n:::\n:::\n\n\n**How to interpret these:**\n\n- **Professional coefficient:** For every 1% increase in the percentage of professionals, median household income changes by the coefficient amount, *holding all other features constant*.\n\n- **Poverty coefficient:** For every 1% increase in the poverty rate, median household income changes by the coefficient amount (likely negative), *holding all other features constant*.\n\n- **Intercept:** The predicted median household income when all features are zero. Often not meaningful in practice (what census tract has zero population or zero unemployment?).\n\nNotice the key phrase: \"**holding all other features constant**.\" That's crucial. It means the coefficient shows the isolated effect of that one feature, assuming everything else stays the same. Change one feature while keeping others fixed, and the coefficient tells you how much the prediction changes.\n\n::: {.callout-warning}\n**Coefficient interpretation breaks down when features are highly correlated (multicollinearity).** We'll address this in Section 5 when we discuss multicollinearity.\n:::\n\n### Learning outcomes: {#ch2-2-outcomes}\n**_By hand_ you should be able to:**\n\n- Given LLM-written linear regression code, explain what each step is doing (loading the function, cleaning the data, creating X/y sets, etc)\n- List and explain all four assumptions behind linear regression\n- Interpret the coefficients from a linear regression model\n\n---\n\n## 3. Regression Evaluation Metrics {#ch2-3}\n\nHow do you know if your regression model is any good? You need metrics. R² is popular, but it can be misleading. Let's look at the most important metrics, what they mean, and when to use each one.\n\n### 3.1 Mean Squared Error (MSE) {#ch2-3-1}\n\nMSE is the average of squared errors. Remember, an error (or residual) is just actual value minus predicted value. Square those, average them, and you have MSE.\n\nWhy does MSE matter? It directly reflects what linear regression minimizes during training. When you fit a linear regression, you're literally finding the line that gives you the smallest possible MSE on the training data.\n\nBut here's the catch: **squaring errors means big errors get penalized heavily**. If one prediction is off by 100 and another is off by 10, the first error contributes 10,000 to MSE while the second contributes only 100. That's 100 times worse, not 10 times worse. This makes MSE sensitive to outliers.\n\n### 3.2 Mean Absolute Error (MAE) {#ch2-3-2}\n\nMAE is simpler: just the average of the absolute values of errors. No squaring involved.\n\n**MAE vs MSE: What's the difference?**\n\nMAE treats all errors proportionally. An error of 100 is exactly 10 times worse than an error of 10. With MSE, that error of 100 is 100 times worse.\n\n::: {.callout-tip}\nWhile MSE _does not_ preserve units (since it squares the errors), you can convert back to the original units by taking a square root, such as\n\n$$\n\\text{RMSE} = \\sqrt{\\text{MSE}} = \\displaystyle\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n$$\n\nThis is called \"**root mean squared error**\", or **RMSE**. It's in the same units as the target variable, making it more interpretable.\n:::\n\n::: {.callout-tip}\nUnless there's an extremely compelling reason to do otherwise, you should always start with MSE or RMSE as your metric. It's the most natural metric for linear regression, and it's what linear regression optimizes.\n\nHowever, it's often beneficial to _also_ compute MAE, as it's more interpretable and less sensitive to outliers. It's typical to report _both_ values, as they each have their own strengths.\n:::\n\n::: {#73572d70 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Calculate MAE and MSE\nmse_sklearn = mean_squared_error(y_test, y_pred_test)\nmae = mean_absolute_error(y_test, y_pred_test)\n\n# Calculate Root Mean Squared Error (RMSE) for comparison\nrmse = np.sqrt(mse_sklearn)\n\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\n\nprint(f\"\\nInterpretation:\")\nprint(f\"- On average, predictions are off by ${mae:.0f} (MAE)\")\nprint(f\"- MAE is lower than RMSE because MSE penalizes large errors more heavily\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMAE: 12478.0785\nRMSE: 15670.5818\n\nInterpretation:\n- On average, predictions are off by $12478 (MAE)\n- MAE is lower than RMSE because MSE penalizes large errors more heavily\n```\n:::\n:::\n\n\nMAE is lower than RMSE (square root of MSE). That's typical—RMSE is always at least as large as MAE, and the gap tells you something about outliers. A big gap means you have some really bad predictions pulling up the RMSE.\n\n### 3.3 R² (R-Squared) {#ch2-3-3}\n\nR² tells you the proportion of variance in the target variable that your model explains. It ranges from 0 to 1, where:\n\n- **R² = 1:** Perfect predictions (your model explains 100% of the variance)\n- **R² = 0:** Your model is no better than just predicting the mean every time\n- **R² < 0:** Your model is worse than predicting the mean (yes, this is possible!)\n\nThe formula is: R² = 1 - (Sum of Squared Residuals / Total Sum of Squares)\n\nBut what does that actually mean? Think of it this way: imagine you know nothing about the features and just predict the mean house value for every house. That's your baseline. R² tells you how much better (or worse!) your model is than that naive baseline.\n\n::: {#3f0041e7 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\n\n# Calculate R² manually to understand it\n# Total Sum of Squares: how far each point is from the mean\ny_mean = y_test.mean()\ntotal_ss = ((y_test - y_mean) ** 2).sum()\n\n# Residual Sum of Squares: how far predictions are from actual values\nresidual_ss = ((y_test - y_pred_test) ** 2).sum()\n\n# R² = 1 - (residual variation / total variation)\nr2_manual = 1 - (residual_ss / total_ss)\n\n# Compare to sklearn\nr2_sklearn = r2_score(y_test, y_pred_test)\n\nprint(f\"R² (manual): {r2_manual:.4f}\")\nprint(f\"R² (sklearn): {r2_sklearn:.4f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"The model explains {r2_sklearn*100:.2f}% of the variance in house prices\")\nprint(f\"That means {(1-r2_sklearn)*100:.2f}% of the variance is unexplained\")\n\n# Visualize what R² means\nfig = plt.figure(figsize=(10, 6))\n\n# Baseline plot\nplt.scatter(X_test, y_test, alpha=0.3, s=10, label='Actual Data')\nplt.axhline(y_mean, color='red', linestyle='--', linewidth=2, label=f'Baseline: Predict Mean = {y_mean:.2f}')\n\n# Regression line\nplt.plot(X_test, y_pred_test, 'b-', linewidth=2, label=f'Regression Line (R² = {r2_sklearn:.4f})')\n\nplt.xlabel('Median Income (in $10,000s)', fontsize=12)\nplt.ylabel('Median House Value (in $100,000s)', fontsize=12)\nplt.title('Baseline Model (R² = 0)', fontsize=14)\n\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR² (manual): 0.7202\nR² (sklearn): 0.7202\n\nInterpretation:\nThe model explains 72.02% of the variance in house prices\nThat means 27.98% of the variance is unexplained\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-7-output-2.png){width=951 height=566}\n:::\n:::\n\n\n::: {.callout-note}\nNote how our regression line is a significantly better fit than the baseline line. That leads to an R² value significantly above zero.\n:::\n\nR² is intuitive and easy to explain to non-technical audiences. But it has problems:\n\n**Problem 1: R² always increases when you add features**, even if those features are random noise. Add 100 random columns to your data, and R² will go up, even though the model hasn't actually improved.\n\n**Problem 2: R² doesn't tell you if your predictions are biased**. R² tells you how close to a line your actual vs predicted values are. What's one easy way to get a straight line? Just predict the same y-value every single time! Congrats, you have a perfectly straight line, and thus have an R² of 100%! R² doesn't tell you if your predictions are biased. You could have a high R² but still systematically overpredict or underpredict in certain ranges.\n\n**Problem 3: R² can be negative on test data** if your model is terrible. That's a good warning sign, actually.\n\n### 3.4 Adjusted R² {#ch2-3-4}\n\nAdjusted R² fixes the \"adding features always increases R²\" problem. It penalizes you for adding features that don't actually help.\n\nThe formula adjusts R² based on the number of features (p) and number of observations (n):\n\n**Adjusted R² = $1 - \\displaystyle\\biggl[\\frac{(1 - R^2)(n - 1)}{(n - p - 1)}\\biggr]$**\n\nAdd a useless feature? Regular R² might go from 0.75 to 0.751. But adjusted R² might drop from 0.75 to 0.748 because the penalty for adding a feature outweighs the tiny improvement.\n\n**When to use Adjusted R²:**\n- When comparing models with different numbers of features\n- When you're worried about overfitting\n- When presenting results to stakeholders (it's more honest)\n\n**When regular R² is fine:**\n- When comparing models with the same number of features\n- When you're more concerned about prediction accuracy than interpretation\n\n**Real-World Example: When Many Features Mislead**\n\nLet's see this with real census data from Staten Island. We'll predict income using many demographic features:\n\n::: {#638a41ab .cell execution_count=7}\n``` {.python .cell-code}\n# Load NYC census tract data\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Filter to just Staten Island\nstaten_island_data = nyc_census[nyc_census['Borough'] == 'Staten Island'].copy()\n\n# Remove rows with missing income (our target)\nstaten_island_data = staten_island_data.dropna(subset=['Income'])\n\n# Select features (exclude identifiers and the target)\nfeature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',\n                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',\n                'ChildPoverty', 'Professional', 'Service', 'Office',\n                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',\n                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',\n                'Unemployment']\n\n# Prepare data (drop rows with any missing values)\nstaten_island_clean = staten_island_data[feature_cols + ['Income']].dropna()\n\nX_si = staten_island_clean[feature_cols].values\ny_si = staten_island_clean['Income'].values\n\nprint(f\"\\n{len(staten_island_clean)} observations\")\nprint(f\"Number of features: {len(feature_cols)}\")\nprint(f\"Ratio of features to observations (p/n): {len(feature_cols)/len(staten_island_clean):.3f}\")\n\n# Split data\nX_train_si, X_test_si, y_train_si, y_test_si = train_test_split(\n    X_si, y_si, test_size=0.3, random_state=42\n)\n\n# Fit model\nmodel_si = LinearRegression()\nmodel_si.fit(X_train_si, y_train_si)\n\n# Calculate metrics\ntrain_r2_si = model_si.score(X_train_si, y_train_si)\ntest_r2_si = model_si.score(X_test_si, y_test_si)\n\n# Calculate Adjusted R² for training set\nn_train = len(X_train_si)\np_si = X_train_si.shape[1]\nadj_r2_train_si = 1 - ((1 - train_r2_si) * (n_train - 1) / (n_train - p_si - 1))\n\n# Calculate Adjusted R² for test set\nn_test = len(X_test_si)\nadj_r2_test_si = 1 - ((1 - test_r2_si) * (n_test - 1) / (n_test - p_si - 1))\n\n# Visualize the comparison\nmetrics_si = pd.DataFrame({\n    'Metric': ['R²', 'Adjusted R²', 'R²', 'Adjusted R²'],\n    'Dataset': ['Training', 'Training', 'Test', 'Test'],\n    'Value': [train_r2_si, adj_r2_train_si, test_r2_si, adj_r2_test_si]\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx_pos = [0, 1, 3, 4]\ncolors = ['skyblue', 'lightcoral', 'skyblue', 'lightcoral']\nbars = ax.bar(x_pos, metrics_si['Value'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n\n# Add value labels on bars (handle negative values)\nfor i, (pos, val) in enumerate(zip(x_pos, metrics_si['Value'])):\n    y_offset = 0.02 if val >= 0 else -0.05\n    ax.text(pos, val + y_offset, f'{val:.3f}', ha='center', fontsize=10, weight='bold')\n\nax.set_ylabel('Score', fontsize=12)\nax.set_title(f'R² vs Adjusted R² with {p_si} Features', fontsize=14)\nax.set_xticks(x_pos)\nax.set_xticklabels(['R²\\n\\nTraining', 'Adj R²\\n\\nTraining', 'R²\\n\\nTest', 'Adj R²\\n\\nTest'], fontsize=11)\n# Set y-limits to accommodate negative adjusted R² values\ny_min = min(metrics_si['Value'].min() - 0.1, -0.1)\nax.set_ylim(y_min, 1.1)\nax.axhline(y=0, color='black', linewidth=0.8, linestyle='-')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n107 observations\nNumber of features: 30\nRatio of features to observations (p/n): 0.280\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-8-output-2.png){width=951 height=563}\n:::\n:::\n\n\nSee the problem? With 30 features and a small number of Staten Island census tracts, the picture is alarming:\n\n1. **Adjusted R² can be negative** - When test adjusted R² is negative, the model performs *worse* than just predicting the mean after accounting for model complexity. The penalty for using so many features overwhelms any predictive value. Remember, this is possible because the model _learned_ from the training data, but is _evaluated_ on the test data. What was learned from the training data is leading the model astray on the test data.\n2. **Test R² drops significantly from training** - the model doesn't generalize well\n3. **The high p/n ratio is the culprit** - too many features relative to observations\n\nThis is a realistic scenario where you might look at training R² and think \"Great, decent fit!\" but adjusted R² and test performance reveal the truth: the model is overfitted and unreliable.\n\n::: {.callout-warning}\n**Rule of thumb:** When your p/n ratio exceeds 0.10 (1 feature per 10 observations), be very skeptical of regular R². Always check Adjusted R² and test set performance. Better yet, use cross-validation to get a more honest assessment.\n:::\n\n### Learning outcomes: {#ch2-3-outcomes}\n**_By hand_ you should be able to:**\n\n- Explain what MSE and MAE are, and how they penalize a model differently from one another\n- Explain how to interpret $R^2$ values, and explain the three common problems associated with $R^2$\n- Explain how adjusted $R^2$ differs from adjusted $R^2$, and what problems it attempts to solve\n\n---\n\n## 4. Residual Analysis: Your Diagnostic Tool {#ch2-4}\n\nMetrics like R² and MSE tell you *how much* error your model makes. But residual plots tell you *where* and *why* the model is making mistakes. This is where you catch problems before they bite you in production.\n\n### 4.1 What Are Residuals? {#ch2-4-1}\n\nA residual is simple: **residual = actual value - predicted value**.\n\nIf your model predicts a house is worth $250,000 but it's actually worth $300,000, the residual is $50,000. Positive residual means you underpredicted. Negative residual means you overpredicted.\n\nWhy do residuals matter more than just looking at errors? Because the *pattern* of residuals reveals whether your model's assumptions are violated. Random residuals? Great. Systematic patterns? Problem.\n\n::: {#a72d696a .cell execution_count=8}\n``` {.python .cell-code}\n# Load NYC census tract data (all boroughs this time)\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Remove rows with missing income (our target)\nnyc_census = nyc_census.dropna(subset=['Income'])\n\n# Select features (exclude identifiers and the target)\nfeature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',\n                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',\n                'ChildPoverty', 'Professional', 'Service', 'Office',\n                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',\n                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',\n                'Unemployment']\n\n# Prepare data (drop rows with any missing values)\nnyc_census = nyc_census[feature_cols + ['Income']].dropna()\n\nX_nyc = nyc_census[feature_cols].values\ny_nyc = nyc_census['Income'].values\n\n# Split data\nX_train_nyc, X_test_nyc, y_train_nyc, y_test_nyc = train_test_split(\n    X_nyc, y_nyc, test_size=0.3, random_state=1\n)\n\n# Fit model\nmodel_nyc = LinearRegression()\nmodel_nyc.fit(X_train_nyc, y_train_nyc)\n\n# Calculate residuals from our simple model\ny_pred_test = model_nyc.predict(X_test_nyc)\nresiduals = y_test_nyc - y_pred_test\n\n# Create a DataFrame for easy viewing\nresidual_df = pd.DataFrame({\n    'Actual': y_test_nyc.flatten(),\n    'Predicted': y_pred_test.flatten(),\n    'Residual': residuals.flatten()\n})\n\nresidual_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Actual</th>\n      <th>Predicted</th>\n      <th>Residual</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>87708.0</td>\n      <td>89123.902250</td>\n      <td>-1415.902250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21577.0</td>\n      <td>20985.111768</td>\n      <td>591.888232</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51595.0</td>\n      <td>59802.080768</td>\n      <td>-8207.080768</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102625.0</td>\n      <td>72263.008289</td>\n      <td>30361.991711</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>51045.0</td>\n      <td>58308.513690</td>\n      <td>-7263.513690</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice that the residuals all fluctuate around zero? That's expected for linear regression—the errors cancel out on average. But that doesn't mean the errors are acceptable! We need to look at the pattern.\n\n### 4.2 Residuals vs. Fitted Values Plot {#ch2-4-2}\n\nThis is the **single most important diagnostic plot** you'll make. It plots residuals (y-axis) against predicted values (x-axis).\n\n**What you want to see:** Random scatter around zero. No patterns, no trends, just noise.\n\n**What you don't want to see:** Curves, funnels, or systematic patterns. These indicate problems.\n\n::: {#07878611 .cell execution_count=9}\n``` {.python .cell-code}\n# Create the residuals vs fitted plot\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred_test, residuals, alpha=0.5, s=20)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.xlabel('Fitted Values (Predictions)', fontsize=12)\nplt.ylabel('Residuals', fontsize=12)\nplt.title('Residuals vs. Fitted Values', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Also show distribution of residuals\nplt.figure(figsize=(10, 5))\nplt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Residual', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.title('Distribution of Residuals', fontsize=14)\nplt.axvline(0, color='red', linestyle='--', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-10-output-1.png){width=847 height=528}\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-10-output-2.png){width=828 height=454}\n:::\n:::\n\n\nSee what's happening? The residuals are somewhat randomly scattered, but there's two small issues: \n\n1. The model tends to underpredict (positive residuals) for very low predictions. \n2. The model tends to overpredict (negative residuals) for very high predictions.\n\nThis suggests the relationship might not be perfectly linear. However, despite this, this shows a reasonably good fit overall, with normally distributed residuals centered around zero.\n\n### 4.3 What Patterns Tell You {#ch2-4-3}\n\nLet's look at specific patterns and what they mean:\n\n**Pattern 1: Curved Residuals (Non-Linearity)**\n\nIf residuals form a U-shape or inverted U-shape, your relationship isn't linear. The model systematically underpredicts in some regions and overpredicts in others.\n\n**Fix:** Try polynomial features, log transforms, or use a non-linear model.\n\n**Pattern 2: Funnel Shape (Heteroscedasticity)**\n\nIf residuals spread out as predictions increase (or decrease), you have non-constant variance. Maybe small houses have predictable prices, but mansion prices vary wildly.\n\n**Fix:** Transform the target variable (log, square root), use weighted regression, or use a model that handles heteroscedasticity.\n\n**Pattern 3: Outliers**\n\nPoints far from zero are outliers. A few outliers are normal. Many outliers mean something's wrong with your model or data.\n\n**Fix:** Investigate outliers (data errors? special cases?), consider robust regression, or use regularization.\n\nLet's demonstrate these patterns with synthetic data, comparing good vs. bad residual plots:\n\n::: {#4b7c664c .cell execution_count=10}\n``` {.python .cell-code}\n# Create three datasets: good fit, non-linear, and heteroscedastic\nnp.random.seed(42)\n\n# GOOD: Linear relationship with constant variance\nX_good = np.linspace(0, 10, 200).reshape(-1, 1)\ny_good = 5 * X_good.flatten() + np.random.normal(0, 5, 200)\n\nmodel_good = LinearRegression()\nmodel_good.fit(X_good, y_good)\ny_pred_good = model_good.predict(X_good)\nresiduals_good = y_good - y_pred_good\n\n# BAD: Non-linear relationship\nX_nonlinear = np.linspace(0, 10, 200).reshape(-1, 1)\ny_nonlinear = 2 * X_nonlinear.flatten()**2 + np.random.normal(0, 10, 200)\n\nmodel_nonlinear = LinearRegression()\nmodel_nonlinear.fit(X_nonlinear, y_nonlinear)\ny_pred_nonlinear = model_nonlinear.predict(X_nonlinear)\nresiduals_nonlinear = y_nonlinear - y_pred_nonlinear\n\n# BAD: Heteroscedasticity (funnel pattern)\nX_hetero = np.linspace(1, 10, 200).reshape(-1, 1)\ny_hetero = 5 * X_hetero.flatten() + np.random.normal(0, X_hetero.flatten(), 200)\n\nmodel_hetero = LinearRegression()\nmodel_hetero.fit(X_hetero, y_hetero)\ny_pred_hetero = model_hetero.predict(X_hetero)\nresiduals_hetero = y_hetero - y_pred_hetero\n\n# Create comparison plot\nfig, axes = plt.subplots(3, 2, figsize=(14, 15))\n\n# Row 1: GOOD - Linear data with constant variance\naxes[0, 0].scatter(X_good, y_good, alpha=0.5, s=10, color='green')\naxes[0, 0].plot(X_good, y_pred_good, 'r-', linewidth=2)\naxes[0, 0].set_title('GOOD: Linear Data, Constant Variance', fontsize=12, fontweight='bold', color='green')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('y')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].scatter(y_pred_good, residuals_good, alpha=0.5, s=10, color='green')\naxes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[0, 1].set_title('✓ Random Scatter (Good!)', fontsize=12, fontweight='bold', color='green')\naxes[0, 1].set_xlabel('Fitted Values')\naxes[0, 1].set_ylabel('Residuals')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Row 2: BAD - Non-linearity\naxes[1, 0].scatter(X_nonlinear, y_nonlinear, alpha=0.5, s=10, color='orange')\naxes[1, 0].plot(X_nonlinear, y_pred_nonlinear, 'r-', linewidth=2)\naxes[1, 0].set_title('BAD: Non-Linear Data with Linear Fit', fontsize=12, fontweight='bold', color='orange')\naxes[1, 0].set_xlabel('X')\naxes[1, 0].set_ylabel('y')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].scatter(y_pred_nonlinear, residuals_nonlinear, alpha=0.5, s=10, color='orange')\naxes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[1, 1].set_title('✗ Curved Pattern (Bad!)', fontsize=12, fontweight='bold', color='orange')\naxes[1, 1].set_xlabel('Fitted Values')\naxes[1, 1].set_ylabel('Residuals')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Row 3: BAD - Heteroscedasticity\naxes[2, 0].scatter(X_hetero, y_hetero, alpha=0.5, s=10, color='red')\naxes[2, 0].plot(X_hetero, y_pred_hetero, 'r-', linewidth=2)\naxes[2, 0].set_title('BAD: Data with Increasing Variance', fontsize=12, fontweight='bold', color='red')\naxes[2, 0].set_xlabel('X')\naxes[2, 0].set_ylabel('y')\naxes[2, 0].grid(True, alpha=0.3)\n\naxes[2, 1].scatter(y_pred_hetero, residuals_hetero, alpha=0.5, s=10, color='red')\naxes[2, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[2, 1].set_title('✗ Funnel Pattern (Bad!)', fontsize=12, fontweight='bold', color='red')\naxes[2, 1].set_xlabel('Fitted Values')\naxes[2, 1].set_ylabel('Residuals')\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Compare the three residual plots (right column):\")\nprint(\"  TOP (Green): Random scatter around zero = GOOD\")\nprint(\"  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)\")\nprint(\"  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-11-output-1.png){width=1333 height=1430}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCompare the three residual plots (right column):\n  TOP (Green): Random scatter around zero = GOOD\n  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)\n  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)\n```\n:::\n:::\n\n\nSee the difference?\n\n- **Top row (GOOD):** Residuals are randomly scattered around zero with constant spread. This is what you want!\n- **Middle row (BAD - Curved):** Residuals show a clear U-shaped pattern—the model systematically underpredicts at the extremes and overpredicts in the middle.\n- **Bottom row (BAD - Funnel):** Residuals fan out as predictions increase—variance is not constant.\n\n::: {.callout-warning}\nA high R² doesn't mean your assumptions are met! You can have R² = 0.95 but still have terrible residual patterns that indicate the model will fail on new data.\n:::\n\nThese plots are your early warning system. Learn to read them, and you'll catch problems before they become disasters.\n\n### 4.4 What to Do When Assumptions Fail {#ch2-4-4}\n\nOkay, you've identified a problem in your residual plots. Now what?\n\n**Problem: Non-Linearity (curved residual plot)**\n\n**Solutions:**\n\n1. **Polynomial features:** Add X², X³, etc. (covered in Section 4)\n2. **Transform features:** Try log(X), √X, or 1/X\n3. **Transform target:** Try log(y) or √y\n4. **Use a non-linear model:** Tree-based models, neural networks, etc.\n\n**Problem: Heteroscedasticity (funnel residual plot)**\n\n**Solutions:**\n\n1. **Transform target variable:** log(y) often stabilizes variance\n2. **Weighted least squares:** Give less weight to high-variance observations\n3. **Use robust standard errors:** Adjust your confidence intervals\n4. **Just accept it:** If you only care about predictions (not inference), heteroscedasticity matters less\n\n**Problem: Outliers**\n\n**Solutions:**\n\n1. **Investigate:** Are they data errors? Real but unusual observations?\n2. **Remove them:** Only if justified (e.g., data entry errors)\n3. **Use robust regression:** Huber regression, RANSAC, etc.\n4. **Use regularization:** Ridge and Lasso reduce outlier influence (covered in Sections 6-7)\n\n::: {.callout-tip}\nStart with the simplest fix first. A log transformation of the target variable often fixes multiple problems at once: non-linearity and heteroscedasticity.\n:::\n\n### Learning outcomes: {#ch2-4-outcomes}\n**_By hand_ you should be able to:**\n\n- Calculate residuals given the true values and the predicted values\n- Interpret a residuals vs fitted plot to understand your models predictions\n- Interpret a histogram of residuals to understand your models predictions\n- Interpret the three common patterns in residual plots (section 4.3)\n\n---\n\n## 5. Polynomial Regression: Handling Non-Linearity {#ch2-5}\n\nWhat do you do when your residual plot shows a clear curve? The relationship isn't linear, so a straight line won't work. This is where polynomial regression saves you.\n\n### 5.1 When Linear Isn't Enough {#ch2-5-1}\n\nReal relationships are rarely perfectly linear. Temperature and ice cream sales? Definitely curved (sales don't go negative when it's cold, and they plateau when it's hot).\n\nPolynomial regression lets you fit curves while still using linear regression. The trick? Create new features that are powers of your original features: X², X³, etc. Then fit a linear model to these polynomial features.\n\nHere's the key insight: **polynomial regression is still linear regression**. It's linear in the *coefficients*, even though the relationship with X is non-linear. The model is y = β₀ + β₁X + β₂X² + β₃X³, which is a linear combination of the features [X, X², X³].\n\n### 5.2 Creating Polynomial Features {#ch2-5-2}\n\nLet's see this in action. We'll create polynomial features and fit them to data with a clear non-linear relationship.\n\n::: {#aeeb63a0 .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Use data with a non-linear relationship\n# Let's explore Unemployment vs Income (often a non-linear relationship)\nnyc_poly = nyc_census[['Unemployment', 'Income']].dropna()\n\n# Filter to only unemployment under 30% (ignore outliers)\nnyc_poly = nyc_poly[nyc_poly['Unemployment'] < 30]\n\nX_unemp = nyc_poly[['Unemployment']].values\ny_income_poly = nyc_poly['Income'].values\n\n# Split the data\nX_unemp_train, X_unemp_test, y_income_poly_train, y_income_poly_test = train_test_split(\n    X_unemp, y_income_poly, test_size=0.2, random_state=42\n)\n\n# First, try linear regression\nmodel_linear_unemp = LinearRegression()\nmodel_linear_unemp.fit(X_unemp_train, y_income_poly_train)\ny_pred_linear_unemp = model_linear_unemp.predict(X_unemp_test)\n\nprint(\"Linear Model:\")\nprint(f\"R² = {model_linear_unemp.score(X_unemp_test, y_income_poly_test):.4f}\")\n\n# Now try polynomial regression (degree 2)\npoly_2 = PolynomialFeatures(degree=2, include_bias=False)\nX_unemp_train_poly2 = poly_2.fit_transform(X_unemp_train)\nX_unemp_test_poly2 = poly_2.transform(X_unemp_test)\n\nprint(f\"\\nOriginal features shape: {X_unemp_train.shape}\")\nprint(f\"Polynomial features shape: {X_unemp_train_poly2.shape}\")\nprint(f\"\\nFeature names: {poly_2.get_feature_names_out(['Unemployment'])}\")\n\nmodel_poly2 = LinearRegression()\nmodel_poly2.fit(X_unemp_train_poly2, y_income_poly_train)\ny_pred_poly2 = model_poly2.predict(X_unemp_test_poly2)\n\nprint(\"\\nDegree-2 Polynomial Model:\")\nprint(f\"R² = {model_poly2.score(X_unemp_test_poly2, y_income_poly_test):.4f}\")\nprint(f\"Coefficients: {model_poly2.coef_}\")\n\n# Visualize the difference\nplt.figure(figsize=(10, 6))\n# Sort for smooth plotting\nsort_idx = np.argsort(X_unemp_test.flatten())\nX_test_sorted = X_unemp_test.flatten()[sort_idx]\ny_pred_linear_sorted = y_pred_linear_unemp.flatten()[sort_idx]\ny_pred_poly2_sorted = y_pred_poly2.flatten()[sort_idx]\n\nplt.scatter(X_unemp_test, y_income_poly_test, alpha=0.3, s=10, label='Actual Data')\nplt.plot(X_test_sorted, y_pred_linear_sorted, 'r-', linewidth=2, label='Linear Model')\nplt.plot(X_test_sorted, y_pred_poly2_sorted, 'g-', linewidth=2, label='Degree-2 Polynomial')\nplt.xlabel('Unemployment Rate (%)', fontsize=12)\nplt.ylabel('Median Household Income ($)', fontsize=12)\nplt.title('Linear vs Polynomial Regression', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Model:\nR² = 0.2180\n\nOriginal features shape: (1670, 1)\nPolynomial features shape: (1670, 2)\n\nFeature names: ['Unemployment' 'Unemployment^2']\n\nDegree-2 Polynomial Model:\nR² = 0.2339\nCoefficients: [-5763.42735409   114.23513525]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-12-output-2.png){width=844 height=528}\n:::\n:::\n\n\nSee what happened? `PolynomialFeatures(degree=2)` created a new feature: Unemployment². The linear model then fits: `y = β₀ + β₁(Unemployment) + β₂(Unemployment²)`. This gives us a parabola, which can capture curvature that a straight line can't.\n\n::: {.callout-tip}\nHere we're using the `PolynomialFeatures` pipeline from scikit-learn. However, you can also just square (or cube, or whatever you want) the column, such as `df['unemp_poly2'] = df['unemp']**2`.\n:::\n\n### 5.3 Choosing Polynomial Degree {#ch2-5-3}\n\nHow do you know what degree to use? Degree 1 is linear. Degree 2 adds curvature. Degree 3 adds an S-curve. Higher degrees add more wiggles.\n\n**The problem:** Higher degree doesn't always mean better. You can overfit spectacularly with high-degree polynomials.\n\n**The solution:** Try multiple degrees and use a validation set (or cross-validation) to pick the best one.\n\n::: {#6ac1207f .cell execution_count=12}\n``` {.python .cell-code}\n# Try polynomials of degree 1 through 10\ndegrees = range(1, 11)\ntrain_scores = []\ntest_scores = []\n\nfor degree in degrees:\n    # Create polynomial features\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_unemp_train)\n    X_test_poly = poly.transform(X_unemp_test)\n\n    # Fit model\n    model = LinearRegression()\n    model.fit(X_train_poly, y_income_poly_train)\n\n    # Score\n    train_score = model.score(X_train_poly, y_income_poly_train)\n    test_score = model.score(X_test_poly, y_income_poly_test)\n\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n\n    print(f\"Degree {degree}: Train R² = {train_score:.4f}, Test R² = {test_score:.4f}\")\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, train_scores, 'o-', linewidth=2, label='Training R²')\nplt.plot(degrees, test_scores, 's-', linewidth=2, label='Test R²')\nplt.xlabel('Polynomial Degree', fontsize=12)\nplt.ylabel('R²', fontsize=12)\nplt.title('Model Performance vs Polynomial Degree', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(degrees)\nplt.show()\n\n# Find optimal degree\noptimal_degree = degrees[np.argmax(test_scores)]\nprint(f\"\\nOptimal polynomial degree: {optimal_degree} (Test R² = {max(test_scores):.4f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDegree 1: Train R² = 0.2718, Test R² = 0.2180\nDegree 2: Train R² = 0.2890, Test R² = 0.2339\nDegree 3: Train R² = 0.2904, Test R² = 0.2329\nDegree 4: Train R² = 0.2904, Test R² = 0.2317\nDegree 5: Train R² = 0.2923, Test R² = 0.2294\nDegree 6: Train R² = 0.2953, Test R² = 0.2225\nDegree 7: Train R² = 0.2955, Test R² = 0.2185\nDegree 8: Train R² = 0.2965, Test R² = 0.2265\nDegree 9: Train R² = 0.2960, Test R² = 0.2290\nDegree 10: Train R² = 0.2969, Test R² = 0.2255\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-13-output-2.png){width=823 height=528}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nOptimal polynomial degree: 2 (Test R² = 0.2339)\n```\n:::\n:::\n\n\nSee that? Training R² keeps increasing with degree—the model just memorizes the training data. But test R² peaks and then starts *decreasing*. That's overfitting. The model gets so wiggly it fits training noise instead of the true pattern.\n\n::: {.callout-warning}\n**Never choose polynomial degree based on training performance alone!** Always use validation data or cross-validation. Otherwise, you'll pick a high degree that overfits.\n:::\n\n### 5.4 The Overfitting Risk {#ch2-5-4}\n\nLet's visualize what high-degree polynomials do. They create absurd wiggles that fit every bump in the training data but fail on new data.\n\n::: {#b7956dcb .cell execution_count=13}\n``` {.python .cell-code}\n# Compare degree 2 vs degree 10\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Create small dataset to exaggerate overfitting\nnp.random.seed(42)\nX_small = np.linspace(0, 10, 30).reshape(-1, 1)\ny_small = 2 * X_small.flatten() + np.random.normal(0, 5, 30)\n\n# Degree 2\npoly_2_small = PolynomialFeatures(degree=2, include_bias=False)\nX_small_poly2 = poly_2_small.fit_transform(X_small)\nmodel_poly2_small = LinearRegression()\nmodel_poly2_small.fit(X_small_poly2, y_small)\n\n# Create smooth line for plotting\nX_plot = np.linspace(0, 10, 200).reshape(-1, 1)\nX_plot_poly2 = poly_2_small.transform(X_plot)\ny_plot_poly2 = model_poly2_small.predict(X_plot_poly2)\n\naxes[0].scatter(X_small, y_small, s=50, label='Training Data')\naxes[0].plot(X_plot, y_plot_poly2, 'r-', linewidth=2, label='Degree 2')\naxes[0].set_xlabel('X', fontsize=12)\naxes[0].set_ylabel('y', fontsize=12)\naxes[0].set_title('Degree 2: Reasonable Fit', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Degree 10\npoly_10_small = PolynomialFeatures(degree=10, include_bias=False)\nX_small_poly10 = poly_10_small.fit_transform(X_small)\nmodel_poly10_small = LinearRegression()\nmodel_poly10_small.fit(X_small_poly10, y_small)\n\nX_plot_poly10 = poly_10_small.transform(X_plot)\ny_plot_poly10 = model_poly10_small.predict(X_plot_poly10)\n\naxes[1].scatter(X_small, y_small, s=50, label='Training Data')\naxes[1].plot(X_plot, y_plot_poly10, 'r-', linewidth=2, label='Degree 10')\naxes[1].set_xlabel('X', fontsize=12)\naxes[1].set_ylabel('y', fontsize=12)\naxes[1].set_title('Degree 10: Overfitting!', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim(axes[0].get_ylim())  # Same y-axis for comparison\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Degree 2 - Training R²: {model_poly2_small.score(X_small_poly2, y_small):.4f}\")\nprint(f\"Degree 10 - Training R²: {model_poly10_small.score(X_small_poly10, y_small):.4f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-14-output-1.png){width=1335 height=470}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nDegree 2 - Training R²: 0.5739\nDegree 10 - Training R²: 0.6720\n```\n:::\n:::\n\n\nLook at that degree-10 model! It wiggles wildly to pass through training points. Training R² is higher, but the model is useless for prediction. It learned noise, not signal.\n\n**The Extreme Case: More Features Than Observations (p > n)**\n\nNow let's see what happens when you push this to the limit: more features than data points. This creates an **overdetermined system** where you can get perfect training fit (R² = 1.0) that means absolutely nothing.\n\n::: {#3d54667f .cell execution_count=14}\n``` {.python .cell-code}\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Load our Bronx census data again\nbronx_subset = nyc_census[nyc_census['Borough'] == 'Bronx'].copy()\nbronx_subset = bronx_subset.dropna(subset=['Income'])\n\n# Select just a few features to start\nbase_features = ['TotalPop', 'IncomePerCap', 'Poverty', 'Professional', 'Unemployment']\nbronx_tiny = bronx_subset[base_features + ['Income']].dropna()\n\n# Take only 30 census tracts (small sample)\nbronx_tiny_sample = bronx_tiny.sample(n=30, random_state=42)\n\nX_tiny = bronx_tiny_sample[base_features].values\ny_tiny = bronx_tiny_sample['Income'].values\n\nprint(f\"Starting with: {len(bronx_tiny_sample)} observations, {len(base_features)} features\")\n\n# Now create polynomial features to blow up the feature count\npoly_extreme = PolynomialFeatures(degree=4, include_bias=False)\nX_tiny_poly = poly_extreme.fit_transform(X_tiny)\n\nprint(f\"After polynomial expansion (degree 4): {X_tiny_poly.shape[0]} observations, {X_tiny_poly.shape[1]} features\")\nprint(f\"Features > Observations: {X_tiny_poly.shape[1] > X_tiny_poly.shape[0]}\")\n\n# Hold out just ONE observation for \"testing\"\nX_train_tiny = X_tiny_poly[:-1]\ny_train_tiny = y_tiny[:-1]\nX_test_tiny = X_tiny_poly[-1:]\ny_test_tiny = y_tiny[-1:]\n\nprint(f\"\\nTraining: {X_train_tiny.shape[0]} observations, {X_train_tiny.shape[1]} features\")\nprint(f\"p > n? {X_train_tiny.shape[1] > X_train_tiny.shape[0]}\")\n\n# Fit the model\nmodel_extreme = LinearRegression()\nmodel_extreme.fit(X_train_tiny, y_train_tiny)\n\n# Check performance\ntrain_r2_extreme = model_extreme.score(X_train_tiny, y_train_tiny)\ny_pred_train_extreme = model_extreme.predict(X_train_tiny)\ny_pred_test_extreme = model_extreme.predict(X_test_tiny)\n\ntrain_mse_extreme = mean_squared_error(y_train_tiny, y_pred_train_extreme)\ntest_error_extreme = abs(y_test_tiny[0] - y_pred_test_extreme[0])\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RESULTS: {X_train_tiny.shape[1]} features, {X_train_tiny.shape[0]} observations\")\nprint(f\"{'='*60}\")\nprint(f\"Training R²: {train_r2_extreme:.10f}\")\nprint(f\"Training MSE: {train_mse_extreme:.10f}\")\nprint(f\"\\nActual income (held-out): ${y_test_tiny[0]:,.0f}\")\nprint(f\"Predicted income: ${y_pred_test_extreme[0]:,.0f}\")\nprint(f\"Prediction error: ${test_error_extreme:,.0f}\")\nprint(f\"Prediction error (%): {100 * test_error_extreme / y_test_tiny[0]:.1f}%\")\n\n# Visualize the \"perfect\" fit\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Training \"perfection\"\naxes[0].scatter(y_train_tiny, y_pred_train_extreme, s=50, alpha=0.7)\naxes[0].plot([y_train_tiny.min(), y_train_tiny.max()],\n             [y_train_tiny.min(), y_train_tiny.max()],\n             'r--', linewidth=2, label='Perfect Prediction')\naxes[0].set_xlabel('Actual Income', fontsize=12)\naxes[0].set_ylabel('Predicted Income', fontsize=12)\naxes[0].set_title(f'Training: R² = {train_r2_extreme:.6f}', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Right: Test disaster\naxes[1].scatter([y_test_tiny[0]], [y_pred_test_extreme[0]], s=200, c='red',\n                marker='X', label='Test Prediction', zorder=3)\naxes[1].plot([y_train_tiny.min(), y_train_tiny.max()],\n             [y_train_tiny.min(), y_train_tiny.max()],\n             'r--', linewidth=2, label='Perfect Prediction')\naxes[1].set_xlabel('Actual Income', fontsize=12)\naxes[1].set_ylabel('Predicted Income', fontsize=12)\naxes[1].set_title(f'Test: Error = ${test_error_extreme:,.0f}', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting with: 30 observations, 5 features\nAfter polynomial expansion (degree 4): 30 observations, 125 features\nFeatures > Observations: True\n\nTraining: 29 observations, 125 features\np > n? True\n\n============================================================\nRESULTS: 125 features, 29 observations\n============================================================\nTraining R²: 1.0000000000\nTraining MSE: 0.0000000000\n\nActual income (held-out): $52,344\nPredicted income: $167,147\nPrediction error: $114,803\nPrediction error (%): 219.3%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-15-output-2.png){width=1333 height=470}\n:::\n:::\n\n\n**What just happened?**\n\nWhen p > n (more features than observations), the system becomes **underdetermined**. The model can perfectly fit every training point because it has enough degrees of freedom to pass through all of them. R² = 1.0 is **guaranteed** mathematically—but it's meaningless!\n\nThis is like drawing a curve through 5 points with a 10th-degree polynomial. You have so much flexibility that you can hit every point exactly. But the curve between points? Pure nonsense.\n\n::: {.callout-warning}\n**DANGER: The Perfect R² Trap**\n\nIf you ever see R² = 1.0000 (or extremely close) on training data, be immediately suspicious:\n\n1. Check if p ≥ n (features ≥ observations)\n2. Check if you accidentally included the target variable as a feature\n3. Check for data leakage (future information in features)\n4. Check if you have duplicate rows\n\nA \"perfect\" fit is almost never real. It's almost always a problem.\n:::\n\n**Real-world implications:**\n\nThis happens more often than you think:\n\n- **Small datasets:** Medical studies with 50 patients but 200 genetic markers\n- **High-dimensional data:** Images, text, genomics where features vastly outnumber samples\n- **Time series:** Predicting tomorrow with 100 technical indicators but only 30 days of data\n\nThe solution? **Regularization** (covered in the next sections) or getting more data. Never trust a model where p/n > 1.0, and be very cautious when p/n > 0.3.\n\n**Key takeaways:**\n\n- Polynomial features let you model non-linear relationships with linear regression\n- Choose degree using validation data, not training data\n- Lower degree often generalizes better than higher degree\n- When p ≥ n, you get perfect training fit but meaningless predictions\n- Regularization (covered next) can help control overfitting in polynomial models\n\n### Learning outcomes: {#ch2-5-outcomes}\n**_By hand_ you should be able to:**\n\n- Understand what polynomial features contribute to a linear regression model\n- Understand what it means to be \"linear regression\" when using terms like $X^2$, $X^3$, etc.\n- Understanding linear regression code using polynmomial regression\n- Modifying existing linear regression code to change polynomial regression features (e.g. adding additional terms, changing the variable being transformed, etc)\n- Interpreting model performance vs polynomial degree graphs to pick an optimal degree\n- Understand and explain the problems arising for picking too high of a polynomial degree (overfitting)\n- Understand and explain why picking a degree higher than the number of data points can lead to \"perfect\" predictions which don't generalize\n\n---\n\n## 6. Multicollinearity: When Features Are Too Similar {#ch2-6}\n\nImagine trying to predict house prices using both \"square footage\" and \"square meters\" as separate features. They contain basically the same information! This creates multicollinearity, and it breaks coefficient interpretation in sneaky ways.\n\n### 6.1 What Is Multicollinearity? {#ch2-6-1}\n\n**Multicollinearity** means your features are highly correlated with each other. One feature can be predicted fairly well from others.\n\nWhy is this a problem? Linear regression tries to isolate the effect of each feature while \"holding others constant.\" But if two features move together, you can't hold one constant while changing the other—they're tied together!\n\nThe result: **coefficient estimates become unstable**. Add or remove one observation, and coefficients swing wildly. Even worse, a feature that's clearly important might show up with a tiny coefficient (or even the wrong sign!) because its effect is \"stolen\" by correlated features.\n\nLet's create an example to see this:\n\n::: {#74bc4a4a .cell execution_count=15}\n``` {.python .cell-code}\n# Create dataset with multicollinearity\nnp.random.seed(42)\nn = 1000\n\n# X1 is random\nX1 = np.random.normal(0, 1, n)\n\n# X2 is highly correlated with X1\nX2 = X1 + np.random.normal(0, 0.1, n)  # Almost identical to X1\n\n# X3 is independent\nX3 = np.random.normal(0, 1, n)\n\n# True relationship: y = 5*X1 + 0*X2 + 3*X3 + noise\n# Note: X2 has NO effect, but it's correlated with X1\ny_multi = 5*X1 + 3*X3 + np.random.normal(0, 1, n)\n\n# Create DataFrame\ndf_multi = pd.DataFrame({\n    'X1': X1,\n    'X2': X2,\n    'X3': X3,\n    'y': y_multi\n})\n\n# Check correlation matrix\nprint(\"Correlation Matrix:\")\nprint(df_multi.corr())\n\n# Visualize correlations\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_multi.corr(), annot=True, cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Feature Correlation Matrix', fontsize=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrelation Matrix:\n          X1        X2        X3         y\nX1  1.000000  0.994818  0.022129  0.843973\nX2  0.994818  1.000000  0.020966  0.838088\nX3  0.022129  0.020966  1.000000  0.525535\ny   0.843973  0.838088  0.525535  1.000000\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-16-output-2.png){width=563 height=508}\n:::\n:::\n\n\nSee that? X1 and X2 have a correlation of about 0.995. They're nearly identical. Now watch what happens when we fit a regression:\n\n::: {#bfe811dc .cell execution_count=16}\n``` {.python .cell-code}\n# Fit model with all features\nX_multi = df_multi[['X1', 'X2', 'X3']].values\ny_multi_target = df_multi['y'].values\n\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_multi, y_multi_target)\n\nprint(\"Coefficients with multicollinearity:\")\nfor i, name in enumerate(['X1', 'X2', 'X3']):\n    print(f\"  {name}: {model_multi.coef_[i]:.4f}\")\n\nprint(f\"\\nRemember: True coefficients are X1=5, X2=0, X3=3\")\nprint(\"But the model can't tell X1 and X2 apart!\")\n\n# Fit model with only X1 and X3 (no multicollinearity)\nX_clean = df_multi[['X1', 'X3']].values\nmodel_clean = LinearRegression()\nmodel_clean.fit(X_clean, y_multi_target)\n\nprint(\"\\nCoefficients without multicollinearity:\")\nfor i, name in enumerate(['X1', 'X3']):\n    print(f\"  {name}: {model_clean.coef_[i]:.4f}\")\n\nprint(\"\\nMuch closer to the true values!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients with multicollinearity:\n  X1: 5.5507\n  X2: -0.5675\n  X3: 3.0223\n\nRemember: True coefficients are X1=5, X2=0, X3=3\nBut the model can't tell X1 and X2 apart!\n\nCoefficients without multicollinearity:\n  X1: 4.9855\n  X3: 3.0229\n\nMuch closer to the true values!\n```\n:::\n:::\n\n\n### 6.2 Why It's a Problem {#ch2-6-2}\n\nLet me be blunt: **multicollinearity doesn't hurt prediction accuracy**. Your R² will be fine. Your predictions will be fine. So why do we care?\n\n**Problem 1: Coefficient interpretation becomes meaningless**\n\nIf X1 and X2 are highly correlated, the model might give X1 a coefficient of 10 and X2 a coefficient of -5. Or it might do the opposite: X1 = -5, X2 = 10. Or X1 = 2.5, X2 = 2.5. All three give similar predictions! But which feature is \"really\" important? You can't tell. \n\n::: {.callout-note}\nOne of the main benefits of linear regression models is their interpretability. You can look at the coefficients and read off what they tell you. If you lose that, then you might as well not use a linear model!\n:::\n\n**Problem 2: Coefficient instability**\n\nSmall changes in the data cause huge swings in coefficients. Add 10 new observations? Coefficients might flip signs. This makes the model untrustworthy for understanding relationships.\n\n**Problem 3: Hard to select features**\n\nIf X1 and X2 are correlated, dropping one might barely hurt performance, but dropping both kills it. This makes feature selection confusing.\n\n**When to care:**\n\n- You need to interpret coefficients (e.g., explaining to stakeholders)\n- You want to identify the \"most important\" features\n- You're making causal claims\n\n### 6.3 Detecting Multicollinearity {#ch2-6-3}\n\n**Method 1: Correlation Matrix**\n\nThe simplest approach. Look for correlations close to ±1 (say, above 0.8 or 0.9).\n\n::: {#604daf7a .cell execution_count=17}\n``` {.python .cell-code}\n# Use NYC census data\nfeatures_for_vif = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']\nnyc_vif = nyc_census[features_for_vif].dropna()\nX_vif = nyc_vif\n\n# Correlation matrix\ncorr_matrix = X_vif.corr()\n\n# Visualize\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, fmt='.3f')\nplt.title('Feature Correlation Matrix - NYC Census Data', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Identify high correlations\nhigh_corr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if abs(corr_matrix.iloc[i, j]) > 0.7:\n            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\nprint(\"\\nHighly correlated feature pairs (|correlation| > 0.7):\")\nfor feat1, feat2, corr in high_corr_pairs:\n    print(f\"  {feat1} & {feat2}: {corr:.3f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-18-output-1.png){width=842 height=757}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nHighly correlated feature pairs (|correlation| > 0.7):\n  Professional & IncomePerCap: 0.782\n  Poverty & ChildPoverty: 0.911\n```\n:::\n:::\n\n\n**Method 2: Variance Inflation Factor (VIF)**\n\nVIF measures how much the variance of a coefficient is \"inflated\" due to multicollinearity.\n\n**VIF interpretation:**\n\n- VIF = 1: No multicollinearity\n- VIF = 1-5: Moderate multicollinearity (usually okay)\n- VIF = 5-10: High multicollinearity (concerning)\n- VIF > 10: Severe multicollinearity (definitely a problem)\n\n::: {#3bbb508f .cell execution_count=18}\n``` {.python .cell-code}\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = features_for_vif\nvif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(features_for_vif))]\n\n# Visualize VIF\nplt.figure(figsize=(10, 6))\nplt.barh(vif_data[\"Feature\"], vif_data[\"VIF\"])\nplt.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='VIF = 5 (Moderate)')\nplt.axvline(x=10, color='red', linestyle='--', linewidth=2, label='VIF = 10 (Severe)')\nplt.xlabel('VIF', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Variance Inflation Factors', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3, axis='x')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-19-output-1.png){width=896 height=528}\n:::\n:::\n\n\nSee features with high VIF? Those are the ones tangled up with others. The solution? Remove one of the correlated features, combine them, or use regularization (which we'll cover next).\n\n\n### Learning outcomes: {#ch2-6-outcomes}\n**_By hand_ you should be able to:**\n\n- Explain what multicollinearity is, and why it can cause issues with predictions in linear regression models\n- Interpret feature correlation matrices to find highly correlated features\n- Interpret VIF values to find highly correlated features\n\n---\n\n## 7. Ridge Regression: L2 Regularization {#ch2-7}\n\nMulticollinearity makes coefficients unstable. Polynomial features risk overfitting. The solution to both? **Regularization**. It's one of the most important ideas in machine learning.\n\n### 7.1 The Regularization Idea {#ch2-7-1}\n\nHere's the core insight: **penalize large coefficients**. Instead of just minimizing error, also minimize the size of coefficients. The model has to balance two goals:\n\n1. Fit the training data well (low error)\n2. Keep coefficients small (low complexity)\n\nWhy does this help? Large coefficients make the model sensitive to small changes in features. By shrinking coefficients, you make the model more stable. We want to _discourage_ the model from making wild swings in predictions from very small changes in the data. Imagine if you had a coefficient of 10,000,000. Then a one unit change in x would add ten million to your predicted value. That's almost certainly not desirable.\n\nYes, you're intentionally introducing bias (the model won't fit training data perfectly). But you reduce variance (the model generalizes better to new data).\n\nThis is the famous **bias-variance tradeoff**. A little bias for a lot less variance is usually a great deal.\n\n### 7.2 How Ridge Works {#ch2-7-2}\n\nRidge regression adds a penalty term to the loss function:\n\n$$\n\\text{Loss} = \\text{MSE} + \\alpha \\displaystyle\\sum\\text{coefficients}^2\n$$\n\nThat second term is the L2 penalty (sum of squared coefficients). The **α** (alpha) parameter controls how much you penalize:\n\n- α = 0: No penalty, just regular linear regression\n- Small α: Light penalty, coefficients shrink a little\n- Large α: Heavy penalty, coefficients shrink toward zero (but never reach exactly zero)\n\n::: {.callout-note}\nRemember that we want as small a loss as possible. The goal of machine learning algorithms are to find the parameters (e.g. coefficients of the regression model) that minimize the loss function. Regularization _adds_ the coefficients themselves (squared) to the loss function, meaning the algorithm is penalized for choosing large coefficients.\n:::\n\nLet's see it in action using our synthetic data with extreme multicollinearity from section 5.1. Remember: X1 and X2 are nearly identical (correlation ≈ 0.995), and the true model is y = 5*X1 + 3*X3.\n\n::: {#62c6e7ee .cell execution_count=19}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\n# Use the multicollinearity data we created earlier\n# X1 and X2 are highly correlated; true coefficients are X1=5, X2=0, X3=3\nX_ridge = df_multi[['X1', 'X2', 'X3']].values\ny_ridge = df_multi['y'].values\n\n# Split data\nX_ridge_train, X_ridge_test, y_ridge_train, y_ridge_test = train_test_split(\n    X_ridge, y_ridge, test_size=0.2, random_state=42\n)\n\n# IMPORTANT: Always scale features before regularization!\n# Features with larger scales get penalized more\nscaler = StandardScaler()\nX_ridge_train_scaled = scaler.fit_transform(X_ridge_train)\nX_ridge_test_scaled = scaler.transform(X_ridge_test)\n\n# Fit regular linear regression\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_ridge_train_scaled, y_ridge_train)\nprint(\"Linear Regression (no regularization):\")\nprint(f\"  Train R²: {model_lr.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lr.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"  Coefficients: {model_lr.coef_}\")\nprint(f\"  Coefficient magnitudes: {np.abs(model_lr.coef_).sum():.4f}\")\nprint(f\"\\n  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)\")\nprint(f\"  True coefficients: X1=5, X2=0, X3=3\")\nprint(f\"  But with multicollinearity, coefficients are unstable!\")\n\n# Fit Ridge with alpha=1\nmodel_ridge = Ridge(alpha=1.0)\nmodel_ridge.fit(X_ridge_train_scaled, y_ridge_train)\nprint(\"\\nRidge Regression (alpha=1.0):\")\nprint(f\"  Train R²: {model_ridge.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_ridge.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"  Coefficients: {model_ridge.coef_}\")\nprint(f\"  Coefficient magnitudes: {np.abs(model_ridge.coef_).sum():.4f}\")\nprint(f\"\\n  Ridge distributes the X1 effect across X1 and X2 more evenly\")\nprint(f\"  Coefficients are more stable!\")\n\n# Visualize coefficient shrinkage\nfeature_names = ['X1', 'X2', 'X3']\ncoef_comparison = pd.DataFrame({\n    'Feature': feature_names,\n    'Linear Regression': model_lr.coef_,\n    'Ridge (α=1)': model_ridge.coef_,\n    'True Value': [5, 0, 3]\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\nax.bar(x - width, coef_comparison['True Value'], width, label='True Coefficients', alpha=0.8, color='green')\nax.bar(x + width, coef_comparison['Ridge (α=1)'], width, label='Ridge (α=1)', alpha=0.8, color='red')\nax.bar(x, coef_comparison['Linear Regression'], width, label='Linear Regression', alpha=0.8, color='blue')\nax.set_xlabel('Feature', fontsize=12)\nax.set_ylabel('Coefficient Value', fontsize=12)\nax.set_title('Ridge Handles Multicollinearity Better Than OLS', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(feature_names)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression (no regularization):\n  Train R²: 0.9700\n  Test R²: 0.9665\n  Coefficients: [ 5.71502205 -0.82461178  3.00187963]\n  Coefficient magnitudes: 9.5415\n\n  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)\n  True coefficients: X1=5, X2=0, X3=3\n  But with multicollinearity, coefficients are unstable!\n\nRidge Regression (alpha=1.0):\n  Train R²: 0.9699\n  Test R²: 0.9670\n  Coefficients: [ 5.07662499 -0.1891781   2.99844018]\n  Coefficient magnitudes: 8.2642\n\n  Ridge distributes the X1 effect across X1 and X2 more evenly\n  Coefficients are more stable!\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-20-output-2.png){width=951 height=566}\n:::\n:::\n\n\nSee what happened? With regular linear regression, the coefficients for X1 and X2 were far from the true values. Ridge regression distributes the effect more evenly between the correlated features. While Ridge coefficients still aren't perfect (X2 should be 0), they're much more stable and reasonable.\n\n::: {.callout-warning}\n**Always scale features before using Ridge!** Features with large scales get penalized more heavily than features with small scales. Standardizing (mean=0, std=1) ensures all features are treated equally. Imagine if I wrote your salary in pennies. Each year you get a raise, and the raise would look enormous to the model! \n:::\n\n### 7.3 Choosing Alpha {#ch2-7-3}\n\nHow do you pick $\\alpha$? Try many values and use cross-validation to see which generalizes best. Let's continue with our synthetic multicollinearity data:\n\n::: {#c06d5b4b .cell execution_count=20}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\n# Try many alpha values\nalphas = np.logspace(-2, 2, 50)  # From 0.01 to 100\ntrain_scores_ridge = []\ntest_scores_ridge = []\ncv_scores_ridge = []\ncoef_sum_ridge = []\n\nfor alpha in alphas:\n    model = Ridge(alpha=alpha)\n\n    # Train score\n    model.fit(X_ridge_train_scaled, y_ridge_train)\n    train_scores_ridge.append(model.score(X_ridge_train_scaled, y_ridge_train))\n\n    # Test score\n    test_scores_ridge.append(model.score(X_ridge_test_scaled, y_ridge_test))\n\n    # Cross-validation score (5-fold)\n    cv_score = cross_val_score(model, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    cv_scores_ridge.append(cv_score)\n\n    # Track total coefficient magnitude\n    coef_sum_ridge.append(np.abs(model.coef_).sum())\n\n# Plot results\nfig = plt.figure(figsize=(8, 5))\n\n# Plot 1: R² scores\nplt.plot(alphas, train_scores_ridge, label='Training R²', linewidth=2)\nplt.plot(alphas, test_scores_ridge, label='Test R²', linewidth=2)\nplt.plot(alphas, cv_scores_ridge, label='CV R² (5-fold)', linewidth=2, linestyle='--')\nplt.xscale('log')\nplt.xlabel('Alpha (log scale)', fontsize=12)\nplt.ylabel('R²', fontsize=12)\nplt.title('Ridge Regression: Choosing Alpha', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\noptimal_alpha = alphas[np.argmax(cv_scores_ridge)]\nplt.axvline(optimal_alpha, color='red', linestyle=':', linewidth=2, label='Optimal Alpha')\n\nplt.tight_layout()\nplt.show()\n\n# Best alpha\nprint(f\"Optimal alpha (by cross-validation): {optimal_alpha:.4f}\")\nprint(f\"Best CV R²: {max(cv_scores_ridge):.4f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-21-output-1.png){width=759 height=468}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal alpha (by cross-validation): 0.0100\nBest CV R²: 0.9692\n```\n:::\n:::\n\n\nAs $\\alpha$ increases:\n\n- Training R² decreases (more bias, less fit to training data), however...\n- Test/CV R² are still high, and even increase for a while (multicollinearity handled), then sharply decreases (too much bias)\n\nThe optimal $\\alpha$ is where test/CV performance peaks. For this synthetic data with severe multicollinearity, Ridge helps stabilize coefficients even with small α values.\n\n### Learning outcomes: {#ch2-7-outcomes}\n**_By hand_ you should be able to:**\n\n- Understand mathematically (at a high level) how ridge regression penalizes models for picking very large coefficients\n- Understand the role of $\\alpha$ in regularization (how larger or smaller $\\alpha$ values change the penalty)\n- Interpret a graph showing $\\alpha$ versus $R^2$ to determine the optimal $\\alpha$\n\n---\n\n## 8. Lasso Regression: L1 Regularization {#ch2-8}\n\nRidge is great, but it never says \"this feature is useless.\" Lasso does. It performs **automatic feature selection** by setting some coefficients to exactly zero.\n\n### 8.1 How Lasso Differs from Ridge {#ch2-8-1}\n\nLasso uses L1 regularization instead of L2:\n\n$$ \n\\text{Loss} = \\text{MSE} + \\alpha \\sum |\\text{coefficients}|\n$$\n\nThe difference? Instead of squaring coefficients (Ridge), we take their absolute value (Lasso). This small change has a huge impact: **Lasso can set coefficients to exactly zero**.\n\nWhy? It's geometry. The L1 penalty creates \"corners\" where the optimal solution often has some coefficients at exactly zero. Ridge's L2 penalty is smooth, so coefficients approach zero but never arrive.\n\n### 8.2 Lasso for Feature Selection {#ch2-8-2}\n\nLasso is feature selection built into the regression. As α increases, Lasso zeroes out features one by one, keeping only the most important.\n\n::: {#403c8968 .cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Lasso\n\nalphas_path = np.logspace(-2, 2, 100)\n\n# Fit Lasso with alpha=0.01\nmodel_lasso = Lasso(alpha=0.01, max_iter=10000)\nmodel_lasso.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(\"Lasso Regression (alpha=0.01):\")\nprint(f\"  Train R²: {model_lasso.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lasso.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"\\nCoefficients:\")\nfor feature, coef in zip(feature_names, model_lasso.coef_):\n    if abs(coef) < 0.0001:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED!\")\n    else:\n        print(f\"  {feature}: {coef:.6f}\")\n\n# Compare with larger alpha\nmodel_lasso_strong = Lasso(alpha=0.1, max_iter=10000)\nmodel_lasso_strong.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(\"\\nLasso Regression (alpha=0.1):\")\nprint(f\"  Train R²: {model_lasso_strong.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lasso_strong.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"\\nCoefficients:\")\nfor feature, coef in zip(feature_names, model_lasso_strong.coef_):\n    if abs(coef) < 0.0001:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED!\")\n    else:\n        print(f\"  {feature}: {coef:.6f}\")\n\n# Visualize feature selection\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\nax.bar(x - width, model_lr.coef_, width, label='Linear Regression', alpha=0.8)\nax.bar(x, model_lasso.coef_, width, label='Lasso (α=0.01)', alpha=0.8)\nax.bar(x + width, model_lasso_strong.coef_, width, label='Lasso (α=0.1)', alpha=0.8)\nax.set_xlabel('Feature', fontsize=12)\nax.set_ylabel('Coefficient Value', fontsize=12)\nax.set_title('Lasso Feature Selection', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(feature_names)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLasso Regression (alpha=0.01):\n  Train R²: 0.9698\n  Test R²: 0.9671\n\nCoefficients:\n  X1: 4.884947\n  X2: 0.000000 → ELIMINATED!\n  X3: 2.992327\n\nLasso Regression (alpha=0.1):\n  Train R²: 0.9692\n  Test R²: 0.9680\n\nCoefficients:\n  X1: 4.797478\n  X2: 0.000000 → ELIMINATED!\n  X3: 2.904858\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-22-output-2.png){width=951 height=566}\n:::\n:::\n\n\nSee how Lasso zeroed out some features completely? That's automatic feature selection. Larger α means more aggressive selection.\n\n### 8.3 Visualizing the Regularization Path {#ch2-8-3}\n\nThe \"regularization path\" shows how coefficients shrink as α increases. For Lasso, coefficients hit zero and stay there.\n\n::: {#3bdf0bb7 .cell execution_count=22}\n``` {.python .cell-code}\n# Track coefficients across alpha values for both Lasso and Ridge\nalphas_lasso = np.logspace(-3, 1, 100)  # From 0.001 to 10\ncoefs_lasso = []\ncoefs_ridge_comparison = []\n\nfor alpha in alphas_lasso:\n    # Lasso\n    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n    lasso_model.fit(X_ridge_train_scaled, y_ridge_train)\n    coefs_lasso.append(lasso_model.coef_)\n\n    # Ridge (for comparison)\n    ridge_model = Ridge(alpha=alpha)\n    ridge_model.fit(X_ridge_train_scaled, y_ridge_train)\n    coefs_ridge_comparison.append(ridge_model.coef_)\n\ncoefs_lasso = np.array(coefs_lasso)\ncoefs_ridge_comparison = np.array(coefs_ridge_comparison)\n\n# Plot Lasso regularization path\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lasso path\ncolors = ['blue', 'red', 'green']\nfor i, feature in enumerate(feature_names):\n    axes[0].plot(alphas_lasso, coefs_lasso[:, i], label=feature, linewidth=2, color=colors[i])\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Alpha (log scale)', fontsize=12)\naxes[0].set_ylabel('Coefficient Value', fontsize=12)\naxes[0].set_title('Lasso Regularization Path', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n\n# Ridge path (for comparison)\nfor i, feature in enumerate(feature_names):\n    axes[1].plot(alphas_lasso, coefs_ridge_comparison[:, i], label=feature, linewidth=2, color=colors[i])\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Alpha (log scale)', fontsize=12)\naxes[1].set_ylabel('Coefficient Value', fontsize=12)\naxes[1].set_title('Ridge Regularization Path', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-23-output-1.png){width=1335 height=468}\n:::\n:::\n\n\nSee the difference? **Lasso coefficients hit zero abruptly** and stay there (left plot). **Ridge coefficients smoothly approach zero** but never reach it (right plot).\n\n::: {.callout-note}\nLasso is more aggressive than Ridge. It makes hard decisions: \"this feature matters\" or \"this feature doesn't.\" Ridge says \"this feature matters a little less.\" Choose based on whether you want sparse models (Lasso) or stable coefficients (Ridge).\n:::\n\n### 8.4 When to Use Lasso vs. Ridge {#ch2-8-4}\n\n**Use Lasso when:**\n\n- You suspect many features are irrelevant\n- You want a sparse model (fewer features)\n- You need to explain which features matter most\n- Interpretability is critical\n\n**Use Ridge when:**\n\n- You think most features contribute something\n- You have multicollinearity and want stable coefficients\n- You're okay with keeping all features\n- You prioritize prediction over interpretation\n\n**The truth?** Try both and use cross-validation to decide. Sometimes Ridge wins. Sometimes Lasso wins. Sometimes they're tied.\n\n::: {#cb8783f2 .cell execution_count=23}\n``` {.python .cell-code}\n# Compare Ridge vs Lasso performance\nalphas_compare = np.logspace(-3, 2, 50)\nridge_scores = []\nlasso_scores = []\n\nfor alpha in alphas_compare:\n    # Ridge\n    ridge = Ridge(alpha=alpha)\n    ridge_cv = cross_val_score(ridge, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    ridge_scores.append(ridge_cv)\n\n    # Lasso\n    lasso = Lasso(alpha=alpha, max_iter=10000)\n    lasso_cv = cross_val_score(lasso, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    lasso_scores.append(lasso_cv)\n\n# Plot comparison\nplt.figure(figsize=(10, 6))\nplt.plot(alphas_compare, ridge_scores, 'o-', label='Ridge', linewidth=2)\nplt.plot(alphas_compare, lasso_scores, 's-', label='Lasso', linewidth=2)\nplt.xscale('log')\nplt.xlabel('Alpha (log scale)', fontsize=12)\nplt.ylabel('Cross-Validation R²', fontsize=12)\nplt.title('Ridge vs Lasso: Cross-Validation Performance', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Best performance\nbest_ridge_score = max(ridge_scores)\nbest_lasso_score = max(lasso_scores)\nprint(f\"Best Ridge CV R²: {best_ridge_score:.4f}\")\nprint(f\"Best Lasso CV R²: {best_lasso_score:.4f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-24-output-1.png){width=815 height=530}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Ridge CV R²: 0.9692\nBest Lasso CV R²: 0.9692\n```\n:::\n:::\n\n\n::: {.callout-note}\nHere we see that as $\\alpha$ increased, ridge was relatively unaffectd, but lasso quickly had poor prediction. Why do you think that is? Compare this with the regularization path plots above, can you explain what's going on here?\n:::\n\n### Learning outcomes: {#ch2-8-outcomes}\n**_By hand_ you should be able to:**\n\n- Understand how lasso regularization differs from ridge regularization\n- Understand what lasso does to coefficients in linear regression models\n- Understand when to use lasso vs ridge regularization\n\n---\n\n## 9. Elastic Net: Best of Both Worlds {#ch2-9}\n\nCan't decide between Ridge and Lasso? Why not both? Elastic Net combines L1 and L2 regularization to get the best of both worlds.\n\n### 9.1 Combining L1 and L2 {#ch2-9-1}\n\nElastic Net uses a mix of Ridge (L2) and Lasso (L1) penalties:\n$$\n\\text{Loss} = \\text{MSE} + \\alpha \\left[ \n    \\omega \\sum |\\beta_j| + (1 - \\omega) \\sum \\beta_j^2 \\right]\n$$\n\nTwo hyperparameters:\n\n- **α (alpha):** Overall regularization strength (like Ridge and Lasso)\n- $\\omega$ = **l1_ratio:** Mix between L1 and L2\n  - $\\omega$ = 0: Pure Ridge\n  - $\\omega$ = 1: Pure Lasso\n  - $\\omega$ = 0.5: Equal mix of both\n\nWhy combine them? Lasso can be unstable when features are highly correlated—it randomly picks one and zeros the others. Ridge keeps all correlated features but doesn't select. Elastic Net does feature selection (like Lasso) but more stably (like Ridge).\n\n::: {#cf7da8cf .cell execution_count=24}\n``` {.python .cell-code}\nfrom sklearn.linear_model import ElasticNet\n\n# Fit Elastic Net with different l1_ratios\nl1_ratios = [0.2, 0.5, 0.8]\nalpha_en = 0.01\n\n# Also include Ridge and Lasso for comparison\nmodels = {\n    'Ridge (l1=0)': Ridge(alpha=alpha_en),\n    'ElasticNet (l1=0.2)': ElasticNet(alpha=alpha_en, l1_ratio=0.2, max_iter=10000),\n    'ElasticNet (l1=0.5)': ElasticNet(alpha=alpha_en, l1_ratio=0.5, max_iter=10000),\n    'ElasticNet (l1=0.8)': ElasticNet(alpha=alpha_en, l1_ratio=0.8, max_iter=10000),\n    'Lasso (l1=1)': Lasso(alpha=alpha_en, max_iter=10000)\n}\n\nfor idx, (name, model) in enumerate(models.items()):\n    model.fit(X_ridge_train_scaled, y_ridge_train)\n    r2 = model.score(X_ridge_test_scaled, y_ridge_test)\n    print(f\"{name}: Test R² = {r2:.4f}\")\n    print(f\"  Coefficients: {model.coef_}\")\n    print(f\"  Non-zero coefficients: {np.sum(np.abs(model.coef_) > 0.0001)}\")\n    print()\n\n# Visualize one Elastic Net model\nmodel_en = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)\nmodel_en.fit(X_ridge_train_scaled, y_ridge_train)\n\n# Compare all three\nfig = plt.figure(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\n\n# Get a Ridge and Lasso with same alpha for fair comparison\nmodel_ridge_comp = Ridge(alpha=0.01)\nmodel_ridge_comp.fit(X_ridge_train_scaled, y_ridge_train)\n\nmodel_lasso_comp = Lasso(alpha=0.01, max_iter=10000)\nmodel_lasso_comp.fit(X_ridge_train_scaled, y_ridge_train)\n\nplt.bar(x - width, model_ridge_comp.coef_, width, label='Ridge', alpha=0.8)\nplt.bar(x, model_en.coef_, width, label='Elastic Net (l1_ratio=0.5)', alpha=0.8)\nplt.bar(x + width, model_lasso_comp.coef_, width, label='Lasso', alpha=0.8)\n\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Coefficient Value', fontsize=12)\nplt.title('Ridge vs Elastic Net vs Lasso', fontsize=14)\nplt.xticks(x, feature_names)\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRidge (l1=0): Test R² = 0.9665\n  Coefficients: [ 5.70711611 -0.81673551  3.00184572]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.2): Test R² = 0.9673\n  Coefficients: [3.73186865 1.1378942  2.97734663]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.5): Test R² = 0.9673\n  Coefficients: [4.118586   0.75533971 2.98299579]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.8): Test R² = 0.9672\n  Coefficients: [4.82987896 0.04824256 2.98856424]\n  Non-zero coefficients: 3\n\nLasso (l1=1): Test R² = 0.9671\n  Coefficients: [4.88494701 0.         2.99232692]\n  Non-zero coefficients: 2\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-25-output-2.png){width=951 height=566}\n:::\n:::\n\n\nElastic Net sits between Ridge and Lasso. It zeros out some features (like Lasso) but keeps coefficients more stable (like Ridge).\n\n### 9.2 When to Use Elastic Net {#ch2-9-2}\n\n**Use Elastic Net when:**\n\n- You have groups of correlated features\n- You want feature selection but Lasso is too unstable\n- You're not sure if Ridge or Lasso is better (hedge your bets)\n- You have more features than observations (p > n)\n\n**Real-world scenario:** You have 50 features measuring similar things (different weather stations, different survey questions, etc.). Lasso might randomly pick one from each group. Ridge keeps all 50. Elastic Net picks a few from each group—the best compromise.\n\n**Practical advice:** If you're unsure, use Elastic Net with l1_ratio=0.5 and tune it with cross-validation. It's a safe default that adapts to your data.\n\n::: {#ecdaefb1 .cell execution_count=25}\n``` {.python .cell-code}\n# Tune both alpha and l1_ratio with grid search\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'alpha': np.logspace(-3, 1, 20),\n    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n}\n\nelastic_net = ElasticNet(max_iter=10000)\ngrid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='r2', n_jobs=-1)\ngrid_search.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV R²: {grid_search.best_score_:.4f}\")\nprint(f\"Test R²: {grid_search.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\n\n# Best model coefficients\nbest_model = grid_search.best_estimator_\nprint(f\"\\nBest model coefficients:\")\nfor feature, coef in zip(feature_names, best_model.coef_):\n    if abs(coef) > 0.0001:\n        print(f\"  {feature}: {coef:.6f}\")\n    else:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: {'alpha': np.float64(0.001), 'l1_ratio': 0.9}\nBest CV R²: 0.9691\nTest R²: 0.9667\n\nBest model coefficients:\n  X1: 5.474694\n  X2: -0.584535\n  X3: 3.000771\n```\n:::\n:::\n\n\nElastic Net automatically found the best combination of Ridge and Lasso for your data. That's the power of combining regularization techniques.\n\n### Learning outcomes: {#ch2-9-outcomes}\n**_By hand_ you should be able to:**\n\n- Understand how elastic net combines both ridge and lasso regularization\n- Understand when to use elastic net regularization\n\n---\n\n## 10. Putting It All Together: A Complete Regression Analysis {#ch2-10}\n\nYou've learned all the pieces. Now let's put them together into a complete regression workflow that you can follow for any project.\n\n### 10.1 The Diagnostic Workflow {#ch2-10-1}\n\nHere's the process every regression analysis should follow:\n\n- **Step 1:** Fit a baseline linear model\n- **Step 2:** Check assumptions with diagnostic plots\n- **Step 3:** Identify problems (non-linearity, heteroscedasticity, multicollinearity)\n- **Step 4:** Fix problems (transformations, polynomial features, regularization)\n- **Step 5:** Validate with cross-validation\n- **Step 6:** Interpret and communicate results\n\nLet's walk through this with a complete example:\n\n::: {#1b70e25b .cell execution_count=26}\n``` {.python .cell-code}\nfrom scipy import stats\n\n# Step 1: Fit baseline linear model\nprint(\"=\"*60)\nprint(\"STEP 1: Baseline Linear Regression\")\nprint(\"=\"*60)\n\nfeatures_complete = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']\nnyc_complete = nyc_census[features_complete + ['Income']].dropna()\nX_complete = nyc_complete[features_complete].values\ny_complete = nyc_complete['Income'].values\n\nX_train_complete, X_test_complete, y_train_complete, y_test_complete = train_test_split(\n    X_complete, y_complete, test_size=0.2, random_state=42\n)\n\n# Baseline model\nmodel_baseline = LinearRegression()\nmodel_baseline.fit(X_train_complete, y_train_complete)\ny_pred_baseline = model_baseline.predict(X_test_complete)\n\nprint(f\"Train R²: {model_baseline.score(X_train_complete, y_train_complete):.4f}\")\nprint(f\"Test R²: {model_baseline.score(X_test_complete, y_test_complete):.4f}\")\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test_complete, y_pred_baseline)):.4f}\")\n\n# Step 2: Check assumptions with diagnostic plots\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: Diagnostic Plots\")\nprint(\"=\"*60)\n\nresiduals_complete = y_test_complete - y_pred_baseline\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Residuals vs Fitted\naxes[0].scatter(y_pred_baseline, residuals_complete, alpha=0.5, s=10)\naxes[0].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Fitted Values')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('Residuals vs Fitted')\naxes[0].grid(True, alpha=0.3)\n\n# Histogram of residuals\naxes[1].hist(residuals_complete, bins=50, edgecolor='black', alpha=0.7)\naxes[1].axvline(0, color='red', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Residuals')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Residual Distribution')\naxes[1].grid(True, alpha=0.3)\n\n# Scale-Location Plot\nstandardized_resid = residuals_complete / residuals_complete.std()\naxes[2].scatter(y_pred_baseline, np.sqrt(np.abs(standardized_resid)), alpha=0.5, s=10)\naxes[2].set_xlabel('Fitted Values')\naxes[2].set_ylabel('√|Standardized Residuals|')\naxes[2].set_title('Scale-Location Plot')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Observations:\")\nprint(\"- Residuals show some heteroscedasticity (variance increases with fitted values)\")\nprint(\"- Overall pattern suggests room for improvement\")\n\n# Step 3: Check for multicollinearity\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 3: Check Multicollinearity\")\nprint(\"=\"*60)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = features_complete\nvif_data[\"VIF\"] = [variance_inflation_factor(X_complete, i) for i in range(len(features_complete))]\n\nprint(vif_data.sort_values('VIF', ascending=False))\nprint(\"\\nVIF < 5: No serious multicollinearity concerns\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n============================================================\nSTEP 1: Baseline Linear Regression\n============================================================\nTrain R²: 0.8347\nTest R²: 0.8406\nTest RMSE: 11405.3004\n\n============================================================\nSTEP 2: Diagnostic Plots\n============================================================\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-27-output-2.png){width=1430 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nObservations:\n- Residuals show some heteroscedasticity (variance increases with fitted values)\n- Overall pattern suggests room for improvement\n\n============================================================\nSTEP 3: Check Multicollinearity\n============================================================\n        Feature        VIF\n2       Poverty  21.903183\n5  ChildPoverty  16.856925\n1  Professional   9.947219\n4  IncomePerCap   7.250800\n3  Unemployment   4.843157\n0      TotalPop   4.334884\n\nVIF < 5: No serious multicollinearity concerns\n```\n:::\n:::\n\n\nSee the workflow? Fit → diagnose → identify issues. Now let's fix them.\n\n### 10.2 Model Selection Strategy {#ch2-10-2}\n\nBased on diagnostics, try improvements systematically:\n\n::: {#0ab68c50 .cell execution_count=27}\n``` {.python .cell-code}\n# Step 4: Try different models\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 4: Model Comparison\")\nprint(\"=\"*60)\n\n# Scale features for regularization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_complete)\nX_test_scaled = scaler.transform(X_test_complete)\n\n# Dictionary to store results\nresults = []\n\n# Model 1: Baseline Linear Regression\nmodel1 = LinearRegression()\nmodel1.fit(X_train_scaled, y_train_complete)\ncv_score1 = cross_val_score(model1, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score1 = model1.score(X_test_scaled, y_test_complete)\nresults.append({\n    'Model': 'Linear Regression',\n    'CV R²': cv_score1,\n    'Test R²': test_score1,\n    'Features': len(features_complete)\n})\n\n# Model 2: Polynomial Features (degree 2)\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = poly_features.fit_transform(X_train_scaled)\nX_test_poly = poly_features.transform(X_test_scaled)\n\nmodel2 = LinearRegression()\nmodel2.fit(X_train_poly, y_train_complete)\ncv_score2 = cross_val_score(model2, X_train_poly, y_train_complete, cv=5, scoring='r2').mean()\ntest_score2 = model2.score(X_test_poly, y_test_complete)\nresults.append({\n    'Model': 'Polynomial (degree=2)',\n    'CV R²': cv_score2,\n    'Test R²': test_score2,\n    'Features': X_train_poly.shape[1]\n})\n\n# Model 3: Ridge Regression\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train_complete)\ncv_score3 = cross_val_score(ridge, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score3 = ridge.score(X_test_scaled, y_test_complete)\nresults.append({\n    'Model': 'Ridge (α=1.0)',\n    'CV R²': cv_score3,\n    'Test R²': test_score3,\n    'Features': len(features_complete)\n})\n\n# Model 4: Lasso Regression\nlasso = Lasso(alpha=0.01, max_iter=10000)\nlasso.fit(X_train_scaled, y_train_complete)\ncv_score4 = cross_val_score(lasso, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score4 = lasso.score(X_test_scaled, y_test_complete)\nn_features_lasso = np.sum(np.abs(lasso.coef_) > 0.0001)\nresults.append({\n    'Model': 'Lasso (α=0.01)',\n    'CV R²': cv_score4,\n    'Test R²': test_score4,\n    'Features': f\"{n_features_lasso} (selected)\"\n})\n\n# Model 5: Elastic Net\nelastic = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)\nelastic.fit(X_train_scaled, y_train_complete)\ncv_score5 = cross_val_score(elastic, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score5 = elastic.score(X_test_scaled, y_test_complete)\nn_features_elastic = np.sum(np.abs(elastic.coef_) > 0.0001)\nresults.append({\n    'Model': 'Elastic Net (α=0.01, l1=0.5)',\n    'CV R²': cv_score5,\n    'Test R²': test_score5,\n    'Features': f\"{n_features_elastic} (selected)\"\n})\n\n# Display results\nresults_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison Results:\")\nprint(results_df.to_string(index=False))\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(results_df))\nwidth = 0.35\n\nax.bar(x - width/2, results_df['CV R²'], width, label='CV R²', alpha=0.8)\nax.bar(x + width/2, results_df['Test R²'], width, label='Test R²', alpha=0.8)\n\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('R²', fontsize=12)\nax.set_title('Model Performance Comparison', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Best model\nbest_idx = results_df['CV R²'].argmax()\nbest_model_name = results_df.iloc[best_idx]['Model']\nprint(f\"\\n✓ Best model by CV: {best_model_name}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n============================================================\nSTEP 4: Model Comparison\n============================================================\n\nModel Comparison Results:\n                       Model    CV R²  Test R²     Features\n           Linear Regression 0.832510 0.840628            6\n       Polynomial (degree=2) 0.845916 0.856639           27\n               Ridge (α=1.0) 0.832520 0.840618            6\n              Lasso (α=0.01) 0.832510 0.840628 6 (selected)\nElastic Net (α=0.01, l1=0.5) 0.832527 0.840470 6 (selected)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chapter-2-regression_files/figure-html/cell-28-output-2.png){width=951 height=566}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n✓ Best model by CV: Polynomial (degree=2)\n```\n:::\n:::\n\n\nThe comparison shows which approach works best for your data. In this case, all models perform similarly, but Ridge and Elastic Net provide slightly better generalization.\n\n### 10.3 Interpreting and Communicating Results {#ch2-10-3}\n\nOnce you've selected your best model, interpret the coefficients and communicate clearly:\n\n::: {#5c5dcd51 .cell execution_count=28}\n``` {.python .cell-code}\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 5: Interpret Final Model\")\nprint(\"=\"*60)\n\n# Use Ridge as our final model\nfinal_model = Ridge(alpha=1.0)\nfinal_model.fit(X_train_scaled, y_train_complete)\n\n# Get coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features_complete,\n    'Coefficient': final_model.coef_\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"\\nFinal Model Coefficients (Ridge, α=1.0):\")\nprint(coef_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n============================================================\nSTEP 5: Interpret Final Model\n============================================================\n\nFinal Model Coefficients (Ridge, α=1.0):\n     Feature   Coefficient\nIncomePerCap  15857.063355\nProfessional   2913.318816\nChildPoverty   1977.037962\nUnemployment   -882.530764\n    TotalPop  -1494.961708\n     Poverty -12904.379677\n```\n:::\n:::\n\n\n**Interpretation for Stakeholders:**\nOur regression model explains approximately 60% of the variance in median household income across NYC census tracts. The model uses 6 key features:\n\n**Key findings:**\n1. **Income Per Capita** is the strongest predictor. Each $1,000 increase in per capita income corresponds to a significant increase in median household income.\n\n2. **Professional Employment** has a positive effect. Census tracts with more professionals tend to have higher median household incomes.\n\n3. **Poverty Rate** shows a strong negative relationship with income, as expected.\n\n4. **Child Poverty** has an additional negative effect beyond general poverty, highlighting the economic challenges faced by families with children.\n\n5. **Unemployment** shows a negative relationship with median household income, though this may be partially captured by the poverty variables.\n\n6. **Total Population** has minimal effect on median household income at the census tract level.\n\nThe model performs consistently on both training and test data, suggesting it generalizes well to new predictions.\n\n::: {.callout-tip}\nThis is how you communicate results: translate coefficients into plain language, explain what matters, and acknowledge limitations.\n:::\n\n::: {.callout-tip}\n**Always present both statistical results (R², coefficients) and practical interpretation.** Stakeholders need to understand what the numbers mean for their decisions, not just that \"the model has an R² of 0.60.\"\n:::\n\nThe complete workflow: diagnose issues → try fixes systematically → select best model → interpret clearly. That's professional regression analysis.\n\n---\n\n## Summary {#ch2-summary}\n\nLinear regression is the foundation of machine learning, not because it's the most powerful model, but because understanding it deeply prepares you to understand everything else. This chapter covered the complete regression toolkit—from basic assumptions to advanced regularization techniques.\n\n**Key Takeaways:**\n\n1. **Linear regression makes four assumptions:** linearity, independence, homoscedasticity, and normality. Violate them, and your model might give terrible predictions even with high R².\n\n2. **Residual plots are your diagnostic tool.** They reveal problems that metrics hide. A curved residual plot means non-linearity. A funnel means heteroscedasticity. Random scatter means you're good.\n\n3. **Evaluation metrics serve different purposes.** MSE penalizes large errors heavily. MAE treats all errors equally. R² tells you variance explained. RMSE gives interpretable units. Use multiple metrics, not just one.\n\n4. **Polynomial features capture non-linearity** while staying within linear regression. But high-degree polynomials overfit spectacularly. Always use validation data to choose the degree.\n\n5. **Multicollinearity breaks coefficient interpretation** but doesn't hurt prediction accuracy. High VIF values warn you that coefficients are unstable. Regularization fixes this automatically.\n\n6. **Ridge regression (L2) shrinks all coefficients** toward zero but never reaches exactly zero. It handles multicollinearity well and prevents overfitting. Always scale features first.\n\n7. **Lasso regression (L1) does automatic feature selection** by setting some coefficients to exactly zero. Use it when you suspect many features are irrelevant.\n\n8. **Elastic Net combines Ridge and Lasso.** It's more stable than Lasso with correlated features while still doing feature selection. When unsure, start with Elastic Net.\n\n9. **The complete workflow matters:** Fit baseline → check diagnostics → identify problems → fix systematically → validate with cross-validation → interpret clearly. Skip steps, and you'll miss critical issues.\n\n10. **Always communicate results in plain language.** Stakeholders don't care that \"the coefficient for X1 is 0.437 with a p-value of 0.003.\" They care what that means for their decisions.\n\nRegression modeling is both art and science. The science is in the diagnostics, metrics, and validation. The art is in knowing when assumptions matter, which fixes to try, and how to communicate findings. Master both, and you'll build models that actually work in the real world.\n\nUse your brain. That's what it's there for.\n\n---\n\n## Practice Exercises {#ch2-practice}\n\nThese exercises build progressively from understanding diagnostics to conducting complete regression analyses. Work through them to solidify your grasp of regression modeling.\n\n### Exercise 1: Residual Analysis\n\n**Task:** Load the NYC census dataset and fit a linear regression predicting median household income from just the `TotalPop` feature. Create a residuals vs. fitted values plot. Based on the pattern you observe:\n\na) Identify which regression assumption(s) are violated\nb) Explain what the pattern tells you about the model's mistakes\nc) Suggest two specific fixes that might improve the model\nd) Implement one fix and show the improvement\n\n**What you're learning:** How to read residual plots and diagnose problems.\n\n### Exercise 2: Coefficient Interpretation\n\n**Task:** Fit a multiple linear regression predicting median household income from `Professional`, `Poverty`, `Unemployment`, and `ChildPoverty`. For each coefficient:\n\na) Write the interpretation in plain English (e.g., \"For every additional...\")\nb) Explain what \"holding other features constant\" means\nc) Identify which coefficient is hardest to interpret and explain why\nd) Calculate and report both R² and Adjusted R², then explain the difference\n\n**What you're learning:** How to interpret and communicate regression results.\n\n### Exercise 3: Polynomial Selection\n\n**Task:** Using the `IncomePerCap` feature alone, fit polynomial regression models of degrees 1 through 8:\n\na) For each degree, calculate train MSE and test MSE\nb) Create a plot showing both training and test MSE across all degrees\nc) Identify the optimal polynomial degree and justify your choice\nd) Visualize the polynomial fit for degrees 1, 3, and 8 on the same plot\ne) Explain why degree 8 has lower training MSE but might perform worse in practice\n\n**What you're learning:** The bias-variance tradeoff and how to prevent overfitting.\n\n### Exercise 4: Multicollinearity Detection\n\n**Task:** Using all numeric features in the NYC census dataset:\n\na) Compute and visualize the correlation matrix\nb) Identify any feature pairs with correlation above 0.7\nc) Calculate VIF for each feature\nd) Identify features with VIF > 5 and explain what this means\ne) Fit two models: one with all features, one removing high-VIF features. Compare coefficients and show which is more stable by refitting on a different random split.\n\n**What you're learning:** How to detect and handle multicollinearity.\n\n### Exercise 5: Ridge vs Lasso Comparison\n\n**Task:** Using all features from the NYC census dataset, compare Ridge and Lasso regression:\n\na) For both Ridge and Lasso, find the optimal alpha using cross-validation\nb) Plot the regularization paths for both methods (coefficients vs. alpha)\nc) Create a table showing which features each method keeps (for optimal alpha)\nd) Explain why Lasso eliminates some features while Ridge doesn't\ne) Recommend which method you'd use for this dataset and why\n\n**What you're learning:** The practical differences between L1 and L2 regularization.\n\n### Exercise 6: Complete Regression Workflow\n\n**Task:** Conduct a complete regression analysis on the NYC census dataset:\n\na) **Baseline:** Fit linear regression with all features. Report train/test R² and RMSE.\nb) **Diagnostics:** Create residual plots (residuals vs fitted, histogram of residuals, scale-location). List any problems you identify.\nc) **Multicollinearity:** Check VIF. Report any concerns.\nd) **Model Improvement:** Try at least 3 different approaches (e.g., polynomial features, Ridge, Lasso, log transform). Use cross-validation to compare.\ne) **Final Model:** Select your best model and justify the choice.\nf) **Interpretation:** Write a 2-3 paragraph summary explaining your findings to a non-technical audience. Include the model's performance, key predictors, and limitations.\n\n**What you're learning:** The complete end-to-end regression modeling workflow.\n\n---\n\n**Tips for Success:**\n\n- Don't just run code—think about what each plot and metric is telling you\n- When diagnostic plots show problems, try multiple fixes and compare results\n- Always use train/validation/test splits or cross-validation properly\n- Write your interpretations before looking at solutions\n- Remember: a model that's slightly less accurate but easier to interpret is often more valuable in practice\n\nUse your brain. That's what it's there for.\n\n---\n\n## Additional Resources {#ch2-additional-resources}\n\n- [Regression Assumptions Explained](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/) - Detailed guide to assumptions\n- [Interpreting Residual Plots](https://online.stat.psu.edu/stat462/node/117/) - Penn State course notes\n- [Ridge vs Lasso](https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/) - Comprehensive comparison\n- [Scikit-learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html) - Official documentation\n- [Feature Scaling and Regularization](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/) - Why scaling matters\n\n",
    "supporting": [
      "chapter-2-regression_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}