{
  "hash": "2f38d6a69c86617c26dce0acdaeda362",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Chapter 4: LLMs for Feature Engineering and Data Extraction\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Chapter Resources\n\n**Related Assignments:**\n\n- [Chapter 4 Homework](../../Assignments/Module%206%20-%20LLMs%20Feature%20Engineering/module-6-homework.qmd)\n\n---\n\n## Introduction\n\nYou've spent weeks learning how to build machine learning models—linear regression, logistic regression, random forests, SVMs. You know how to tune hyperparameters, evaluate performance, and diagnose problems. But here's the thing: all those models need one critical ingredient before they can work their magic.\n\n**Features.**\n\nAnd not just any features—numeric or categorical features that capture the information hidden in your data. If you have structured data (age, income, house size), you're set. But what about unstructured data? What about customer reviews, support tickets, job descriptions, social media posts, news articles? Most real-world data lives in text, and traditional machine learning models can't directly consume text.\n\nThis is where Large Language Models (LLMs) come in—not as the end goal, but as powerful **feature engineering tools**. Think of LLMs as intelligent extractors that can read text, understand context, and pull out structured information. Need to know if a review is positive or negative? LLM. Want to extract job requirements from a posting? LLM. Need to categorize thousands of support tickets? LLM.\n\nThe beautiful part? You don't need to train these models. You don't need GPUs. You don't even need to understand how they work internally. You just call an API, send some text with instructions, and get back structured data ready for your ML pipeline.\n\nBut LLMs aren't free, and they aren't perfect. Every API call costs money. Extraction quality varies. Some tasks work beautifully with simpler versions of LLMs, such as GPT-4o, Gemini Flash, Claude Haiku, while others need more powerful models like GPT-5, Gemini Pro, or Claude Sonnet. Sometimes a simple regex pattern works better than an expensive LLM call. The skill isn't just using LLMs—it's knowing **when** to use them, **which** one to use, and **how** to validate the results.\n\nThis chapter teaches you to use LLMs as practical data science tools. You'll learn to write prompts that extract information reliably, parse and validate LLM responses, calculate costs, integrate extracted features into ML pipelines, and most importantly—judge when LLMs add value versus when simpler approaches suffice.\n\nLet's jump in.\n\n---\n\n## 1. The Feature Engineering Challenge: From Text to Numbers\n\n### 1.1 Why Traditional ML Needs Structured Features\n\nRemember our housing price predictor from earlier modules? The input was clean: square footage (numeric), number of bedrooms (numeric), neighborhood (categorical). Easy. Train-test split, fit the model, done.\n\nBut look at this product review:\n\n> \"This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud, but I can live with that for this price.\"\n\nWhat features can we extract? What information is hidden here that might help predict if other customers will find this review helpful?\n\n::: {#dde74ae8 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Example: Product review data\nreview_text = \"This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud, but I can live with that for this price.\"\n\n# Traditional ML needs numbers or categories\n# We could count words, but that misses the meaning\nword_count = len(review_text.split())\nprint(f\"Word count: {word_count}\")\n\n# We could look for specific keywords\nhas_positive_words = any(word in review_text.lower() for word in ['amazing', 'great', 'love'])\nhas_negative_words = any(word in review_text.lower() for word in ['bad', 'terrible', 'hate'])\nprint(f\"Has positive words: {has_positive_words}\")\nprint(f\"Has negative words: {has_negative_words}\")\n\n# But we're missing so much: \n# - sentiment nuance\n# - specific features mentioned\n# - overall tone\n# - ...\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord count: 28\nHas positive words: True\nHas negative words: False\n```\n:::\n:::\n\n\nWord counts and keyword matching capture some information, but they miss the **meaning**. This review is mostly positive despite mentioning a downside. A keyword approach might rate it as mixed because it has both positive and negative words. But a human reading it understands: this person likes the product.\n\n### 1.2 What Information Is Hidden in Text?\n\nText contains structured information waiting to be extracted:\n\n**Sentiment/Opinion:**\n\n- Overall positive/negative/neutral\n- Strength of sentiment (mildly positive vs. extremely positive)\n- Sentiment about specific aspects (loves taste, dislikes noise)\n\n**Categories/Classification:**\n\n- Product category (coffee maker, not coffee beans)\n- Issue type in support tickets (billing vs. technical)\n- Job seniority level (entry vs. senior)\n\n**Entities and Attributes:**\n\n- Brands mentioned\n- Specific features discussed (speed, taste, noise)\n- Requirements (in job postings: \"5 years experience\", \"Python required\")\n\n**Numeric Values:**\n\n- Implicit ratings (\"amazing\" = 5 stars, \"okay\" = 3 stars)\n- Quantities mentioned\n- Price ranges\n\n### 1.3 Traditional NLP Approaches\n\nBefore LLMs, we had a few options:\n\n**1. Keyword/Regex Matching:**\n\nThis approach attempts to search for specific keywords in the text. For example, look at the `positive_words` and `negative_words` in the code below.\n\n::: {#4fb6e56e .cell execution_count=2}\n``` {.python .cell-code}\nimport re\n\ndef simple_sentiment(text):\n    \"\"\"Basic sentiment using keyword matching\"\"\"\n    positive_words = ['amazing', 'great', 'excellent', 'love', 'perfect']\n    negative_words = ['bad', 'terrible', 'hate', 'awful', 'waste']\n\n    text_lower = text.lower()\n    pos_count = sum(1 for word in positive_words if word in text_lower)\n    neg_count = sum(1 for word in negative_words if word in text_lower)\n\n    if pos_count > neg_count:\n        return \"positive\"\n    elif neg_count > pos_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\nreview = \"This coffee maker is amazing! Brews quickly and the coffee tastes great.\"\nprint(f\"Simple sentiment: {simple_sentiment(review)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimple sentiment: positive\n```\n:::\n:::\n\n\n**When to use this:**\n\n- When you have specific words you're looking for, such as students mentioning a specific course\n\n**When this fails:**\n\n- This works for simple cases but breaks easily:\n    - \"This product is not bad\" → Incorrectly classified as negative\n    - \"I expected amazing quality but got terrible service\" → Confusing mix\n    - Doesn't handle sarcasm, context, or nuance\n\n**2. Bag-of-Words + ML:**\n\"Bag of words\" is a simple way to represent text as a vector of word counts. For example, we might assign:\n\n- \"apple\" = 0\n- \"banana\" = 1\n- \"orange\" = 2\n\nThese number represent the _index_ of each word in our vector. So \"apple\" is at index 0, \"banana\" is at index 1, and \"orange\" is at index 2. Then we might count up how many times each word appears in the text. For example, the text \"I have an apple and a banana\" would be represented as [1, 1, 0] (\"apple\" apears once as indicated in index zero, and \"banana\" appears once, but \"orange\" never appears).\n\nThis gives us numeric values, which can then be used in any of the machine learning models you've learned so far this semester.\n\nProblems with this approach include:\n\n- Need a large labeled dataset, which is expensive to create\n- Need to retrain for new domains (i.e. the dataset you have may not cover the domain you're interested in)\n- Separate model for each extraction task (i.e. you need a separate model for each feature you want to extract, such as sentiment, brand, price, etc.)\n\n**3. Other approaches:**\nTraditionally, people used many, many different approaches for feature extraction, such as TF-IDF, word embeddings, and more. These approaches were often ad-hoc and required a lot of expertise to implement. \n\nOnce LLMs arose, people quickly realized that we can simply _ask the LLM_ to extract what we want. While not always perfect, this approach is highly flexible, tuneable, and easily implemented. Because of that, we'll focus the majority of our efforts on this technique.\n\n### 1.4 The LLM Advantage: Zero-Shot Extraction\n\nLLMs offer something revolutionary: **zero-shot learning**, which means that you can extract information without training on your specific task, without labeled data, just by asking clearly.\n\nHere's a preview (we'll implement this properly soon):\n\n> **Prompt:** \"What is the sentiment of this review? Answer with just 'positive', 'negative', or 'neutral': This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud.\"\n>\n> **LLM Response:** \"positive\"\n\nNo training. No labeled data. Just clear instructions and the text. The LLM understands context, handles negation, grasps nuance. It knows \"only downside\" indicates a minor criticism in an otherwise positive review.\n\nThis is powerful. But it's also expensive, sometimes inconsistent, and not always necessary. In addition, it's clearly a black box, since we have no idea how the LLM decided this should be a positive review. Of course, we could continually tweak instructions to get outcomes closer to what we want. This is the art and science of working with LLMs.\n\n---\n\n## 2. Your First LLM Extraction: Sentiment Analysis\n\n### 2.1 Setting Up API Access\n\nTo use LLMs, you need API access. The main options:\n\n- **OpenAI (GPT-5):** Most popular, good quality, moderate cost\n- **Anthropic (Claude):** High quality, good for long texts, moderate cost\n- **Google (Gemini):** Competitive quality, often cheaper\n- **Open-source via Hugging Face:** Free but requires more setup\n\nFor this chapter, we'll use OpenAI's API since it's widely available. The concepts transfer to other providers.\n\n::: {.callout-note}\n**About Running These Examples:**\nThe code examples in this chapter that call the OpenAI API will require an API key to run. If you don't have an API key set, the examples will be skipped during rendering. This is intentional - you can still learn from reading the code and understanding the patterns. When you're ready to run these examples yourself:\n\n1. Sign up for an OpenAI API account at [platform.openai.com](https://platform.openai.com)\n2. Generate an API key\n3. Set it as an environment variable: `export OPENAI_API_KEY=\"your-key-here\"`\n4. Be mindful of costs - start with small tests before processing large datasets\n:::\n\n::: {#fecf62c5 .cell execution_count=3}\n``` {.python .cell-code}\n# First, install the OpenAI library if needed\n# pip install openai\n\nfrom openai import OpenAI\nimport os\n\n# Set up your API key\n# IMPORTANT: Never hard-code API keys! Use environment variables\n# Set this in your terminal: export OPENAI_API_KEY=\"your-key-here\"\n\n# Check if API key is set\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    print(\"⚠️  API key not found. Set OPENAI_API_KEY environment variable.\")\n    print(\"⚠️  API calls in this chapter will be skipped.\")\n    client = None\nelse:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    print(\"✓ API key is set\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n⚠️  API key not found. Set OPENAI_API_KEY environment variable.\n⚠️  API calls in this chapter will be skipped.\n```\n:::\n:::\n\n\n::: {.callout-warning}\n**Never commit API keys to GitHub!** Use environment variables or config files that are in `.gitignore`. Exposed keys can lead to unexpected charges or account suspension.\n:::\n\n### 2.2 Making Your First Extraction Call\n\nLet's extract sentiment from a product review:\n\n::: {#f8266043 .cell execution_count=4}\n``` {.python .cell-code}\ndef extract_sentiment_simple(review_text):\n    \"\"\"\n    Extract sentiment using GPT-3.5\n    Returns: 'positive', 'negative', or 'neutral'\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return \"neutral\"  # Default response\n\n    # Create the prompt\n    prompt = f\"\"\"What is the sentiment of this review?\nAnswer with just one word: 'positive', 'negative', or 'neutral'.\n\nReview: {review_text}\n\nSentiment:\"\"\"\n\n    # Call the API\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # Cheaper, faster model\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0,  # Deterministic output\n        max_tokens=10   # We only need one word\n    )\n\n    # Extract the response\n    sentiment = response.choices[0].message.content.strip().lower()\n    return sentiment\n\n# Test it (only if API key is available)\nif client is not None:\n    review1 = \"This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud.\"\n    review2 = \"Total waste of money. Broke after two uses. Terrible product.\"\n    review3 = \"It's okay. Does the job but nothing special.\"\n\n    print(f\"Review 1 sentiment: {extract_sentiment_simple(review1)}\")\n    print(f\"Review 2 sentiment: {extract_sentiment_simple(review2)}\")\n    print(f\"Review 3 sentiment: {extract_sentiment_simple(review3)}\")\nelse:\n    print(\"Skipping examples - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Review 1 sentiment: positive\")\n    print(\"Review 2 sentiment: negative\")\n    print(\"Review 3 sentiment: neutral\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkipping examples - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nReview 1 sentiment: positive\nReview 2 sentiment: negative\nReview 3 sentiment: neutral\n```\n:::\n:::\n\n\nWe sent a clear instruction, included the text, and got back exactly what we asked for. No training, no labeled data, no complex preprocessing. Notice how the LLM correctly identifies Review 1 as positive despite it mentioning a downside—it understands that \"only downside\" indicates a minor complaint in an otherwise positive review.\n\n### 2.3 Understanding the API Call\n\nLet's break down the important parameters:\n\n**model:** Which LLM to use\n\n- `gpt-3.5-turbo`: Cheaper ($0.0005 per 1K tokens), faster, good for most tasks\n- `gpt-4`: More expensive ($0.03 per 1K tokens), smarter, better for complex extraction\n- Start with 3.5, upgrade to 4 if quality isn't sufficient\n\n**temperature:** Controls randomness (0 to 2)\n\n- `0`: Deterministic, same input → same output\n- `0.7-1.0`: More creative/varied (for generation tasks)\n- For extraction: use `0` for consistency\n\n**max_tokens:** Maximum length of response\n\n- Tokens ≈ words × 1.3 (rough estimate)\n- For simple extraction: 10-50 tokens\n- For detailed extraction: 100-500 tokens\n- More tokens = higher cost\n\n::: {#bb7fe024 .cell execution_count=5}\n``` {.python .cell-code}\n# Let's see token usage and cost\nif client is not None:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n        temperature=0\n    )\n\n    usage = response.usage\n    print(f\"Prompt tokens: {usage.prompt_tokens}\")\n    print(f\"Completion tokens: {usage.completion_tokens}\")\n    print(f\"Total tokens: {usage.total_tokens}\")\n\n    # Calculate cost (GPT-3.5-turbo pricing as of 2024)\n    cost_per_1k_tokens = 0.0005\n    cost = (usage.total_tokens / 1000) * cost_per_1k_tokens\n    print(f\"Cost for this call: ${cost:.6f}\")\nelse:\n    print(\"Skipping - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Prompt tokens: 14\")\n    print(\"Completion tokens: 1\")\n    print(\"Total tokens: 15\")\n    print(\"Cost for this call: $0.000008\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkipping - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nPrompt tokens: 14\nCompletion tokens: 1\nTotal tokens: 15\nCost for this call: $0.000008\n```\n:::\n:::\n\n\nFor our simple sentiment extraction, we're using about 50-100 tokens per review. At $0.0005 per 1K tokens, that's $0.00005 per review. To process 10,000 reviews: $0.50. Cheap!\n\n### 2.4 Handling Errors and Edge Cases\n\nLLMs don't always follow instructions perfectly. Let's make our extraction more robust:\n\n::: {#eaabb135 .cell execution_count=6}\n``` {.python .cell-code}\ndef extract_sentiment_robust(review_text):\n    \"\"\"\n    Robust sentiment extraction with error handling\n    \"\"\"\n    try:\n        prompt = f\"\"\"What is the sentiment of this review?\nAnswer with ONLY one of these words: positive, negative, neutral\n\nReview: {review_text}\n\nSentiment:\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=10\n        )\n\n        sentiment = response.choices[0].message.content.strip().lower()\n\n        # Validate the response\n        valid_sentiments = ['positive', 'negative', 'neutral']\n        if sentiment in valid_sentiments:\n            return sentiment\n        else:\n            # Try to extract valid sentiment from response\n            for valid in valid_sentiments:\n                if valid in sentiment:\n                    return valid\n            # If we still can't find it, return None\n            return None\n\n    except Exception as e:\n        print(f\"Error during extraction: {e}\")\n        return None\n\n# Test with various inputs\nreviews = [\n    \"Love it!\",\n    \"Terrible product.\",\n    \"It's fine.\",\n    \"\"  # Empty review - will this break?\n]\n\nif client is not None:\n    for review in reviews:\n        result = extract_sentiment_robust(review)\n        print(f\"'{review}' → {result}\")\nelse:\n    print(\"Skipping examples - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"'Love it!' → positive\")\n    print(\"'Terrible product.' → negative\")\n    print(\"'It's fine.' → neutral\")\n    print(\"'' → None\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkipping examples - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\n'Love it!' → positive\n'Terrible product.' → negative\n'It's fine.' → neutral\n'' → None\n```\n:::\n:::\n\n\nKey improvements:\n\n1. **Try-except block** catches API errors\n2. **Validation** checks if response matches expected values\n3. **Fallback logic** tries to extract valid sentiment if response is wordy\n4. **Returns None** if extraction fails (rather than crashing)\n\nNotice how the function handles the empty review gracefully by returning `None` instead of crashing.\n\n---\n\n## 3. Prompt Engineering for Reliable Extraction\n\n### 3.1 The Anatomy of a Good Extraction Prompt\n\nA good prompt has four parts:\n\n1. **Clear task description:** What do you want extracted?\n2. **Output format specification:** Exactly how should the response look?\n3. **The input data:** The text to analyze\n4. **Any constraints or examples:** Helps guide the LLM\n\nLet's compare bad vs. good prompts:\n\n::: {#a59deb66 .cell execution_count=7}\n``` {.python .cell-code}\n# BAD PROMPT - Vague, no format specified\nbad_prompt = \"\"\"\nTell me about this review:\n{review_text}\n\"\"\"\n\n# GOOD PROMPT - Clear, specific format\ngood_prompt = \"\"\"\nExtract the following information from this product review:\n- Sentiment: positive, negative, or neutral\n- Rating: estimated star rating from 1-5\n- Main complaint: brief description, or \"none\" if no complaints\n\nFormat your response as JSON:\n{{\"sentiment\": \"positive/negative/neutral\", \"rating\": 1-5, \"complaint\": \"text or none\"}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\nreview = \"Great coffee maker! Makes excellent coffee quickly. Wish it was quieter though. 4 stars.\"\n\n# The good prompt will give us structured, parseable output\n```\n:::\n\n\nSee the difference? The good prompt:\n- Lists exactly what to extract\n- Specifies valid values for each field\n- Requests JSON format for easy parsing\n- Shows the structure we expect\n\n### 3.2 Requesting Structured Output (JSON)\n\nJSON is your best friend for extraction. It's easy to parse and works with any number of fields:\n\n::: {#bdbd1b66 .cell execution_count=8}\n``` {.python .cell-code}\nimport json\n\ndef extract_review_features(review_text):\n    \"\"\"\n    Extract multiple features from a review using JSON output\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return None\n\n    prompt = f\"\"\"Extract information from this product review.\n\nRespond with ONLY valid JSON in this exact format:\n{{\n    \"sentiment\": \"positive/negative/neutral\",\n    \"rating\": 1-5,\n    \"pros\": [\"list\", \"of\", \"positive\", \"aspects\"],\n    \"cons\": [\"list\", \"of\", \"negative\", \"aspects\"]\n}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=200\n    )\n\n    # Parse the JSON response\n    try:\n        result_text = response.choices[0].message.content.strip()\n        # Sometimes LLMs add markdown formatting, remove it\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n        result = json.loads(result_text)\n        return result\n    except json.JSONDecodeError as e:\n        print(f\"Failed to parse JSON: {e}\")\n        print(f\"Raw response: {result_text}\")\n        return None\n\n# Test it (only if API key is available)\nif client is not None:\n    review = \"\"\"This coffee maker is fantastic! The coffee tastes amazing and\n    it's very fast. Design is sleek. Only complaint is it's somewhat loud\n    during brewing, but that's minor. Highly recommend!\"\"\"\n\n    features = extract_review_features(review)\n    if features:\n        print(\"Extracted features:\")\n        print(json.dumps(features, indent=2))\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Extracted features:\")\n    print(\"\"\"{\n  \"sentiment\": \"positive\",\n  \"rating\": 4,\n  \"pros\": [\n    \"Coffee tastes amazing\",\n    \"Very fast\",\n    \"Sleek design\"\n  ],\n  \"cons\": [\n    \"Somewhat loud during brewing\"\n  ]\n}\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nExtracted features:\n{\n  \"sentiment\": \"positive\",\n  \"rating\": 4,\n  \"pros\": [\n    \"Coffee tastes amazing\",\n    \"Very fast\",\n    \"Sleek design\"\n  ],\n  \"cons\": [\n    \"Somewhat loud during brewing\"\n  ]\n}\n```\n:::\n:::\n\n\nThis gives us structured data we can immediately put into a DataFrame! Notice how the LLM extracted multiple pieces of information from a single review: overall sentiment, an estimated rating, specific positive aspects, and even the one complaint mentioned.\n\n### 3.3 Few-Shot Learning: Teaching by Example\n\nSometimes clear instructions aren't enough. LLMs learn better with examples:\n\n::: {#941b1d9a .cell execution_count=9}\n``` {.python .cell-code}\ndef extract_with_examples(review_text):\n    \"\"\"\n    Use few-shot learning - provide examples of correct extraction\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return \"neutral\"\n\n    prompt = f\"\"\"Extract sentiment from product reviews.\n\nExamples:\n\nReview: \"Best purchase ever! Love this product.\"\nSentiment: positive\n\nReview: \"Broke after one day. Waste of money.\"\nSentiment: negative\n\nReview: \"It's okay. Does what it says.\"\nSentiment: neutral\n\nNow extract sentiment from this review:\n\nReview: {review_text}\nSentiment:\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=10\n    )\n\n    return response.choices[0].message.content.strip().lower()\n\n# Test with tricky example (only if API key is available)\nif client is not None:\n    tricky_review = \"I wanted to love this, but it's just not good enough.\"\n    sentiment = extract_with_examples(tricky_review)\n    print(f\"Tricky review sentiment: {sentiment}\")\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Tricky review sentiment: negative\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nTricky review sentiment: negative\n```\n:::\n:::\n\n\nFew-shot prompting (providing 2-5 examples) helps with:\n- Ambiguous edge cases\n- Specific formatting requirements\n- Consistency across similar inputs\n- Domain-specific language\n\n::: {.callout-tip}\n**Few-Shot Prompting Best Practices:**\n- Use 2-5 examples (more doesn't always help)\n- Examples should cover different scenarios (positive, negative, neutral)\n- Keep examples concise\n- Examples cost tokens—balance quality vs. cost\n:::\n\n### 3.4 Iterating on Prompts: A Real Example\n\nPrompts rarely work perfectly the first time. Let's iterate:\n\n::: {#f223ebdf .cell execution_count=10}\n``` {.python .cell-code}\n# V1: First attempt - too vague\nprompt_v1 = \"What category is this job posting?\"\n\n# V2: More specific\nprompt_v2 = \"\"\"What job category is this posting?\nChoose from: Engineering, Marketing, Sales, Customer Support, Other\"\"\"\n\n# V3: Even more specific with format\nprompt_v3 = \"\"\"Classify this job posting into ONE category.\nValid categories: Engineering, Marketing, Sales, Customer Support, Other\nRespond with ONLY the category name, nothing else.\n\nJob posting: {text}\n\nCategory:\"\"\"\n\n# V4: Add examples for edge cases\nprompt_v4 = \"\"\"Classify this job posting into ONE category.\n\nValid categories: Engineering, Marketing, Sales, Customer Support, Other\n\nExamples:\n\"Senior Python Developer needed\" → Engineering\n\"Social Media Manager wanted\" → Marketing\n\"Account Executive\" → Sales\n\nJob posting: {text}\n\nCategory:\"\"\"\n\n# This iterative process is normal and expected\n# Start simple, add specificity based on failures\n```\n:::\n\n\nThe key is testing your prompts on real data and refining based on errors. We'll see more on validation in the next section.\n\n---\n\n## 4. Parsing, Validating, and Converting to DataFrames\n\n### 4.1 Parsing JSON Responses\n\nLet's build a robust system for extracting and parsing:\n\n::: {#14a1e158 .cell execution_count=11}\n``` {.python .cell-code}\ndef extract_job_features(job_text):\n    \"\"\"\n    Extract structured information from job postings\n    Returns a dictionary or None if extraction fails\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return None\n\n    prompt = f\"\"\"Extract these fields from the job posting.\n\nRespond with valid JSON:\n{{\n    \"title\": \"job title\",\n    \"category\": \"Engineering/Marketing/Sales/Support/Other\",\n    \"experience_level\": \"Entry/Mid/Senior/Lead\",\n    \"remote_policy\": \"Remote/Hybrid/Onsite\",\n    \"key_skills\": [\"skill1\", \"skill2\", \"skill3\"]\n}}\n\nJob posting: {job_text}\n\nJSON:\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=300\n        )\n\n        # Get response text\n        result_text = response.choices[0].message.content.strip()\n\n        # Clean up markdown formatting if present\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n\n        # Parse JSON\n        result = json.loads(result_text)\n\n        return result\n\n    except json.JSONDecodeError as e:\n        print(f\"JSON parsing error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"API error: {e}\")\n        return None\n\n# Test it (only if API key is available)\nif client is not None:\n    job_posting = \"\"\"\n    Senior Data Scientist - Remote\n    We're seeking an experienced data scientist with 5+ years experience.\n    Required: Python, SQL, machine learning, deep learning.\n    Fully remote position.\n    \"\"\"\n\n    features = extract_job_features(job_posting)\n    if features:\n        print(\"Extracted job features:\")\n        for key, value in features.items():\n            print(f\"  {key}: {value}\")\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Extracted job features:\")\n    print(\"  title: Senior Data Scientist\")\n    print(\"  category: Engineering\")\n    print(\"  experience_level: Senior\")\n    print(\"  remote_policy: Remote\")\n    print(\"  key_skills: ['Python', 'SQL', 'machine learning', 'deep learning']\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nExtracted job features:\n  title: Senior Data Scientist\n  category: Engineering\n  experience_level: Senior\n  remote_policy: Remote\n  key_skills: ['Python', 'SQL', 'machine learning', 'deep learning']\n```\n:::\n:::\n\n\n### 4.2 Validating Extractions\n\nAlways validate LLM outputs—they don't always follow instructions:\n\n::: {#3bc82a6a .cell execution_count=12}\n``` {.python .cell-code}\ndef validate_job_features(features):\n    \"\"\"\n    Validate that extracted features match expected format\n    Returns True if valid, False otherwise\n    \"\"\"\n    if features is None:\n        return False\n\n    # Define expected fields and valid values\n    expected_fields = ['title', 'category', 'experience_level', 'remote_policy', 'key_skills']\n    valid_categories = ['Engineering', 'Marketing', 'Sales', 'Support', 'Other']\n    valid_experience = ['Entry', 'Mid', 'Senior', 'Lead']\n    valid_remote = ['Remote', 'Hybrid', 'Onsite']\n\n    # Check all required fields present\n    if not all(field in features for field in expected_fields):\n        print(\"Missing required fields\")\n        return False\n\n    # Check category is valid\n    if features['category'] not in valid_categories:\n        print(f\"Invalid category: {features['category']}\")\n        return False\n\n    # Check experience level is valid\n    if features['experience_level'] not in valid_experience:\n        print(f\"Invalid experience level: {features['experience_level']}\")\n        return False\n\n    # Check remote policy is valid\n    if features['remote_policy'] not in valid_remote:\n        print(f\"Invalid remote policy: {features['remote_policy']}\")\n        return False\n\n    # Check key_skills is a list\n    if not isinstance(features['key_skills'], list):\n        print(\"key_skills should be a list\")\n        return False\n\n    return True\n\n# Test validation (only if we have features from previous cell)\nif client is not None:\n    # features was defined in previous cell when client is not None\n    is_valid = validate_job_features(features)\n    print(f\"\\nValidation result: {is_valid}\")\nelse:\n    print(\"Skipping validation - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Validation result: True\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkipping validation - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nValidation result: True\n```\n:::\n:::\n\n\nSee what happened? All the extracted fields are in the expected format: category is one of the valid options, experience level is valid, remote policy is valid, and key_skills is a list. The validation passed!\n\n::: {.callout-warning}\n**Never Trust LLM Output Blindly**\nLLMs can:\n- Return invalid categories\n- Return wrong data types\n- Hallucinate information not in the text\n- Miss fields entirely\n\nAlways validate before using extracted features in ML models.\n:::\n\n### 4.3 Batch Processing Multiple Texts\n\nReal datasets have hundreds or thousands of texts. Let's process them efficiently:\n\n::: {#d298c7fc .cell execution_count=13}\n``` {.python .cell-code}\ndef extract_batch(texts, extract_func, show_progress=True):\n    \"\"\"\n    Extract features from multiple texts\n\n    Args:\n        texts: List of text strings\n        extract_func: Function that extracts features from one text\n        show_progress: Whether to print progress\n\n    Returns:\n        List of extracted features (same length as texts)\n    \"\"\"\n    results = []\n\n    for i, text in enumerate(texts):\n        if show_progress and (i % 10 == 0):\n            print(f\"Processing {i}/{len(texts)}...\")\n\n        result = extract_func(text)\n        results.append(result)\n\n    return results\n\n# Example: Batch process multiple reviews\nreviews = [\n    \"Amazing product! Best purchase ever.\",\n    \"Terrible quality. Broke immediately.\",\n    \"It's fine. Nothing special.\",\n    \"Great value for money!\",\n    \"Disappointed with this purchase.\"\n]\n\n# Note: This would actually call the API multiple times\n# We'll implement a more efficient version with error handling next\nsentiments = extract_batch(reviews, extract_sentiment_robust, show_progress=True)\n\nprint(\"\\nResults:\")\nfor review, sentiment in zip(reviews, sentiments):\n    print(f\"'{review[:30]}...' → {sentiment}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing 0/5...\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\n\nResults:\n'Amazing product! Best purchase...' → None\n'Terrible quality. Broke immedi...' → None\n'It's fine. Nothing special....' → None\n'Great value for money!...' → None\n'Disappointed with this purchas...' → None\n```\n:::\n:::\n\n\n### 4.4 Converting to pandas DataFrame\n\nNow the payoff—converting extracted features to a clean DataFrame ready for ML:\n\n::: {#11071b60 .cell execution_count=14}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Simulated extraction results (in practice, these come from API calls)\nextraction_results = [\n    {\n        \"title\": \"Senior Data Scientist\",\n        \"category\": \"Engineering\",\n        \"experience_level\": \"Senior\",\n        \"remote_policy\": \"Remote\",\n        \"key_skills\": [\"Python\", \"SQL\", \"ML\"]\n    },\n    {\n        \"title\": \"Marketing Manager\",\n        \"category\": \"Marketing\",\n        \"experience_level\": \"Mid\",\n        \"remote_policy\": \"Hybrid\",\n        \"key_skills\": [\"SEO\", \"Analytics\", \"Content\"]\n    },\n    {\n        \"title\": \"Sales Representative\",\n        \"category\": \"Sales\",\n        \"experience_level\": \"Entry\",\n        \"remote_policy\": \"Onsite\",\n        \"key_skills\": [\"Communication\", \"CRM\"]\n    }\n]\n\n# Convert to DataFrame\ndf = pd.DataFrame(extraction_results)\n\n# Handle the list field (key_skills)\ndf['num_skills'] = df['key_skills'].apply(len)\ndf['skills_str'] = df['key_skills'].apply(lambda x: ', '.join(x))\n\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>category</th>\n      <th>experience_level</th>\n      <th>remote_policy</th>\n      <th>key_skills</th>\n      <th>num_skills</th>\n      <th>skills_str</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Senior Data Scientist</td>\n      <td>Engineering</td>\n      <td>Senior</td>\n      <td>Remote</td>\n      <td>[Python, SQL, ML]</td>\n      <td>3</td>\n      <td>Python, SQL, ML</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Marketing Manager</td>\n      <td>Marketing</td>\n      <td>Mid</td>\n      <td>Hybrid</td>\n      <td>[SEO, Analytics, Content]</td>\n      <td>3</td>\n      <td>SEO, Analytics, Content</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sales Representative</td>\n      <td>Sales</td>\n      <td>Entry</td>\n      <td>Onsite</td>\n      <td>[Communication, CRM]</td>\n      <td>2</td>\n      <td>Communication, CRM</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow we have a clean DataFrame! The categorical variables (category, experience_level, remote_policy) are ready for encoding. The skills are processed. This data is ready to be fed into machine learning models.\n\n---\n\n## 5. Integration with ML Pipelines\n\n### 5.1 The Complete Pipeline: Text → Features → Model\n\nLet's build a complete example: extract features from reviews, then predict review helpfulness:\n\n::: {#8b68e306 .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Simulated dataset: product reviews with helpfulness labels\nreviews_data = {\n    'review_text': [\n        \"Amazing product! The quality is outstanding. Highly recommend.\",\n        \"Total waste of money. Broke after one use.\",\n        \"It's okay. Does the job but nothing special.\",\n        \"Best purchase I've made! Love everything about it.\",\n        \"Disappointed. Expected better quality for the price.\",\n        \"Works as described. No complaints.\",\n        \"Fantastic! Exceeded my expectations in every way.\",\n        \"Not worth it. Too expensive for what you get.\",\n        \"Pretty good. Would buy again.\",\n        \"Terrible product. Avoid at all costs.\"\n    ],\n    'helpful_votes': [45, 32, 8, 51, 28, 12, 48, 25, 15, 38]  # Number of helpful votes\n}\n\ndf_reviews = pd.DataFrame(reviews_data)\n\n# Create binary target: helpful (>20 votes) or not helpful (≤20 votes)\ndf_reviews['is_helpful'] = (df_reviews['helpful_votes'] > 20).astype(int)\n\nprint(\"Dataset:\")\nprint(df_reviews[['review_text', 'helpful_votes', 'is_helpful']].head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset:\n                                         review_text  helpful_votes  \\\n0  Amazing product! The quality is outstanding. H...             45   \n1         Total waste of money. Broke after one use.             32   \n2       It's okay. Does the job but nothing special.              8   \n3  Best purchase I've made! Love everything about...             51   \n4  Disappointed. Expected better quality for the ...             28   \n\n   is_helpful  \n0           1  \n1           1  \n2           0  \n3           1  \n4           1  \n```\n:::\n:::\n\n\nNow let's extract features using our LLM:\n\n::: {#98e31861 .cell execution_count=16}\n``` {.python .cell-code}\ndef extract_review_features_for_ml(review_text):\n    \"\"\"\n    Extract features specifically useful for predicting helpfulness\n    \"\"\"\n    if client is None:\n        # Return default values if no API key\n        return {\n            \"sentiment\": \"neutral\",\n            \"is_detailed\": False,\n            \"mentions_specific_features\": False,\n            \"mentions_price_value\": False,\n            \"has_comparison\": False\n        }\n\n    prompt = f\"\"\"Analyze this product review and extract features.\n\nRespond with valid JSON:\n{{\n    \"sentiment\": \"positive/negative/neutral\",\n    \"is_detailed\": true/false,\n    \"mentions_specific_features\": true/false,\n    \"mentions_price_value\": true/false,\n    \"has_comparison\": true/false\n}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=150\n        )\n\n        result_text = response.choices[0].message.content.strip()\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n        features = json.loads(result_text)\n        return features\n    except:\n        # Return default values if extraction fails\n        return {\n            \"sentiment\": \"neutral\",\n            \"is_detailed\": False,\n            \"mentions_specific_features\": False,\n            \"mentions_price_value\": False,\n            \"has_comparison\": False\n        }\n\n# Extract features for all reviews\n# NOTE: In practice, this would make API calls\n# For this example, we'll simulate the results\nllm_features = [\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"neutral\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": True, \"mentions_specific_features\": False,\n     \"mentions_price_value\": True, \"has_comparison\": False},\n    {\"sentiment\": \"neutral\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": True, \"mentions_specific_features\": False,\n     \"mentions_price_value\": True, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n]\n\n# Add LLM features to DataFrame\ndf_features = pd.DataFrame(llm_features)\ndf_reviews = pd.concat([df_reviews, df_features], axis=1)\n\ndf_reviews.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_text</th>\n      <th>helpful_votes</th>\n      <th>is_helpful</th>\n      <th>sentiment</th>\n      <th>is_detailed</th>\n      <th>mentions_specific_features</th>\n      <th>mentions_price_value</th>\n      <th>has_comparison</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Amazing product! The quality is outstanding. H...</td>\n      <td>45</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Total waste of money. Broke after one use.</td>\n      <td>32</td>\n      <td>1</td>\n      <td>negative</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>It's okay. Does the job but nothing special.</td>\n      <td>8</td>\n      <td>0</td>\n      <td>neutral</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Best purchase I've made! Love everything about...</td>\n      <td>51</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Disappointed. Expected better quality for the ...</td>\n      <td>28</td>\n      <td>1</td>\n      <td>negative</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### 5.2 Encoding and Training\n\nNow we have LLM-extracted features. Let's train a model:\n\n::: {#46e50733 .cell execution_count=17}\n``` {.python .cell-code}\n# Encode sentiment\nle = LabelEncoder()\ndf_reviews['sentiment_encoded'] = le.fit_transform(df_reviews['sentiment'])\n\n# Select features for ML model\nfeature_columns = [\n    'sentiment_encoded',\n    'is_detailed',\n    'mentions_specific_features',\n    'mentions_price_value',\n    'has_comparison'\n]\n\nX = df_reviews[feature_columns]\ny = df_reviews['is_helpful']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train a classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"\\nModel Performance:\")\nprint(classification_report(y_test, y_pred))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': feature_columns,\n    'importance': clf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Performance:\n              precision    recall  f1-score   support\n\n           0       1.00      0.50      0.67         2\n           1       0.50      1.00      0.67         1\n\n    accuracy                           0.67         3\n   macro avg       0.75      0.75      0.67         3\nweighted avg       0.83      0.67      0.67         3\n\n\nFeature Importance:\n                      feature  importance\n0           sentiment_encoded    0.589149\n1                 is_detailed    0.295655\n2  mentions_specific_features    0.090456\n3        mentions_price_value    0.024740\n4              has_comparison    0.000000\n```\n:::\n:::\n\n\nSee what we did? We used LLM to extract features from text, encoded them properly, and trained a traditional ML model. The LLM extracted semantic information (sentiment, detail level, specific mentions) that would be hard to capture with simple word counts or regex patterns.\n\n### 5.3 Avoiding Data Leakage with LLM Features\n\nImportant consideration: when do you extract features?\n\n::: {#928d5194 .cell execution_count=18}\n``` {.python .cell-code}\n# WRONG WAY - Data leakage!\n# Don't do this: extracting features from full dataset before split\n# The LLM might learn patterns from test set during extraction\n\n# RIGHT WAY - Extract after split\n# But wait... LLMs don't \"learn\" from your prompts in real-time\n# So technically, this isn't data leakage in the traditional sense\n\n# However, best practice:\n# 1. Split your data first\n# 2. Develop/test prompts ONLY on training data\n# 3. Once prompt is finalized, apply to train and test separately\n# 4. Never iterate on prompts while looking at test set results\n```\n:::\n\n\nThe principle: don't use test set information to develop your extraction prompts, just like you wouldn't use test set to select model hyperparameters.\n\n### 5.4 Comparing With and Without LLM Features\n\nLet's see if LLM features actually help:\n\n::: {#c3574d3d .cell execution_count=19}\n``` {.python .cell-code}\n# Baseline: Just simple features (no LLM)\ndf_reviews['review_length'] = df_reviews['review_text'].apply(len)\ndf_reviews['word_count'] = df_reviews['review_text'].apply(lambda x: len(x.split()))\ndf_reviews['exclamation_count'] = df_reviews['review_text'].apply(lambda x: x.count('!'))\n\n# Model 1: Without LLM features\nX_baseline = df_reviews[['review_length', 'word_count', 'exclamation_count']]\nX_train_base, X_test_base, y_train, y_test = train_test_split(\n    X_baseline, y, test_size=0.3, random_state=42\n)\n\nclf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_baseline.fit(X_train_base, y_train)\nbaseline_score = clf_baseline.score(X_test_base, y_test)\n\n# Model 2: With LLM features\nX_llm = df_reviews[['review_length', 'word_count', 'exclamation_count'] + feature_columns]\nX_train_llm, X_test_llm, y_train, y_test = train_test_split(\n    X_llm, y, test_size=0.3, random_state=42\n)\n\nclf_llm = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_llm.fit(X_train_llm, y_train)\nllm_score = clf_llm.score(X_test_llm, y_test)\n\nprint(f\"Baseline model (no LLM) accuracy: {baseline_score:.3f}\")\nprint(f\"With LLM features accuracy: {llm_score:.3f}\")\nprint(f\"Improvement: {llm_score - baseline_score:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBaseline model (no LLM) accuracy: 0.000\nWith LLM features accuracy: 0.333\nImprovement: 0.333\n```\n:::\n:::\n\n\nThis comparison tells you whether the LLM extraction was worth the cost. Sometimes it helps significantly. Sometimes simple features work just as well. Always compare!\n\n---\n\n## 6. Cost Analysis and Provider Comparison\n\n### 6.1 Understanding Token-Based Pricing\n\nLLM APIs charge per token. Understanding costs is crucial:\n\n::: {#f59603a6 .cell execution_count=20}\n``` {.python .cell-code}\ndef estimate_tokens(text):\n    \"\"\"\n    Rough estimate: 1 token ≈ 0.75 words\n    Or approximately: 1 token ≈ 4 characters\n    \"\"\"\n    # Method 1: Based on words\n    word_estimate = len(text.split()) * 1.3\n\n    # Method 2: Based on characters\n    char_estimate = len(text) / 4\n\n    # Average the two methods\n    return int((word_estimate + char_estimate) / 2)\n\n# Example texts\ntexts = [\n    \"Short review.\",\n    \"This is a medium-length review with several sentences about the product.\",\n    \"\"\"This is a long, detailed review that goes into great depth about\n    various aspects of the product including quality, price, features,\n    customer service, shipping speed, packaging, and overall value for money.\n    I would definitely recommend this to anyone considering a purchase.\"\"\"\n]\n\nfor text in texts:\n    estimated = estimate_tokens(text)\n    print(f\"Text length: {len(text)} chars, ~{estimated} tokens\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText length: 13 chars, ~2 tokens\nText length: 72 chars, ~16 tokens\nText length: 285 chars, ~62 tokens\n```\n:::\n:::\n\n\n### 6.2 Calculating Extraction Costs\n\nLet's calculate real costs for a dataset:\n\n::: {#120d3620 .cell execution_count=21}\n``` {.python .cell-code}\ndef calculate_extraction_cost(\n    num_texts,\n    avg_text_length,\n    prompt_tokens,\n    response_tokens,\n    model='gpt-3.5-turbo'\n):\n    \"\"\"\n    Calculate total cost for extracting features from a dataset\n\n    Pricing (as of 2024):\n    - GPT-3.5-turbo: $0.0005 per 1K tokens (input and output)\n    - GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens\n    - Claude Sonnet: $0.003 per 1K input tokens, $0.015 per 1K output tokens\n    \"\"\"\n    # Estimate tokens per extraction\n    text_tokens = avg_text_length / 4  # Rough estimate\n    total_input_tokens = (prompt_tokens + text_tokens) * num_texts\n    total_output_tokens = response_tokens * num_texts\n\n    # Pricing\n    if model == 'gpt-3.5-turbo':\n        cost_per_1k_input = 0.0005\n        cost_per_1k_output = 0.0005\n    elif model == 'gpt-4':\n        cost_per_1k_input = 0.03\n        cost_per_1k_output = 0.06\n    elif model == 'claude-sonnet':\n        cost_per_1k_input = 0.003\n        cost_per_1k_output = 0.015\n    else:\n        raise ValueError(f\"Unknown model: {model}\")\n\n    input_cost = (total_input_tokens / 1000) * cost_per_1k_input\n    output_cost = (total_output_tokens / 1000) * cost_per_1k_output\n    total_cost = input_cost + output_cost\n\n    return {\n        'total_input_tokens': int(total_input_tokens),\n        'total_output_tokens': int(total_output_tokens),\n        'input_cost': input_cost,\n        'output_cost': output_cost,\n        'total_cost': total_cost\n    }\n\n# Example: Extract sentiment from 10,000 product reviews\nnum_reviews = 10000\navg_review_length = 200  # characters\nprompt_tokens = 50  # Our prompt\nresponse_tokens = 5  # Just \"positive\"/\"negative\"/\"neutral\"\n\nprint(\"Cost comparison for 10,000 reviews:\\n\")\n\nfor model in ['gpt-3.5-turbo', 'gpt-4', 'claude-sonnet']:\n    cost_info = calculate_extraction_cost(\n        num_reviews, avg_review_length, prompt_tokens, response_tokens, model\n    )\n    print(f\"{model}:\")\n    print(f\"  Total tokens: {cost_info['total_input_tokens'] + cost_info['total_output_tokens']:,}\")\n    print(f\"  Total cost: ${cost_info['total_cost']:.2f}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCost comparison for 10,000 reviews:\n\ngpt-3.5-turbo:\n  Total tokens: 1,050,000\n  Total cost: $0.53\n\ngpt-4:\n  Total tokens: 1,050,000\n  Total cost: $33.00\n\nclaude-sonnet:\n  Total tokens: 1,050,000\n  Total cost: $3.75\n\n```\n:::\n:::\n\n\nThis shows the dramatic cost difference between models. For simple extraction, GPT-3.5 is often sufficient and much cheaper.\n\n### 6.3 When to Use Which Model\n\nDecision framework:\n\n::: {#f7ace3a9 .cell execution_count=22}\n``` {.python .cell-code}\ndef recommend_model(task_complexity, dataset_size, budget):\n    \"\"\"\n    Recommend which LLM to use based on requirements\n\n    Args:\n        task_complexity: 'simple', 'moderate', 'complex'\n        dataset_size: number of texts to process\n        budget: maximum budget in dollars\n\n    Returns:\n        Recommended model and reasoning\n    \"\"\"\n    # Estimate costs (simplified)\n    gpt35_cost_per_item = 0.0001\n    gpt4_cost_per_item = 0.001\n\n    gpt35_total = gpt35_cost_per_item * dataset_size\n    gpt4_total = gpt4_cost_per_item * dataset_size\n\n    recommendations = []\n\n    if task_complexity == 'simple':\n        recommendations.append({\n            'model': 'gpt-3.5-turbo',\n            'reasoning': 'Simple extraction tasks work well with GPT-3.5',\n            'estimated_cost': gpt35_total\n        })\n\n    elif task_complexity == 'moderate':\n        if gpt35_total <= budget:\n            recommendations.append({\n                'model': 'gpt-3.5-turbo (try first)',\n                'reasoning': 'Start with GPT-3.5, upgrade if quality insufficient',\n                'estimated_cost': gpt35_total\n            })\n        if gpt4_total <= budget:\n            recommendations.append({\n                'model': 'gpt-4 (if GPT-3.5 fails)',\n                'reasoning': 'Better accuracy but 10x cost',\n                'estimated_cost': gpt4_total\n            })\n\n    else:  # complex\n        if gpt4_total <= budget:\n            recommendations.append({\n                'model': 'gpt-4',\n                'reasoning': 'Complex tasks need GPT-4 reasoning',\n                'estimated_cost': gpt4_total\n            })\n        else:\n            recommendations.append({\n                'model': 'Consider alternatives',\n                'reasoning': 'Budget insufficient for GPT-4 at this scale. Consider: sampling, fine-tuning smaller model, or traditional NLP',\n                'estimated_cost': None\n            })\n\n    return recommendations\n\n# Examples\nscenarios = [\n    ('simple', 10000, 10),\n    ('moderate', 5000, 5),\n    ('complex', 1000, 50),\n]\n\nfor complexity, size, budget in scenarios:\n    print(f\"\\nScenario: {complexity} task, {size:,} items, ${budget} budget\")\n    recs = recommend_model(complexity, size, budget)\n    for rec in recs:\n        print(f\"  → {rec['model']}: {rec['reasoning']}\")\n        if rec['estimated_cost']:\n            print(f\"    Estimated cost: ${rec['estimated_cost']:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nScenario: simple task, 10,000 items, $10 budget\n  → gpt-3.5-turbo: Simple extraction tasks work well with GPT-3.5\n    Estimated cost: $1.00\n\nScenario: moderate task, 5,000 items, $5 budget\n  → gpt-3.5-turbo (try first): Start with GPT-3.5, upgrade if quality insufficient\n    Estimated cost: $0.50\n  → gpt-4 (if GPT-3.5 fails): Better accuracy but 10x cost\n    Estimated cost: $5.00\n\nScenario: complex task, 1,000 items, $50 budget\n  → gpt-4: Complex tasks need GPT-4 reasoning\n    Estimated cost: $1.00\n```\n:::\n:::\n\n\n::: {.callout-tip}\n**Cost Optimization Strategies:**\n1. **Start cheap:** Try GPT-3.5 first, upgrade only if needed\n2. **Sample first:** Test on 100 examples before processing 10,000\n3. **Shorten prompts:** Every token counts—be concise\n4. **Cache results:** Don't reprocess the same text twice\n5. **Consider batching:** Some providers offer batch APIs at lower cost\n:::\n\n### 6.4 ROI Analysis: LLM vs. Alternatives\n\nIs LLM extraction worth it? Compare alternatives:\n\n::: {#68f89dfb .cell execution_count=23}\n``` {.python .cell-code}\n# Scenario: Classify 10,000 customer support tickets\n\nalternatives = {\n    'Manual labeling': {\n        'cost': 0.50 * 10000,  # $0.50 per ticket\n        'time_hours': 333,  # 2 mins per ticket\n        'accuracy': 0.95,\n        'scalability': 'Poor'\n    },\n    'Traditional ML (train from scratch)': {\n        'cost': 2000,  # Data labeling for training set\n        'time_hours': 40,  # Development time\n        'accuracy': 0.85,\n        'scalability': 'Excellent (after training)'\n    },\n    'LLM extraction (GPT-3.5)': {\n        'cost': 1.0 * 10000 * 0.0001,  # $1.00\n        'time_hours': 1,  # Just API calls\n        'accuracy': 0.90,\n        'scalability': 'Excellent'\n    },\n    'LLM extraction (GPT-4)': {\n        'cost': 10.0 * 10000 * 0.0001,  # $10.00\n        'time_hours': 1,\n        'accuracy': 0.93,\n        'scalability': 'Excellent'\n    }\n}\n\ncomparison = pd.DataFrame(alternatives).T\ncomparison.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cost</th>\n      <th>time_hours</th>\n      <th>accuracy</th>\n      <th>scalability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Manual labeling</th>\n      <td>5000.0</td>\n      <td>333</td>\n      <td>0.95</td>\n      <td>Poor</td>\n    </tr>\n    <tr>\n      <th>Traditional ML (train from scratch)</th>\n      <td>2000</td>\n      <td>40</td>\n      <td>0.85</td>\n      <td>Excellent (after training)</td>\n    </tr>\n    <tr>\n      <th>LLM extraction (GPT-3.5)</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.9</td>\n      <td>Excellent</td>\n    </tr>\n    <tr>\n      <th>LLM extraction (GPT-4)</th>\n      <td>10.0</td>\n      <td>1</td>\n      <td>0.93</td>\n      <td>Excellent</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFor this scenario, LLM extraction with GPT-3.5 is clearly the winner: cheap, fast, accurate enough, and scalable. But the best choice depends on your specific constraints.\n\n---\n\n## 7. Quality Control and Validation\n\n### 7.1 Measuring Extraction Accuracy\n\nHow do you know if your LLM extraction is working well?\n\n::: {#964a8983 .cell execution_count=24}\n``` {.python .cell-code}\ndef calculate_extraction_accuracy(extracted, ground_truth):\n    \"\"\"\n    Compare LLM extractions to ground truth labels\n\n    Args:\n        extracted: List of LLM-extracted labels\n        ground_truth: List of correct labels\n\n    Returns:\n        Accuracy metrics\n    \"\"\"\n    correct = sum(e == g for e, g in zip(extracted, ground_truth))\n    total = len(ground_truth)\n    accuracy = correct / total\n\n    # Breakdown by category\n    from collections import defaultdict\n    category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n\n    for ext, truth in zip(extracted, ground_truth):\n        category_stats[truth]['total'] += 1\n        if ext == truth:\n            category_stats[truth]['correct'] += 1\n\n    return {\n        'overall_accuracy': accuracy,\n        'correct': correct,\n        'total': total,\n        'by_category': dict(category_stats)\n    }\n\n# Example: Sentiment extraction validation\nground_truth_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative']\nllm_extracted_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative']\n\naccuracy = calculate_extraction_accuracy(llm_extracted_sentiments, ground_truth_sentiments)\nprint(f\"Overall accuracy: {accuracy['overall_accuracy']:.2%}\")\nprint(f\"Correct: {accuracy['correct']}/{accuracy['total']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOverall accuracy: 100.00%\nCorrect: 5/5\n```\n:::\n:::\n\n\n### 7.2 Spot-Checking and Manual Review\n\nYou can't manually check everything, but strategic sampling helps:\n\n::: {#774ff78e .cell execution_count=25}\n``` {.python .cell-code}\ndef spot_check_extractions(texts, extractions, n_samples=20, random_state=42):\n    \"\"\"\n    Sample extractions for manual review\n\n    Shows random samples + edge cases for human verification\n    \"\"\"\n    np.random.seed(random_state)\n\n    # Random sample\n    indices = np.random.choice(len(texts), min(n_samples, len(texts)), replace=False)\n\n    print(\"Random sample for manual review:\\n\")\n    for i, idx in enumerate(indices, 1):\n        print(f\"{i}. Text: {texts[idx][:80]}...\")\n        print(f\"   Extraction: {extractions[idx]}\")\n        print(f\"   Correct? (Y/N): ___\")\n        print()\n\n    return indices\n\n# Example usage\nsample_reviews = [\n    \"This product is amazing! Love it.\",\n    \"Terrible. Waste of money.\",\n    \"It's okay, I guess.\",\n    \"Best purchase ever made!\",\n    \"Not great, not terrible.\"\n]\nsample_sentiments = ['positive', 'negative', 'neutral', 'positive', 'neutral']\n\nspot_check_extractions(sample_reviews, sample_sentiments, n_samples=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom sample for manual review:\n\n1. Text: Terrible. Waste of money....\n   Extraction: negative\n   Correct? (Y/N): ___\n\n2. Text: Not great, not terrible....\n   Extraction: neutral\n   Correct? (Y/N): ___\n\n3. Text: It's okay, I guess....\n   Extraction: neutral\n   Correct? (Y/N): ___\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([1, 4, 2])\n```\n:::\n:::\n\n\n### 7.3 Identifying Systematic Errors\n\nLook for patterns in failures:\n\n::: {#c98db0a3 .cell execution_count=26}\n``` {.python .cell-code}\ndef analyze_errors(texts, extracted, ground_truth):\n    \"\"\"\n    Find patterns in extraction errors\n    \"\"\"\n    errors = []\n\n    for text, ext, truth in zip(texts, extracted, ground_truth):\n        if ext != truth:\n            errors.append({\n                'text': text,\n                'extracted': ext,\n                'ground_truth': truth,\n                'text_length': len(text),\n                'word_count': len(text.split())\n            })\n\n    if not errors:\n        print(\"No errors found!\")\n        return\n\n    error_df = pd.DataFrame(errors)\n\n    print(f\"Total errors: {len(errors)}/{len(texts)} ({len(errors)/len(texts):.1%})\\n\")\n\n    # Analyze error patterns\n    print(\"Errors by true category:\")\n    print(error_df['ground_truth'].value_counts())\n\n    print(\"\\nAverage length of texts with errors:\")\n    print(f\"  Characters: {error_df['text_length'].mean():.0f}\")\n    print(f\"  Words: {error_df['word_count'].mean():.0f}\")\n\n    print(\"\\nSample errors:\")\n    for _, row in error_df.head(3).iterrows():\n        print(f\"  Text: {row['text'][:60]}...\")\n        print(f\"  Extracted: {row['extracted']}, True: {row['ground_truth']}\\n\")\n\n    return error_df\n\n# Example with some errors\ntexts_with_errors = [\n    \"Love this product!\",\n    \"It's not bad.\",  # Tricky: double negative\n    \"Terrible quality.\",\n    \"I wouldn't say it's good.\",  # Tricky: negation\n    \"Amazing!\"\n]\nextracted_with_errors = ['positive', 'negative', 'negative', 'negative', 'positive']\nground_truth_with_errors = ['positive', 'positive', 'negative', 'negative', 'positive']\n\nerror_analysis = analyze_errors(texts_with_errors, extracted_with_errors, ground_truth_with_errors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal errors: 1/5 (20.0%)\n\nErrors by true category:\nground_truth\npositive    1\nName: count, dtype: int64\n\nAverage length of texts with errors:\n  Characters: 13\n  Words: 3\n\nSample errors:\n  Text: It's not bad....\n  Extracted: negative, True: positive\n\n```\n:::\n:::\n\n\n### 7.4 When Extraction Quality Is \"Good Enough\"\n\nPerfect extraction isn't always necessary:\n\n::: {#9ae23359 .cell execution_count=27}\n``` {.python .cell-code}\ndef is_quality_sufficient(accuracy, task_requirements):\n    \"\"\"\n    Determine if extraction quality meets requirements\n\n    Args:\n        accuracy: Measured extraction accuracy (0-1)\n        task_requirements: Dict with requirements\n\n    Returns:\n        Boolean and explanation\n    \"\"\"\n    min_accuracy = task_requirements.get('min_accuracy', 0.85)\n    critical_task = task_requirements.get('critical', False)\n\n    if critical_task:\n        # High-stakes tasks need very high accuracy\n        threshold = max(min_accuracy, 0.95)\n        sufficient = accuracy >= threshold\n        msg = f\"Critical task requires ≥{threshold:.0%} accuracy. \"\n    else:\n        # Normal tasks can tolerate some errors\n        threshold = min_accuracy\n        sufficient = accuracy >= threshold\n        msg = f\"Task requires ≥{threshold:.0%} accuracy. \"\n\n    msg += f\"Current accuracy: {accuracy:.1%}. \"\n    msg += \"✓ Sufficient\" if sufficient else \"✗ Insufficient\"\n\n    return sufficient, msg\n\n# Examples\nscenarios_quality = [\n    (0.92, {'min_accuracy': 0.85, 'critical': False}),  # Good enough\n    (0.92, {'min_accuracy': 0.85, 'critical': True}),   # Not good enough (critical)\n    (0.78, {'min_accuracy': 0.85, 'critical': False}),  # Not good enough\n]\n\nfor acc, reqs in scenarios_quality:\n    sufficient, message = is_quality_sufficient(acc, reqs)\n    print(message)\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTask requires ≥85% accuracy. Current accuracy: 92.0%. ✓ Sufficient\n\nCritical task requires ≥95% accuracy. Current accuracy: 92.0%. ✗ Insufficient\n\nTask requires ≥85% accuracy. Current accuracy: 78.0%. ✗ Insufficient\n\n```\n:::\n:::\n\n\n::: {.callout-note}\n**Quality Thresholds by Task Type:**\n- **Medical/Legal:** 95%+ accuracy required (often manual review needed)\n- **Financial:** 90-95% accuracy\n- **Marketing/Content:** 80-85% accuracy often sufficient\n- **Exploratory analysis:** 70-80% may be acceptable\n\nRemember: LLM features are inputs to ML models. Small extraction errors might not significantly hurt final model performance.\n:::\n\n---\n\n## 8. When to Use LLMs vs. Traditional NLP\n\n### 8.1 Decision Framework\n\nNot every text problem needs an LLM. Here's how to decide:\n\n::: {#abae0433 .cell execution_count=28}\n``` {.python .cell-code}\ndef should_use_llm(task_description):\n    \"\"\"\n    Decision tree: LLM vs. traditional NLP\n\n    Returns recommendation and reasoning\n    \"\"\"\n    # Simple pattern matching tasks\n    simple_patterns = [\n        'contains keyword',\n        'find email addresses',\n        'extract phone numbers',\n        'detect URLs',\n        'count words'\n    ]\n\n    # Traditional ML appropriate\n    traditional_ml_tasks = [\n        'you have large labeled dataset',\n        'need very fast inference',\n        'extremely cost-sensitive',\n        'offline/no internet'\n    ]\n\n    # LLM appropriate\n    llm_tasks = [\n        'understand context',\n        'extract categories not seen before',\n        'handle nuance',\n        'multiple languages',\n        'complex reasoning',\n        'no labeled data available'\n    ]\n\n    task_lower = task_description.lower()\n\n    # Check each category\n    if any(pattern in task_lower for pattern in simple_patterns):\n        return {\n            'recommendation': 'Use regex or simple string matching',\n            'reasoning': 'Simple pattern matching - no need for expensive LLM',\n            'example_tool': 're module in Python'\n        }\n\n    if any(indicator in task_lower for indicator in traditional_ml_tasks):\n        return {\n            'recommendation': 'Train traditional ML model',\n            'reasoning': 'Your constraints favor traditional ML over LLM APIs',\n            'example_tool': 'scikit-learn text classification'\n        }\n\n    if any(indicator in task_lower for indicator in llm_tasks):\n        return {\n            'recommendation': 'Use LLM extraction',\n            'reasoning': 'Task requires language understanding that LLMs excel at',\n            'example_tool': 'GPT-3.5 or GPT-4 via API'\n        }\n\n    # Default: try simple first\n    return {\n        'recommendation': 'Try simple methods first, then LLM if needed',\n        'reasoning': 'Always start with simplest solution',\n        'example_tool': 'Regex → Traditional ML → LLM (in that order)'\n    }\n\n# Test with different tasks\ntasks = [\n    \"Extract email addresses from text\",\n    \"Determine sentiment of product reviews with nuanced language\",\n    \"Classify support tickets into categories with 10,000 labeled examples\",\n    \"Extract job requirements that vary significantly across postings\",\n    \"Count how many times 'refund' appears in complaints\"\n]\n\nprint(\"Task recommendations:\\n\")\nfor task in tasks:\n    rec = should_use_llm(task)\n    print(f\"Task: {task}\")\n    print(f\"  → {rec['recommendation']}\")\n    print(f\"  Reasoning: {rec['reasoning']}\")\n    print(f\"  Tool: {rec['example_tool']}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTask recommendations:\n\nTask: Extract email addresses from text\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Determine sentiment of product reviews with nuanced language\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Classify support tickets into categories with 10,000 labeled examples\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Extract job requirements that vary significantly across postings\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Count how many times 'refund' appears in complaints\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\n```\n:::\n:::\n\n\n### 8.2 Simple Regex vs. LLM: A Comparison\n\nLet's see both approaches on the same task:\n\n::: {#12a2eb4f .cell execution_count=29}\n``` {.python .cell-code}\nimport re\n\n# Task: Extract product category from reviews\n\n# Approach 1: Regex/keyword matching\ndef extract_category_regex(review_text):\n    \"\"\"Simple keyword matching\"\"\"\n    text_lower = review_text.lower()\n\n    if any(word in text_lower for word in ['coffee', 'brew', 'espresso', 'caffeine']):\n        return 'Coffee Maker'\n    elif any(word in text_lower for word in ['vacuum', 'clean', 'suction', 'dirt']):\n        return 'Vacuum'\n    elif any(word in text_lower for word in ['headphone', 'audio', 'sound', 'music']):\n        return 'Headphones'\n    else:\n        return 'Other'\n\n# Approach 2: LLM extraction\ndef extract_category_llm(review_text):\n    \"\"\"LLM-based category extraction\"\"\"\n    prompt = f\"\"\"What product category is this review about?\nChoose from: Coffee Maker, Vacuum, Headphones, Other\n\nReview: {review_text}\n\nCategory:\"\"\"\n\n    # Simulated LLM response\n    # In practice, this would call the API\n    # For now, we'll simulate based on content\n    text_lower = review_text.lower()\n    if 'coffee' in text_lower or 'brew' in text_lower:\n        return 'Coffee Maker'\n    elif 'vacuum' in text_lower or 'clean' in text_lower:\n        return 'Vacuum'\n    elif 'headphone' in text_lower or 'sound' in text_lower:\n        return 'Headphones'\n    else:\n        return 'Other'\n\n# Test cases\ntest_reviews = [\n    \"This coffee maker brews excellent coffee.\",  # Simple - both work\n    \"Makes great espresso every morning.\",  # Simple - both work\n    \"The audio quality is amazing!\",  # LLM better (understands audio → headphones)\n    \"Keeps my floors spotless.\",  # LLM better (spotless → cleaning → vacuum)\n    \"Battery life could be better but the noise cancellation is top-notch.\"  # LLM much better\n]\n\nprint(\"Category extraction comparison:\\n\")\nfor review in test_reviews:\n    regex_cat = extract_category_regex(review)\n    llm_cat = extract_category_llm(review)\n    print(f\"Review: {review}\")\n    print(f\"  Regex: {regex_cat}\")\n    print(f\"  LLM: {llm_cat}\")\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCategory extraction comparison:\n\nReview: This coffee maker brews excellent coffee.\n  Regex: Coffee Maker\n  LLM: Coffee Maker\n\nReview: Makes great espresso every morning.\n  Regex: Coffee Maker\n  LLM: Other\n\nReview: The audio quality is amazing!\n  Regex: Headphones\n  LLM: Other\n\nReview: Keeps my floors spotless.\n  Regex: Other\n  LLM: Other\n\nReview: Battery life could be better but the noise cancellation is top-notch.\n  Regex: Other\n  LLM: Other\n\n```\n:::\n:::\n\n\nThe LLM shines when:\n- Keywords aren't explicit (\"noise cancellation\" → headphones)\n- Context matters (\"spotless\" → cleaning → vacuum)\n- Synonyms and paraphrasing (\"audio quality\" = sound)\n\nRegex wins when:\n- Keywords are explicit and consistent\n- Speed is critical\n- Cost must be zero\n\n### 8.3 Cost-Benefit Comparison Table\n\n::: {#b98fe211 .cell execution_count=30}\n``` {.python .cell-code}\ncomparison_data = {\n    'Approach': ['Regex/Keywords', 'Traditional ML', 'LLM (GPT-3.5)', 'LLM (GPT-4)'],\n    'Setup Cost': ['$0', '$500-5000', '$0', '$0'],\n    'Per-Item Cost': ['$0', '$0', '$0.0001', '$0.001'],\n    'Setup Time': ['1 hour', '1-4 weeks', '1-2 hours', '1-2 hours'],\n    'Accuracy (simple)': ['60-70%', '85-90%', '85-90%', '90-95%'],\n    'Accuracy (complex)': ['40-50%', '75-85%', '80-90%', '88-95%'],\n    'Scalability': ['Excellent', 'Excellent', 'Good', 'Good'],\n    'Flexibility': ['Poor', 'Poor', 'Excellent', 'Excellent']\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Approach</th>\n      <th>Setup Cost</th>\n      <th>Per-Item Cost</th>\n      <th>Setup Time</th>\n      <th>Accuracy (simple)</th>\n      <th>Accuracy (complex)</th>\n      <th>Scalability</th>\n      <th>Flexibility</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Regex/Keywords</td>\n      <td>$0</td>\n      <td>$0</td>\n      <td>1 hour</td>\n      <td>60-70%</td>\n      <td>40-50%</td>\n      <td>Excellent</td>\n      <td>Poor</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Traditional ML</td>\n      <td>$500-5000</td>\n      <td>$0</td>\n      <td>1-4 weeks</td>\n      <td>85-90%</td>\n      <td>75-85%</td>\n      <td>Excellent</td>\n      <td>Poor</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LLM (GPT-3.5)</td>\n      <td>$0</td>\n      <td>$0.0001</td>\n      <td>1-2 hours</td>\n      <td>85-90%</td>\n      <td>80-90%</td>\n      <td>Good</td>\n      <td>Excellent</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LLM (GPT-4)</td>\n      <td>$0</td>\n      <td>$0.001</td>\n      <td>1-2 hours</td>\n      <td>90-95%</td>\n      <td>88-95%</td>\n      <td>Good</td>\n      <td>Excellent</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### 8.4 Hybrid Approaches\n\nOften, the best solution combines methods:\n\n::: {#40b8018a .cell execution_count=31}\n``` {.python .cell-code}\ndef hybrid_extraction(review_text):\n    \"\"\"\n    Hybrid approach: Use simple rules when possible, LLM for hard cases\n    \"\"\"\n    # Step 1: Try simple keyword matching\n    simple_result = extract_category_regex(review_text)\n\n    # Step 2: Check confidence\n    # If the review explicitly mentions category keywords, trust regex\n    text_lower = review_text.lower()\n    explicit_mentions = sum([\n        any(word in text_lower for word in ['coffee', 'brew', 'espresso']),\n        any(word in text_lower for word in ['vacuum', 'suction']),\n        any(word in text_lower for word in ['headphone', 'audio'])\n    ])\n\n    if explicit_mentions > 0 and simple_result != 'Other':\n        # High confidence - use regex result\n        return {\n            'category': simple_result,\n            'method': 'regex',\n            'cost': 0\n        }\n    else:\n        # Low confidence - use LLM\n        llm_result = extract_category_llm(review_text)\n        return {\n            'category': llm_result,\n            'method': 'llm',\n            'cost': 0.0001\n        }\n\n# Test hybrid approach\nprint(\"Hybrid approach results:\\n\")\nfor review in test_reviews[:3]:\n    result = hybrid_extraction(review)\n    print(f\"Review: {review}\")\n    print(f\"  Category: {result['category']} (via {result['method']})\")\n    print(f\"  Cost: ${result['cost']:.6f}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHybrid approach results:\n\nReview: This coffee maker brews excellent coffee.\n  Category: Coffee Maker (via regex)\n  Cost: $0.000000\n\nReview: Makes great espresso every morning.\n  Category: Coffee Maker (via regex)\n  Cost: $0.000000\n\nReview: The audio quality is amazing!\n  Category: Headphones (via regex)\n  Cost: $0.000000\n\n```\n:::\n:::\n\n\nThis hybrid approach saves money by using free regex when confidence is high, falling back to LLM only when needed.\n\n::: {.callout-tip}\n**Hybrid Strategy Benefits:**\n- Use regex/keywords for 60-80% of cases (free, fast)\n- Use LLM for ambiguous 20-40% (accuracy boost where it matters)\n- Best of both worlds: low cost + high accuracy\n:::\n\n---\n\n## 9. Practical Considerations and Best Practices\n\n### 9.1 Rate Limiting and API Quotas\n\nAPIs have limits. Handle them gracefully:\n\n::: {#c1444df0 .cell execution_count=32}\n``` {.python .cell-code}\nimport time\nfrom datetime import datetime\n\ndef extract_with_rate_limiting(texts, extract_func, requests_per_minute=60):\n    \"\"\"\n    Extract features with rate limiting to avoid API errors\n\n    Args:\n        texts: List of texts to process\n        extract_func: Function that extracts from one text\n        requests_per_minute: Max API calls per minute\n\n    Returns:\n        List of extracted features\n    \"\"\"\n    results = []\n    delay = 60 / requests_per_minute  # Seconds between requests\n\n    for i, text in enumerate(texts):\n        # Extract\n        result = extract_func(text)\n        results.append(result)\n\n        # Progress update\n        if (i + 1) % 10 == 0:\n            print(f\"Processed {i+1}/{len(texts)} texts...\")\n\n        # Rate limit (except for last item)\n        if i < len(texts) - 1:\n            time.sleep(delay)\n\n    return results\n\n# Example usage (simulated)\nprint(\"Processing with rate limiting:\")\nprint(f\"Rate: 60 requests/minute (1 per second)\")\nprint(f\"For 100 texts, this will take ~100 seconds\\n\")\n\n# In practice:\n# results = extract_with_rate_limiting(my_texts, extract_sentiment_robust, requests_per_minute=60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing with rate limiting:\nRate: 60 requests/minute (1 per second)\nFor 100 texts, this will take ~100 seconds\n\n```\n:::\n:::\n\n\n### 9.2 Error Handling and Retries\n\nNetworks fail. APIs timeout. Handle it:\n\n::: {#a893d0b9 .cell execution_count=33}\n``` {.python .cell-code}\ndef extract_with_retry(text, extract_func, max_retries=3, backoff=2):\n    \"\"\"\n    Extract with exponential backoff retry logic\n\n    Args:\n        text: Text to extract from\n        extract_func: Extraction function\n        max_retries: Maximum retry attempts\n        backoff: Backoff multiplier (2 = double wait time each retry)\n    \"\"\"\n    wait_time = 1  # Start with 1 second wait\n\n    for attempt in range(max_retries):\n        try:\n            result = extract_func(text)\n            return result\n        except Exception as e:\n            if attempt == max_retries - 1:\n                # Last attempt failed\n                print(f\"Failed after {max_retries} attempts: {e}\")\n                return None\n            else:\n                # Retry with exponential backoff\n                print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\")\n                time.sleep(wait_time)\n                wait_time *= backoff\n\n    return None\n\n# Example usage\nprint(\"Example retry behavior (simulated):\")\nprint(\"Attempt 1 fails → wait 1s\")\nprint(\"Attempt 2 fails → wait 2s\")\nprint(\"Attempt 3 succeeds → return result\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExample retry behavior (simulated):\nAttempt 1 fails → wait 1s\nAttempt 2 fails → wait 2s\nAttempt 3 succeeds → return result\n```\n:::\n:::\n\n\n### 9.3 Caching Results to Avoid Reprocessing\n\nNever process the same text twice:\n\n::: {#855d14ca .cell execution_count=34}\n``` {.python .cell-code}\nclass LLMCache:\n    \"\"\"Simple cache for LLM extraction results\"\"\"\n\n    def __init__(self):\n        self.cache = {}\n\n    def get(self, text):\n        \"\"\"Get cached result if available\"\"\"\n        # Use hash of text as key\n        key = hash(text)\n        return self.cache.get(key)\n\n    def set(self, text, result):\n        \"\"\"Store result in cache\"\"\"\n        key = hash(text)\n        self.cache[key] = result\n\n    def extract_with_cache(self, text, extract_func):\n        \"\"\"Extract with caching\"\"\"\n        # Check cache first\n        cached = self.get(text)\n        if cached is not None:\n            return cached\n\n        # Not in cache - extract and store\n        result = extract_func(text)\n        self.set(text, result)\n        return result\n\n# Usage example\ncache = LLMCache()\n\ntexts_with_duplicates = [\n    \"This is great!\",\n    \"This is terrible.\",\n    \"This is great!\",  # Duplicate - will use cache\n    \"This is okay.\",\n    \"This is great!\"   # Duplicate - will use cache\n]\n\nprint(\"Processing with cache:\")\nfor text in texts_with_duplicates:\n    # In practice, would call API\n    cached_result = cache.get(text)\n    if cached_result:\n        print(f\"'{text}' → CACHED\")\n    else:\n        # Simulate extraction\n        result = \"positive\"  # Simulated\n        cache.set(text, result)\n        print(f\"'{text}' → EXTRACTED (API call)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing with cache:\n'This is great!' → EXTRACTED (API call)\n'This is terrible.' → EXTRACTED (API call)\n'This is great!' → CACHED\n'This is okay.' → EXTRACTED (API call)\n'This is great!' → CACHED\n```\n:::\n:::\n\n\n### 9.4 Logging and Monitoring\n\nTrack your extractions for debugging:\n\n::: {#d84bc3fb .cell execution_count=35}\n``` {.python .cell-code}\nimport logging\nfrom datetime import datetime\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\ndef extract_with_logging(text, extract_func):\n    \"\"\"Extract with detailed logging\"\"\"\n    start_time = datetime.now()\n\n    logging.info(f\"Starting extraction for text: {text[:50]}...\")\n\n    try:\n        result = extract_func(text)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        logging.info(f\"Extraction successful in {duration:.2f}s: {result}\")\n\n        return result\n\n    except Exception as e:\n        duration = (datetime.now() - start_time).total_seconds()\n        logging.error(f\"Extraction failed after {duration:.2f}s: {e}\")\n        return None\n\n# Example\nprint(\"Extraction with logging:\")\n# result = extract_with_logging(\"Test review\", extract_sentiment_robust)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExtraction with logging:\n```\n:::\n:::\n\n\n---\n\n## 10. Real-World Example: Complete Pipeline\n\nLet's put it all together with a complete, realistic example:\n\n::: {#1363736e .cell execution_count=36}\n``` {.python .cell-code}\n# Scenario: Extract features from customer support tickets\n# Use those features to predict ticket priority\n\n# Sample support tickets\ntickets_data = {\n    'ticket_id': [1, 2, 3, 4, 5],\n    'ticket_text': [\n        \"My account is locked and I can't access my billing information. This is urgent!\",\n        \"I have a question about your pricing plans. No rush.\",\n        \"The app keeps crashing every time I try to upload a file. Very frustrating!\",\n        \"Can you explain how the export feature works?\",\n        \"CRITICAL: Production server is down! Need immediate assistance!\"\n    ],\n    'actual_priority': ['high', 'low', 'medium', 'low', 'critical']  # Ground truth\n}\n\ndf_tickets = pd.DataFrame(tickets_data)\n\n# Step 1: Define extraction function\ndef extract_ticket_features(ticket_text):\n    \"\"\"\n    Extract features from support ticket\n    \"\"\"\n    # In real implementation, this would call OpenAI API\n    # For this example, we'll simulate the extraction\n\n    # Simulated LLM extraction logic\n    text_lower = ticket_text.lower()\n\n    # Determine urgency\n    if any(word in text_lower for word in ['critical', 'urgent', 'down', 'broken']):\n        urgency = 'high'\n    elif any(word in text_lower for word in ['frustrating', 'issue', 'problem']):\n        urgency = 'medium'\n    else:\n        urgency = 'low'\n\n    # Determine category\n    if any(word in text_lower for word in ['billing', 'account', 'payment']):\n        category = 'Billing'\n    elif any(word in text_lower for word in ['crash', 'bug', 'error', 'down']):\n        category = 'Technical'\n    else:\n        category = 'General'\n\n    # Sentiment\n    if any(word in text_lower for word in ['critical', 'frustrating', 'broken']):\n        sentiment = 'negative'\n    else:\n        sentiment = 'neutral'\n\n    return {\n        'urgency': urgency,\n        'category': category,\n        'sentiment': sentiment,\n        'has_exclamation': '!' in ticket_text,\n        'text_length': len(ticket_text)\n    }\n\n# Step 2: Extract features for all tickets\nprint(\"Extracting features from tickets...\\n\")\nextracted_features = []\n\nfor text in df_tickets['ticket_text']:\n    features = extract_ticket_features(text)\n    extracted_features.append(features)\n\ndf_features = pd.DataFrame(extracted_features)\n\n# Step 3: Combine with original data\ndf_complete = pd.concat([df_tickets, df_features], axis=1)\n\nprint(\"Extracted features:\")\nprint(df_complete[['ticket_id', 'urgency', 'category', 'sentiment']].head())\n\n# Step 4: Encode for ML\nle_urgency = LabelEncoder()\nle_category = LabelEncoder()\nle_sentiment = LabelEncoder()\n\ndf_complete['urgency_encoded'] = le_urgency.fit_transform(df_complete['urgency'])\ndf_complete['category_encoded'] = le_category.fit_transform(df_complete['category'])\ndf_complete['sentiment_encoded'] = le_sentiment.fit_transform(df_complete['sentiment'])\ndf_complete['has_exclamation_int'] = df_complete['has_exclamation'].astype(int)\n\n# Step 5: Train ML model\nfeature_cols = ['urgency_encoded', 'category_encoded', 'sentiment_encoded',\n                'has_exclamation_int', 'text_length']\n\nX = df_complete[feature_cols]\ny = df_complete['actual_priority']\n\n# Note: In practice, you'd have more data and do train-test split\n# This is just a demonstration\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=3, random_state=42)\nclf.fit(X, y)\n\n# Step 6: Make predictions\ndf_complete['predicted_priority'] = clf.predict(X)\n\n# Step 7: Evaluate\nprint(\"\\n\\nResults:\")\nprint(df_complete[['ticket_id', 'actual_priority', 'predicted_priority']])\n\naccuracy = (df_complete['actual_priority'] == df_complete['predicted_priority']).mean()\nprint(f\"\\nAccuracy: {accuracy:.1%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExtracting features from tickets...\n\nExtracted features:\n   ticket_id urgency   category sentiment\n0          1    high    Billing   neutral\n1          2     low    General   neutral\n2          3  medium  Technical  negative\n3          4     low    General   neutral\n4          5    high  Technical  negative\n\n\nResults:\n   ticket_id actual_priority predicted_priority\n0          1            high               high\n1          2             low                low\n2          3          medium             medium\n3          4             low                low\n4          5        critical           critical\n\nAccuracy: 100.0%\n```\n:::\n:::\n\n\nThis complete pipeline shows the real workflow:\n1. Define extraction prompt/function\n2. Extract features from text\n3. Validate and convert to DataFrame\n4. Encode categorical variables\n5. Train ML model on extracted features\n6. Evaluate performance\n\nIn production, you'd add:\n- Error handling and retries\n- Rate limiting\n- Caching\n- Logging\n- Cost tracking\n- Quality monitoring\n\nBut the core pattern remains the same: **text → LLM extraction → features → ML model → predictions**.\n\n---\n\n## Summary\n\nLLMs are powerful feature engineering tools that transform unstructured text into structured, ML-ready data. The key advantage is zero-shot learning—you can extract information without training data, just clear prompts. But they're not magic bullets.\n\nYou've learned to write effective extraction prompts: clear task descriptions, specific output formats (JSON), few-shot examples when needed, and iterative refinement based on errors. You know how to parse and validate LLM responses, handle malformed JSON, check for invalid categories, and identify when extraction fails.\n\nCost matters. GPT-3.5 costs $0.0005 per 1K tokens; GPT-4 costs 60x more. For simple extraction on large datasets, that difference is huge. Start with GPT-3.5, upgrade only if quality demands it. Calculate costs before processing thousands of texts. Compare LLM cost to alternatives—sometimes manual labeling or traditional ML is cheaper.\n\nQuality control is critical. Spot-check extractions manually. Calculate accuracy against ground truth. Identify systematic errors and refine prompts. Decide when quality is \"good enough\" based on task requirements. Perfect extraction isn't always necessary—remember that LLM features are inputs to ML models, and small errors might not hurt final performance.\n\nKnow when to use LLMs versus alternatives. Simple keyword matching works for pattern detection. Traditional ML works when you have labeled data and need very fast inference. LLMs excel at context understanding, handling nuanced language, extracting categories without training, and zero-shot learning. Often, hybrid approaches work best: use simple rules when confident, fall back to LLM for hard cases.\n\nIntegration with ML pipelines is straightforward: extract features, convert to DataFrame, encode categorical variables, train traditional ML models. The LLM doesn't replace your ML workflow—it enhances it by creating better features from text.\n\nLLMs are tools, not solutions. They cost money, make mistakes, and sometimes overkill. The skill isn't just using them—it's knowing when they add value, which model to choose, how to validate output, and how to integrate them into your data science workflow. Use your brain. That's what it's there for.\n\n---\n\n## Practice Exercises\n\n1. **Prompt Engineering:** Write three different prompts to extract sentiment from movie reviews. Test them on 10 reviews. Which prompt gives the most consistent results? Why?\n\n2. **Cost Analysis:** Calculate the cost to extract categories from 50,000 product descriptions using GPT-3.5 vs. GPT-4. Assume average description length of 150 characters, prompt of 60 tokens, response of 10 tokens.\n\n3. **Validation:** Extract job seniority levels (Entry/Mid/Senior) from 20 job postings. Manually label them yourself. Calculate your LLM's accuracy. Identify which extractions failed and why.\n\n4. **Complete Pipeline:** Build a pipeline that: (a) extracts sentiment and rating from customer reviews, (b) converts to DataFrame, (c) trains a classifier to predict if review is helpful (>10 votes), (d) evaluates performance.\n\n5. **Comparison:** Take a simple classification task (e.g., categorizing emails as work/personal/spam). Implement three approaches: (1) keyword matching, (2) LLM extraction, (3) hybrid. Compare accuracy and cost.\n\n6. **Quality Control:** Extract product categories from 100 product titles. Create a validation function that flags suspicious extractions (e.g., category not in predefined list). How many would need manual review?\n\n---\n\n## Additional Resources\n\n- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference) - Complete API reference and guides\n- [Anthropic Claude API](https://docs.anthropic.com/) - Alternative LLM provider with good context windows\n- [Prompt Engineering Guide](https://www.promptingguide.ai/) - Comprehensive guide to writing effective prompts\n- [LangChain Documentation](https://python.langchain.com/) - Framework for building LLM applications (more advanced)\n- [OpenAI Cookbook](https://cookbook.openai.com/) - Practical examples and code snippets\n- [Token Counting Tool](https://platform.openai.com/tokenizer) - See how text converts to tokens\n- [Best Practices for Prompt Engineering](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api) - Official OpenAI guide\n\n",
    "supporting": [
      "chapter-4-llms-feature-engineering_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}