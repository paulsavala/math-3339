<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 2: Regression Models – MATH 3339: Introduction to Data Science with LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/textbook-nav.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#ch2-resources" id="toc-ch2-resources" class="nav-link active" data-scroll-target="#ch2-resources">Chapter Resources</a></li>
  <li><a href="#ch2-intro" id="toc-ch2-intro" class="nav-link" data-scroll-target="#ch2-intro">Introduction</a></li>
  <li><a href="#ch2-1" id="toc-ch2-1" class="nav-link" data-scroll-target="#ch2-1">1. Train/Validation/Test Methodology</a>
  <ul class="collapse">
  <li><a href="#ch2-1-1" id="toc-ch2-1-1" class="nav-link" data-scroll-target="#ch2-1-1">1.1 Why Three Sets?</a></li>
  <li><a href="#ch2-1-2" id="toc-ch2-1-2" class="nav-link" data-scroll-target="#ch2-1-2">1.2 Cross-Validation: Getting More Reliable Estimates</a></li>
  <li><a href="#ch2-2-outcomes" id="toc-ch2-2-outcomes" class="nav-link" data-scroll-target="#ch2-2-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-2" id="toc-ch2-2" class="nav-link" data-scroll-target="#ch2-2">2. Linear Regression</a>
  <ul class="collapse">
  <li><a href="#ch2-2-1" id="toc-ch2-2-1" class="nav-link" data-scroll-target="#ch2-2-1">2.1 The Line of Best Fit</a></li>
  <li><a href="#ch2-2-2" id="toc-ch2-2-2" class="nav-link" data-scroll-target="#ch2-2-2">2.2 The Assumptions Behind Linear Regression</a></li>
  <li><a href="#ch2-2-3" id="toc-ch2-2-3" class="nav-link" data-scroll-target="#ch2-2-3">2.3 Interpreting Coefficients</a></li>
  <li><a href="#ch2-2-outcomes" id="toc-ch2-2-outcomes" class="nav-link" data-scroll-target="#ch2-2-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-3" id="toc-ch2-3" class="nav-link" data-scroll-target="#ch2-3">3. Regression Evaluation Metrics</a>
  <ul class="collapse">
  <li><a href="#ch2-3-1" id="toc-ch2-3-1" class="nav-link" data-scroll-target="#ch2-3-1">3.1 Mean Squared Error (MSE)</a></li>
  <li><a href="#ch2-3-2" id="toc-ch2-3-2" class="nav-link" data-scroll-target="#ch2-3-2">3.2 Mean Absolute Error (MAE)</a></li>
  <li><a href="#ch2-3-3" id="toc-ch2-3-3" class="nav-link" data-scroll-target="#ch2-3-3">3.3 R² (R-Squared)</a></li>
  <li><a href="#ch2-3-4" id="toc-ch2-3-4" class="nav-link" data-scroll-target="#ch2-3-4">3.4 Adjusted R²</a></li>
  <li><a href="#ch2-3-outcomes" id="toc-ch2-3-outcomes" class="nav-link" data-scroll-target="#ch2-3-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-4" id="toc-ch2-4" class="nav-link" data-scroll-target="#ch2-4">4. Residual Analysis: Your Diagnostic Tool</a>
  <ul class="collapse">
  <li><a href="#ch2-4-1" id="toc-ch2-4-1" class="nav-link" data-scroll-target="#ch2-4-1">4.1 What Are Residuals?</a></li>
  <li><a href="#ch2-4-2" id="toc-ch2-4-2" class="nav-link" data-scroll-target="#ch2-4-2">4.2 Residuals vs.&nbsp;Fitted Values Plot</a></li>
  <li><a href="#ch2-4-3" id="toc-ch2-4-3" class="nav-link" data-scroll-target="#ch2-4-3">4.3 What Patterns Tell You</a></li>
  <li><a href="#ch2-4-4" id="toc-ch2-4-4" class="nav-link" data-scroll-target="#ch2-4-4">4.4 What to Do When Assumptions Fail</a></li>
  <li><a href="#ch2-4-outcomes" id="toc-ch2-4-outcomes" class="nav-link" data-scroll-target="#ch2-4-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-5" id="toc-ch2-5" class="nav-link" data-scroll-target="#ch2-5">5. Polynomial Regression: Handling Non-Linearity</a>
  <ul class="collapse">
  <li><a href="#ch2-5-1" id="toc-ch2-5-1" class="nav-link" data-scroll-target="#ch2-5-1">5.1 When Linear Isn’t Enough</a></li>
  <li><a href="#ch2-5-2" id="toc-ch2-5-2" class="nav-link" data-scroll-target="#ch2-5-2">5.2 Creating Polynomial Features</a></li>
  <li><a href="#ch2-5-3" id="toc-ch2-5-3" class="nav-link" data-scroll-target="#ch2-5-3">5.3 Choosing Polynomial Degree</a></li>
  <li><a href="#ch2-5-4" id="toc-ch2-5-4" class="nav-link" data-scroll-target="#ch2-5-4">5.4 The Overfitting Risk</a></li>
  <li><a href="#ch2-5-outcomes" id="toc-ch2-5-outcomes" class="nav-link" data-scroll-target="#ch2-5-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-6" id="toc-ch2-6" class="nav-link" data-scroll-target="#ch2-6">6. Multicollinearity: When Features Are Too Similar</a>
  <ul class="collapse">
  <li><a href="#ch2-6-1" id="toc-ch2-6-1" class="nav-link" data-scroll-target="#ch2-6-1">6.1 What Is Multicollinearity?</a></li>
  <li><a href="#ch2-6-2" id="toc-ch2-6-2" class="nav-link" data-scroll-target="#ch2-6-2">6.2 Why It’s a Problem</a></li>
  <li><a href="#ch2-6-3" id="toc-ch2-6-3" class="nav-link" data-scroll-target="#ch2-6-3">6.3 Detecting Multicollinearity</a></li>
  <li><a href="#ch2-6-outcomes" id="toc-ch2-6-outcomes" class="nav-link" data-scroll-target="#ch2-6-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-7" id="toc-ch2-7" class="nav-link" data-scroll-target="#ch2-7">7. Ridge Regression: L2 Regularization</a>
  <ul class="collapse">
  <li><a href="#ch2-7-1" id="toc-ch2-7-1" class="nav-link" data-scroll-target="#ch2-7-1">7.1 The Regularization Idea</a></li>
  <li><a href="#ch2-7-2" id="toc-ch2-7-2" class="nav-link" data-scroll-target="#ch2-7-2">7.2 How Ridge Works</a></li>
  <li><a href="#ch2-7-3" id="toc-ch2-7-3" class="nav-link" data-scroll-target="#ch2-7-3">7.3 Choosing Alpha</a></li>
  <li><a href="#ch2-7-outcomes" id="toc-ch2-7-outcomes" class="nav-link" data-scroll-target="#ch2-7-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-8" id="toc-ch2-8" class="nav-link" data-scroll-target="#ch2-8">8. Lasso Regression: L1 Regularization</a>
  <ul class="collapse">
  <li><a href="#ch2-8-1" id="toc-ch2-8-1" class="nav-link" data-scroll-target="#ch2-8-1">8.1 How Lasso Differs from Ridge</a></li>
  <li><a href="#ch2-8-2" id="toc-ch2-8-2" class="nav-link" data-scroll-target="#ch2-8-2">8.2 Lasso for Feature Selection</a></li>
  <li><a href="#ch2-8-3" id="toc-ch2-8-3" class="nav-link" data-scroll-target="#ch2-8-3">8.3 Visualizing the Regularization Path</a></li>
  <li><a href="#ch2-8-4" id="toc-ch2-8-4" class="nav-link" data-scroll-target="#ch2-8-4">8.4 When to Use Lasso vs.&nbsp;Ridge</a></li>
  <li><a href="#ch2-8-outcomes" id="toc-ch2-8-outcomes" class="nav-link" data-scroll-target="#ch2-8-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-9" id="toc-ch2-9" class="nav-link" data-scroll-target="#ch2-9">9. Elastic Net: Best of Both Worlds</a>
  <ul class="collapse">
  <li><a href="#ch2-9-1" id="toc-ch2-9-1" class="nav-link" data-scroll-target="#ch2-9-1">9.1 Combining L1 and L2</a></li>
  <li><a href="#ch2-9-2" id="toc-ch2-9-2" class="nav-link" data-scroll-target="#ch2-9-2">9.2 When to Use Elastic Net</a></li>
  <li><a href="#ch2-9-outcomes" id="toc-ch2-9-outcomes" class="nav-link" data-scroll-target="#ch2-9-outcomes">Learning outcomes:</a></li>
  </ul></li>
  <li><a href="#ch2-10" id="toc-ch2-10" class="nav-link" data-scroll-target="#ch2-10">10. Putting It All Together: A Complete Regression Analysis</a>
  <ul class="collapse">
  <li><a href="#ch2-10-1" id="toc-ch2-10-1" class="nav-link" data-scroll-target="#ch2-10-1">10.1 The Diagnostic Workflow</a></li>
  <li><a href="#ch2-10-2" id="toc-ch2-10-2" class="nav-link" data-scroll-target="#ch2-10-2">10.2 Model Selection Strategy</a></li>
  <li><a href="#ch2-10-3" id="toc-ch2-10-3" class="nav-link" data-scroll-target="#ch2-10-3">10.3 Interpreting and Communicating Results</a></li>
  </ul></li>
  <li><a href="#ch2-summary" id="toc-ch2-summary" class="nav-link" data-scroll-target="#ch2-summary">Summary</a></li>
  <li><a href="#ch2-practice" id="toc-ch2-practice" class="nav-link" data-scroll-target="#ch2-practice">Practice Exercises</a>
  <ul class="collapse">
  <li><a href="#exercise-1-residual-analysis" id="toc-exercise-1-residual-analysis" class="nav-link" data-scroll-target="#exercise-1-residual-analysis">Exercise 1: Residual Analysis</a></li>
  <li><a href="#exercise-2-coefficient-interpretation" id="toc-exercise-2-coefficient-interpretation" class="nav-link" data-scroll-target="#exercise-2-coefficient-interpretation">Exercise 2: Coefficient Interpretation</a></li>
  <li><a href="#exercise-3-polynomial-selection" id="toc-exercise-3-polynomial-selection" class="nav-link" data-scroll-target="#exercise-3-polynomial-selection">Exercise 3: Polynomial Selection</a></li>
  <li><a href="#exercise-4-multicollinearity-detection" id="toc-exercise-4-multicollinearity-detection" class="nav-link" data-scroll-target="#exercise-4-multicollinearity-detection">Exercise 4: Multicollinearity Detection</a></li>
  <li><a href="#exercise-5-ridge-vs-lasso-comparison" id="toc-exercise-5-ridge-vs-lasso-comparison" class="nav-link" data-scroll-target="#exercise-5-ridge-vs-lasso-comparison">Exercise 5: Ridge vs Lasso Comparison</a></li>
  <li><a href="#exercise-6-complete-regression-workflow" id="toc-exercise-6-complete-regression-workflow" class="nav-link" data-scroll-target="#exercise-6-complete-regression-workflow">Exercise 6: Complete Regression Workflow</a></li>
  </ul></li>
  <li><a href="#ch2-additional-resources" id="toc-ch2-additional-resources" class="nav-link" data-scroll-target="#ch2-additional-resources">Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<nav class="chapter-nav" aria-label="Textbook navigation">
  <a class="chapter-nav__link" href="https://paulsavala.github.io/math-3339-intro-data-science-with-llms/" aria-label="Course home" title="Course home">
    <i class="bi bi-house"></i>
  </a>
  <a class="chapter-nav__link" href="../table-of-contents.html" aria-label="Table of contents" title="Table of contents">
    <i class="bi bi-list"></i>
  </a>
</nav>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Chapter 2: Regression Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="ch2-resources" class="level2">
<h2 class="anchored" data-anchor-id="ch2-resources">Chapter Resources</h2>
<p><strong>Related Assignments:</strong></p>
<ul>
<li><a href="../../Assignments/Chapter 2 - Regression/chapter-2-homework.html">Chapter 2 Homework</a></li>
</ul>
<hr>
</section>
<section id="ch2-intro" class="level2">
<h2 class="anchored" data-anchor-id="ch2-intro">Introduction</h2>
<p>Linear regression is perhaps the most important algorithm in machine learning. Not because it’s the most powerful—it’s not. Not because it always works—it doesn’t. Linear regression matters because understanding it deeply gives you the foundation to understand almost everything else in machine learning.</p>
<p>Here’s the thing: most machine learning is just sophisticated regression. Neural networks? Stacked regressions with non-linear activation functions. Logistic regression? Linear regression passed through a special function. Even tree-based methods are essentially breaking the data into regions and fitting constants (which are just simple regressions) in each region.</p>
<p>But linear regression has a dirty secret: it only works well when certain assumptions are met. Violate those assumptions, and your beautiful model with a high R² might be making terrible predictions. This is where many beginners get burned. They fit a model, see a nice R² value, and think they’re done. Then they deploy it to production and watch it fail spectacularly.</p>
<p>This chapter is about becoming a regression expert. Not just someone who can call <code>LinearRegression().fit()</code>, but someone who knows when regression will work, when it won’t, and how to fix it when it breaks. We’ll cover the assumptions behind linear regression, how to check if they’re violated, what to do when they are, and how regularization techniques can save you when things get messy.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>You may assume that linear regression is only useful when your data has a linear relationship. But this is not true! Even data with complex relationships can be modeled using linear regression with the right features, transformations, and regularization techniques.</p>
</div>
</div>
<p>By the end of this chapter, you’ll understand:</p>
<ul>
<li>What assumptions linear regression makes and why they matter</li>
<li>How to diagnose problems using residual plots</li>
<li>When and how to use polynomial features for non-linear relationships</li>
<li>How multicollinearity breaks coefficient interpretation</li>
<li>How Ridge, Lasso, and Elastic Net regularization fix overfitting and multicollinearity</li>
<li>How to choose between different regression approaches</li>
</ul>
<p>Let’s jump in.</p>
<hr>
</section>
<section id="ch2-1" class="level2">
<h2 class="anchored" data-anchor-id="ch2-1">1. Train/Validation/Test Methodology</h2>
<p>Before we dive into specific regression models, we need to establish a crucial foundation: how to properly split and evaluate our data. Understanding train/validation/test methodology is essential for building models that actually generalize to new data.</p>
<section id="ch2-1-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-1-1">1.1 Why Three Sets?</h3>
<p>You’ve seen train/test splits. But proper ML methodology requires <strong>three</strong> sets. Here’s why:</p>
<ul>
<li><strong>Training set:</strong> Used to fit the model (learn parameters)</li>
<li><strong>Validation set:</strong> Used to tune hyperparameters and select models</li>
<li><strong>Test set:</strong> Used ONCE at the very end to get an unbiased estimate of performance</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before you freak out at the huge block of code below, slow down! You’re not expected to write code like this from scratch. Instead, what you should do is go through, look at the code comments (everything after the hashtag <code>#</code>), and get a rough feeling for what it’s doing. In practice, an LLM would write this code for you based on your prompt.</p>
</div>
</div>
<div id="f1194192" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load California housing data for demonstration</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>housing_df <span class="op">=</span> pd.read_csv(<span class="st">'../data/housing.csv'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Select features and target</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> housing_df[[<span class="st">'median_income'</span>, <span class="st">'housing_median_age'</span>, <span class="st">'total_rooms'</span>, <span class="st">'population'</span>]]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> housing_df[<span class="st">'median_house_value'</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove missing values</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>X_clean <span class="op">=</span> X.dropna()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>y_clean <span class="op">=</span> y[X_clean.index]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Proper three-way split</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_val_test_split(X, y, train_size<span class="op">=</span><span class="fl">0.6</span>, val_size<span class="op">=</span><span class="fl">0.2</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Split data into train, validation, and test sets"""</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First split: separate test set</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    X_temp, X_test, y_temp, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        X, y, test_size<span class="op">=</span>test_size, random_state<span class="op">=</span>random_state</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second split: separate train and validation</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># val_size needs to be recalculated relative to remaining data</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    val_size_adjusted <span class="op">=</span> val_size <span class="op">/</span> (train_size <span class="op">+</span> val_size)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        X_temp, y_temp, test_size<span class="op">=</span>val_size_adjusted, random_state<span class="op">=</span>random_state</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X_train, X_val, X_test, y_train, y_val, y_test</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>X_train, X_val, X_test, y_train, y_val, y_test <span class="op">=</span> train_val_test_split(X_clean, y_clean)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dataset sizes (California housing):"</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training: </span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">}</span><span class="ss"> samples (</span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="op">/</span><span class="bu">len</span>(X_clean)<span class="op">*</span><span class="dv">100</span><span class="sc">:.0f}</span><span class="ss">%)"</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation: </span><span class="sc">{</span><span class="bu">len</span>(X_val)<span class="sc">}</span><span class="ss"> samples (</span><span class="sc">{</span><span class="bu">len</span>(X_val)<span class="op">/</span><span class="bu">len</span>(X_clean)<span class="op">*</span><span class="dv">100</span><span class="sc">:.0f}</span><span class="ss">%)"</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test: </span><span class="sc">{</span><span class="bu">len</span>(X_test)<span class="sc">}</span><span class="ss"> samples (</span><span class="sc">{</span><span class="bu">len</span>(X_test)<span class="op">/</span><span class="bu">len</span>(X_clean)<span class="op">*</span><span class="dv">100</span><span class="sc">:.0f}</span><span class="ss">%)"</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total: </span><span class="sc">{</span><span class="bu">len</span>(X_clean)<span class="sc">}</span><span class="ss"> samples"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset sizes (California housing):
Training: 12384 samples (60%)
Validation: 4128 samples (20%)
Test: 4128 samples (20%)
Total: 20640 samples</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Never use the test set for model selection or hyperparameter tuning.</strong> If you evaluate multiple models on the test set and pick the best, you’ve contaminated it. The test set should be used exactly once, at the very end, to get an honest estimate of how your chosen model will perform on new data.</p>
</div>
</div>
</section>
<section id="ch2-1-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-1-2">1.2 Cross-Validation: Getting More Reliable Estimates</h3>
<p>A single train/validation split can be unlucky—maybe the validation set happens to be easy or hard. <strong>Cross-validation</strong> solves this by using multiple validation sets.</p>
<p>In cross-validation, we split the training data into multiple “folds”. The model is fit on some folds and validated on the remaining fold. This process is repeated multiple times, with each fold serving as the validation set once. The final cross-validation score is the average of all validation scores.</p>
<p>The benefits of this are that:</p>
<ol type="1">
<li>We can better understand the variability of our predictions, because we can see how much the model’s performance changes when we use different validation sets.</li>
<li>We can keep our test set “clean”, and only use it once at the very end.</li>
</ol>
<p>Let’s implement this in code. Here we’ll use cross-validation to find the optimal number of neighbors for a KNN regression model. We’ll choose between 2, 5, 10, 20, 30, 40, and 50 neighbors. Rather than doing a single fit on the training set and then validating on the validation set, we’ll use cross-validation. Doing so allows us to only use the test set once at the very end, and also to better understand the variability of our predictions, which we’ll show as error bars.</p>
<div id="09157da4" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Use cross-validation to find the optimal number of neighbors</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Test different values of n_neighbors</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>n_neighbors_options <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> n_neighbors_options:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create model with this value of n_neighbors</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> KNeighborsRegressor(n_neighbors<span class="op">=</span>n)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform 5-fold cross-validation</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    cv_scores <span class="op">=</span> cross_val_score(model, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>                                 scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to MSE (scores are negative)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    mse_scores <span class="op">=</span> <span class="op">-</span>cv_scores</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    mean_mse <span class="op">=</span> mse_scores.mean()</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    std_mse <span class="op">=</span> mse_scores.std()</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    results.append({</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_neighbors'</span>: n,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">'mean_mse'</span>: mean_mse,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">'std_mse'</span>: std_mse</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the best n_neighbors</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>best_result <span class="op">=</span> <span class="bu">min</span>(results, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="st">'mean_mse'</span>])</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the results</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>n_values <span class="op">=</span> [r[<span class="st">'n_neighbors'</span>] <span class="cf">for</span> r <span class="kw">in</span> results]</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>mean_mses <span class="op">=</span> [r[<span class="st">'mean_mse'</span>] <span class="cf">for</span> r <span class="kw">in</span> results]</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>std_mses <span class="op">=</span> [r[<span class="st">'std_mse'</span>] <span class="cf">for</span> r <span class="kw">in</span> results]</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>plt.errorbar(n_values, mean_mses, yerr<span class="op">=</span>std_mses, marker<span class="op">=</span><span class="st">'o'</span>, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>             capsize<span class="op">=</span><span class="dv">5</span>, capthick<span class="op">=</span><span class="dv">2</span>, markersize<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Neighbors'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean Squared Error'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Cross-Validation: Finding Optimal n_neighbors'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-3-output-1.png" width="815" height="528" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We see that 10 neighbors appears optimal, as it has the lowest mean squared error with reasonably low variation. This means that it generalizes well to new data.</p>
<p>Cross-validation gives you:</p>
<ol type="1">
<li><strong>More reliable estimates:</strong> Averages over multiple validation sets</li>
<li><strong>Uncertainty quantification:</strong> Standard deviation tells you how variable performance is</li>
<li><strong>Better use of data:</strong> Every example gets to be in the validation set once</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Use cross-validation for model selection when you have limited data. For very large datasets, a single train/val/test split is often sufficient and much faster.</p>
</div>
</div>
</section>
<section id="ch2-2-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-2-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Explain why a train-test-validation split is key for producing reliable findings, and the role of each of the three sets (training, testing, validation)</li>
<li>Explain what cross validation is, and when it’s beneficial</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-2" class="level2">
<h2 class="anchored" data-anchor-id="ch2-2">2. Linear Regression</h2>
<section id="ch2-2-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-2-1">2.1 The Line of Best Fit</h3>
<p>You’ve probably heard linear regression described as “finding the line of best fit.” But what does “best” actually mean? Best according to what criteria?</p>
<p>The answer: <strong>best means the line that minimizes the sum of squared errors</strong>. For each data point, we calculate how far the prediction is from the actual value (the error), square it, and add up all these squared errors. We can either minimize this sum, or minimize the average, they’re equivalent. The line that makes this value as small as possible is our “best fit” line. The equation is</p>
<p><span class="math display">\[
\frac{1}{n}\displaystyle\sum_{i=1}^n (y_i - \hat{y}_i)^2
\]</span></p>
<p>This is called mean squared error (MSE), which is the loss function for linear regression. The goal of linear regression is to find the line that minimizes this loss.</p>
<p>Let’s see this in action with the NYC census data:</p>
<div id="543c0b32" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load NYC census data</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>nyc_census <span class="op">=</span> pd.read_csv(<span class="st">'../data/nyc_census_tracts.csv'</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter to complete cases for income prediction</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>nyc_clean <span class="op">=</span> nyc_census[[<span class="st">'IncomePerCap'</span>, <span class="st">'Income'</span>]].dropna()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's predict median household income from per-capita income</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Start with just one feature to visualize easily</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> nyc_clean[[<span class="st">'IncomePerCap'</span>]].values  <span class="co"># Income per capita</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nyc_clean[<span class="st">'Income'</span>].values  <span class="co"># Median household income</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear regression</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>y_pred_train <span class="op">=</span> model.predict(X_train)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> model.predict(X_test)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the fit</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train, y_train, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, label<span class="op">=</span><span class="st">'Training Data'</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">sorted</span>(X_train.flatten()), model.predict(<span class="bu">sorted</span>(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))),</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Best Fit Line'</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Income Per Capita ($)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Median Household Income ($)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Linear Regression: Per Capita Income vs Household Income'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Slope (coefficient): </span><span class="sc">{</span>model<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Intercept: </span><span class="sc">{</span>model<span class="sc">.</span>intercept_<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training R²: </span><span class="sc">{</span>model<span class="sc">.</span>score(X_train, y_train)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>model<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-4-output-1.png" width="851" height="528" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Slope (coefficient): 0.9894
Intercept: 27540.5059
Training R²: 0.6878
Test R²: 0.7202</code></pre>
</div>
</div>
<p>The regression line tries to find the balance that minimizes total squared error across all points. The slope and y-intercept are learned (machine learning) such that the sum of squared residuals, or MSE, is as small as possible.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>You <em>are not</em> expected to write code like this from hand. However, you <em>are</em> expected to <em>understand</em> it! You will be given quiz questions with blocks of code like the code above, and asked questions about it, which you must answer without the help of an LLM.</p>
</div>
</div>
</section>
<section id="ch2-2-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-2-2">2.2 The Assumptions Behind Linear Regression</h3>
<p>Linear regression makes four key assumptions. Violate them, and your model might give you terrible predictions even if the R² looks good.</p>
<p><strong>The Four Assumptions:</strong></p>
<ol type="1">
<li><p><strong>Linearity:</strong> The relationship between X and y is actually linear. If it’s curved, a straight line won’t fit well (however, we’ll handle those cases later in this chapter).</p></li>
<li><p><strong>Independence:</strong> Each observation is independent. One house’s price doesn’t directly affect another’s. (This can be violated with time series or spatial data.)</p></li>
<li><p><strong>Homoscedasticity:</strong> The “spread” of points around the line should be roughly the same everywhere. If errors are tiny for low X values but huge for high X values, that’s a problem. (The fancy term means “constant variance.”)</p></li>
<li><p><strong>Normality:</strong> The residuals (errors) are normally distributed. This matters more for statistical inference than prediction.</p></li>
</ol>
<p><strong>What happens when you violate these?</strong></p>
<ul>
<li>Violate <strong>linearity</strong>: Your predictions will be systematically wrong in certain ranges</li>
<li>Violate <strong>independence</strong>: Your confidence intervals and p-values become unreliable</li>
<li>Violate <strong>homoscedasticity</strong>: Some predictions are much more uncertain than others (but you won’t know which ones!)</li>
<li>Violate <strong>normality</strong>: Your confidence intervals might be wrong, but predictions can still be okay</li>
</ul>
<p>We’ll learn how to check these assumptions using diagnostic plots in Section 3.</p>
</section>
<section id="ch2-2-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-2-3">2.3 Interpreting Coefficients</h3>
<p>Coefficients tell you the relationship between features and the target. Let’s extract and interpret them:</p>
<div id="de6e64ab" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a model with multiple features</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'TotalPop'</span>, <span class="st">'Professional'</span>, <span class="st">'Poverty'</span>, <span class="st">'Unemployment'</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>nyc_multi <span class="op">=</span> nyc_census[features <span class="op">+</span> [<span class="st">'Income'</span>]].dropna()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>X_multi <span class="op">=</span> nyc_multi[features].values</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>y_multi <span class="op">=</span> nyc_multi[<span class="st">'Income'</span>].values</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>X_train_multi, X_test_multi, y_train_multi, y_test_multi <span class="op">=</span> train_test_split(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    X_multi, y_multi, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>model_multi <span class="op">=</span> LinearRegression()</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>model_multi.fit(X_train_multi, y_train_multi)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Display coefficients</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>coef_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Feature'</span>: features,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Coefficient'</span>: model_multi.coef_</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>}).sort_values(<span class="st">'Coefficient'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Regression Coefficients:"</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef_df)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Intercept: </span><span class="sc">{</span>model_multi<span class="sc">.</span>intercept_<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Regression Coefficients:
        Feature  Coefficient
1  Professional   777.977464
0      TotalPop    -0.184356
3  Unemployment  -191.478521
2       Poverty -1036.991744

Intercept: 53635.7278</code></pre>
</div>
</div>
<p><strong>How to interpret these:</strong></p>
<ul>
<li><p><strong>Professional coefficient:</strong> For every 1% increase in the percentage of professionals, median household income changes by the coefficient amount, <em>holding all other features constant</em>.</p></li>
<li><p><strong>Poverty coefficient:</strong> For every 1% increase in the poverty rate, median household income changes by the coefficient amount (likely negative), <em>holding all other features constant</em>.</p></li>
<li><p><strong>Intercept:</strong> The predicted median household income when all features are zero. Often not meaningful in practice (what census tract has zero population or zero unemployment?).</p></li>
</ul>
<p>Notice the key phrase: “<strong>holding all other features constant</strong>.” That’s crucial. It means the coefficient shows the isolated effect of that one feature, assuming everything else stays the same. Change one feature while keeping others fixed, and the coefficient tells you how much the prediction changes.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Coefficient interpretation breaks down when features are highly correlated (multicollinearity).</strong> We’ll address this in Section 5 when we discuss multicollinearity.</p>
</div>
</div>
</section>
<section id="ch2-2-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-2-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Given LLM-written linear regression code, explain what each step is doing (loading the function, cleaning the data, creating X/y sets, etc)</li>
<li>List and explain all four assumptions behind linear regression</li>
<li>Interpret the coefficients from a linear regression model</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-3" class="level2">
<h2 class="anchored" data-anchor-id="ch2-3">3. Regression Evaluation Metrics</h2>
<p>How do you know if your regression model is any good? You need metrics. R² is popular, but it can be misleading. Let’s look at the most important metrics, what they mean, and when to use each one.</p>
<section id="ch2-3-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-3-1">3.1 Mean Squared Error (MSE)</h3>
<p>MSE is the average of squared errors. Remember, an error (or residual) is just actual value minus predicted value. Square those, average them, and you have MSE.</p>
<p>Why does MSE matter? It directly reflects what linear regression minimizes during training. When you fit a linear regression, you’re literally finding the line that gives you the smallest possible MSE on the training data.</p>
<p>But here’s the catch: <strong>squaring errors means big errors get penalized heavily</strong>. If one prediction is off by 100 and another is off by 10, the first error contributes 10,000 to MSE while the second contributes only 100. That’s 100 times worse, not 10 times worse. This makes MSE sensitive to outliers.</p>
</section>
<section id="ch2-3-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-3-2">3.2 Mean Absolute Error (MAE)</h3>
<p>MAE is simpler: just the average of the absolute values of errors. No squaring involved.</p>
<p><strong>MAE vs MSE: What’s the difference?</strong></p>
<p>MAE treats all errors proportionally. An error of 100 is exactly 10 times worse than an error of 10. With MSE, that error of 100 is 100 times worse.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>While MSE <em>does not</em> preserve units (since it squares the errors), you can convert back to the original units by taking a square root, such as</p>
<p><span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}} = \displaystyle\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}
\]</span></p>
<p>This is called “<strong>root mean squared error</strong>”, or <strong>RMSE</strong>. It’s in the same units as the target variable, making it more interpretable.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Unless there’s an extremely compelling reason to do otherwise, you should always start with MSE or RMSE as your metric. It’s the most natural metric for linear regression, and it’s what linear regression optimizes.</p>
<p>However, it’s often beneficial to <em>also</em> compute MAE, as it’s more interpretable and less sensitive to outliers. It’s typical to report <em>both</em> values, as they each have their own strengths.</p>
</div>
</div>
<div id="2f9b8a8a" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error, mean_squared_error</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate MAE and MSE</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>mse_sklearn <span class="op">=</span> mean_squared_error(y_test, y_pred_test)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y_test, y_pred_test)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Root Mean Squared Error (RMSE) for comparison</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mse_sklearn)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAE: </span><span class="sc">{</span>mae<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>rmse<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Interpretation:"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"- On average, predictions are off by $</span><span class="sc">{</span>mae<span class="sc">:.0f}</span><span class="ss"> (MAE)"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"- MAE is lower than RMSE because MSE penalizes large errors more heavily"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>MAE: 12478.0785
RMSE: 15670.5818

Interpretation:
- On average, predictions are off by $12478 (MAE)
- MAE is lower than RMSE because MSE penalizes large errors more heavily</code></pre>
</div>
</div>
<p>MAE is lower than RMSE (square root of MSE). That’s typical—RMSE is always at least as large as MAE, and the gap tells you something about outliers. A big gap means you have some really bad predictions pulling up the RMSE.</p>
</section>
<section id="ch2-3-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-3-3">3.3 R² (R-Squared)</h3>
<p>R² tells you the proportion of variance in the target variable that your model explains. It ranges from 0 to 1, where:</p>
<ul>
<li><strong>R² = 1:</strong> Perfect predictions (your model explains 100% of the variance)</li>
<li><strong>R² = 0:</strong> Your model is no better than just predicting the mean every time</li>
<li><strong>R² &lt; 0:</strong> Your model is worse than predicting the mean (yes, this is possible!)</li>
</ul>
<p>The formula is: R² = 1 - (Sum of Squared Residuals / Total Sum of Squares)</p>
<p>But what does that actually mean? Think of it this way: imagine you know nothing about the features and just predict the mean house value for every house. That’s your baseline. R² tells you how much better (or worse!) your model is than that naive baseline.</p>
<div id="a5749350" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> r2_score</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate R² manually to understand it</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Total Sum of Squares: how far each point is from the mean</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>y_mean <span class="op">=</span> y_test.mean()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>total_ss <span class="op">=</span> ((y_test <span class="op">-</span> y_mean) <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Residual Sum of Squares: how far predictions are from actual values</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>residual_ss <span class="op">=</span> ((y_test <span class="op">-</span> y_pred_test) <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># R² = 1 - (residual variation / total variation)</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>r2_manual <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (residual_ss <span class="op">/</span> total_ss)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare to sklearn</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>r2_sklearn <span class="op">=</span> r2_score(y_test, y_pred_test)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R² (manual): </span><span class="sc">{</span>r2_manual<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R² (sklearn): </span><span class="sc">{</span>r2_sklearn<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Interpretation:"</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The model explains </span><span class="sc">{</span>r2_sklearn<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% of the variance in house prices"</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"That means </span><span class="sc">{</span>(<span class="dv">1</span><span class="op">-</span>r2_sklearn)<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% of the variance is unexplained"</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize what R² means</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline plot</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'Actual Data'</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plt.axhline(y_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Baseline: Predict Mean = </span><span class="sc">{</span>y_mean<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression line</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, y_pred_test, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Regression Line (R² = </span><span class="sc">{</span>r2_sklearn<span class="sc">:.4f}</span><span class="ss">)'</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Median Income (in $10,000s)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Median House Value (in $100,000s)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Baseline Model (R² = 0)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>R² (manual): 0.7202
R² (sklearn): 0.7202

Interpretation:
The model explains 72.02% of the variance in house prices
That means 27.98% of the variance is unexplained</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-7-output-2.png" width="951" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how our regression line is a significantly better fit than the baseline line. That leads to an R² value significantly above zero.</p>
</div>
</div>
<p>R² is intuitive and easy to explain to non-technical audiences. But it has problems:</p>
<p><strong>Problem 1: R² always increases when you add features</strong>, even if those features are random noise. Add 100 random columns to your data, and R² will go up, even though the model hasn’t actually improved.</p>
<p><strong>Problem 2: R² doesn’t tell you if your predictions are biased</strong>. R² tells you how close to a line your actual vs predicted values are. What’s one easy way to get a straight line? Just predict the same y-value every single time! Congrats, you have a perfectly straight line, and thus have an R² of 100%! R² doesn’t tell you if your predictions are biased. You could have a high R² but still systematically overpredict or underpredict in certain ranges.</p>
<p><strong>Problem 3: R² can be negative on test data</strong> if your model is terrible. That’s a good warning sign, actually.</p>
</section>
<section id="ch2-3-4" class="level3">
<h3 class="anchored" data-anchor-id="ch2-3-4">3.4 Adjusted R²</h3>
<p>Adjusted R² fixes the “adding features always increases R²” problem. It penalizes you for adding features that don’t actually help.</p>
<p>The formula adjusts R² based on the number of features (p) and number of observations (n):</p>
<p><strong>Adjusted R² = <span class="math inline">\(1 - \displaystyle\biggl[\frac{(1 - R^2)(n - 1)}{(n - p - 1)}\biggr]\)</span></strong></p>
<p>Add a useless feature? Regular R² might go from 0.75 to 0.751. But adjusted R² might drop from 0.75 to 0.748 because the penalty for adding a feature outweighs the tiny improvement.</p>
<p><strong>When to use Adjusted R²:</strong> - When comparing models with different numbers of features - When you’re worried about overfitting - When presenting results to stakeholders (it’s more honest)</p>
<p><strong>When regular R² is fine:</strong> - When comparing models with the same number of features - When you’re more concerned about prediction accuracy than interpretation</p>
<p><strong>Real-World Example: When Many Features Mislead</strong></p>
<p>Let’s see this with real census data from Staten Island. We’ll predict income using many demographic features:</p>
<div id="4abe9d2b" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load NYC census tract data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>nyc_census <span class="op">=</span> pd.read_csv(<span class="st">'../data/nyc_census_tracts.csv'</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter to just Staten Island</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>staten_island_data <span class="op">=</span> nyc_census[nyc_census[<span class="st">'Borough'</span>] <span class="op">==</span> <span class="st">'Staten Island'</span>].copy()</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove rows with missing income (our target)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>staten_island_data <span class="op">=</span> staten_island_data.dropna(subset<span class="op">=</span>[<span class="st">'Income'</span>])</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Select features (exclude identifiers and the target)</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>feature_cols <span class="op">=</span> [<span class="st">'TotalPop'</span>, <span class="st">'Men'</span>, <span class="st">'Women'</span>, <span class="st">'Hispanic'</span>, <span class="st">'White'</span>, <span class="st">'Black'</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Native'</span>, <span class="st">'Asian'</span>, <span class="st">'Citizen'</span>, <span class="st">'IncomePerCap'</span>, <span class="st">'Poverty'</span>,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>                <span class="st">'ChildPoverty'</span>, <span class="st">'Professional'</span>, <span class="st">'Service'</span>, <span class="st">'Office'</span>,</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Construction'</span>, <span class="st">'Production'</span>, <span class="st">'Drive'</span>, <span class="st">'Carpool'</span>, <span class="st">'Transit'</span>,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Walk'</span>, <span class="st">'OtherTransp'</span>, <span class="st">'WorkAtHome'</span>, <span class="st">'MeanCommute'</span>, <span class="st">'Employed'</span>,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>                <span class="st">'PrivateWork'</span>, <span class="st">'PublicWork'</span>, <span class="st">'SelfEmployed'</span>, <span class="st">'FamilyWork'</span>,</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Unemployment'</span>]</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data (drop rows with any missing values)</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>staten_island_clean <span class="op">=</span> staten_island_data[feature_cols <span class="op">+</span> [<span class="st">'Income'</span>]].dropna()</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>X_si <span class="op">=</span> staten_island_clean[feature_cols].values</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>y_si <span class="op">=</span> staten_island_clean[<span class="st">'Income'</span>].values</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="bu">len</span>(staten_island_clean)<span class="sc">}</span><span class="ss"> observations"</span>)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of features: </span><span class="sc">{</span><span class="bu">len</span>(feature_cols)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ratio of features to observations (p/n): </span><span class="sc">{</span><span class="bu">len</span>(feature_cols)<span class="op">/</span><span class="bu">len</span>(staten_island_clean)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>X_train_si, X_test_si, y_train_si, y_test_si <span class="op">=</span> train_test_split(</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    X_si, y_si, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>model_si <span class="op">=</span> LinearRegression()</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>model_si.fit(X_train_si, y_train_si)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>train_r2_si <span class="op">=</span> model_si.score(X_train_si, y_train_si)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>test_r2_si <span class="op">=</span> model_si.score(X_test_si, y_test_si)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Adjusted R² for training set</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>n_train <span class="op">=</span> <span class="bu">len</span>(X_train_si)</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>p_si <span class="op">=</span> X_train_si.shape[<span class="dv">1</span>]</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>adj_r2_train_si <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> ((<span class="dv">1</span> <span class="op">-</span> train_r2_si) <span class="op">*</span> (n_train <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (n_train <span class="op">-</span> p_si <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Adjusted R² for test set</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>n_test <span class="op">=</span> <span class="bu">len</span>(X_test_si)</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>adj_r2_test_si <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> ((<span class="dv">1</span> <span class="op">-</span> test_r2_si) <span class="op">*</span> (n_test <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (n_test <span class="op">-</span> p_si <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the comparison</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>metrics_si <span class="op">=</span> pd.DataFrame({</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Metric'</span>: [<span class="st">'R²'</span>, <span class="st">'Adjusted R²'</span>, <span class="st">'R²'</span>, <span class="st">'Adjusted R²'</span>],</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Dataset'</span>: [<span class="st">'Training'</span>, <span class="st">'Training'</span>, <span class="st">'Test'</span>, <span class="st">'Test'</span>],</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Value'</span>: [train_r2_si, adj_r2_train_si, test_r2_si, adj_r2_test_si]</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>x_pos <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">4</span>]</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'skyblue'</span>, <span class="st">'lightcoral'</span>, <span class="st">'skyblue'</span>, <span class="st">'lightcoral'</span>]</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> ax.bar(x_pos, metrics_si[<span class="st">'Value'</span>], color<span class="op">=</span>colors, alpha<span class="op">=</span><span class="fl">0.8</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Add value labels on bars (handle negative values)</span></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (pos, val) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x_pos, metrics_si[<span class="st">'Value'</span>])):</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>    y_offset <span class="op">=</span> <span class="fl">0.02</span> <span class="cf">if</span> val <span class="op">&gt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="op">-</span><span class="fl">0.05</span></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>    ax.text(pos, val <span class="op">+</span> y_offset, <span class="ss">f'</span><span class="sc">{</span>val<span class="sc">:.3f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Score'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'R² vs Adjusted R² with </span><span class="sc">{</span>p_si<span class="sc">}</span><span class="ss"> Features'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x_pos)</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">'R²</span><span class="ch">\n\n</span><span class="st">Training'</span>, <span class="st">'Adj R²</span><span class="ch">\n\n</span><span class="st">Training'</span>, <span class="st">'R²</span><span class="ch">\n\n</span><span class="st">Test'</span>, <span class="st">'Adj R²</span><span class="ch">\n\n</span><span class="st">Test'</span>], fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Set y-limits to accommodate negative adjusted R² values</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>y_min <span class="op">=</span> <span class="bu">min</span>(metrics_si[<span class="st">'Value'</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.1</span>)</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(y_min, <span class="fl">1.1</span>)</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>, linestyle<span class="op">=</span><span class="st">'-'</span>)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
107 observations
Number of features: 30
Ratio of features to observations (p/n): 0.280</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-8-output-2.png" width="951" height="563" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See the problem? With 30 features and a small number of Staten Island census tracts, the picture is alarming:</p>
<ol type="1">
<li><strong>Adjusted R² can be negative</strong> - When test adjusted R² is negative, the model performs <em>worse</em> than just predicting the mean after accounting for model complexity. The penalty for using so many features overwhelms any predictive value. Remember, this is possible because the model <em>learned</em> from the training data, but is <em>evaluated</em> on the test data. What was learned from the training data is leading the model astray on the test data.</li>
<li><strong>Test R² drops significantly from training</strong> - the model doesn’t generalize well</li>
<li><strong>The high p/n ratio is the culprit</strong> - too many features relative to observations</li>
</ol>
<p>This is a realistic scenario where you might look at training R² and think “Great, decent fit!” but adjusted R² and test performance reveal the truth: the model is overfitted and unreliable.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Rule of thumb:</strong> When your p/n ratio exceeds 0.10 (1 feature per 10 observations), be very skeptical of regular R². Always check Adjusted R² and test set performance. Better yet, use cross-validation to get a more honest assessment.</p>
</div>
</div>
</section>
<section id="ch2-3-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-3-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Explain what MSE and MAE are, and how they penalize a model differently from one another</li>
<li>Explain how to interpret <span class="math inline">\(R^2\)</span> values, and explain the three common problems associated with <span class="math inline">\(R^2\)</span></li>
<li>Explain how adjusted <span class="math inline">\(R^2\)</span> differs from adjusted <span class="math inline">\(R^2\)</span>, and what problems it attempts to solve</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-4" class="level2">
<h2 class="anchored" data-anchor-id="ch2-4">4. Residual Analysis: Your Diagnostic Tool</h2>
<p>Metrics like R² and MSE tell you <em>how much</em> error your model makes. But residual plots tell you <em>where</em> and <em>why</em> the model is making mistakes. This is where you catch problems before they bite you in production.</p>
<section id="ch2-4-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-4-1">4.1 What Are Residuals?</h3>
<p>A residual is simple: <strong>residual = actual value - predicted value</strong>.</p>
<p>If your model predicts a house is worth $250,000 but it’s actually worth $300,000, the residual is $50,000. Positive residual means you underpredicted. Negative residual means you overpredicted.</p>
<p>Why do residuals matter more than just looking at errors? Because the <em>pattern</em> of residuals reveals whether your model’s assumptions are violated. Random residuals? Great. Systematic patterns? Problem.</p>
<div id="2246745c" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load NYC census tract data (all boroughs this time)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>nyc_census <span class="op">=</span> pd.read_csv(<span class="st">'../data/nyc_census_tracts.csv'</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove rows with missing income (our target)</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>nyc_census <span class="op">=</span> nyc_census.dropna(subset<span class="op">=</span>[<span class="st">'Income'</span>])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Select features (exclude identifiers and the target)</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>feature_cols <span class="op">=</span> [<span class="st">'TotalPop'</span>, <span class="st">'Men'</span>, <span class="st">'Women'</span>, <span class="st">'Hispanic'</span>, <span class="st">'White'</span>, <span class="st">'Black'</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Native'</span>, <span class="st">'Asian'</span>, <span class="st">'Citizen'</span>, <span class="st">'IncomePerCap'</span>, <span class="st">'Poverty'</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>                <span class="st">'ChildPoverty'</span>, <span class="st">'Professional'</span>, <span class="st">'Service'</span>, <span class="st">'Office'</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Construction'</span>, <span class="st">'Production'</span>, <span class="st">'Drive'</span>, <span class="st">'Carpool'</span>, <span class="st">'Transit'</span>,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Walk'</span>, <span class="st">'OtherTransp'</span>, <span class="st">'WorkAtHome'</span>, <span class="st">'MeanCommute'</span>, <span class="st">'Employed'</span>,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>                <span class="st">'PrivateWork'</span>, <span class="st">'PublicWork'</span>, <span class="st">'SelfEmployed'</span>, <span class="st">'FamilyWork'</span>,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Unemployment'</span>]</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data (drop rows with any missing values)</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>nyc_census <span class="op">=</span> nyc_census[feature_cols <span class="op">+</span> [<span class="st">'Income'</span>]].dropna()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>X_nyc <span class="op">=</span> nyc_census[feature_cols].values</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>y_nyc <span class="op">=</span> nyc_census[<span class="st">'Income'</span>].values</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>X_train_nyc, X_test_nyc, y_train_nyc, y_test_nyc <span class="op">=</span> train_test_split(</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    X_nyc, y_nyc, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>model_nyc <span class="op">=</span> LinearRegression()</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>model_nyc.fit(X_train_nyc, y_train_nyc)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate residuals from our simple model</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> model_nyc.predict(X_test_nyc)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_test_nyc <span class="op">-</span> y_pred_test</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame for easy viewing</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>residual_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Actual'</span>: y_test_nyc.flatten(),</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Predicted'</span>: y_pred_test.flatten(),</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Residual'</span>: residuals.flatten()</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>residual_df.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Actual</th>
<th data-quarto-table-cell-role="th">Predicted</th>
<th data-quarto-table-cell-role="th">Residual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>87708.0</td>
<td>89123.902250</td>
<td>-1415.902250</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>21577.0</td>
<td>20985.111768</td>
<td>591.888232</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>51595.0</td>
<td>59802.080768</td>
<td>-8207.080768</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>102625.0</td>
<td>72263.008289</td>
<td>30361.991711</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>51045.0</td>
<td>58308.513690</td>
<td>-7263.513690</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Notice that the residuals all fluctuate around zero? That’s expected for linear regression—the errors cancel out on average. But that doesn’t mean the errors are acceptable! We need to look at the pattern.</p>
</section>
<section id="ch2-4-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-4-2">4.2 Residuals vs.&nbsp;Fitted Values Plot</h3>
<p>This is the <strong>single most important diagnostic plot</strong> you’ll make. It plots residuals (y-axis) against predicted values (x-axis).</p>
<p><strong>What you want to see:</strong> Random scatter around zero. No patterns, no trends, just noise.</p>
<p><strong>What you don’t want to see:</strong> Curves, funnels, or systematic patterns. These indicate problems.</p>
<div id="c9364b72" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the residuals vs fitted plot</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_pred_test, residuals, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Fitted Values (Predictions)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Residuals'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Residuals vs. Fitted Values'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Also show distribution of residuals</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>plt.hist(residuals, bins<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Residual'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distribution of Residuals'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-10-output-1.png" width="847" height="528" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-10-output-2.png" width="828" height="454" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See what’s happening? The residuals are somewhat randomly scattered, but there’s two small issues:</p>
<ol type="1">
<li>The model tends to underpredict (positive residuals) for very low predictions.</li>
<li>The model tends to overpredict (negative residuals) for very high predictions.</li>
</ol>
<p>This suggests the relationship might not be perfectly linear. However, despite this, this shows a reasonably good fit overall, with normally distributed residuals centered around zero.</p>
</section>
<section id="ch2-4-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-4-3">4.3 What Patterns Tell You</h3>
<p>Let’s look at specific patterns and what they mean:</p>
<p><strong>Pattern 1: Curved Residuals (Non-Linearity)</strong></p>
<p>If residuals form a U-shape or inverted U-shape, your relationship isn’t linear. The model systematically underpredicts in some regions and overpredicts in others.</p>
<p><strong>Fix:</strong> Try polynomial features, log transforms, or use a non-linear model.</p>
<p><strong>Pattern 2: Funnel Shape (Heteroscedasticity)</strong></p>
<p>If residuals spread out as predictions increase (or decrease), you have non-constant variance. Maybe small houses have predictable prices, but mansion prices vary wildly.</p>
<p><strong>Fix:</strong> Transform the target variable (log, square root), use weighted regression, or use a model that handles heteroscedasticity.</p>
<p><strong>Pattern 3: Outliers</strong></p>
<p>Points far from zero are outliers. A few outliers are normal. Many outliers mean something’s wrong with your model or data.</p>
<p><strong>Fix:</strong> Investigate outliers (data errors? special cases?), consider robust regression, or use regularization.</p>
<p>Let’s demonstrate these patterns with synthetic data, comparing good vs.&nbsp;bad residual plots:</p>
<div id="9c7294a1" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create three datasets: good fit, non-linear, and heteroscedastic</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># GOOD: Linear relationship with constant variance</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>X_good <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>y_good <span class="op">=</span> <span class="dv">5</span> <span class="op">*</span> X_good.flatten() <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">200</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>model_good <span class="op">=</span> LinearRegression()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>model_good.fit(X_good, y_good)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>y_pred_good <span class="op">=</span> model_good.predict(X_good)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>residuals_good <span class="op">=</span> y_good <span class="op">-</span> y_pred_good</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: Non-linear relationship</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>X_nonlinear <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>y_nonlinear <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> X_nonlinear.flatten()<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>model_nonlinear <span class="op">=</span> LinearRegression()</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>model_nonlinear.fit(X_nonlinear, y_nonlinear)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>y_pred_nonlinear <span class="op">=</span> model_nonlinear.predict(X_nonlinear)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>residuals_nonlinear <span class="op">=</span> y_nonlinear <span class="op">-</span> y_pred_nonlinear</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: Heteroscedasticity (funnel pattern)</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>X_hetero <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>y_hetero <span class="op">=</span> <span class="dv">5</span> <span class="op">*</span> X_hetero.flatten() <span class="op">+</span> np.random.normal(<span class="dv">0</span>, X_hetero.flatten(), <span class="dv">200</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>model_hetero <span class="op">=</span> LinearRegression()</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>model_hetero.fit(X_hetero, y_hetero)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>y_pred_hetero <span class="op">=</span> model_hetero.predict(X_hetero)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>residuals_hetero <span class="op">=</span> y_hetero <span class="op">-</span> y_pred_hetero</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Create comparison plot</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">15</span>))</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Row 1: GOOD - Linear data with constant variance</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].scatter(X_good, y_good, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].plot(X_good, y_pred_good, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'GOOD: Linear Data, Constant Variance'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].scatter(y_pred_good, residuals_good, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'✓ Random Scatter (Good!)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Fitted Values'</span>)</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Row 2: BAD - Non-linearity</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].scatter(X_nonlinear, y_nonlinear, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'orange'</span>)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].plot(X_nonlinear, y_pred_nonlinear, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'BAD: Non-Linear Data with Linear Fit'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, color<span class="op">=</span><span class="st">'orange'</span>)</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].scatter(y_pred_nonlinear, residuals_nonlinear, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'orange'</span>)</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'✗ Curved Pattern (Bad!)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, color<span class="op">=</span><span class="st">'orange'</span>)</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Fitted Values'</span>)</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Row 3: BAD - Heteroscedasticity</span></span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].scatter(X_hetero, y_hetero, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].plot(X_hetero, y_pred_hetero, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].set_title(<span class="st">'BAD: Data with Increasing Variance'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].scatter(y_pred_hetero, residuals_hetero, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].set_title(<span class="st">'✗ Funnel Pattern (Bad!)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Fitted Values'</span>)</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>, <span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Compare the three residual plots (right column):"</span>)</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  TOP (Green): Random scatter around zero = GOOD"</span>)</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)"</span>)</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-11-output-1.png" width="1333" height="1430" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Compare the three residual plots (right column):
  TOP (Green): Random scatter around zero = GOOD
  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)
  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)</code></pre>
</div>
</div>
<p>See the difference?</p>
<ul>
<li><strong>Top row (GOOD):</strong> Residuals are randomly scattered around zero with constant spread. This is what you want!</li>
<li><strong>Middle row (BAD - Curved):</strong> Residuals show a clear U-shaped pattern—the model systematically underpredicts at the extremes and overpredicts in the middle.</li>
<li><strong>Bottom row (BAD - Funnel):</strong> Residuals fan out as predictions increase—variance is not constant.</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>A high R² doesn’t mean your assumptions are met! You can have R² = 0.95 but still have terrible residual patterns that indicate the model will fail on new data.</p>
</div>
</div>
<p>These plots are your early warning system. Learn to read them, and you’ll catch problems before they become disasters.</p>
</section>
<section id="ch2-4-4" class="level3">
<h3 class="anchored" data-anchor-id="ch2-4-4">4.4 What to Do When Assumptions Fail</h3>
<p>Okay, you’ve identified a problem in your residual plots. Now what?</p>
<p><strong>Problem: Non-Linearity (curved residual plot)</strong></p>
<p><strong>Solutions:</strong></p>
<ol type="1">
<li><strong>Polynomial features:</strong> Add X², X³, etc. (covered in Section 4)</li>
<li><strong>Transform features:</strong> Try log(X), √X, or 1/X</li>
<li><strong>Transform target:</strong> Try log(y) or √y</li>
<li><strong>Use a non-linear model:</strong> Tree-based models, neural networks, etc.</li>
</ol>
<p><strong>Problem: Heteroscedasticity (funnel residual plot)</strong></p>
<p><strong>Solutions:</strong></p>
<ol type="1">
<li><strong>Transform target variable:</strong> log(y) often stabilizes variance</li>
<li><strong>Weighted least squares:</strong> Give less weight to high-variance observations</li>
<li><strong>Use robust standard errors:</strong> Adjust your confidence intervals</li>
<li><strong>Just accept it:</strong> If you only care about predictions (not inference), heteroscedasticity matters less</li>
</ol>
<p><strong>Problem: Outliers</strong></p>
<p><strong>Solutions:</strong></p>
<ol type="1">
<li><strong>Investigate:</strong> Are they data errors? Real but unusual observations?</li>
<li><strong>Remove them:</strong> Only if justified (e.g., data entry errors)</li>
<li><strong>Use robust regression:</strong> Huber regression, RANSAC, etc.</li>
<li><strong>Use regularization:</strong> Ridge and Lasso reduce outlier influence (covered in Sections 6-7)</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Start with the simplest fix first. A log transformation of the target variable often fixes multiple problems at once: non-linearity and heteroscedasticity.</p>
</div>
</div>
</section>
<section id="ch2-4-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-4-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Calculate residuals given the true values and the predicted values</li>
<li>Interpret a residuals vs fitted plot to understand your models predictions</li>
<li>Interpret a histogram of residuals to understand your models predictions</li>
<li>Interpret the three common patterns in residual plots (section 4.3)</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-5" class="level2">
<h2 class="anchored" data-anchor-id="ch2-5">5. Polynomial Regression: Handling Non-Linearity</h2>
<p>What do you do when your residual plot shows a clear curve? The relationship isn’t linear, so a straight line won’t work. This is where polynomial regression saves you.</p>
<section id="ch2-5-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-5-1">5.1 When Linear Isn’t Enough</h3>
<p>Real relationships are rarely perfectly linear. Temperature and ice cream sales? Definitely curved (sales don’t go negative when it’s cold, and they plateau when it’s hot).</p>
<p>Polynomial regression lets you fit curves while still using linear regression. The trick? Create new features that are powers of your original features: X², X³, etc. Then fit a linear model to these polynomial features.</p>
<p>Here’s the key insight: <strong>polynomial regression is still linear regression</strong>. It’s linear in the <em>coefficients</em>, even though the relationship with X is non-linear. The model is y = β₀ + β₁X + β₂X² + β₃X³, which is a linear combination of the features [X, X², X³].</p>
</section>
<section id="ch2-5-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-5-2">5.2 Creating Polynomial Features</h3>
<p>Let’s see this in action. We’ll create polynomial features and fit them to data with a clear non-linear relationship.</p>
<div id="fc01b461" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Use data with a non-linear relationship</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's explore Unemployment vs Income (often a non-linear relationship)</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>nyc_poly <span class="op">=</span> nyc_census[[<span class="st">'Unemployment'</span>, <span class="st">'Income'</span>]].dropna()</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter to only unemployment under 30% (ignore outliers)</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>nyc_poly <span class="op">=</span> nyc_poly[nyc_poly[<span class="st">'Unemployment'</span>] <span class="op">&lt;</span> <span class="dv">30</span>]</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>X_unemp <span class="op">=</span> nyc_poly[[<span class="st">'Unemployment'</span>]].values</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>y_income_poly <span class="op">=</span> nyc_poly[<span class="st">'Income'</span>].values</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>X_unemp_train, X_unemp_test, y_income_poly_train, y_income_poly_test <span class="op">=</span> train_test_split(</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    X_unemp, y_income_poly, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co"># First, try linear regression</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>model_linear_unemp <span class="op">=</span> LinearRegression()</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>model_linear_unemp.fit(X_unemp_train, y_income_poly_train)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>y_pred_linear_unemp <span class="op">=</span> model_linear_unemp.predict(X_unemp_test)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Model:"</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R² = </span><span class="sc">{</span>model_linear_unemp<span class="sc">.</span>score(X_unemp_test, y_income_poly_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Now try polynomial regression (degree 2)</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>poly_2 <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>X_unemp_train_poly2 <span class="op">=</span> poly_2.fit_transform(X_unemp_train)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>X_unemp_test_poly2 <span class="op">=</span> poly_2.transform(X_unemp_test)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Original features shape: </span><span class="sc">{</span>X_unemp_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Polynomial features shape: </span><span class="sc">{</span>X_unemp_train_poly2<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Feature names: </span><span class="sc">{</span>poly_2<span class="sc">.</span>get_feature_names_out([<span class="st">'Unemployment'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>model_poly2 <span class="op">=</span> LinearRegression()</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>model_poly2.fit(X_unemp_train_poly2, y_income_poly_train)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>y_pred_poly2 <span class="op">=</span> model_poly2.predict(X_unemp_test_poly2)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Degree-2 Polynomial Model:"</span>)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R² = </span><span class="sc">{</span>model_poly2<span class="sc">.</span>score(X_unemp_test_poly2, y_income_poly_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coefficients: </span><span class="sc">{</span>model_poly2<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the difference</span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort for smooth plotting</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>sort_idx <span class="op">=</span> np.argsort(X_unemp_test.flatten())</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>X_test_sorted <span class="op">=</span> X_unemp_test.flatten()[sort_idx]</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>y_pred_linear_sorted <span class="op">=</span> y_pred_linear_unemp.flatten()[sort_idx]</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>y_pred_poly2_sorted <span class="op">=</span> y_pred_poly2.flatten()[sort_idx]</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_unemp_test, y_income_poly_test, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'Actual Data'</span>)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test_sorted, y_pred_linear_sorted, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Linear Model'</span>)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test_sorted, y_pred_poly2_sorted, <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Degree-2 Polynomial'</span>)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Unemployment Rate (%)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Median Household Income ($)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Linear vs Polynomial Regression'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Model:
R² = 0.2180

Original features shape: (1670, 1)
Polynomial features shape: (1670, 2)

Feature names: ['Unemployment' 'Unemployment^2']

Degree-2 Polynomial Model:
R² = 0.2339
Coefficients: [-5763.42735409   114.23513525]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-12-output-2.png" width="844" height="528" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See what happened? <code>PolynomialFeatures(degree=2)</code> created a new feature: Unemployment². The linear model then fits: <code>y = β₀ + β₁(Unemployment) + β₂(Unemployment²)</code>. This gives us a parabola, which can capture curvature that a straight line can’t.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here we’re using the <code>PolynomialFeatures</code> pipeline from scikit-learn. However, you can also just square (or cube, or whatever you want) the column, such as <code>df['unemp_poly2'] = df['unemp']**2</code>.</p>
</div>
</div>
</section>
<section id="ch2-5-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-5-3">5.3 Choosing Polynomial Degree</h3>
<p>How do you know what degree to use? Degree 1 is linear. Degree 2 adds curvature. Degree 3 adds an S-curve. Higher degrees add more wiggles.</p>
<p><strong>The problem:</strong> Higher degree doesn’t always mean better. You can overfit spectacularly with high-degree polynomials.</p>
<p><strong>The solution:</strong> Try multiple degrees and use a validation set (or cross-validation) to pick the best one.</p>
<div id="fcd84ca1" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Try polynomials of degree 1 through 10</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>train_scores <span class="op">=</span> []</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>test_scores <span class="op">=</span> []</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> degree <span class="kw">in</span> degrees:</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create polynomial features</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    poly <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>degree, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    X_train_poly <span class="op">=</span> poly.fit_transform(X_unemp_train)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    X_test_poly <span class="op">=</span> poly.transform(X_unemp_test)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit model</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression()</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train_poly, y_income_poly_train)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Score</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    train_score <span class="op">=</span> model.score(X_train_poly, y_income_poly_train)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    test_score <span class="op">=</span> model.score(X_test_poly, y_income_poly_test)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    train_scores.append(train_score)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    test_scores.append(test_score)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Degree </span><span class="sc">{</span>degree<span class="sc">}</span><span class="ss">: Train R² = </span><span class="sc">{</span>train_score<span class="sc">:.4f}</span><span class="ss">, Test R² = </span><span class="sc">{</span>test_score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>plt.plot(degrees, train_scores, <span class="st">'o-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Training R²'</span>)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>plt.plot(degrees, test_scores, <span class="st">'s-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Test R²'</span>)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Polynomial Degree'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'R²'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Performance vs Polynomial Degree'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>plt.xticks(degrees)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal degree</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>optimal_degree <span class="op">=</span> degrees[np.argmax(test_scores)]</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Optimal polynomial degree: </span><span class="sc">{</span>optimal_degree<span class="sc">}</span><span class="ss"> (Test R² = </span><span class="sc">{</span><span class="bu">max</span>(test_scores)<span class="sc">:.4f}</span><span class="ss">)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Degree 1: Train R² = 0.2718, Test R² = 0.2180
Degree 2: Train R² = 0.2890, Test R² = 0.2339
Degree 3: Train R² = 0.2904, Test R² = 0.2329
Degree 4: Train R² = 0.2904, Test R² = 0.2317
Degree 5: Train R² = 0.2923, Test R² = 0.2294
Degree 6: Train R² = 0.2953, Test R² = 0.2225
Degree 7: Train R² = 0.2955, Test R² = 0.2185
Degree 8: Train R² = 0.2965, Test R² = 0.2265
Degree 9: Train R² = 0.2960, Test R² = 0.2290
Degree 10: Train R² = 0.2969, Test R² = 0.2255</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-13-output-2.png" width="823" height="528" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Optimal polynomial degree: 2 (Test R² = 0.2339)</code></pre>
</div>
</div>
<p>See that? Training R² keeps increasing with degree—the model just memorizes the training data. But test R² peaks and then starts <em>decreasing</em>. That’s overfitting. The model gets so wiggly it fits training noise instead of the true pattern.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Never choose polynomial degree based on training performance alone!</strong> Always use validation data or cross-validation. Otherwise, you’ll pick a high degree that overfits.</p>
</div>
</div>
</section>
<section id="ch2-5-4" class="level3">
<h3 class="anchored" data-anchor-id="ch2-5-4">5.4 The Overfitting Risk</h3>
<p>Let’s visualize what high-degree polynomials do. They create absurd wiggles that fit every bump in the training data but fail on new data.</p>
<div id="aff30cdb" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare degree 2 vs degree 10</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create small dataset to exaggerate overfitting</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>X_small <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">30</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>y_small <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> X_small.flatten() <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">30</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Degree 2</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>poly_2_small <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>X_small_poly2 <span class="op">=</span> poly_2_small.fit_transform(X_small)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>model_poly2_small <span class="op">=</span> LinearRegression()</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>model_poly2_small.fit(X_small_poly2, y_small)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create smooth line for plotting</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>X_plot_poly2 <span class="op">=</span> poly_2_small.transform(X_plot)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>y_plot_poly2 <span class="op">=</span> model_poly2_small.predict(X_plot_poly2)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(X_small, y_small, s<span class="op">=</span><span class="dv">50</span>, label<span class="op">=</span><span class="st">'Training Data'</span>)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(X_plot, y_plot_poly2, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Degree 2'</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'X'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'y'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Degree 2: Reasonable Fit'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Degree 10</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>poly_10_small <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">10</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>X_small_poly10 <span class="op">=</span> poly_10_small.fit_transform(X_small)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>model_poly10_small <span class="op">=</span> LinearRegression()</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>model_poly10_small.fit(X_small_poly10, y_small)</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>X_plot_poly10 <span class="op">=</span> poly_10_small.transform(X_plot)</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>y_plot_poly10 <span class="op">=</span> model_poly10_small.predict(X_plot_poly10)</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(X_small, y_small, s<span class="op">=</span><span class="dv">50</span>, label<span class="op">=</span><span class="st">'Training Data'</span>)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(X_plot, y_plot_poly10, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Degree 10'</span>)</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'X'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'y'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Degree 10: Overfitting!'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylim(axes[<span class="dv">0</span>].get_ylim())  <span class="co"># Same y-axis for comparison</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Degree 2 - Training R²: </span><span class="sc">{</span>model_poly2_small<span class="sc">.</span>score(X_small_poly2, y_small)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Degree 10 - Training R²: </span><span class="sc">{</span>model_poly10_small<span class="sc">.</span>score(X_small_poly10, y_small)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-14-output-1.png" width="1335" height="470" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Degree 2 - Training R²: 0.5739
Degree 10 - Training R²: 0.6720</code></pre>
</div>
</div>
<p>Look at that degree-10 model! It wiggles wildly to pass through training points. Training R² is higher, but the model is useless for prediction. It learned noise, not signal.</p>
<p><strong>The Extreme Case: More Features Than Observations (p &gt; n)</strong></p>
<p>Now let’s see what happens when you push this to the limit: more features than data points. This creates an <strong>overdetermined system</strong> where you can get perfect training fit (R² = 1.0) that means absolutely nothing.</p>
<div id="5e52a08a" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>nyc_census <span class="op">=</span> pd.read_csv(<span class="st">'../data/nyc_census_tracts.csv'</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load our Bronx census data again</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>bronx_subset <span class="op">=</span> nyc_census[nyc_census[<span class="st">'Borough'</span>] <span class="op">==</span> <span class="st">'Bronx'</span>].copy()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>bronx_subset <span class="op">=</span> bronx_subset.dropna(subset<span class="op">=</span>[<span class="st">'Income'</span>])</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Select just a few features to start</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>base_features <span class="op">=</span> [<span class="st">'TotalPop'</span>, <span class="st">'IncomePerCap'</span>, <span class="st">'Poverty'</span>, <span class="st">'Professional'</span>, <span class="st">'Unemployment'</span>]</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>bronx_tiny <span class="op">=</span> bronx_subset[base_features <span class="op">+</span> [<span class="st">'Income'</span>]].dropna()</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Take only 30 census tracts (small sample)</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>bronx_tiny_sample <span class="op">=</span> bronx_tiny.sample(n<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>X_tiny <span class="op">=</span> bronx_tiny_sample[base_features].values</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>y_tiny <span class="op">=</span> bronx_tiny_sample[<span class="st">'Income'</span>].values</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Starting with: </span><span class="sc">{</span><span class="bu">len</span>(bronx_tiny_sample)<span class="sc">}</span><span class="ss"> observations, </span><span class="sc">{</span><span class="bu">len</span>(base_features)<span class="sc">}</span><span class="ss"> features"</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Now create polynomial features to blow up the feature count</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>poly_extreme <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">4</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>X_tiny_poly <span class="op">=</span> poly_extreme.fit_transform(X_tiny)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"After polynomial expansion (degree 4): </span><span class="sc">{</span>X_tiny_poly<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> observations, </span><span class="sc">{</span>X_tiny_poly<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features"</span>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Features &gt; Observations: </span><span class="sc">{</span>X_tiny_poly<span class="sc">.</span>shape[<span class="dv">1</span>] <span class="op">&gt;</span> X_tiny_poly<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Hold out just ONE observation for "testing"</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>X_train_tiny <span class="op">=</span> X_tiny_poly[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>y_train_tiny <span class="op">=</span> y_tiny[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>X_test_tiny <span class="op">=</span> X_tiny_poly[<span class="op">-</span><span class="dv">1</span>:]</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>y_test_tiny <span class="op">=</span> y_tiny[<span class="op">-</span><span class="dv">1</span>:]</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Training: </span><span class="sc">{</span>X_train_tiny<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> observations, </span><span class="sc">{</span>X_train_tiny<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features"</span>)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p &gt; n? </span><span class="sc">{</span>X_train_tiny<span class="sc">.</span>shape[<span class="dv">1</span>] <span class="op">&gt;</span> X_train_tiny<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>model_extreme <span class="op">=</span> LinearRegression()</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>model_extreme.fit(X_train_tiny, y_train_tiny)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Check performance</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>train_r2_extreme <span class="op">=</span> model_extreme.score(X_train_tiny, y_train_tiny)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>y_pred_train_extreme <span class="op">=</span> model_extreme.predict(X_train_tiny)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>y_pred_test_extreme <span class="op">=</span> model_extreme.predict(X_test_tiny)</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>train_mse_extreme <span class="op">=</span> mean_squared_error(y_train_tiny, y_pred_train_extreme)</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>test_error_extreme <span class="op">=</span> <span class="bu">abs</span>(y_test_tiny[<span class="dv">0</span>] <span class="op">-</span> y_pred_test_extreme[<span class="dv">0</span>])</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RESULTS: </span><span class="sc">{</span>X_train_tiny<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features, </span><span class="sc">{</span>X_train_tiny<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> observations"</span>)</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training R²: </span><span class="sc">{</span>train_r2_extreme<span class="sc">:.10f}</span><span class="ss">"</span>)</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training MSE: </span><span class="sc">{</span>train_mse_extreme<span class="sc">:.10f}</span><span class="ss">"</span>)</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Actual income (held-out): $</span><span class="sc">{</span>y_test_tiny[<span class="dv">0</span>]<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted income: $</span><span class="sc">{</span>y_pred_test_extreme[<span class="dv">0</span>]<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prediction error: $</span><span class="sc">{</span>test_error_extreme<span class="sc">:,.0f}</span><span class="ss">"</span>)</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prediction error (%): </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> test_error_extreme <span class="op">/</span> y_test_tiny[<span class="dv">0</span>]<span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the "perfect" fit</span></span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Left: Training "perfection"</span></span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(y_train_tiny, y_pred_train_extreme, s<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot([y_train_tiny.<span class="bu">min</span>(), y_train_tiny.<span class="bu">max</span>()],</span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>             [y_train_tiny.<span class="bu">min</span>(), y_train_tiny.<span class="bu">max</span>()],</span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>             <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Perfect Prediction'</span>)</span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Actual Income'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Predicted Income'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="ss">f'Training: R² = </span><span class="sc">{</span>train_r2_extreme<span class="sc">:.6f}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Right: Test disaster</span></span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter([y_test_tiny[<span class="dv">0</span>]], [y_pred_test_extreme[<span class="dv">0</span>]], s<span class="op">=</span><span class="dv">200</span>, c<span class="op">=</span><span class="st">'red'</span>,</span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>                marker<span class="op">=</span><span class="st">'X'</span>, label<span class="op">=</span><span class="st">'Test Prediction'</span>, zorder<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot([y_train_tiny.<span class="bu">min</span>(), y_train_tiny.<span class="bu">max</span>()],</span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>             [y_train_tiny.<span class="bu">min</span>(), y_train_tiny.<span class="bu">max</span>()],</span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a>             <span class="st">'r--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Perfect Prediction'</span>)</span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Actual Income'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Predicted Income'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="ss">f'Test: Error = $</span><span class="sc">{</span>test_error_extreme<span class="sc">:,.0f}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting with: 30 observations, 5 features
After polynomial expansion (degree 4): 30 observations, 125 features
Features &gt; Observations: True

Training: 29 observations, 125 features
p &gt; n? True

============================================================
RESULTS: 125 features, 29 observations
============================================================
Training R²: 1.0000000000
Training MSE: 0.0000000000

Actual income (held-out): $52,344
Predicted income: $167,147
Prediction error: $114,803
Prediction error (%): 219.3%</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-15-output-2.png" width="1333" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>What just happened?</strong></p>
<p>When p &gt; n (more features than observations), the system becomes <strong>underdetermined</strong>. The model can perfectly fit every training point because it has enough degrees of freedom to pass through all of them. R² = 1.0 is <strong>guaranteed</strong> mathematically—but it’s meaningless!</p>
<p>This is like drawing a curve through 5 points with a 10th-degree polynomial. You have so much flexibility that you can hit every point exactly. But the curve between points? Pure nonsense.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>DANGER: The Perfect R² Trap</strong></p>
<p>If you ever see R² = 1.0000 (or extremely close) on training data, be immediately suspicious:</p>
<ol type="1">
<li>Check if p ≥ n (features ≥ observations)</li>
<li>Check if you accidentally included the target variable as a feature</li>
<li>Check for data leakage (future information in features)</li>
<li>Check if you have duplicate rows</li>
</ol>
<p>A “perfect” fit is almost never real. It’s almost always a problem.</p>
</div>
</div>
<p><strong>Real-world implications:</strong></p>
<p>This happens more often than you think:</p>
<ul>
<li><strong>Small datasets:</strong> Medical studies with 50 patients but 200 genetic markers</li>
<li><strong>High-dimensional data:</strong> Images, text, genomics where features vastly outnumber samples</li>
<li><strong>Time series:</strong> Predicting tomorrow with 100 technical indicators but only 30 days of data</li>
</ul>
<p>The solution? <strong>Regularization</strong> (covered in the next sections) or getting more data. Never trust a model where p/n &gt; 1.0, and be very cautious when p/n &gt; 0.3.</p>
<p><strong>Key takeaways:</strong></p>
<ul>
<li>Polynomial features let you model non-linear relationships with linear regression</li>
<li>Choose degree using validation data, not training data</li>
<li>Lower degree often generalizes better than higher degree</li>
<li>When p ≥ n, you get perfect training fit but meaningless predictions</li>
<li>Regularization (covered next) can help control overfitting in polynomial models</li>
</ul>
</section>
<section id="ch2-5-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-5-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Understand what polynomial features contribute to a linear regression model</li>
<li>Understand what it means to be “linear regression” when using terms like <span class="math inline">\(X^2\)</span>, <span class="math inline">\(X^3\)</span>, etc.</li>
<li>Understanding linear regression code using polynmomial regression</li>
<li>Modifying existing linear regression code to change polynomial regression features (e.g.&nbsp;adding additional terms, changing the variable being transformed, etc)</li>
<li>Interpreting model performance vs polynomial degree graphs to pick an optimal degree</li>
<li>Understand and explain the problems arising for picking too high of a polynomial degree (overfitting)</li>
<li>Understand and explain why picking a degree higher than the number of data points can lead to “perfect” predictions which don’t generalize</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-6" class="level2">
<h2 class="anchored" data-anchor-id="ch2-6">6. Multicollinearity: When Features Are Too Similar</h2>
<p>Imagine trying to predict house prices using both “square footage” and “square meters” as separate features. They contain basically the same information! This creates multicollinearity, and it breaks coefficient interpretation in sneaky ways.</p>
<section id="ch2-6-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-6-1">6.1 What Is Multicollinearity?</h3>
<p><strong>Multicollinearity</strong> means your features are highly correlated with each other. One feature can be predicted fairly well from others.</p>
<p>Why is this a problem? Linear regression tries to isolate the effect of each feature while “holding others constant.” But if two features move together, you can’t hold one constant while changing the other—they’re tied together!</p>
<p>The result: <strong>coefficient estimates become unstable</strong>. Add or remove one observation, and coefficients swing wildly. Even worse, a feature that’s clearly important might show up with a tiny coefficient (or even the wrong sign!) because its effect is “stolen” by correlated features.</p>
<p>Let’s create an example to see this:</p>
<div id="6e1e286d" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset with multicollinearity</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co"># X1 is random</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># X2 is highly correlated with X1</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> X1 <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, n)  <span class="co"># Almost identical to X1</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># X3 is independent</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co"># True relationship: y = 5*X1 + 0*X2 + 3*X3 + noise</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: X2 has NO effect, but it's correlated with X1</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>y_multi <span class="op">=</span> <span class="dv">5</span><span class="op">*</span>X1 <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>X3 <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>df_multi <span class="op">=</span> pd.DataFrame({</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'X1'</span>: X1,</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'X2'</span>: X2,</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'X3'</span>: X3,</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'y'</span>: y_multi</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Check correlation matrix</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Correlation Matrix:"</span>)</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_multi.corr())</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize correlations</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>sns.heatmap(df_multi.corr(), annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, center<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>            square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="dv">1</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">0.8</span>})</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Correlation Matrix'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation Matrix:
          X1        X2        X3         y
X1  1.000000  0.994818  0.022129  0.843973
X2  0.994818  1.000000  0.020966  0.838088
X3  0.022129  0.020966  1.000000  0.525535
y   0.843973  0.838088  0.525535  1.000000</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-16-output-2.png" width="563" height="508" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See that? X1 and X2 have a correlation of about 0.995. They’re nearly identical. Now watch what happens when we fit a regression:</p>
<div id="61463cbb" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with all features</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>X_multi <span class="op">=</span> df_multi[[<span class="st">'X1'</span>, <span class="st">'X2'</span>, <span class="st">'X3'</span>]].values</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>y_multi_target <span class="op">=</span> df_multi[<span class="st">'y'</span>].values</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>model_multi <span class="op">=</span> LinearRegression()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>model_multi.fit(X_multi, y_multi_target)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coefficients with multicollinearity:"</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, name <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">'X1'</span>, <span class="st">'X2'</span>, <span class="st">'X3'</span>]):</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>model_multi<span class="sc">.</span>coef_[i]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Remember: True coefficients are X1=5, X2=0, X3=3"</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"But the model can't tell X1 and X2 apart!"</span>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with only X1 and X3 (no multicollinearity)</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>X_clean <span class="op">=</span> df_multi[[<span class="st">'X1'</span>, <span class="st">'X3'</span>]].values</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>model_clean <span class="op">=</span> LinearRegression()</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>model_clean.fit(X_clean, y_multi_target)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Coefficients without multicollinearity:"</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, name <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">'X1'</span>, <span class="st">'X3'</span>]):</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>model_clean<span class="sc">.</span>coef_[i]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Much closer to the true values!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Coefficients with multicollinearity:
  X1: 5.5507
  X2: -0.5675
  X3: 3.0223

Remember: True coefficients are X1=5, X2=0, X3=3
But the model can't tell X1 and X2 apart!

Coefficients without multicollinearity:
  X1: 4.9855
  X3: 3.0229

Much closer to the true values!</code></pre>
</div>
</div>
</section>
<section id="ch2-6-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-6-2">6.2 Why It’s a Problem</h3>
<p>Let me be blunt: <strong>multicollinearity doesn’t hurt prediction accuracy</strong>. Your R² will be fine. Your predictions will be fine. So why do we care?</p>
<p><strong>Problem 1: Coefficient interpretation becomes meaningless</strong></p>
<p>If X1 and X2 are highly correlated, the model might give X1 a coefficient of 10 and X2 a coefficient of -5. Or it might do the opposite: X1 = -5, X2 = 10. Or X1 = 2.5, X2 = 2.5. All three give similar predictions! But which feature is “really” important? You can’t tell.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>One of the main benefits of linear regression models is their interpretability. You can look at the coefficients and read off what they tell you. If you lose that, then you might as well not use a linear model!</p>
</div>
</div>
<p><strong>Problem 2: Coefficient instability</strong></p>
<p>Small changes in the data cause huge swings in coefficients. Add 10 new observations? Coefficients might flip signs. This makes the model untrustworthy for understanding relationships.</p>
<p><strong>Problem 3: Hard to select features</strong></p>
<p>If X1 and X2 are correlated, dropping one might barely hurt performance, but dropping both kills it. This makes feature selection confusing.</p>
<p><strong>When to care:</strong></p>
<ul>
<li>You need to interpret coefficients (e.g., explaining to stakeholders)</li>
<li>You want to identify the “most important” features</li>
<li>You’re making causal claims</li>
</ul>
</section>
<section id="ch2-6-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-6-3">6.3 Detecting Multicollinearity</h3>
<p><strong>Method 1: Correlation Matrix</strong></p>
<p>The simplest approach. Look for correlations close to ±1 (say, above 0.8 or 0.9).</p>
<div id="202a47d8" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use NYC census data</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>features_for_vif <span class="op">=</span> [<span class="st">'TotalPop'</span>, <span class="st">'Professional'</span>, <span class="st">'Poverty'</span>, <span class="st">'Unemployment'</span>, <span class="st">'IncomePerCap'</span>, <span class="st">'ChildPoverty'</span>]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>nyc_vif <span class="op">=</span> nyc_census[features_for_vif].dropna()</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>X_vif <span class="op">=</span> nyc_vif</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlation matrix</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>corr_matrix <span class="op">=</span> X_vif.corr()</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>sns.heatmap(corr_matrix, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, center<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>            square<span class="op">=</span><span class="va">True</span>, linewidths<span class="op">=</span><span class="dv">1</span>, cbar_kws<span class="op">=</span>{<span class="st">"shrink"</span>: <span class="fl">0.8</span>}, fmt<span class="op">=</span><span class="st">'.3f'</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Correlation Matrix - NYC Census Data'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify high correlations</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>high_corr_pairs <span class="op">=</span> []</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(corr_matrix.columns)):</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="bu">len</span>(corr_matrix.columns)):</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(corr_matrix.iloc[i, j]) <span class="op">&gt;</span> <span class="fl">0.7</span>:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Highly correlated feature pairs (|correlation| &gt; 0.7):"</span>)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feat1, feat2, corr <span class="kw">in</span> high_corr_pairs:</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feat1<span class="sc">}</span><span class="ss"> &amp; </span><span class="sc">{</span>feat2<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>corr<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-18-output-1.png" width="842" height="757" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Highly correlated feature pairs (|correlation| &gt; 0.7):
  Professional &amp; IncomePerCap: 0.782
  Poverty &amp; ChildPoverty: 0.911</code></pre>
</div>
</div>
<p><strong>Method 2: Variance Inflation Factor (VIF)</strong></p>
<p>VIF measures how much the variance of a coefficient is “inflated” due to multicollinearity.</p>
<p><strong>VIF interpretation:</strong></p>
<ul>
<li>VIF = 1: No multicollinearity</li>
<li>VIF = 1-5: Moderate multicollinearity (usually okay)</li>
<li>VIF = 5-10: High multicollinearity (concerning)</li>
<li>VIF &gt; 10: Severe multicollinearity (definitely a problem)</li>
</ul>
<div id="55122fcd" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate VIF for each feature</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>vif_data <span class="op">=</span> pd.DataFrame()</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">"Feature"</span>] <span class="op">=</span> features_for_vif</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">"VIF"</span>] <span class="op">=</span> [variance_inflation_factor(X_vif.values, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(features_for_vif))]</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize VIF</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>plt.barh(vif_data[<span class="st">"Feature"</span>], vif_data[<span class="st">"VIF"</span>])</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">5</span>, color<span class="op">=</span><span class="st">'orange'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'VIF = 5 (Moderate)'</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'VIF = 10 (Severe)'</span>)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'VIF'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Variance Inflation Factors'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-19-output-1.png" width="896" height="528" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See features with high VIF? Those are the ones tangled up with others. The solution? Remove one of the correlated features, combine them, or use regularization (which we’ll cover next).</p>
</section>
<section id="ch2-6-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-6-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Explain what multicollinearity is, and why it can cause issues with predictions in linear regression models</li>
<li>Interpret feature correlation matrices to find highly correlated features</li>
<li>Interpret VIF values to find highly correlated features</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-7" class="level2">
<h2 class="anchored" data-anchor-id="ch2-7">7. Ridge Regression: L2 Regularization</h2>
<p>Multicollinearity makes coefficients unstable. Polynomial features risk overfitting. The solution to both? <strong>Regularization</strong>. It’s one of the most important ideas in machine learning.</p>
<section id="ch2-7-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-7-1">7.1 The Regularization Idea</h3>
<p>Here’s the core insight: <strong>penalize large coefficients</strong>. Instead of just minimizing error, also minimize the size of coefficients. The model has to balance two goals:</p>
<ol type="1">
<li>Fit the training data well (low error)</li>
<li>Keep coefficients small (low complexity)</li>
</ol>
<p>Why does this help? Large coefficients make the model sensitive to small changes in features. By shrinking coefficients, you make the model more stable. We want to <em>discourage</em> the model from making wild swings in predictions from very small changes in the data. Imagine if you had a coefficient of 10,000,000. Then a one unit change in x would add ten million to your predicted value. That’s almost certainly not desirable.</p>
<p>Yes, you’re intentionally introducing bias (the model won’t fit training data perfectly). But you reduce variance (the model generalizes better to new data).</p>
<p>This is the famous <strong>bias-variance tradeoff</strong>. A little bias for a lot less variance is usually a great deal.</p>
</section>
<section id="ch2-7-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-7-2">7.2 How Ridge Works</h3>
<p>Ridge regression adds a penalty term to the loss function:</p>
<p><span class="math display">\[
\text{Loss} = \text{MSE} + \alpha \displaystyle\sum\text{coefficients}^2
\]</span></p>
<p>That second term is the L2 penalty (sum of squared coefficients). The <strong>α</strong> (alpha) parameter controls how much you penalize:</p>
<ul>
<li>α = 0: No penalty, just regular linear regression</li>
<li>Small α: Light penalty, coefficients shrink a little</li>
<li>Large α: Heavy penalty, coefficients shrink toward zero (but never reach exactly zero)</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember that we want as small a loss as possible. The goal of machine learning algorithms are to find the parameters (e.g.&nbsp;coefficients of the regression model) that minimize the loss function. Regularization <em>adds</em> the coefficients themselves (squared) to the loss function, meaning the algorithm is penalized for choosing large coefficients.</p>
</div>
</div>
<p>Let’s see it in action using our synthetic data with extreme multicollinearity from section 5.1. Remember: X1 and X2 are nearly identical (correlation ≈ 0.995), and the true model is y = 5<em>X1 + 3</em>X3.</p>
<div id="c07bfd8e" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the multicollinearity data we created earlier</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># X1 and X2 are highly correlated; true coefficients are X1=5, X2=0, X3=3</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>X_ridge <span class="op">=</span> df_multi[[<span class="st">'X1'</span>, <span class="st">'X2'</span>, <span class="st">'X3'</span>]].values</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>y_ridge <span class="op">=</span> df_multi[<span class="st">'y'</span>].values</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>X_ridge_train, X_ridge_test, y_ridge_train, y_ridge_test <span class="op">=</span> train_test_split(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    X_ridge, y_ridge, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co"># IMPORTANT: Always scale features before regularization!</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Features with larger scales get penalized more</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>X_ridge_train_scaled <span class="op">=</span> scaler.fit_transform(X_ridge_train)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>X_ridge_test_scaled <span class="op">=</span> scaler.transform(X_ridge_test)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit regular linear regression</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>model_lr <span class="op">=</span> LinearRegression()</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>model_lr.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Regression (no regularization):"</span>)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Train R²: </span><span class="sc">{</span>model_lr<span class="sc">.</span>score(X_ridge_train_scaled, y_ridge_train)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Test R²: </span><span class="sc">{</span>model_lr<span class="sc">.</span>score(X_ridge_test_scaled, y_ridge_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Coefficients: </span><span class="sc">{</span>model_lr<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Coefficient magnitudes: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">abs</span>(model_lr.coef_)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)"</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  True coefficients: X1=5, X2=0, X3=3"</span>)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  But with multicollinearity, coefficients are unstable!"</span>)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Ridge with alpha=1</span></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>model_ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>model_ridge.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Ridge Regression (alpha=1.0):"</span>)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Train R²: </span><span class="sc">{</span>model_ridge<span class="sc">.</span>score(X_ridge_train_scaled, y_ridge_train)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Test R²: </span><span class="sc">{</span>model_ridge<span class="sc">.</span>score(X_ridge_test_scaled, y_ridge_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Coefficients: </span><span class="sc">{</span>model_ridge<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Coefficient magnitudes: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">abs</span>(model_ridge.coef_)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Ridge distributes the X1 effect across X1 and X2 more evenly"</span>)</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Coefficients are more stable!"</span>)</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize coefficient shrinkage</span></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> [<span class="st">'X1'</span>, <span class="st">'X2'</span>, <span class="st">'X3'</span>]</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>coef_comparison <span class="op">=</span> pd.DataFrame({</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Feature'</span>: feature_names,</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Linear Regression'</span>: model_lr.coef_,</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Ridge (α=1)'</span>: model_ridge.coef_,</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">'True Value'</span>: [<span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">3</span>]</span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(feature_names))</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">-</span> width, coef_comparison[<span class="st">'True Value'</span>], width, label<span class="op">=</span><span class="st">'True Coefficients'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">+</span> width, coef_comparison[<span class="st">'Ridge (α=1)'</span>], width, label<span class="op">=</span><span class="st">'Ridge (α=1)'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>ax.bar(x, coef_comparison[<span class="st">'Linear Regression'</span>], width, label<span class="op">=</span><span class="st">'Linear Regression'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Feature'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Coefficient Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Ridge Handles Multicollinearity Better Than OLS'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(feature_names)</span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Regression (no regularization):
  Train R²: 0.9700
  Test R²: 0.9665
  Coefficients: [ 5.71502205 -0.82461178  3.00187963]
  Coefficient magnitudes: 9.5415

  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)
  True coefficients: X1=5, X2=0, X3=3
  But with multicollinearity, coefficients are unstable!

Ridge Regression (alpha=1.0):
  Train R²: 0.9699
  Test R²: 0.9670
  Coefficients: [ 5.07662499 -0.1891781   2.99844018]
  Coefficient magnitudes: 8.2642

  Ridge distributes the X1 effect across X1 and X2 more evenly
  Coefficients are more stable!</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-20-output-2.png" width="951" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See what happened? With regular linear regression, the coefficients for X1 and X2 were far from the true values. Ridge regression distributes the effect more evenly between the correlated features. While Ridge coefficients still aren’t perfect (X2 should be 0), they’re much more stable and reasonable.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Always scale features before using Ridge!</strong> Features with large scales get penalized more heavily than features with small scales. Standardizing (mean=0, std=1) ensures all features are treated equally. Imagine if I wrote your salary in pennies. Each year you get a raise, and the raise would look enormous to the model!</p>
</div>
</div>
</section>
<section id="ch2-7-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-7-3">7.3 Choosing Alpha</h3>
<p>How do you pick <span class="math inline">\(\alpha\)</span>? Try many values and use cross-validation to see which generalizes best. Let’s continue with our synthetic multicollinearity data:</p>
<div id="51dcd1c8" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Try many alpha values</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">50</span>)  <span class="co"># From 0.01 to 100</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>train_scores_ridge <span class="op">=</span> []</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>test_scores_ridge <span class="op">=</span> []</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>cv_scores_ridge <span class="op">=</span> []</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>coef_sum_ridge <span class="op">=</span> []</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Ridge(alpha<span class="op">=</span>alpha)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train score</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    model.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    train_scores_ridge.append(model.score(X_ridge_train_scaled, y_ridge_train))</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test score</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    test_scores_ridge.append(model.score(X_ridge_test_scaled, y_ridge_test))</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cross-validation score (5-fold)</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    cv_score <span class="op">=</span> cross_val_score(model, X_ridge_train_scaled, y_ridge_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    cv_scores_ridge.append(cv_score)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Track total coefficient magnitude</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    coef_sum_ridge.append(np.<span class="bu">abs</span>(model.coef_).<span class="bu">sum</span>())</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: R² scores</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas, train_scores_ridge, label<span class="op">=</span><span class="st">'Training R²'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas, test_scores_ridge, label<span class="op">=</span><span class="st">'Test R²'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas, cv_scores_ridge, label<span class="op">=</span><span class="st">'CV R² (5-fold)'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Alpha (log scale)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'R²'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Ridge Regression: Choosing Alpha'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>optimal_alpha <span class="op">=</span> alphas[np.argmax(cv_scores_ridge)]</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>plt.axvline(optimal_alpha, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Optimal Alpha'</span>)</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Best alpha</span></span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Optimal alpha (by cross-validation): </span><span class="sc">{</span>optimal_alpha<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best CV R²: </span><span class="sc">{</span><span class="bu">max</span>(cv_scores_ridge)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-21-output-1.png" width="759" height="468" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal alpha (by cross-validation): 0.0100
Best CV R²: 0.9692</code></pre>
</div>
</div>
<p>As <span class="math inline">\(\alpha\)</span> increases:</p>
<ul>
<li>Training R² decreases (more bias, less fit to training data), however…</li>
<li>Test/CV R² are still high, and even increase for a while (multicollinearity handled), then sharply decreases (too much bias)</li>
</ul>
<p>The optimal <span class="math inline">\(\alpha\)</span> is where test/CV performance peaks. For this synthetic data with severe multicollinearity, Ridge helps stabilize coefficients even with small α values.</p>
</section>
<section id="ch2-7-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-7-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Understand mathematically (at a high level) how ridge regression penalizes models for picking very large coefficients</li>
<li>Understand the role of <span class="math inline">\(\alpha\)</span> in regularization (how larger or smaller <span class="math inline">\(\alpha\)</span> values change the penalty)</li>
<li>Interpret a graph showing <span class="math inline">\(\alpha\)</span> versus <span class="math inline">\(R^2\)</span> to determine the optimal <span class="math inline">\(\alpha\)</span></li>
</ul>
<hr>
</section>
</section>
<section id="ch2-8" class="level2">
<h2 class="anchored" data-anchor-id="ch2-8">8. Lasso Regression: L1 Regularization</h2>
<p>Ridge is great, but it never says “this feature is useless.” Lasso does. It performs <strong>automatic feature selection</strong> by setting some coefficients to exactly zero.</p>
<section id="ch2-8-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-8-1">8.1 How Lasso Differs from Ridge</h3>
<p>Lasso uses L1 regularization instead of L2:</p>
<p><span class="math display">\[
\text{Loss} = \text{MSE} + \alpha \sum |\text{coefficients}|
\]</span></p>
<p>The difference? Instead of squaring coefficients (Ridge), we take their absolute value (Lasso). This small change has a huge impact: <strong>Lasso can set coefficients to exactly zero</strong>.</p>
<p>Why? It’s geometry. The L1 penalty creates “corners” where the optimal solution often has some coefficients at exactly zero. Ridge’s L2 penalty is smooth, so coefficients approach zero but never arrive.</p>
</section>
<section id="ch2-8-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-8-2">8.2 Lasso for Feature Selection</h3>
<p>Lasso is feature selection built into the regression. As α increases, Lasso zeroes out features one by one, keeping only the most important.</p>
<div id="b2b7ec04" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>alphas_path <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Lasso with alpha=0.01</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>model_lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>model_lasso.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Lasso Regression (alpha=0.01):"</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Train R²: </span><span class="sc">{</span>model_lasso<span class="sc">.</span>score(X_ridge_train_scaled, y_ridge_train)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Test R²: </span><span class="sc">{</span>model_lasso<span class="sc">.</span>score(X_ridge_test_scaled, y_ridge_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Coefficients:"</span>)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature, coef <span class="kw">in</span> <span class="bu">zip</span>(feature_names, model_lasso.coef_):</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">abs</span>(coef) <span class="op">&lt;</span> <span class="fl">0.0001</span>:</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef<span class="sc">:.6f}</span><span class="ss"> → ELIMINATED!"</span>)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare with larger alpha</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>model_lasso_strong <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>model_lasso_strong.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Lasso Regression (alpha=0.1):"</span>)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Train R²: </span><span class="sc">{</span>model_lasso_strong<span class="sc">.</span>score(X_ridge_train_scaled, y_ridge_train)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Test R²: </span><span class="sc">{</span>model_lasso_strong<span class="sc">.</span>score(X_ridge_test_scaled, y_ridge_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Coefficients:"</span>)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature, coef <span class="kw">in</span> <span class="bu">zip</span>(feature_names, model_lasso_strong.coef_):</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">abs</span>(coef) <span class="op">&lt;</span> <span class="fl">0.0001</span>:</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef<span class="sc">:.6f}</span><span class="ss"> → ELIMINATED!"</span>)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize feature selection</span></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(feature_names))</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">-</span> width, model_lr.coef_, width, label<span class="op">=</span><span class="st">'Linear Regression'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>ax.bar(x, model_lasso.coef_, width, label<span class="op">=</span><span class="st">'Lasso (α=0.01)'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">+</span> width, model_lasso_strong.coef_, width, label<span class="op">=</span><span class="st">'Lasso (α=0.1)'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Feature'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Coefficient Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Lasso Feature Selection'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(feature_names)</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Lasso Regression (alpha=0.01):
  Train R²: 0.9698
  Test R²: 0.9671

Coefficients:
  X1: 4.884947
  X2: 0.000000 → ELIMINATED!
  X3: 2.992327

Lasso Regression (alpha=0.1):
  Train R²: 0.9692
  Test R²: 0.9680

Coefficients:
  X1: 4.797478
  X2: 0.000000 → ELIMINATED!
  X3: 2.904858</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-22-output-2.png" width="951" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See how Lasso zeroed out some features completely? That’s automatic feature selection. Larger α means more aggressive selection.</p>
</section>
<section id="ch2-8-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-8-3">8.3 Visualizing the Regularization Path</h3>
<p>The “regularization path” shows how coefficients shrink as α increases. For Lasso, coefficients hit zero and stay there.</p>
<div id="3f7f4910" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Track coefficients across alpha values for both Lasso and Ridge</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>alphas_lasso <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">100</span>)  <span class="co"># From 0.001 to 10</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>coefs_lasso <span class="op">=</span> []</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>coefs_ridge_comparison <span class="op">=</span> []</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas_lasso:</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lasso</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    lasso_model <span class="op">=</span> Lasso(alpha<span class="op">=</span>alpha, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    lasso_model.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    coefs_lasso.append(lasso_model.coef_)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ridge (for comparison)</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    ridge_model <span class="op">=</span> Ridge(alpha<span class="op">=</span>alpha)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    ridge_model.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    coefs_ridge_comparison.append(ridge_model.coef_)</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>coefs_lasso <span class="op">=</span> np.array(coefs_lasso)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>coefs_ridge_comparison <span class="op">=</span> np.array(coefs_ridge_comparison)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Lasso regularization path</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Lasso path</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'blue'</span>, <span class="st">'red'</span>, <span class="st">'green'</span>]</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, feature <span class="kw">in</span> <span class="bu">enumerate</span>(feature_names):</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].plot(alphas_lasso, coefs_lasso[:, i], label<span class="op">=</span>feature, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>colors[i])</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xscale(<span class="st">'log'</span>)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Alpha (log scale)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Coefficient Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Lasso Regularization Path'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge path (for comparison)</span></span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, feature <span class="kw">in</span> <span class="bu">enumerate</span>(feature_names):</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].plot(alphas_lasso, coefs_ridge_comparison[:, i], label<span class="op">=</span>feature, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>colors[i])</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xscale(<span class="st">'log'</span>)</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Alpha (log scale)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Coefficient Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Ridge Regularization Path'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-23-output-1.png" width="1335" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See the difference? <strong>Lasso coefficients hit zero abruptly</strong> and stay there (left plot). <strong>Ridge coefficients smoothly approach zero</strong> but never reach it (right plot).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Lasso is more aggressive than Ridge. It makes hard decisions: “this feature matters” or “this feature doesn’t.” Ridge says “this feature matters a little less.” Choose based on whether you want sparse models (Lasso) or stable coefficients (Ridge).</p>
</div>
</div>
</section>
<section id="ch2-8-4" class="level3">
<h3 class="anchored" data-anchor-id="ch2-8-4">8.4 When to Use Lasso vs.&nbsp;Ridge</h3>
<p><strong>Use Lasso when:</strong></p>
<ul>
<li>You suspect many features are irrelevant</li>
<li>You want a sparse model (fewer features)</li>
<li>You need to explain which features matter most</li>
<li>Interpretability is critical</li>
</ul>
<p><strong>Use Ridge when:</strong></p>
<ul>
<li>You think most features contribute something</li>
<li>You have multicollinearity and want stable coefficients</li>
<li>You’re okay with keeping all features</li>
<li>You prioritize prediction over interpretation</li>
</ul>
<p><strong>The truth?</strong> Try both and use cross-validation to decide. Sometimes Ridge wins. Sometimes Lasso wins. Sometimes they’re tied.</p>
<div id="1ac485c5" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare Ridge vs Lasso performance</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>alphas_compare <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">50</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>ridge_scores <span class="op">=</span> []</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>lasso_scores <span class="op">=</span> []</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas_compare:</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ridge</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span>alpha)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    ridge_cv <span class="op">=</span> cross_val_score(ridge, X_ridge_train_scaled, y_ridge_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    ridge_scores.append(ridge_cv)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lasso</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span>alpha, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    lasso_cv <span class="op">=</span> cross_val_score(lasso, X_ridge_train_scaled, y_ridge_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    lasso_scores.append(lasso_cv)</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot comparison</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas_compare, ridge_scores, <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Ridge'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas_compare, lasso_scores, <span class="st">'s-'</span>, label<span class="op">=</span><span class="st">'Lasso'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Alpha (log scale)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-Validation R²'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Ridge vs Lasso: Cross-Validation Performance'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Best performance</span></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>best_ridge_score <span class="op">=</span> <span class="bu">max</span>(ridge_scores)</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>best_lasso_score <span class="op">=</span> <span class="bu">max</span>(lasso_scores)</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Ridge CV R²: </span><span class="sc">{</span>best_ridge_score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Lasso CV R²: </span><span class="sc">{</span>best_lasso_score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-24-output-1.png" width="815" height="530" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Ridge CV R²: 0.9692
Best Lasso CV R²: 0.9692</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here we see that as <span class="math inline">\(\alpha\)</span> increased, ridge was relatively unaffectd, but lasso quickly had poor prediction. Why do you think that is? Compare this with the regularization path plots above, can you explain what’s going on here?</p>
</div>
</div>
</section>
<section id="ch2-8-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-8-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Understand how lasso regularization differs from ridge regularization</li>
<li>Understand what lasso does to coefficients in linear regression models</li>
<li>Understand when to use lasso vs ridge regularization</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-9" class="level2">
<h2 class="anchored" data-anchor-id="ch2-9">9. Elastic Net: Best of Both Worlds</h2>
<p>Can’t decide between Ridge and Lasso? Why not both? Elastic Net combines L1 and L2 regularization to get the best of both worlds.</p>
<section id="ch2-9-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-9-1">9.1 Combining L1 and L2</h3>
<p>Elastic Net uses a mix of Ridge (L2) and Lasso (L1) penalties: <span class="math display">\[
\text{Loss} = \text{MSE} + \alpha \left[
    \omega \sum |\beta_j| + (1 - \omega) \sum \beta_j^2 \right]
\]</span></p>
<p>Two hyperparameters:</p>
<ul>
<li><strong>α (alpha):</strong> Overall regularization strength (like Ridge and Lasso)</li>
<li><span class="math inline">\(\omega\)</span> = <strong>l1_ratio:</strong> Mix between L1 and L2
<ul>
<li><span class="math inline">\(\omega\)</span> = 0: Pure Ridge</li>
<li><span class="math inline">\(\omega\)</span> = 1: Pure Lasso</li>
<li><span class="math inline">\(\omega\)</span> = 0.5: Equal mix of both</li>
</ul></li>
</ul>
<p>Why combine them? Lasso can be unstable when features are highly correlated—it randomly picks one and zeros the others. Ridge keeps all correlated features but doesn’t select. Elastic Net does feature selection (like Lasso) but more stably (like Ridge).</p>
<div id="9bcc1ce2" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> ElasticNet</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Elastic Net with different l1_ratios</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>l1_ratios <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>]</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>alpha_en <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Also include Ridge and Lasso for comparison</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Ridge (l1=0)'</span>: Ridge(alpha<span class="op">=</span>alpha_en),</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ElasticNet (l1=0.2)'</span>: ElasticNet(alpha<span class="op">=</span>alpha_en, l1_ratio<span class="op">=</span><span class="fl">0.2</span>, max_iter<span class="op">=</span><span class="dv">10000</span>),</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ElasticNet (l1=0.5)'</span>: ElasticNet(alpha<span class="op">=</span>alpha_en, l1_ratio<span class="op">=</span><span class="fl">0.5</span>, max_iter<span class="op">=</span><span class="dv">10000</span>),</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ElasticNet (l1=0.8)'</span>: ElasticNet(alpha<span class="op">=</span>alpha_en, l1_ratio<span class="op">=</span><span class="fl">0.8</span>, max_iter<span class="op">=</span><span class="dv">10000</span>),</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Lasso (l1=1)'</span>: Lasso(alpha<span class="op">=</span>alpha_en, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, (name, model) <span class="kw">in</span> <span class="bu">enumerate</span>(models.items()):</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    model.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    r2 <span class="op">=</span> model.score(X_ridge_test_scaled, y_ridge_test)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: Test R² = </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Coefficients: </span><span class="sc">{</span>model<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Non-zero coefficients: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(np.<span class="bu">abs</span>(model.coef_) <span class="op">&gt;</span> <span class="fl">0.0001</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize one Elastic Net model</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>model_en <span class="op">=</span> ElasticNet(alpha<span class="op">=</span><span class="fl">0.01</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>model_en.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare all three</span></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(feature_names))</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a Ridge and Lasso with same alpha for fair comparison</span></span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>model_ridge_comp <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>model_ridge_comp.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>model_lasso_comp <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>model_lasso_comp.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>plt.bar(x <span class="op">-</span> width, model_ridge_comp.coef_, width, label<span class="op">=</span><span class="st">'Ridge'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>plt.bar(x, model_en.coef_, width, label<span class="op">=</span><span class="st">'Elastic Net (l1_ratio=0.5)'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a>plt.bar(x <span class="op">+</span> width, model_lasso_comp.coef_, width, label<span class="op">=</span><span class="st">'Lasso'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Coefficient Value'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Ridge vs Elastic Net vs Lasso'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a>plt.xticks(x, feature_names)</span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb43-52"><a href="#cb43-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ridge (l1=0): Test R² = 0.9665
  Coefficients: [ 5.70711611 -0.81673551  3.00184572]
  Non-zero coefficients: 3

ElasticNet (l1=0.2): Test R² = 0.9673
  Coefficients: [3.73186865 1.1378942  2.97734663]
  Non-zero coefficients: 3

ElasticNet (l1=0.5): Test R² = 0.9673
  Coefficients: [4.118586   0.75533971 2.98299579]
  Non-zero coefficients: 3

ElasticNet (l1=0.8): Test R² = 0.9672
  Coefficients: [4.82987896 0.04824256 2.98856424]
  Non-zero coefficients: 3

Lasso (l1=1): Test R² = 0.9671
  Coefficients: [4.88494701 0.         2.99232692]
  Non-zero coefficients: 2
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-25-output-2.png" width="951" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Elastic Net sits between Ridge and Lasso. It zeros out some features (like Lasso) but keeps coefficients more stable (like Ridge).</p>
</section>
<section id="ch2-9-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-9-2">9.2 When to Use Elastic Net</h3>
<p><strong>Use Elastic Net when:</strong></p>
<ul>
<li>You have groups of correlated features</li>
<li>You want feature selection but Lasso is too unstable</li>
<li>You’re not sure if Ridge or Lasso is better (hedge your bets)</li>
<li>You have more features than observations (p &gt; n)</li>
</ul>
<p><strong>Real-world scenario:</strong> You have 50 features measuring similar things (different weather stations, different survey questions, etc.). Lasso might randomly pick one from each group. Ridge keeps all 50. Elastic Net picks a few from each group—the best compromise.</p>
<p><strong>Practical advice:</strong> If you’re unsure, use Elastic Net with l1_ratio=0.5 and tune it with cross-validation. It’s a safe default that adapts to your data.</p>
<div id="4b940221" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune both alpha and l1_ratio with grid search</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'alpha'</span>: np.logspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'l1_ratio'</span>: [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>elastic_net <span class="op">=</span> ElasticNet(max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(elastic_net, param_grid, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_ridge_train_scaled, y_ridge_train)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best parameters: </span><span class="sc">{</span>grid_search<span class="sc">.</span>best_params_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best CV R²: </span><span class="sc">{</span>grid_search<span class="sc">.</span>best_score_<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>grid_search<span class="sc">.</span>score(X_ridge_test_scaled, y_ridge_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Best model coefficients</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best model coefficients:"</span>)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature, coef <span class="kw">in</span> <span class="bu">zip</span>(feature_names, best_model.coef_):</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">abs</span>(coef) <span class="op">&gt;</span> <span class="fl">0.0001</span>:</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>coef<span class="sc">:.6f}</span><span class="ss"> → ELIMINATED"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<div class="ansi-escaped-output">
<pre>Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"folder","base64_name" for automatic cleanup: unknown resource type ("L3Zhci9mb2xkZXJzL3pmLzdkMnBkbWxzN3lsZDk2Z2owMmR6Yl9naDAwMDBnbi9UL2pvYmxpYl9tZW1tYXBwaW5nX2ZvbGRlcl85OTA4M180OTQ0ODM3YTBiZDk0ODhiYWQxZGE1YzFiMTFhN2FlYl9mYjcxODE2YzliNmI0MTgwYjMxNjMyZDJkMGNhZDY4Mw=="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtM3JfMnA4OHI="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtZDlzOHU3OHA="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMta2NuenA5djQ="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtZWNhaGt6ODU="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtZHRlc3o4eHg="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtcTVoNml6cDc="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"folder","base64_name" for automatic cleanup: unknown resource type ("L3Zhci9mb2xkZXJzL3pmLzdkMnBkbWxzN3lsZDk2Z2owMmR6Yl9naDAwMDBnbi9UL2pvYmxpYl9tZW1tYXBwaW5nX2ZvbGRlcl85OTA4M180OTQ0ODM3YTBiZDk0ODhiYWQxZGE1YzFiMTFhN2FlYl8xMWM4YTMwMTQ1OTM0MmZkYTFiODFhZDZlMmVkYWMzOA=="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMteDNlaWo5Ym0="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtc2Y3ejlrNno="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtdm9jYnR6N2w="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtaHFiNm5uY3Y="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtaml1c2w1em0="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtc2Y2bnQ2YTU="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtNHBhZGxzMDU="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>

Traceback (most recent call last):

  File <span class="ansi-magenta-fg">"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py"</span>, line <span class="ansi-magenta-fg">297</span>, in <span class="ansi-magenta-fg">main</span>

    raise ValueError(

    ...&lt;4 lines&gt;...

    )

<span class="ansi-magenta-fg ansi-bold">ValueError</span>: <span class="ansi-magenta-fg">Cannot register "REGISTER","rtype":"semlock","base64_name" for automatic cleanup: unknown resource type ("L2xva3ktOTkwODMtcnIyMmRlbzA="}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']</span>
</pre>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters: {'alpha': np.float64(0.001), 'l1_ratio': 0.9}
Best CV R²: 0.9691
Test R²: 0.9667

Best model coefficients:
  X1: 5.474694
  X2: -0.584535
  X3: 3.000771</code></pre>
</div>
</div>
<p>Elastic Net automatically found the best combination of Ridge and Lasso for your data. That’s the power of combining regularization techniques.</p>
</section>
<section id="ch2-9-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="ch2-9-outcomes">Learning outcomes:</h3>
<p><strong><em>By hand</em> you should be able to:</strong></p>
<ul>
<li>Understand how elastic net combines both ridge and lasso regularization</li>
<li>Understand when to use elastic net regularization</li>
</ul>
<hr>
</section>
</section>
<section id="ch2-10" class="level2">
<h2 class="anchored" data-anchor-id="ch2-10">10. Putting It All Together: A Complete Regression Analysis</h2>
<p>You’ve learned all the pieces. Now let’s put them together into a complete regression workflow that you can follow for any project.</p>
<section id="ch2-10-1" class="level3">
<h3 class="anchored" data-anchor-id="ch2-10-1">10.1 The Diagnostic Workflow</h3>
<p>Here’s the process every regression analysis should follow:</p>
<ul>
<li><strong>Step 1:</strong> Fit a baseline linear model</li>
<li><strong>Step 2:</strong> Check assumptions with diagnostic plots</li>
<li><strong>Step 3:</strong> Identify problems (non-linearity, heteroscedasticity, multicollinearity)</li>
<li><strong>Step 4:</strong> Fix problems (transformations, polynomial features, regularization)</li>
<li><strong>Step 5:</strong> Validate with cross-validation</li>
<li><strong>Step 6:</strong> Interpret and communicate results</li>
</ul>
<p>Let’s walk through this with a complete example:</p>
<div id="b1e2f982" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Fit baseline linear model</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"STEP 1: Baseline Linear Regression"</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>features_complete <span class="op">=</span> [<span class="st">'TotalPop'</span>, <span class="st">'Professional'</span>, <span class="st">'Poverty'</span>, <span class="st">'Unemployment'</span>, <span class="st">'IncomePerCap'</span>, <span class="st">'ChildPoverty'</span>]</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>nyc_complete <span class="op">=</span> nyc_census[features_complete <span class="op">+</span> [<span class="st">'Income'</span>]].dropna()</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>X_complete <span class="op">=</span> nyc_complete[features_complete].values</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>y_complete <span class="op">=</span> nyc_complete[<span class="st">'Income'</span>].values</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>X_train_complete, X_test_complete, y_train_complete, y_test_complete <span class="op">=</span> train_test_split(</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>    X_complete, y_complete, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline model</span></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>model_baseline <span class="op">=</span> LinearRegression()</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>model_baseline.fit(X_train_complete, y_train_complete)</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>y_pred_baseline <span class="op">=</span> model_baseline.predict(X_test_complete)</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train R²: </span><span class="sc">{</span>model_baseline<span class="sc">.</span>score(X_train_complete, y_train_complete)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>model_baseline<span class="sc">.</span>score(X_test_complete, y_test_complete)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(mean_squared_error(y_test_complete, y_pred_baseline))<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Check assumptions with diagnostic plots</span></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"STEP 2: Diagnostic Plots"</span>)</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>residuals_complete <span class="op">=</span> y_test_complete <span class="op">-</span> y_pred_baseline</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Residuals vs Fitted</span></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(y_pred_baseline, residuals_complete, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Fitted Values'</span>)</span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Residuals vs Fitted'</span>)</span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Histogram of residuals</span></span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].hist(residuals_complete, bins<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb47-46"><a href="#cb47-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Residuals'</span>)</span>
<span id="cb47-47"><a href="#cb47-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb47-48"><a href="#cb47-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Residual Distribution'</span>)</span>
<span id="cb47-49"><a href="#cb47-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb47-50"><a href="#cb47-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-51"><a href="#cb47-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale-Location Plot</span></span>
<span id="cb47-52"><a href="#cb47-52" aria-hidden="true" tabindex="-1"></a>standardized_resid <span class="op">=</span> residuals_complete <span class="op">/</span> residuals_complete.std()</span>
<span id="cb47-53"><a href="#cb47-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].scatter(y_pred_baseline, np.sqrt(np.<span class="bu">abs</span>(standardized_resid)), alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb47-54"><a href="#cb47-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'Fitted Values'</span>)</span>
<span id="cb47-55"><a href="#cb47-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'√|Standardized Residuals|'</span>)</span>
<span id="cb47-56"><a href="#cb47-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Scale-Location Plot'</span>)</span>
<span id="cb47-57"><a href="#cb47-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb47-58"><a href="#cb47-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-59"><a href="#cb47-59" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb47-60"><a href="#cb47-60" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb47-61"><a href="#cb47-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-62"><a href="#cb47-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Observations:"</span>)</span>
<span id="cb47-63"><a href="#cb47-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Residuals show some heteroscedasticity (variance increases with fitted values)"</span>)</span>
<span id="cb47-64"><a href="#cb47-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Overall pattern suggests room for improvement"</span>)</span>
<span id="cb47-65"><a href="#cb47-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-66"><a href="#cb47-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Check for multicollinearity</span></span>
<span id="cb47-67"><a href="#cb47-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb47-68"><a href="#cb47-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"STEP 3: Check Multicollinearity"</span>)</span>
<span id="cb47-69"><a href="#cb47-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb47-70"><a href="#cb47-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-71"><a href="#cb47-71" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor</span>
<span id="cb47-72"><a href="#cb47-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-73"><a href="#cb47-73" aria-hidden="true" tabindex="-1"></a>vif_data <span class="op">=</span> pd.DataFrame()</span>
<span id="cb47-74"><a href="#cb47-74" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">"Feature"</span>] <span class="op">=</span> features_complete</span>
<span id="cb47-75"><a href="#cb47-75" aria-hidden="true" tabindex="-1"></a>vif_data[<span class="st">"VIF"</span>] <span class="op">=</span> [variance_inflation_factor(X_complete, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(features_complete))]</span>
<span id="cb47-76"><a href="#cb47-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-77"><a href="#cb47-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vif_data.sort_values(<span class="st">'VIF'</span>, ascending<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb47-78"><a href="#cb47-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">VIF &lt; 5: No serious multicollinearity concerns"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>============================================================
STEP 1: Baseline Linear Regression
============================================================
Train R²: 0.8347
Test R²: 0.8406
Test RMSE: 11405.3004

============================================================
STEP 2: Diagnostic Plots
============================================================</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-27-output-2.png" width="1430" height="374" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Observations:
- Residuals show some heteroscedasticity (variance increases with fitted values)
- Overall pattern suggests room for improvement

============================================================
STEP 3: Check Multicollinearity
============================================================
        Feature        VIF
2       Poverty  21.903183
5  ChildPoverty  16.856925
1  Professional   9.947219
4  IncomePerCap   7.250800
3  Unemployment   4.843157
0      TotalPop   4.334884

VIF &lt; 5: No serious multicollinearity concerns</code></pre>
</div>
</div>
<p>See the workflow? Fit → diagnose → identify issues. Now let’s fix them.</p>
</section>
<section id="ch2-10-2" class="level3">
<h3 class="anchored" data-anchor-id="ch2-10-2">10.2 Model Selection Strategy</h3>
<p>Based on diagnostics, try improvements systematically:</p>
<div id="26689ed7" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Try different models</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"STEP 4: Model Comparison"</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale features for regularization</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> scaler.fit_transform(X_train_complete)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> scaler.transform(X_test_complete)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Dictionary to store results</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 1: Baseline Linear Regression</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> LinearRegression()</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>model1.fit(X_train_scaled, y_train_complete)</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>cv_score1 <span class="op">=</span> cross_val_score(model1, X_train_scaled, y_train_complete, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>test_score1 <span class="op">=</span> model1.score(X_test_scaled, y_test_complete)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>results.append({</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Model'</span>: <span class="st">'Linear Regression'</span>,</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CV R²'</span>: cv_score1,</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Test R²'</span>: test_score1,</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Features'</span>: <span class="bu">len</span>(features_complete)</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 2: Polynomial Features (degree 2)</span></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>poly_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>X_train_poly <span class="op">=</span> poly_features.fit_transform(X_train_scaled)</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>X_test_poly <span class="op">=</span> poly_features.transform(X_test_scaled)</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> LinearRegression()</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>model2.fit(X_train_poly, y_train_complete)</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>cv_score2 <span class="op">=</span> cross_val_score(model2, X_train_poly, y_train_complete, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>test_score2 <span class="op">=</span> model2.score(X_test_poly, y_test_complete)</span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>results.append({</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Model'</span>: <span class="st">'Polynomial (degree=2)'</span>,</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CV R²'</span>: cv_score2,</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Test R²'</span>: test_score2,</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Features'</span>: X_train_poly.shape[<span class="dv">1</span>]</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 3: Ridge Regression</span></span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>ridge.fit(X_train_scaled, y_train_complete)</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>cv_score3 <span class="op">=</span> cross_val_score(ridge, X_train_scaled, y_train_complete, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>test_score3 <span class="op">=</span> ridge.score(X_test_scaled, y_test_complete)</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>results.append({</span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Model'</span>: <span class="st">'Ridge (α=1.0)'</span>,</span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CV R²'</span>: cv_score3,</span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Test R²'</span>: test_score3,</span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Features'</span>: <span class="bu">len</span>(features_complete)</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 4: Lasso Regression</span></span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.01</span>, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a>lasso.fit(X_train_scaled, y_train_complete)</span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a>cv_score4 <span class="op">=</span> cross_val_score(lasso, X_train_scaled, y_train_complete, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a>test_score4 <span class="op">=</span> lasso.score(X_test_scaled, y_test_complete)</span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>n_features_lasso <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(lasso.coef_) <span class="op">&gt;</span> <span class="fl">0.0001</span>)</span>
<span id="cb50-60"><a href="#cb50-60" aria-hidden="true" tabindex="-1"></a>results.append({</span>
<span id="cb50-61"><a href="#cb50-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Model'</span>: <span class="st">'Lasso (α=0.01)'</span>,</span>
<span id="cb50-62"><a href="#cb50-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CV R²'</span>: cv_score4,</span>
<span id="cb50-63"><a href="#cb50-63" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Test R²'</span>: test_score4,</span>
<span id="cb50-64"><a href="#cb50-64" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Features'</span>: <span class="ss">f"</span><span class="sc">{</span>n_features_lasso<span class="sc">}</span><span class="ss"> (selected)"</span></span>
<span id="cb50-65"><a href="#cb50-65" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-66"><a href="#cb50-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-67"><a href="#cb50-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 5: Elastic Net</span></span>
<span id="cb50-68"><a href="#cb50-68" aria-hidden="true" tabindex="-1"></a>elastic <span class="op">=</span> ElasticNet(alpha<span class="op">=</span><span class="fl">0.01</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>, max_iter<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb50-69"><a href="#cb50-69" aria-hidden="true" tabindex="-1"></a>elastic.fit(X_train_scaled, y_train_complete)</span>
<span id="cb50-70"><a href="#cb50-70" aria-hidden="true" tabindex="-1"></a>cv_score5 <span class="op">=</span> cross_val_score(elastic, X_train_scaled, y_train_complete, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'r2'</span>).mean()</span>
<span id="cb50-71"><a href="#cb50-71" aria-hidden="true" tabindex="-1"></a>test_score5 <span class="op">=</span> elastic.score(X_test_scaled, y_test_complete)</span>
<span id="cb50-72"><a href="#cb50-72" aria-hidden="true" tabindex="-1"></a>n_features_elastic <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(elastic.coef_) <span class="op">&gt;</span> <span class="fl">0.0001</span>)</span>
<span id="cb50-73"><a href="#cb50-73" aria-hidden="true" tabindex="-1"></a>results.append({</span>
<span id="cb50-74"><a href="#cb50-74" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Model'</span>: <span class="st">'Elastic Net (α=0.01, l1=0.5)'</span>,</span>
<span id="cb50-75"><a href="#cb50-75" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CV R²'</span>: cv_score5,</span>
<span id="cb50-76"><a href="#cb50-76" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Test R²'</span>: test_score5,</span>
<span id="cb50-77"><a href="#cb50-77" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Features'</span>: <span class="ss">f"</span><span class="sc">{</span>n_features_elastic<span class="sc">}</span><span class="ss"> (selected)"</span></span>
<span id="cb50-78"><a href="#cb50-78" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-79"><a href="#cb50-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-80"><a href="#cb50-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb50-81"><a href="#cb50-81" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(results)</span>
<span id="cb50-82"><a href="#cb50-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Model Comparison Results:"</span>)</span>
<span id="cb50-83"><a href="#cb50-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df.to_string(index<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb50-84"><a href="#cb50-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-85"><a href="#cb50-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb50-86"><a href="#cb50-86" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb50-87"><a href="#cb50-87" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(results_df))</span>
<span id="cb50-88"><a href="#cb50-88" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.35</span></span>
<span id="cb50-89"><a href="#cb50-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-90"><a href="#cb50-90" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">-</span> width<span class="op">/</span><span class="dv">2</span>, results_df[<span class="st">'CV R²'</span>], width, label<span class="op">=</span><span class="st">'CV R²'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb50-91"><a href="#cb50-91" aria-hidden="true" tabindex="-1"></a>ax.bar(x <span class="op">+</span> width<span class="op">/</span><span class="dv">2</span>, results_df[<span class="st">'Test R²'</span>], width, label<span class="op">=</span><span class="st">'Test R²'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb50-92"><a href="#cb50-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-93"><a href="#cb50-93" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Model'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb50-94"><a href="#cb50-94" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'R²'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb50-95"><a href="#cb50-95" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Model Performance Comparison'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb50-96"><a href="#cb50-96" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x)</span>
<span id="cb50-97"><a href="#cb50-97" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(results_df[<span class="st">'Model'</span>], rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb50-98"><a href="#cb50-98" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb50-99"><a href="#cb50-99" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb50-100"><a href="#cb50-100" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb50-101"><a href="#cb50-101" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb50-102"><a href="#cb50-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-103"><a href="#cb50-103" aria-hidden="true" tabindex="-1"></a><span class="co"># Best model</span></span>
<span id="cb50-104"><a href="#cb50-104" aria-hidden="true" tabindex="-1"></a>best_idx <span class="op">=</span> results_df[<span class="st">'CV R²'</span>].argmax()</span>
<span id="cb50-105"><a href="#cb50-105" aria-hidden="true" tabindex="-1"></a>best_model_name <span class="op">=</span> results_df.iloc[best_idx][<span class="st">'Model'</span>]</span>
<span id="cb50-106"><a href="#cb50-106" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">✓ Best model by CV: </span><span class="sc">{</span>best_model_name<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
============================================================
STEP 4: Model Comparison
============================================================

Model Comparison Results:
                       Model    CV R²  Test R²     Features
           Linear Regression 0.832510 0.840628            6
       Polynomial (degree=2) 0.845916 0.856639           27
               Ridge (α=1.0) 0.832520 0.840618            6
              Lasso (α=0.01) 0.832510 0.840628 6 (selected)
Elastic Net (α=0.01, l1=0.5) 0.832527 0.840470 6 (selected)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter-2-regression_files/figure-html/cell-28-output-2.png" width="951" height="566" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
✓ Best model by CV: Polynomial (degree=2)</code></pre>
</div>
</div>
<p>The comparison shows which approach works best for your data. In this case, all models perform similarly, but Ridge and Elastic Net provide slightly better generalization.</p>
</section>
<section id="ch2-10-3" class="level3">
<h3 class="anchored" data-anchor-id="ch2-10-3">10.3 Interpreting and Communicating Results</h3>
<p>Once you’ve selected your best model, interpret the coefficients and communicate clearly:</p>
<div id="b510c23d" class="cell" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"STEP 5: Interpret Final Model"</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Ridge as our final model</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>final_model.fit(X_train_scaled, y_train_complete)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Get coefficients</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>coef_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Feature'</span>: features_complete,</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Coefficient'</span>: final_model.coef_</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>}).sort_values(<span class="st">'Coefficient'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Final Model Coefficients (Ridge, α=1.0):"</span>)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef_df.to_string(index<span class="op">=</span><span class="va">False</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
============================================================
STEP 5: Interpret Final Model
============================================================

Final Model Coefficients (Ridge, α=1.0):
     Feature   Coefficient
IncomePerCap  15857.063355
Professional   2913.318816
ChildPoverty   1977.037962
Unemployment   -882.530764
    TotalPop  -1494.961708
     Poverty -12904.379677</code></pre>
</div>
</div>
<p><strong>Interpretation for Stakeholders:</strong> Our regression model explains approximately 60% of the variance in median household income across NYC census tracts. The model uses 6 key features:</p>
<p><strong>Key findings:</strong> 1. <strong>Income Per Capita</strong> is the strongest predictor. Each $1,000 increase in per capita income corresponds to a significant increase in median household income.</p>
<ol start="2" type="1">
<li><p><strong>Professional Employment</strong> has a positive effect. Census tracts with more professionals tend to have higher median household incomes.</p></li>
<li><p><strong>Poverty Rate</strong> shows a strong negative relationship with income, as expected.</p></li>
<li><p><strong>Child Poverty</strong> has an additional negative effect beyond general poverty, highlighting the economic challenges faced by families with children.</p></li>
<li><p><strong>Unemployment</strong> shows a negative relationship with median household income, though this may be partially captured by the poverty variables.</p></li>
<li><p><strong>Total Population</strong> has minimal effect on median household income at the census tract level.</p></li>
</ol>
<p>The model performs consistently on both training and test data, suggesting it generalizes well to new predictions.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is how you communicate results: translate coefficients into plain language, explain what matters, and acknowledge limitations.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Always present both statistical results (R², coefficients) and practical interpretation.</strong> Stakeholders need to understand what the numbers mean for their decisions, not just that “the model has an R² of 0.60.”</p>
</div>
</div>
<p>The complete workflow: diagnose issues → try fixes systematically → select best model → interpret clearly. That’s professional regression analysis.</p>
<hr>
</section>
</section>
<section id="ch2-summary" class="level2">
<h2 class="anchored" data-anchor-id="ch2-summary">Summary</h2>
<p>Linear regression is the foundation of machine learning, not because it’s the most powerful model, but because understanding it deeply prepares you to understand everything else. This chapter covered the complete regression toolkit—from basic assumptions to advanced regularization techniques.</p>
<p><strong>Key Takeaways:</strong></p>
<ol type="1">
<li><p><strong>Linear regression makes four assumptions:</strong> linearity, independence, homoscedasticity, and normality. Violate them, and your model might give terrible predictions even with high R².</p></li>
<li><p><strong>Residual plots are your diagnostic tool.</strong> They reveal problems that metrics hide. A curved residual plot means non-linearity. A funnel means heteroscedasticity. Random scatter means you’re good.</p></li>
<li><p><strong>Evaluation metrics serve different purposes.</strong> MSE penalizes large errors heavily. MAE treats all errors equally. R² tells you variance explained. RMSE gives interpretable units. Use multiple metrics, not just one.</p></li>
<li><p><strong>Polynomial features capture non-linearity</strong> while staying within linear regression. But high-degree polynomials overfit spectacularly. Always use validation data to choose the degree.</p></li>
<li><p><strong>Multicollinearity breaks coefficient interpretation</strong> but doesn’t hurt prediction accuracy. High VIF values warn you that coefficients are unstable. Regularization fixes this automatically.</p></li>
<li><p><strong>Ridge regression (L2) shrinks all coefficients</strong> toward zero but never reaches exactly zero. It handles multicollinearity well and prevents overfitting. Always scale features first.</p></li>
<li><p><strong>Lasso regression (L1) does automatic feature selection</strong> by setting some coefficients to exactly zero. Use it when you suspect many features are irrelevant.</p></li>
<li><p><strong>Elastic Net combines Ridge and Lasso.</strong> It’s more stable than Lasso with correlated features while still doing feature selection. When unsure, start with Elastic Net.</p></li>
<li><p><strong>The complete workflow matters:</strong> Fit baseline → check diagnostics → identify problems → fix systematically → validate with cross-validation → interpret clearly. Skip steps, and you’ll miss critical issues.</p></li>
<li><p><strong>Always communicate results in plain language.</strong> Stakeholders don’t care that “the coefficient for X1 is 0.437 with a p-value of 0.003.” They care what that means for their decisions.</p></li>
</ol>
<p>Regression modeling is both art and science. The science is in the diagnostics, metrics, and validation. The art is in knowing when assumptions matter, which fixes to try, and how to communicate findings. Master both, and you’ll build models that actually work in the real world.</p>
<p>Use your brain. That’s what it’s there for.</p>
<hr>
</section>
<section id="ch2-practice" class="level2">
<h2 class="anchored" data-anchor-id="ch2-practice">Practice Exercises</h2>
<p>These exercises build progressively from understanding diagnostics to conducting complete regression analyses. Work through them to solidify your grasp of regression modeling.</p>
<section id="exercise-1-residual-analysis" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1-residual-analysis">Exercise 1: Residual Analysis</h3>
<p><strong>Task:</strong> Load the NYC census dataset and fit a linear regression predicting median household income from just the <code>TotalPop</code> feature. Create a residuals vs.&nbsp;fitted values plot. Based on the pattern you observe:</p>
<ol type="a">
<li>Identify which regression assumption(s) are violated</li>
<li>Explain what the pattern tells you about the model’s mistakes</li>
<li>Suggest two specific fixes that might improve the model</li>
<li>Implement one fix and show the improvement</li>
</ol>
<p><strong>What you’re learning:</strong> How to read residual plots and diagnose problems.</p>
</section>
<section id="exercise-2-coefficient-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="exercise-2-coefficient-interpretation">Exercise 2: Coefficient Interpretation</h3>
<p><strong>Task:</strong> Fit a multiple linear regression predicting median household income from <code>Professional</code>, <code>Poverty</code>, <code>Unemployment</code>, and <code>ChildPoverty</code>. For each coefficient:</p>
<ol type="a">
<li>Write the interpretation in plain English (e.g., “For every additional…”)</li>
<li>Explain what “holding other features constant” means</li>
<li>Identify which coefficient is hardest to interpret and explain why</li>
<li>Calculate and report both R² and Adjusted R², then explain the difference</li>
</ol>
<p><strong>What you’re learning:</strong> How to interpret and communicate regression results.</p>
</section>
<section id="exercise-3-polynomial-selection" class="level3">
<h3 class="anchored" data-anchor-id="exercise-3-polynomial-selection">Exercise 3: Polynomial Selection</h3>
<p><strong>Task:</strong> Using the <code>IncomePerCap</code> feature alone, fit polynomial regression models of degrees 1 through 8:</p>
<ol type="a">
<li>For each degree, calculate train MSE and test MSE</li>
<li>Create a plot showing both training and test MSE across all degrees</li>
<li>Identify the optimal polynomial degree and justify your choice</li>
<li>Visualize the polynomial fit for degrees 1, 3, and 8 on the same plot</li>
<li>Explain why degree 8 has lower training MSE but might perform worse in practice</li>
</ol>
<p><strong>What you’re learning:</strong> The bias-variance tradeoff and how to prevent overfitting.</p>
</section>
<section id="exercise-4-multicollinearity-detection" class="level3">
<h3 class="anchored" data-anchor-id="exercise-4-multicollinearity-detection">Exercise 4: Multicollinearity Detection</h3>
<p><strong>Task:</strong> Using all numeric features in the NYC census dataset:</p>
<ol type="a">
<li>Compute and visualize the correlation matrix</li>
<li>Identify any feature pairs with correlation above 0.7</li>
<li>Calculate VIF for each feature</li>
<li>Identify features with VIF &gt; 5 and explain what this means</li>
<li>Fit two models: one with all features, one removing high-VIF features. Compare coefficients and show which is more stable by refitting on a different random split.</li>
</ol>
<p><strong>What you’re learning:</strong> How to detect and handle multicollinearity.</p>
</section>
<section id="exercise-5-ridge-vs-lasso-comparison" class="level3">
<h3 class="anchored" data-anchor-id="exercise-5-ridge-vs-lasso-comparison">Exercise 5: Ridge vs Lasso Comparison</h3>
<p><strong>Task:</strong> Using all features from the NYC census dataset, compare Ridge and Lasso regression:</p>
<ol type="a">
<li>For both Ridge and Lasso, find the optimal alpha using cross-validation</li>
<li>Plot the regularization paths for both methods (coefficients vs.&nbsp;alpha)</li>
<li>Create a table showing which features each method keeps (for optimal alpha)</li>
<li>Explain why Lasso eliminates some features while Ridge doesn’t</li>
<li>Recommend which method you’d use for this dataset and why</li>
</ol>
<p><strong>What you’re learning:</strong> The practical differences between L1 and L2 regularization.</p>
</section>
<section id="exercise-6-complete-regression-workflow" class="level3">
<h3 class="anchored" data-anchor-id="exercise-6-complete-regression-workflow">Exercise 6: Complete Regression Workflow</h3>
<p><strong>Task:</strong> Conduct a complete regression analysis on the NYC census dataset:</p>
<ol type="a">
<li><strong>Baseline:</strong> Fit linear regression with all features. Report train/test R² and RMSE.</li>
<li><strong>Diagnostics:</strong> Create residual plots (residuals vs fitted, histogram of residuals, scale-location). List any problems you identify.</li>
<li><strong>Multicollinearity:</strong> Check VIF. Report any concerns.</li>
<li><strong>Model Improvement:</strong> Try at least 3 different approaches (e.g., polynomial features, Ridge, Lasso, log transform). Use cross-validation to compare.</li>
<li><strong>Final Model:</strong> Select your best model and justify the choice.</li>
<li><strong>Interpretation:</strong> Write a 2-3 paragraph summary explaining your findings to a non-technical audience. Include the model’s performance, key predictors, and limitations.</li>
</ol>
<p><strong>What you’re learning:</strong> The complete end-to-end regression modeling workflow.</p>
<hr>
<p><strong>Tips for Success:</strong></p>
<ul>
<li>Don’t just run code—think about what each plot and metric is telling you</li>
<li>When diagnostic plots show problems, try multiple fixes and compare results</li>
<li>Always use train/validation/test splits or cross-validation properly</li>
<li>Write your interpretations before looking at solutions</li>
<li>Remember: a model that’s slightly less accurate but easier to interpret is often more valuable in practice</li>
</ul>
<p>Use your brain. That’s what it’s there for.</p>
<hr>
</section>
</section>
<section id="ch2-additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="ch2-additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/">Regression Assumptions Explained</a> - Detailed guide to assumptions</li>
<li><a href="https://online.stat.psu.edu/stat462/node/117/">Interpreting Residual Plots</a> - Penn State course notes</li>
<li><a href="https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/">Ridge vs Lasso</a> - Comprehensive comparison</li>
<li><a href="https://scikit-learn.org/stable/modules/linear_model.html">Scikit-learn Linear Models</a> - Official documentation</li>
<li><a href="https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/">Feature Scaling and Regularization</a> - Why scaling matters</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/paulsavala\.github\.io\/math-3339-intro-data-science-with-llms\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>