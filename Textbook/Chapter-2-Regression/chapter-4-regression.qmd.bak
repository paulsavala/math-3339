---
title: "Chapter 4: Regression Models"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
jupyter: python3
---

## Module Resources

**Related Assignments:**

- [Module 4 Homework](../../Assignments/Module%204%20-%20Regression%20Models/module-4-homework.qmd)
- [Module 4 Quiz](../../Assignments/Module%204%20-%20Regression%20Models/module-4-quiz.qmd)

---

## Introduction

Linear regression is perhaps the most important algorithm in machine learning. Not because it's the most powerful—it's not. Not because it always works—it doesn't. Linear regression matters because understanding it deeply gives you the foundation to understand almost everything else in machine learning.

Here's the thing: most machine learning is just sophisticated regression. Neural networks? Stacked regressions with non-linear activation functions. Logistic regression? Linear regression passed through a special function. Even tree-based methods are essentially breaking the data into regions and fitting constants (which are just simple regressions) in each region.

But linear regression has a dirty secret: it only works well when certain assumptions are met. Violate those assumptions, and your beautiful model with a high R² might be making terrible predictions. This is where many beginners get burned. They fit a model, see a nice R² value, and think they're done. Then they deploy it to production and watch it fail spectacularly.

This chapter is about becoming a regression expert. Not just someone who can call `LinearRegression().fit()`, but someone who knows when regression will work, when it won't, and how to fix it when it breaks. We'll cover the assumptions behind linear regression, how to check if they're violated, what to do when they are, and how regularization techniques can save you when things get messy.

::: {.callout-tip}
You may assume that linear regression is only useful when your data has a linear relationship. But this is not true! Even data with complex relationships can be modeled using linear regression with the right features, transformations, and regularization techniques.
:::

By the end of this chapter, you'll understand:

- What assumptions linear regression makes and why they matter
- How to diagnose problems using residual plots
- When and how to use polynomial features for non-linear relationships
- How multicollinearity breaks coefficient interpretation
- How Ridge, Lasso, and Elastic Net regularization fix overfitting and multicollinearity
- How to choose between different regression approaches

Let's jump in.

---

## 1. Linear Regression Revisited

### 1.1 The Line of Best Fit

You've probably heard linear regression described as "finding the line of best fit." But what does "best" actually mean? Best according to what criteria?

The answer: **best means the line that minimizes the sum of squared errors**. For each data point, we calculate how far the prediction is from the actual value (the error), square it, and add up all these squared errors. We can either minimize this sum, or minimize the average, they're equivalent. The line that makes this value as small as possible is our "best fit" line. The equation is

$$
\frac{1}{n}\displaystyle\sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

Does this look familiar? It's just mean squared error (MSE), which is the loss function for linear regression. The goal of linear regression is to find the line that minimizes this loss.

Let's see this in action with the NYC census data:

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load NYC census data
nyc_census = pd.read_csv('../data/nyc_census_tracts.csv')

# Filter to complete cases for income prediction
nyc_clean = nyc_census[['IncomePerCap', 'Income']].dropna()

# Let's predict median household income from per-capita income
# Start with just one feature to visualize easily
X = nyc_clean[['IncomePerCap']].values  # Income per capita
y = nyc_clean['Income'].values  # Median household income

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit linear regression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Visualize the fit
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, alpha=0.5, s=20, label='Training Data')
plt.plot(sorted(X_train.flatten()), model.predict(sorted(X_train.reshape(-1, 1))),
         'r-', linewidth=2, label='Best Fit Line')
plt.xlabel('Income Per Capita ($)', fontsize=12)
plt.ylabel('Median Household Income ($)', fontsize=12)
plt.title('Linear Regression: Per Capita Income vs Household Income', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"Slope (coefficient): {model.coef_[0]:.4f}")
print(f"Intercept: {model.intercept_:.4f}")
print(f"Training R²: {model.score(X_train, y_train):.4f}")
print(f"Test R²: {model.score(X_test, y_test):.4f}")
```

The regression line tries to find the balance that minimizes total squared error across all points. The slope and y-intercept are learned (machine learning) such that the sum of squared residuals, or MSE, is as small as possible.

### 1.2 The Assumptions Behind Linear Regression

Linear regression makes four key assumptions. Violate them, and your model might give you terrible predictions even if the R² looks good.

**The Four Assumptions:**

1. **Linearity:** The relationship between X and y is actually linear. If it's curved, a straight line won't fit well (however, we'll handle those cases later in this chapter).

2. **Independence:** Each observation is independent. One house's price doesn't directly affect another's. (This can be violated with time series or spatial data.)

3. **Homoscedasticity:** The "spread" of points around the line should be roughly the same everywhere. If errors are tiny for low X values but huge for high X values, that's a problem. (The fancy term means "constant variance.")

4. **Normality:** The residuals (errors) are normally distributed. This matters more for statistical inference than prediction.

**What happens when you violate these?**

- Violate **linearity**: Your predictions will be systematically wrong in certain ranges
- Violate **independence**: Your confidence intervals and p-values become unreliable
- Violate **homoscedasticity**: Some predictions are much more uncertain than others (but you won't know which ones!)
- Violate **normality**: Your confidence intervals might be wrong, but predictions can still be okay

We'll learn how to check these assumptions using diagnostic plots in Section 3.

### 1.3 Interpreting Coefficients

Coefficients tell you the relationship between features and the target. Let's extract and interpret them:

```{python}
# Fit a model with multiple features
features = ['TotalPop', 'Professional', 'Poverty', 'Unemployment']
nyc_multi = nyc_census[features + ['Income']].dropna()
X_multi = nyc_multi[features].values
y_multi = nyc_multi['Income'].values

X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
    X_multi, y_multi, test_size=0.2, random_state=42
)

# Fit model
model_multi = LinearRegression()
model_multi.fit(X_train_multi, y_train_multi)

# Display coefficients
coef_df = pd.DataFrame({
    'Feature': features,
    'Coefficient': model_multi.coef_
}).sort_values('Coefficient', ascending=False)

print("Regression Coefficients:")
print(coef_df)
print(f"\nIntercept: {model_multi.intercept_:.4f}")
```

**How to interpret these:**

- **Professional coefficient:** For every 1% increase in the percentage of professionals, median household income changes by the coefficient amount, *holding all other features constant*.

- **Poverty coefficient:** For every 1% increase in the poverty rate, median household income changes by the coefficient amount (likely negative), *holding all other features constant*.

- **Intercept:** The predicted median household income when all features are zero. Often not meaningful in practice (what census tract has zero population or zero unemployment?).

Notice the key phrase: "**holding all other features constant**." That's crucial. It means the coefficient shows the isolated effect of that one feature, assuming everything else stays the same. Change one feature while keeping others fixed, and the coefficient tells you how much the prediction changes.

::: {.callout-warning}
**Coefficient interpretation breaks down when features are highly correlated (multicollinearity).** We'll address this in Section 6 when we discuss regularization.
:::

---

## 2. Regression Evaluation Metrics

How do you know if your regression model is any good? You need metrics. R² is popular, but it can be misleading. Let's look at the most important metrics, what they mean, and when to use each one.

### 2.1 Mean Squared Error (MSE)

MSE is the average of squared errors. Remember, an error (or residual) is just actual value minus predicted value. Square those, average them, and you have MSE.

Why does MSE matter? It directly reflects what linear regression minimizes during training. When you fit a linear regression, you're literally finding the line that gives you the smallest possible MSE on the training data.

But here's the catch: **squaring errors means big errors get penalized heavily**. If one prediction is off by 100 and another is off by 10, the first error contributes 10,000 to MSE while the second contributes only 100. That's 100 times worse, not 10 times worse. This makes MSE sensitive to outliers.

### 2.2 Mean Absolute Error (MAE)

MAE is simpler: just the average of the absolute values of errors. No squaring involved.

**MAE vs MSE: What's the difference?**

MAE treats all errors proportionally. An error of 100 is exactly 10 times worse than an error of 10. With MSE, that error of 100 is 100 times worse.

::: {.callout-tip}
While MSE _does not_ preserve units (since it squares the errors), you can convert back to the original units by taking a square root, such as

$$
\text{RMSE} = \sqrt{\text{MSE}} = \displaystyle\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}
$$

This is called "**root mean squared error**", or **RMSE**. It's in the same units as the target variable, making it more interpretable.
:::

::: {.callout-tip}
Unless there's an extremely compelling reason to do otherwise, you should always start with MSE or RMSE as your metric. It's the most natural metric for linear regression, and it's what linear regression optimizes.

However, it's often beneficial to _also_ compute MAE, as it's more interpretable and less sensitive to outliers. It's typical to report _both_ values, as they each have their own strengths.
:::

```{python}
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Calculate MAE and MSE
mse_sklearn = mean_squared_error(y_test, y_pred_test)
mae = mean_absolute_error(y_test, y_pred_test)

# Calculate Root Mean Squared Error (RMSE) for comparison
rmse = np.sqrt(mse_sklearn)

print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")

print(f"\nInterpretation:")
print(f"- On average, predictions are off by ${mae:.0f} (MAE)")
print(f"- MAE is lower than RMSE because MSE penalizes large errors more heavily")
```

MAE is lower than RMSE (square root of MSE). That's typical—RMSE is always at least as large as MAE, and the gap tells you something about outliers. A big gap means you have some really bad predictions pulling up the RMSE.

### 2.3 R² (R-Squared)

R² tells you the proportion of variance in the target variable that your model explains. It ranges from 0 to 1, where:

- **R² = 1:** Perfect predictions (your model explains 100% of the variance)
- **R² = 0:** Your model is no better than just predicting the mean every time
- **R² < 0:** Your model is worse than predicting the mean (yes, this is possible!)

The formula is: R² = 1 - (Sum of Squared Residuals / Total Sum of Squares)

But what does that actually mean? Think of it this way: imagine you know nothing about the features and just predict the mean house value for every house. That's your baseline. R² tells you how much better (or worse!) your model is than that naive baseline.

```{python}
from sklearn.metrics import r2_score

# Calculate R² manually to understand it
# Total Sum of Squares: how far each point is from the mean
y_mean = y_test.mean()
total_ss = ((y_test - y_mean) ** 2).sum()

# Residual Sum of Squares: how far predictions are from actual values
residual_ss = ((y_test - y_pred_test) ** 2).sum()

# R² = 1 - (residual variation / total variation)
r2_manual = 1 - (residual_ss / total_ss)

# Compare to sklearn
r2_sklearn = r2_score(y_test, y_pred_test)

print(f"R² (manual): {r2_manual:.4f}")
print(f"R² (sklearn): {r2_sklearn:.4f}")
print(f"\nInterpretation:")
print(f"The model explains {r2_sklearn*100:.2f}% of the variance in house prices")
print(f"That means {(1-r2_sklearn)*100:.2f}% of the variance is unexplained")

# Visualize what R² means
fig = plt.figure(figsize=(10, 6))

# Baseline plot
plt.scatter(X_test, y_test, alpha=0.3, s=10, label='Actual Data')
plt.axhline(y_mean, color='red', linestyle='--', linewidth=2, label=f'Baseline: Predict Mean = {y_mean:.2f}')

# Regression line
plt.plot(X_test, y_pred_test, 'b-', linewidth=2, label=f'Regression Line (R² = {r2_sklearn:.4f})')

plt.xlabel('Median Income (in $10,000s)', fontsize=12)
plt.ylabel('Median House Value (in $100,000s)', fontsize=12)
plt.title('Baseline Model (R² = 0)', fontsize=14)

plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

::: {.callout-note}
Note how our regression line is a significantly better fit than the baseline line. That leads to an R² value significantly above zero.
:::

R² is intuitive and easy to explain to non-technical audiences. But it has problems:

**Problem 1: R² always increases when you add features**, even if those features are random noise. Add 100 random columns to your data, and R² will go up, even though the model hasn't actually improved.

**Problem 2: R² doesn't tell you if your predictions are biased**. R² tells you how close to a line your actual vs predicted values are. What's one easy way to get a straight line? Just predict the same y-value every single time! Congrats, you have a perfectly straight line, and thus have an R² of 100%! R² doesn't tell you if your predictions are biased. You could have a high R² but still systematically overpredict or underpredict in certain ranges.

**Problem 3: R² can be negative on test data** if your model is terrible. That's a good warning sign, actually.

### 2.4 Adjusted R²

Adjusted R² fixes the "adding features always increases R²" problem. It penalizes you for adding features that don't actually help.

The formula adjusts R² based on the number of features (p) and number of observations (n):

**Adjusted R² = $1 - \displaystyle\biggl[\frac{(1 - R^2)(n - 1)}{(n - p - 1)}\biggr]$**

Add a useless feature? Regular R² might go from 0.75 to 0.751. But adjusted R² might drop from 0.75 to 0.748 because the penalty for adding a feature outweighs the tiny improvement.

**When to use Adjusted R²:**
- When comparing models with different numbers of features
- When you're worried about overfitting
- When presenting results to stakeholders (it's more honest)

**When regular R² is fine:**
- When comparing models with the same number of features
- When you're more concerned about prediction accuracy than interpretation

**Real-World Example: When Many Features Mislead**

Let's see this with real census data from Staten Island. We'll predict income using many demographic features:

```{python}
# Load NYC census tract data
nyc_census = pd.read_csv('../data/nyc_census_tracts.csv')

# Filter to just Staten Island
staten_island_data = nyc_census[nyc_census['Borough'] == 'Staten Island'].copy()

# Remove rows with missing income (our target)
staten_island_data = staten_island_data.dropna(subset=['Income'])

# Select features (exclude identifiers and the target)
feature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',
                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',
                'ChildPoverty', 'Professional', 'Service', 'Office',
                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',
                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',
                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',
                'Unemployment']

# Prepare data (drop rows with any missing values)
staten_island_clean = staten_island_data[feature_cols + ['Income']].dropna()

X_si = staten_island_clean[feature_cols].values
y_si = staten_island_clean['Income'].values

print(f"\n{len(staten_island_clean)} observations")
print(f"Number of features: {len(feature_cols)}")
print(f"Ratio of features to observations (p/n): {len(feature_cols)/len(staten_island_clean):.3f}")

# Split data
X_train_si, X_test_si, y_train_si, y_test_si = train_test_split(
    X_si, y_si, test_size=0.3, random_state=42
)

# Fit model
model_si = LinearRegression()
model_si.fit(X_train_si, y_train_si)

# Calculate metrics
train_r2_si = model_si.score(X_train_si, y_train_si)
test_r2_si = model_si.score(X_test_si, y_test_si)

# Calculate Adjusted R² for training set
n_train = len(X_train_si)
p_si = X_train_si.shape[1]
adj_r2_train_si = 1 - ((1 - train_r2_si) * (n_train - 1) / (n_train - p_si - 1))

# Calculate Adjusted R² for test set
n_test = len(X_test_si)
adj_r2_test_si = 1 - ((1 - test_r2_si) * (n_test - 1) / (n_test - p_si - 1))

# Visualize the comparison
metrics_si = pd.DataFrame({
    'Metric': ['R²', 'Adjusted R²', 'R²', 'Adjusted R²'],
    'Dataset': ['Training', 'Training', 'Test', 'Test'],
    'Value': [train_r2_si, adj_r2_train_si, test_r2_si, adj_r2_test_si]
})

fig, ax = plt.subplots(figsize=(10, 6))
x_pos = [0, 1, 3, 4]
colors = ['skyblue', 'lightcoral', 'skyblue', 'lightcoral']
bars = ax.bar(x_pos, metrics_si['Value'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)

# Add value labels on bars (handle negative values)
for i, (pos, val) in enumerate(zip(x_pos, metrics_si['Value'])):
    y_offset = 0.02 if val >= 0 else -0.05
    ax.text(pos, val + y_offset, f'{val:.3f}', ha='center', fontsize=10, weight='bold')

ax.set_ylabel('Score', fontsize=12)
ax.set_title(f'R² vs Adjusted R² with {p_si} Features', fontsize=14)
ax.set_xticks(x_pos)
ax.set_xticklabels(['R²\n\nTraining', 'Adj R²\n\nTraining', 'R²\n\nTest', 'Adj R²\n\nTest'], fontsize=11)
# Set y-limits to accommodate negative adjusted R² values
y_min = min(metrics_si['Value'].min() - 0.1, -0.1)
ax.set_ylim(y_min, 1.1)
ax.axhline(y=0, color='black', linewidth=0.8, linestyle='-')
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

See the problem? With 30 features and a small number of Staten Island census tracts, the picture is alarming:

1. **Adjusted R² can be negative** - When test adjusted R² is negative, the model performs *worse* than just predicting the mean after accounting for model complexity. The penalty for using so many features overwhelms any predictive value. Remember, this is possible because the model _learned_ from the training data, but is _evaluated_ on the test data. What was learned from the training data is leading the model astray on the test data.
2. **Test R² drops significantly from training** - the model doesn't generalize well
3. **The high p/n ratio is the culprit** - too many features relative to observations

This is a realistic scenario where you might look at training R² and think "Great, decent fit!" but adjusted R² and test performance reveal the truth: the model is overfitted and unreliable.

::: {.callout-warning}
**Rule of thumb:** When your p/n ratio exceeds 0.10 (1 feature per 10 observations), be very skeptical of regular R². Always check Adjusted R² and test set performance. Better yet, use cross-validation to get a more honest assessment.
:::

---

## 3. Residual Analysis: Your Diagnostic Tool

Metrics like R² and MSE tell you *how much* error your model makes. But residual plots tell you *where* and *why* the model is making mistakes. This is where you catch problems before they bite you in production.

### 3.1 What Are Residuals?

A residual is simple: **residual = actual value - predicted value**.

If your model predicts a house is worth $250,000 but it's actually worth $300,000, the residual is $50,000. Positive residual means you underpredicted. Negative residual means you overpredicted.

Why do residuals matter more than just looking at errors? Because the *pattern* of residuals reveals whether your model's assumptions are violated. Random residuals? Great. Systematic patterns? Problem.

```{python}
# Load NYC census tract data (all boroughs this time)
nyc_census = pd.read_csv('../data/nyc_census_tracts.csv')

# Remove rows with missing income (our target)
nyc_census = nyc_census.dropna(subset=['Income'])

# Select features (exclude identifiers and the target)
feature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',
                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',
                'ChildPoverty', 'Professional', 'Service', 'Office',
                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',
                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',
                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',
                'Unemployment']

# Prepare data (drop rows with any missing values)
nyc_census = nyc_census[feature_cols + ['Income']].dropna()

X_nyc = nyc_census[feature_cols].values
y_nyc = nyc_census['Income'].values

# Split data
X_train_nyc, X_test_nyc, y_train_nyc, y_test_nyc = train_test_split(
    X_nyc, y_nyc, test_size=0.3, random_state=1
)

# Fit model
model_nyc = LinearRegression()
model_nyc.fit(X_train_nyc, y_train_nyc)

# Calculate residuals from our simple model
y_pred_test = model_nyc.predict(X_test_nyc)
residuals = y_test_nyc - y_pred_test

# Create a DataFrame for easy viewing
residual_df = pd.DataFrame({
    'Actual': y_test_nyc.flatten(),
    'Predicted': y_pred_test.flatten(),
    'Residual': residuals.flatten()
})

residual_df.head()
```

Notice that the residuals all fluctuate around zero? That's expected for linear regression—the errors cancel out on average. But that doesn't mean the errors are acceptable! We need to look at the pattern.

### 3.2 Residuals vs. Fitted Values Plot

This is the **single most important diagnostic plot** you'll make. It plots residuals (y-axis) against predicted values (x-axis).

**What you want to see:** Random scatter around zero. No patterns, no trends, just noise.

**What you don't want to see:** Curves, funnels, or systematic patterns. These indicate problems.

```{python}
# Create the residuals vs fitted plot
plt.figure(figsize=(10, 6))
plt.scatter(y_pred_test, residuals, alpha=0.5, s=20)
plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
plt.xlabel('Fitted Values (Predictions)', fontsize=12)
plt.ylabel('Residuals', fontsize=12)
plt.title('Residuals vs. Fitted Values', fontsize=14)
plt.grid(True, alpha=0.3)
plt.show()

# Also show distribution of residuals
plt.figure(figsize=(10, 5))
plt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel('Residual', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Residuals', fontsize=14)
plt.axvline(0, color='red', linestyle='--', linewidth=2)
plt.grid(True, alpha=0.3)
plt.show()
```

See what's happening? The residuals are somewhat randomly scattered, but there's two small issues: 

1. The model tends to underpredict (positive residuals) for very low predictions. 
2. The model tends to overpredict (negative residuals) for very high predictions.

This suggests the relationship might not be perfectly linear. However, despite this, this shows a reasonably good fit overall, with normally distributed residuals centered around zero.

### 3.3 What Patterns Tell You

Let's look at specific patterns and what they mean:

**Pattern 1: Curved Residuals (Non-Linearity)**

If residuals form a U-shape or inverted U-shape, your relationship isn't linear. The model systematically underpredicts in some regions and overpredicts in others.

**Fix:** Try polynomial features, log transforms, or use a non-linear model.

**Pattern 2: Funnel Shape (Heteroscedasticity)**

If residuals spread out as predictions increase (or decrease), you have non-constant variance. Maybe small houses have predictable prices, but mansion prices vary wildly.

**Fix:** Transform the target variable (log, square root), use weighted regression, or use a model that handles heteroscedasticity.

**Pattern 3: Outliers**

Points far from zero are outliers. A few outliers are normal. Many outliers mean something's wrong with your model or data.

**Fix:** Investigate outliers (data errors? special cases?), consider robust regression, or use regularization.

Let's demonstrate these patterns with synthetic data, comparing good vs. bad residual plots:

```{python}
# Create three datasets: good fit, non-linear, and heteroscedastic
np.random.seed(42)

# GOOD: Linear relationship with constant variance
X_good = np.linspace(0, 10, 200).reshape(-1, 1)
y_good = 5 * X_good.flatten() + np.random.normal(0, 5, 200)

model_good = LinearRegression()
model_good.fit(X_good, y_good)
y_pred_good = model_good.predict(X_good)
residuals_good = y_good - y_pred_good

# BAD: Non-linear relationship
X_nonlinear = np.linspace(0, 10, 200).reshape(-1, 1)
y_nonlinear = 2 * X_nonlinear.flatten()**2 + np.random.normal(0, 10, 200)

model_nonlinear = LinearRegression()
model_nonlinear.fit(X_nonlinear, y_nonlinear)
y_pred_nonlinear = model_nonlinear.predict(X_nonlinear)
residuals_nonlinear = y_nonlinear - y_pred_nonlinear

# BAD: Heteroscedasticity (funnel pattern)
X_hetero = np.linspace(1, 10, 200).reshape(-1, 1)
y_hetero = 5 * X_hetero.flatten() + np.random.normal(0, X_hetero.flatten(), 200)

model_hetero = LinearRegression()
model_hetero.fit(X_hetero, y_hetero)
y_pred_hetero = model_hetero.predict(X_hetero)
residuals_hetero = y_hetero - y_pred_hetero

# Create comparison plot
fig, axes = plt.subplots(3, 2, figsize=(14, 15))

# Row 1: GOOD - Linear data with constant variance
axes[0, 0].scatter(X_good, y_good, alpha=0.5, s=10, color='green')
axes[0, 0].plot(X_good, y_pred_good, 'r-', linewidth=2)
axes[0, 0].set_title('GOOD: Linear Data, Constant Variance', fontsize=12, fontweight='bold', color='green')
axes[0, 0].set_xlabel('X')
axes[0, 0].set_ylabel('y')
axes[0, 0].grid(True, alpha=0.3)

axes[0, 1].scatter(y_pred_good, residuals_good, alpha=0.5, s=10, color='green')
axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[0, 1].set_title('✓ Random Scatter (Good!)', fontsize=12, fontweight='bold', color='green')
axes[0, 1].set_xlabel('Fitted Values')
axes[0, 1].set_ylabel('Residuals')
axes[0, 1].grid(True, alpha=0.3)

# Row 2: BAD - Non-linearity
axes[1, 0].scatter(X_nonlinear, y_nonlinear, alpha=0.5, s=10, color='orange')
axes[1, 0].plot(X_nonlinear, y_pred_nonlinear, 'r-', linewidth=2)
axes[1, 0].set_title('BAD: Non-Linear Data with Linear Fit', fontsize=12, fontweight='bold', color='orange')
axes[1, 0].set_xlabel('X')
axes[1, 0].set_ylabel('y')
axes[1, 0].grid(True, alpha=0.3)

axes[1, 1].scatter(y_pred_nonlinear, residuals_nonlinear, alpha=0.5, s=10, color='orange')
axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1, 1].set_title('✗ Curved Pattern (Bad!)', fontsize=12, fontweight='bold', color='orange')
axes[1, 1].set_xlabel('Fitted Values')
axes[1, 1].set_ylabel('Residuals')
axes[1, 1].grid(True, alpha=0.3)

# Row 3: BAD - Heteroscedasticity
axes[2, 0].scatter(X_hetero, y_hetero, alpha=0.5, s=10, color='red')
axes[2, 0].plot(X_hetero, y_pred_hetero, 'r-', linewidth=2)
axes[2, 0].set_title('BAD: Data with Increasing Variance', fontsize=12, fontweight='bold', color='red')
axes[2, 0].set_xlabel('X')
axes[2, 0].set_ylabel('y')
axes[2, 0].grid(True, alpha=0.3)

axes[2, 1].scatter(y_pred_hetero, residuals_hetero, alpha=0.5, s=10, color='red')
axes[2, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[2, 1].set_title('✗ Funnel Pattern (Bad!)', fontsize=12, fontweight='bold', color='red')
axes[2, 1].set_xlabel('Fitted Values')
axes[2, 1].set_ylabel('Residuals')
axes[2, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("Compare the three residual plots (right column):")
print("  TOP (Green): Random scatter around zero = GOOD")
print("  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)")
print("  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)")
```

See the difference?

- **Top row (GOOD):** Residuals are randomly scattered around zero with constant spread. This is what you want!
- **Middle row (BAD - Curved):** Residuals show a clear U-shaped pattern—the model systematically underpredicts at the extremes and overpredicts in the middle.
- **Bottom row (BAD - Funnel):** Residuals fan out as predictions increase—variance is not constant.

::: {.callout-warning}
A high R² doesn't mean your assumptions are met! You can have R² = 0.95 but still have terrible residual patterns that indicate the model will fail on new data.
:::

These plots are your early warning system. Learn to read them, and you'll catch problems before they become disasters.

### 3.4 What to Do When Assumptions Fail

Okay, you've identified a problem in your residual plots. Now what?

**Problem: Non-Linearity (curved residual plot)**

**Solutions:**
1. **Polynomial features:** Add X², X³, etc. (covered in Section 4)
2. **Transform features:** Try log(X), √X, or 1/X
3. **Transform target:** Try log(y) or √y
4. **Use a non-linear model:** Tree-based models, neural networks, etc.

**Problem: Heteroscedasticity (funnel residual plot)**

**Solutions:**
1. **Transform target variable:** log(y) often stabilizes variance
2. **Weighted least squares:** Give less weight to high-variance observations
3. **Use robust standard errors:** Adjust your confidence intervals
4. **Just accept it:** If you only care about predictions (not inference), heteroscedasticity matters less

**Problem: Outliers**

**Solutions:**
1. **Investigate:** Are they data errors? Real but unusual observations?
2. **Remove them:** Only if justified (e.g., data entry errors)
3. **Use robust regression:** Huber regression, RANSAC, etc.
4. **Use regularization:** Ridge and Lasso reduce outlier influence (covered in Sections 6-7)

::: {.callout-tip}
Start with the simplest fix first. A log transformation of the target variable often fixes multiple problems at once: non-linearity and heteroscedasticity.
:::

Let's demonstrate a log transformation fix:

```{python}
# Our NYC census data actually benefits from log transformation
# Income values have a long right tail (a few very high-income census tracts)

# First, fit original model for comparison
model_original = LinearRegression()
model_original.fit(X_train, y_train)
y_pred_original = model_original.predict(X_test)
residuals_original = y_test.flatten() - y_pred_original.flatten()

# Fit model with log-transformed target
y_train_log = np.log(y_train)
y_test_log = np.log(y_test)

model_log = LinearRegression()
model_log.fit(X_train, y_train_log)
y_pred_log = model_log.predict(X_test)

# Calculate residuals
residuals_log = y_test_log.flatten() - y_pred_log.flatten()

# Compare residual plots
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Original scale
axes[0].scatter(y_pred_original, residuals_original, alpha=0.5, s=20)
axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[0].set_xlabel('Fitted Values', fontsize=12)
axes[0].set_ylabel('Residuals', fontsize=12)
axes[0].set_title('Original Scale Residuals', fontsize=14)
axes[0].grid(True, alpha=0.3)

# Log scale
axes[1].scatter(y_pred_log, residuals_log, alpha=0.5, s=20)
axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1].set_xlabel('Fitted Values (log scale)', fontsize=12)
axes[1].set_ylabel('Residuals', fontsize=12)
axes[1].set_title('Log-Transformed Residuals', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Original scale R²: {r2_score(y_test, y_pred_original):.4f}")
print(f"Log scale R²: {r2_score(y_test_log, y_pred_log):.4f}")
```

Notice how the log transformation often creates more evenly distributed residuals? This is a common trick in regression.

---

## 4. Polynomial Regression: Handling Non-Linearity

What do you do when your residual plot shows a clear curve? The relationship isn't linear, so a straight line won't work. This is where polynomial regression saves you.

### 4.1 When Linear Isn't Enough

Real relationships are rarely perfectly linear. Income and house price? Probably has some curvature. Temperature and ice cream sales? Definitely curved (sales don't go negative when it's cold, and they plateau when it's hot).

Polynomial regression lets you fit curves while still using linear regression. The trick? Create new features that are powers of your original features: X², X³, etc. Then fit a linear model to these polynomial features.

Here's the key insight: **polynomial regression is still linear regression**. It's linear in the *coefficients*, even though the relationship with X is non-linear. The model is y = β₀ + β₁X + β₂X² + β₃X³, which is a linear combination of the features [X, X², X³].

### 4.2 Creating Polynomial Features

Let's see this in action. We'll create polynomial features and fit them to data with a clear non-linear relationship.

```{python}
from sklearn.preprocessing import PolynomialFeatures

# Use data with a non-linear relationship
# Let's explore Unemployment vs Income (often a non-linear relationship)
nyc_poly = nyc_census[['Unemployment', 'Income']].dropna()
X_unemp = nyc_poly[['Unemployment']].values
y_income_poly = nyc_poly['Income'].values

# Split the data
X_unemp_train, X_unemp_test, y_income_poly_train, y_income_poly_test = train_test_split(
    X_unemp, y_income_poly, test_size=0.2, random_state=42
)

# First, try linear regression
model_linear_unemp = LinearRegression()
model_linear_unemp.fit(X_unemp_train, y_income_poly_train)
y_pred_linear_unemp = model_linear_unemp.predict(X_unemp_test)

print("Linear Model:")
print(f"R² = {model_linear_unemp.score(X_unemp_test, y_income_poly_test):.4f}")

# Now try polynomial regression (degree 2)
poly_2 = PolynomialFeatures(degree=2, include_bias=False)
X_unemp_train_poly2 = poly_2.fit_transform(X_unemp_train)
X_unemp_test_poly2 = poly_2.transform(X_unemp_test)

print(f"\nOriginal features shape: {X_unemp_train.shape}")
print(f"Polynomial features shape: {X_unemp_train_poly2.shape}")
print(f"Feature names: {poly_2.get_feature_names_out(['Unemployment'])}")

model_poly2 = LinearRegression()
model_poly2.fit(X_unemp_train_poly2, y_income_poly_train)
y_pred_poly2 = model_poly2.predict(X_unemp_test_poly2)

print("\nDegree-2 Polynomial Model:")
print(f"R² = {model_poly2.score(X_unemp_test_poly2, y_income_poly_test):.4f}")
print(f"Coefficients: {model_poly2.coef_}")

# Visualize the difference
plt.figure(figsize=(10, 6))
# Sort for smooth plotting
sort_idx = np.argsort(X_unemp_test.flatten())
X_test_sorted = X_unemp_test.flatten()[sort_idx]
y_pred_linear_sorted = y_pred_linear_unemp.flatten()[sort_idx]
y_pred_poly2_sorted = y_pred_poly2.flatten()[sort_idx]

plt.scatter(X_unemp_test, y_income_poly_test, alpha=0.3, s=10, label='Actual Data')
plt.plot(X_test_sorted, y_pred_linear_sorted, 'r-', linewidth=2, label='Linear Model')
plt.plot(X_test_sorted, y_pred_poly2_sorted, 'g-', linewidth=2, label='Degree-2 Polynomial')
plt.xlabel('Unemployment Rate (%)', fontsize=12)
plt.ylabel('Median Household Income ($)', fontsize=12)
plt.title('Linear vs Polynomial Regression', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

See what happened? `PolynomialFeatures(degree=2)` created a new feature: Unemployment². The linear model then fits: `y = β₀ + β₁(Unemployment) + β₂(Unemployment²)`. This gives us a parabola, which can capture curvature that a straight line can't.

### 4.3 Choosing Polynomial Degree

How do you know what degree to use? Degree 1 is linear. Degree 2 adds curvature. Degree 3 adds an S-curve. Higher degrees add more wiggles.

**The problem:** Higher degree doesn't always mean better. You can overfit spectacularly with high-degree polynomials.

**The solution:** Try multiple degrees and use a validation set (or cross-validation) to pick the best one.

```{python}
# Try polynomials of degree 1 through 10
degrees = range(1, 11)
train_scores = []
test_scores = []

for degree in degrees:
    # Create polynomial features
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_train_poly = poly.fit_transform(X_unemp_train)
    X_test_poly = poly.transform(X_unemp_test)

    # Fit model
    model = LinearRegression()
    model.fit(X_train_poly, y_income_poly_train)

    # Score
    train_score = model.score(X_train_poly, y_income_poly_train)
    test_score = model.score(X_test_poly, y_income_poly_test)

    train_scores.append(train_score)
    test_scores.append(test_score)

    print(f"Degree {degree}: Train R² = {train_score:.4f}, Test R² = {test_score:.4f}")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(degrees, train_scores, 'o-', linewidth=2, label='Training R²')
plt.plot(degrees, test_scores, 's-', linewidth=2, label='Test R²')
plt.xlabel('Polynomial Degree', fontsize=12)
plt.ylabel('R²', fontsize=12)
plt.title('Model Performance vs Polynomial Degree', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(degrees)
plt.show()

# Find optimal degree
optimal_degree = degrees[np.argmax(test_scores)]
print(f"\nOptimal polynomial degree: {optimal_degree} (Test R² = {max(test_scores):.4f})")
```

See that? Training R² keeps increasing with degree—the model just memorizes the training data. But test R² peaks and then starts *decreasing*. That's overfitting. The model gets so wiggly it fits training noise instead of the true pattern.

::: {.callout-warning}
**Never choose polynomial degree based on training performance alone!** Always use validation data or cross-validation. Otherwise, you'll pick a high degree that overfits.
:::

### 4.4 The Overfitting Risk

Let's visualize what high-degree polynomials do. They create absurd wiggles that fit every bump in the training data but fail on new data.

```{python}
# Compare degree 2 vs degree 10
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Create small dataset to exaggerate overfitting
np.random.seed(42)
X_small = np.linspace(0, 10, 30).reshape(-1, 1)
y_small = 2 * X_small.flatten() + np.random.normal(0, 5, 30)

# Degree 2
poly_2_small = PolynomialFeatures(degree=2, include_bias=False)
X_small_poly2 = poly_2_small.fit_transform(X_small)
model_poly2_small = LinearRegression()
model_poly2_small.fit(X_small_poly2, y_small)

# Create smooth line for plotting
X_plot = np.linspace(0, 10, 200).reshape(-1, 1)
X_plot_poly2 = poly_2_small.transform(X_plot)
y_plot_poly2 = model_poly2_small.predict(X_plot_poly2)

axes[0].scatter(X_small, y_small, s=50, label='Training Data')
axes[0].plot(X_plot, y_plot_poly2, 'r-', linewidth=2, label='Degree 2')
axes[0].set_xlabel('X', fontsize=12)
axes[0].set_ylabel('y', fontsize=12)
axes[0].set_title('Degree 2: Reasonable Fit', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Degree 10
poly_10_small = PolynomialFeatures(degree=10, include_bias=False)
X_small_poly10 = poly_10_small.fit_transform(X_small)
model_poly10_small = LinearRegression()
model_poly10_small.fit(X_small_poly10, y_small)

X_plot_poly10 = poly_10_small.transform(X_plot)
y_plot_poly10 = model_poly10_small.predict(X_plot_poly10)

axes[1].scatter(X_small, y_small, s=50, label='Training Data')
axes[1].plot(X_plot, y_plot_poly10, 'r-', linewidth=2, label='Degree 10')
axes[1].set_xlabel('X', fontsize=12)
axes[1].set_ylabel('y', fontsize=12)
axes[1].set_title('Degree 10: Overfitting!', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_ylim(axes[0].get_ylim())  # Same y-axis for comparison

plt.tight_layout()
plt.show()

print(f"Degree 2 - Training R²: {model_poly2_small.score(X_small_poly2, y_small):.4f}")
print(f"Degree 10 - Training R²: {model_poly10_small.score(X_small_poly10, y_small):.4f}")
```

Look at that degree-10 model! It wiggles wildly to pass through training points. Training R² is higher, but the model is useless for prediction. It learned noise, not signal.

**The Extreme Case: More Features Than Observations (p > n)**

Now let's see what happens when you push this to the limit: more features than data points. This creates an **overdetermined system** where you can get perfect training fit (R² = 1.0) that means absolutely nothing.

```{python}
nyc_census = pd.read_csv('../data/nyc_census_tracts.csv')

# Load our Bronx census data again
bronx_subset = nyc_census[nyc_census['Borough'] == 'Bronx'].copy()
bronx_subset = bronx_subset.dropna(subset=['Income'])

# Select just a few features to start
base_features = ['TotalPop', 'IncomePerCap', 'Poverty', 'Professional', 'Unemployment']
bronx_tiny = bronx_subset[base_features + ['Income']].dropna()

# Take only 30 census tracts (small sample)
bronx_tiny_sample = bronx_tiny.sample(n=30, random_state=42)

X_tiny = bronx_tiny_sample[base_features].values
y_tiny = bronx_tiny_sample['Income'].values

print(f"Starting with: {len(bronx_tiny_sample)} observations, {len(base_features)} features")

# Now create polynomial features to blow up the feature count
poly_extreme = PolynomialFeatures(degree=4, include_bias=False)
X_tiny_poly = poly_extreme.fit_transform(X_tiny)

print(f"After polynomial expansion (degree 4): {X_tiny_poly.shape[0]} observations, {X_tiny_poly.shape[1]} features")
print(f"Features > Observations: {X_tiny_poly.shape[1] > X_tiny_poly.shape[0]}")

# Hold out just ONE observation for "testing"
X_train_tiny = X_tiny_poly[:-1]
y_train_tiny = y_tiny[:-1]
X_test_tiny = X_tiny_poly[-1:]
y_test_tiny = y_tiny[-1:]

print(f"\nTraining: {X_train_tiny.shape[0]} observations, {X_train_tiny.shape[1]} features")
print(f"p > n? {X_train_tiny.shape[1] > X_train_tiny.shape[0]}")

# Fit the model
model_extreme = LinearRegression()
model_extreme.fit(X_train_tiny, y_train_tiny)

# Check performance
train_r2_extreme = model_extreme.score(X_train_tiny, y_train_tiny)
y_pred_train_extreme = model_extreme.predict(X_train_tiny)
y_pred_test_extreme = model_extreme.predict(X_test_tiny)

train_mse_extreme = mean_squared_error(y_train_tiny, y_pred_train_extreme)
test_error_extreme = abs(y_test_tiny[0] - y_pred_test_extreme[0])

print(f"\n{'='*60}")
print(f"RESULTS: {X_train_tiny.shape[1]} features, {X_train_tiny.shape[0]} observations")
print(f"{'='*60}")
print(f"Training R²: {train_r2_extreme:.10f}")
print(f"Training MSE: {train_mse_extreme:.10f}")
print(f"\nActual income (held-out): ${y_test_tiny[0]:,.0f}")
print(f"Predicted income: ${y_pred_test_extreme[0]:,.0f}")
print(f"Prediction error: ${test_error_extreme:,.0f}")
print(f"Prediction error (%): {100 * test_error_extreme / y_test_tiny[0]:.1f}%")

# Visualize the "perfect" fit
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Training "perfection"
axes[0].scatter(y_train_tiny, y_pred_train_extreme, s=50, alpha=0.7)
axes[0].plot([y_train_tiny.min(), y_train_tiny.max()],
             [y_train_tiny.min(), y_train_tiny.max()],
             'r--', linewidth=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual Income', fontsize=12)
axes[0].set_ylabel('Predicted Income', fontsize=12)
axes[0].set_title(f'Training: R² = {train_r2_extreme:.6f}', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Right: Test disaster
axes[1].scatter([y_test_tiny[0]], [y_pred_test_extreme[0]], s=200, c='red',
                marker='X', label='Test Prediction', zorder=3)
axes[1].plot([y_train_tiny.min(), y_train_tiny.max()],
             [y_train_tiny.min(), y_train_tiny.max()],
             'r--', linewidth=2, label='Perfect Prediction')
axes[1].set_xlabel('Actual Income', fontsize=12)
axes[1].set_ylabel('Predicted Income', fontsize=12)
axes[1].set_title(f'Test: Error = ${test_error_extreme:,.0f}', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**What just happened?**

When p > n (more features than observations), the system becomes **underdetermined**. The model can perfectly fit every training point because it has enough degrees of freedom to pass through all of them. R² = 1.0 is **guaranteed** mathematically—but it's meaningless!

This is like drawing a curve through 5 points with a 10th-degree polynomial. You have so much flexibility that you can hit every point exactly. But the curve between points? Pure nonsense.

::: {.callout-warning}
**DANGER: The Perfect R² Trap**

If you ever see R² = 1.0000 (or extremely close) on training data, be immediately suspicious:

1. Check if p ≥ n (features ≥ observations)
2. Check if you accidentally included the target variable as a feature
3. Check for data leakage (future information in features)
4. Check if you have duplicate rows

A "perfect" fit is almost never real. It's almost always a problem.
:::

**Real-world implications:**

This happens more often than you think:
- **Small datasets:** Medical studies with 50 patients but 200 genetic markers
- **High-dimensional data:** Images, text, genomics where features vastly outnumber samples
- **Time series:** Predicting tomorrow with 100 technical indicators but only 30 days of data

The solution? **Regularization** (covered in the next sections) or getting more data. Never trust a model where p/n > 1.0, and be very cautious when p/n > 0.3.

**Key takeaways:**
- Polynomial features let you model non-linear relationships with linear regression
- Choose degree using validation data, not training data
- Lower degree often generalizes better than higher degree
- When p ≥ n, you get perfect training fit but meaningless predictions
- Regularization (covered next) can help control overfitting in polynomial models

---

## 6. Multicollinearity: When Features Are Too Similar

Imagine trying to predict house prices using both "square footage" and "square meters" as separate features. They contain basically the same information! This creates multicollinearity, and it breaks coefficient interpretation in sneaky ways.

### 6.1 What Is Multicollinearity?

**Multicollinearity** means your features are highly correlated with each other. One feature can be predicted fairly well from others.

Why is this a problem? Linear regression tries to isolate the effect of each feature while "holding others constant." But if two features move together, you can't hold one constant while changing the other—they're tied together!

The result: **coefficient estimates become unstable**. Add or remove one observation, and coefficients swing wildly. Even worse, a feature that's clearly important might show up with a tiny coefficient (or even the wrong sign!) because its effect is "stolen" by correlated features.

Let's create an example to see this:

```{python}
# Create dataset with multicollinearity
np.random.seed(42)
n = 1000

# X1 is random
X1 = np.random.normal(0, 1, n)

# X2 is highly correlated with X1
X2 = X1 + np.random.normal(0, 0.1, n)  # Almost identical to X1

# X3 is independent
X3 = np.random.normal(0, 1, n)

# True relationship: y = 5*X1 + 0*X2 + 3*X3 + noise
# Note: X2 has NO effect, but it's correlated with X1
y_multi = 5*X1 + 3*X3 + np.random.normal(0, 1, n)

# Create DataFrame
df_multi = pd.DataFrame({
    'X1': X1,
    'X2': X2,
    'X3': X3,
    'y': y_multi
})

# Check correlation matrix
print("Correlation Matrix:")
print(df_multi.corr())

# Visualize correlations
plt.figure(figsize=(8, 6))
sns.heatmap(df_multi.corr(), annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Matrix', fontsize=14)
plt.show()
```

See that? X1 and X2 have a correlation of about 0.995. They're nearly identical. Now watch what happens when we fit a regression:

```{python}
# Fit model with all features
X_multi = df_multi[['X1', 'X2', 'X3']].values
y_multi_target = df_multi['y'].values

model_multi = LinearRegression()
model_multi.fit(X_multi, y_multi_target)

print("Coefficients with multicollinearity:")
for i, name in enumerate(['X1', 'X2', 'X3']):
    print(f"  {name}: {model_multi.coef_[i]:.4f}")

print(f"\nRemember: True coefficients are X1=5, X2=0, X3=3")
print("But the model can't tell X1 and X2 apart!")

# Fit model with only X1 and X3 (no multicollinearity)
X_clean = df_multi[['X1', 'X3']].values
model_clean = LinearRegression()
model_clean.fit(X_clean, y_multi_target)

print("\nCoefficients without multicollinearity:")
for i, name in enumerate(['X1', 'X3']):
    print(f"  {name}: {model_clean.coef_[i]:.4f}")

print("\nMuch closer to the true values!")
```

### 6.2 Why It's a Problem

Let me be blunt: **multicollinearity doesn't hurt prediction accuracy**. Your R² will be fine. Your predictions will be fine. So why do we care?

**Problem 1: Coefficient interpretation becomes meaningless**

If X1 and X2 are highly correlated, the model might give X1 a coefficient of 10 and X2 a coefficient of -5. Or it might do the opposite: X1 = -5, X2 = 10. Or X1 = 2.5, X2 = 2.5. All three give similar predictions! But which feature is "really" important? You can't tell.

**Problem 2: Coefficient instability**

Small changes in the data cause huge swings in coefficients. Add 10 new observations? Coefficients might flip signs. This makes the model untrustworthy for understanding relationships.

**Problem 3: Hard to select features**

If X1 and X2 are correlated, dropping one might barely hurt performance, but dropping both kills it. This makes feature selection confusing.

**When to care:**
- You need to interpret coefficients (e.g., explaining to stakeholders)
- You want to identify the "most important" features
- You're making causal claims

**When NOT to care:**
- You only need accurate predictions
- You're using regularization (which handles multicollinearity automatically)

### 6.3 Detecting Multicollinearity

**Method 1: Correlation Matrix**

The simplest approach. Look for correlations close to ±1 (say, above 0.8 or 0.9).

```{python}
# Use NYC census data
features_for_vif = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']
nyc_vif = nyc_census[features_for_vif].dropna()
X_vif = nyc_vif

# Correlation matrix
corr_matrix = X_vif.corr()

print("Correlation Matrix:")
print(corr_matrix)

# Visualize
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8}, fmt='.3f')
plt.title('Feature Correlation Matrix - NYC Census Data', fontsize=14)
plt.tight_layout()
plt.show()

# Identify high correlations
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        if abs(corr_matrix.iloc[i, j]) > 0.7:
            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

print("\nHighly correlated feature pairs (|correlation| > 0.7):")
for feat1, feat2, corr in high_corr_pairs:
    print(f"  {feat1} & {feat2}: {corr:.3f}")
```

**Method 2: Variance Inflation Factor (VIF)**

VIF measures how much the variance of a coefficient is "inflated" due to multicollinearity.

**VIF interpretation:**
- VIF = 1: No multicollinearity
- VIF = 1-5: Moderate multicollinearity (usually okay)
- VIF = 5-10: High multicollinearity (concerning)
- VIF > 10: Severe multicollinearity (definitely a problem)

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = features_for_vif
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(features_for_vif))]

print("Variance Inflation Factors:")
print(vif_data.sort_values('VIF', ascending=False))

# Visualize VIF
plt.figure(figsize=(10, 6))
plt.barh(vif_data["Feature"], vif_data["VIF"])
plt.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='VIF = 5 (Moderate)')
plt.axvline(x=10, color='red', linestyle='--', linewidth=2, label='VIF = 10 (Severe)')
plt.xlabel('VIF', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.title('Variance Inflation Factors', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3, axis='x')
plt.show()
```

See features with high VIF? Those are the ones tangled up with others. The solution? Remove one of the correlated features, combine them, or use regularization (which we'll cover next).

---

## 7. Ridge Regression: L2 Regularization

Multicollinearity makes coefficients unstable. Polynomial features risk overfitting. The solution to both? **Regularization**. It's one of the most important ideas in machine learning.

### 7.1 The Regularization Idea

Here's the core insight: **penalize large coefficients**. Instead of just minimizing error, also minimize the size of coefficients. The model has to balance two goals:
1. Fit the training data well (low error)
2. Keep coefficients small (low complexity)

Why does this help? Large coefficients make the model sensitive to small changes in features. By shrinking coefficients, you make the model more stable. Yes, you're intentionally introducing bias (the model won't fit training data perfectly). But you reduce variance (the model generalizes better to new data).

This is the famous **bias-variance tradeoff**. A little bias for a lot less variance is usually a great deal.

### 7.2 How Ridge Works

Ridge regression adds a penalty term to the loss function:

**Loss = MSE + α × Σ(coefficients²)**

That second term is the L2 penalty (sum of squared coefficients). The **α** (alpha) parameter controls how much you penalize:
- α = 0: No penalty, just regular linear regression
- Small α: Light penalty, coefficients shrink a little
- Large α: Heavy penalty, coefficients shrink toward zero (but never reach exactly zero)

Let's see it in action:

```{python}
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler

# Use multiple features for richer example
features_ridge = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']
nyc_ridge = nyc_census[features_ridge + ['Income']].dropna()
X_ridge = nyc_ridge[features_ridge].values
y_ridge = nyc_ridge['Income'].values

# Split data
X_ridge_train, X_ridge_test, y_ridge_train, y_ridge_test = train_test_split(
    X_ridge, y_ridge, test_size=0.2, random_state=42
)

# IMPORTANT: Always scale features before regularization!
# Features with larger scales get penalized more
scaler = StandardScaler()
X_ridge_train_scaled = scaler.fit_transform(X_ridge_train)
X_ridge_test_scaled = scaler.transform(X_ridge_test)

# Fit regular linear regression
model_lr = LinearRegression()
model_lr.fit(X_ridge_train_scaled, y_ridge_train)
print("Linear Regression (no regularization):")
print(f"  Train R²: {model_lr.score(X_ridge_train_scaled, y_ridge_train):.4f}")
print(f"  Test R²: {model_lr.score(X_ridge_test_scaled, y_ridge_test):.4f}")
print(f"  Coefficients: {model_lr.coef_}")
print(f"  Coefficient magnitudes: {np.abs(model_lr.coef_).sum():.4f}")

# Fit Ridge with alpha=1
model_ridge = Ridge(alpha=1.0)
model_ridge.fit(X_ridge_train_scaled, y_ridge_train)
print("\nRidge Regression (alpha=1.0):")
print(f"  Train R²: {model_ridge.score(X_ridge_train_scaled, y_ridge_train):.4f}")
print(f"  Test R²: {model_ridge.score(X_ridge_test_scaled, y_ridge_test):.4f}")
print(f"  Coefficients: {model_ridge.coef_}")
print(f"  Coefficient magnitudes: {np.abs(model_ridge.coef_).sum():.4f}")

# Visualize coefficient shrinkage
coef_comparison = pd.DataFrame({
    'Feature': features_ridge,
    'Linear Regression': model_lr.coef_,
    'Ridge (α=1)': model_ridge.coef_
})

fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(features_ridge))
width = 0.35
ax.bar(x - width/2, coef_comparison['Linear Regression'], width, label='Linear Regression', alpha=0.8)
ax.bar(x + width/2, coef_comparison['Ridge (α=1)'], width, label='Ridge (α=1)', alpha=0.8)
ax.set_xlabel('Feature', fontsize=12)
ax.set_ylabel('Coefficient Value', fontsize=12)
ax.set_title('Coefficient Shrinkage with Ridge Regularization', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(features_ridge, rotation=45, ha='right')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

See that? Ridge shrank all coefficients toward zero. Training R² dropped slightly (the model fits training data less perfectly), but test R² stayed similar or even improved (better generalization).

::: {.callout-warning}
**Always scale features before using Ridge!** Features with large scales get penalized more heavily than features with small scales. Standardizing (mean=0, std=1) ensures all features are treated equally.
:::

### 7.3 Choosing Alpha

How do you pick α? Try many values and use cross-validation to see which generalizes best.

```{python}
from sklearn.model_selection import cross_val_score

# Try many alpha values
alphas = np.logspace(-2, 3, 50)  # From 0.01 to 1000
train_scores_ridge = []
test_scores_ridge = []
cv_scores_ridge = []

for alpha in alphas:
    model = Ridge(alpha=alpha)

    # Train score
    model.fit(X_ridge_train_scaled, y_ridge_train)
    train_scores_ridge.append(model.score(X_ridge_train_scaled, y_ridge_train))

    # Test score
    test_scores_ridge.append(model.score(X_ridge_test_scaled, y_ridge_test))

    # Cross-validation score (5-fold)
    cv_score = cross_val_score(model, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()
    cv_scores_ridge.append(cv_score)

# Plot results
plt.figure(figsize=(10, 6))
plt.plot(alphas, train_scores_ridge, label='Training R²', linewidth=2)
plt.plot(alphas, test_scores_ridge, label='Test R²', linewidth=2)
plt.plot(alphas, cv_scores_ridge, label='CV R² (5-fold)', linewidth=2, linestyle='--')
plt.xscale('log')
plt.xlabel('Alpha (log scale)', fontsize=12)
plt.ylabel('R²', fontsize=12)
plt.title('Ridge Regression: Choosing Alpha', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.axvline(alphas[np.argmax(cv_scores_ridge)], color='red', linestyle=':', linewidth=2, label='Optimal Alpha')
plt.show()

# Best alpha
optimal_alpha = alphas[np.argmax(cv_scores_ridge)]
print(f"Optimal alpha (by cross-validation): {optimal_alpha:.4f}")
print(f"Best CV R²: {max(cv_scores_ridge):.4f}")
```

See the pattern? As α increases:
- Training R² decreases (more bias, less fit to training data)
- Test/CV R² first increases (less overfitting), then decreases (too much bias)

The optimal α is where test/CV performance peaks.

### 7.4 Interpreting Ridge Coefficients

Ridge shrinks all coefficients, but **none become exactly zero**. Even if a feature is useless, its coefficient will be tiny but non-zero.

This is both good and bad:
- **Good:** Ridge handles multicollinearity well. Correlated features share the penalty, and coefficients stay stable.
- **Bad:** Ridge doesn't do feature selection. All features stay in the model.

Let's see how coefficients change with α:

```{python}
# Track coefficient paths as alpha changes
alphas_path = np.logspace(-2, 3, 100)
coefs_ridge = []

for alpha in alphas_path:
    model = Ridge(alpha=alpha)
    model.fit(X_ridge_train_scaled, y_ridge_train)
    coefs_ridge.append(model.coef_)

coefs_ridge = np.array(coefs_ridge)

# Plot coefficient paths
plt.figure(figsize=(10, 6))
for i, feature in enumerate(features_ridge):
    plt.plot(alphas_path, coefs_ridge[:, i], label=feature, linewidth=2)

plt.xscale('log')
plt.xlabel('Alpha (log scale)', fontsize=12)
plt.ylabel('Coefficient Value', fontsize=12)
plt.title('Ridge Coefficient Paths', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(0, color='black', linestyle='-', linewidth=0.5)
plt.show()
```

Notice how all coefficients smoothly shrink toward zero as α increases, but none ever reach exactly zero? That's Ridge. Smooth, stable, but keeps all features.

---

## 8. Lasso Regression: L1 Regularization

Ridge is great, but it never says "this feature is useless." Lasso does. It performs **automatic feature selection** by setting some coefficients to exactly zero.

### 8.1 How Lasso Differs from Ridge

Lasso uses L1 regularization instead of L2:

**Loss = MSE + α × Σ|coefficients|**

The difference? Instead of squaring coefficients (Ridge), we take their absolute value (Lasso). This small change has a huge impact: **Lasso can set coefficients to exactly zero**.

Why? It's geometry. The L1 penalty creates "corners" where the optimal solution often has some coefficients at exactly zero. Ridge's L2 penalty is smooth, so coefficients approach zero but never arrive.

### 8.2 Lasso for Feature Selection

Lasso is feature selection built into the regression. As α increases, Lasso zeroes out features one by one, keeping only the most important.

```{python}
from sklearn.linear_model import Lasso

# Fit Lasso with alpha=0.01
model_lasso = Lasso(alpha=0.01, max_iter=10000)
model_lasso.fit(X_ridge_train_scaled, y_ridge_train)

print("Lasso Regression (alpha=0.01):")
print(f"  Train R²: {model_lasso.score(X_ridge_train_scaled, y_ridge_train):.4f}")
print(f"  Test R²: {model_lasso.score(X_ridge_test_scaled, y_ridge_test):.4f}")
print(f"\nCoefficients:")
for feature, coef in zip(features_ridge, model_lasso.coef_):
    if abs(coef) < 0.0001:
        print(f"  {feature}: {coef:.6f} → ELIMINATED!")
    else:
        print(f"  {feature}: {coef:.6f}")

# Compare with larger alpha
model_lasso_strong = Lasso(alpha=0.1, max_iter=10000)
model_lasso_strong.fit(X_ridge_train_scaled, y_ridge_train)

print("\nLasso Regression (alpha=0.1):")
print(f"  Train R²: {model_lasso_strong.score(X_ridge_train_scaled, y_ridge_train):.4f}")
print(f"  Test R²: {model_lasso_strong.score(X_ridge_test_scaled, y_ridge_test):.4f}")
print(f"\nCoefficients:")
for feature, coef in zip(features_ridge, model_lasso_strong.coef_):
    if abs(coef) < 0.0001:
        print(f"  {feature}: {coef:.6f} → ELIMINATED!")
    else:
        print(f"  {feature}: {coef:.6f}")

# Visualize feature selection
fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(features_ridge))
width = 0.25
ax.bar(x - width, model_lr.coef_, width, label='Linear Regression', alpha=0.8)
ax.bar(x, model_lasso.coef_, width, label='Lasso (α=0.01)', alpha=0.8)
ax.bar(x + width, model_lasso_strong.coef_, width, label='Lasso (α=0.1)', alpha=0.8)
ax.set_xlabel('Feature', fontsize=12)
ax.set_ylabel('Coefficient Value', fontsize=12)
ax.set_title('Lasso Feature Selection', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(features_ridge, rotation=45, ha='right')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
ax.axhline(0, color='black', linewidth=0.5)
plt.tight_layout()
plt.show()
```

See how Lasso zeroed out some features completely? That's automatic feature selection. Larger α means more aggressive selection.

### 8.3 Visualizing the Regularization Path

The "regularization path" shows how coefficients shrink as α increases. For Lasso, coefficients hit zero and stay there.

```{python}
# Track Lasso coefficients across alpha values
alphas_lasso = np.logspace(-3, 1, 100)  # From 0.001 to 10
coefs_lasso = []

for alpha in alphas_lasso:
    model = Lasso(alpha=alpha, max_iter=10000)
    model.fit(X_ridge_train_scaled, y_ridge_train)
    coefs_lasso.append(model.coef_)

coefs_lasso = np.array(coefs_lasso)

# Plot Lasso regularization path
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Lasso path
for i, feature in enumerate(features_ridge):
    axes[0].plot(alphas_lasso, coefs_lasso[:, i], label=feature, linewidth=2)

axes[0].set_xscale('log')
axes[0].set_xlabel('Alpha (log scale)', fontsize=12)
axes[0].set_ylabel('Coefficient Value', fontsize=12)
axes[0].set_title('Lasso Regularization Path', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)
axes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)

# Ridge path (for comparison)
for i, feature in enumerate(features_ridge):
    axes[1].plot(alphas_path, coefs_ridge[:, i], label=feature, linewidth=2)

axes[1].set_xscale('log')
axes[1].set_xlabel('Alpha (log scale)', fontsize=12)
axes[1].set_ylabel('Coefficient Value', fontsize=12)
axes[1].set_title('Ridge Regularization Path', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)

plt.tight_layout()
plt.show()
```

See the difference? **Lasso coefficients hit zero abruptly** and stay there (left plot). **Ridge coefficients smoothly approach zero** but never reach it (right plot).

::: {.callout-note}
Lasso is more aggressive than Ridge. It makes hard decisions: "this feature matters" or "this feature doesn't." Ridge says "this feature matters a little less." Choose based on whether you want sparse models (Lasso) or stable coefficients (Ridge).
:::

### 8.4 When to Use Lasso vs. Ridge

**Use Lasso when:**
- You suspect many features are irrelevant
- You want a sparse model (fewer features)
- You need to explain which features matter most
- Interpretability is critical

**Use Ridge when:**
- You think most features contribute something
- You have multicollinearity and want stable coefficients
- You're okay with keeping all features
- You prioritize prediction over interpretation

**The truth?** Try both and use cross-validation to decide. Sometimes Ridge wins. Sometimes Lasso wins. Sometimes they're tied.

```{python}
# Compare Ridge vs Lasso performance
alphas_compare = np.logspace(-3, 2, 50)
ridge_scores = []
lasso_scores = []

for alpha in alphas_compare:
    # Ridge
    ridge = Ridge(alpha=alpha)
    ridge_cv = cross_val_score(ridge, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()
    ridge_scores.append(ridge_cv)

    # Lasso
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso_cv = cross_val_score(lasso, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()
    lasso_scores.append(lasso_cv)

# Plot comparison
plt.figure(figsize=(10, 6))
plt.plot(alphas_compare, ridge_scores, 'o-', label='Ridge', linewidth=2)
plt.plot(alphas_compare, lasso_scores, 's-', label='Lasso', linewidth=2)
plt.xscale('log')
plt.xlabel('Alpha (log scale)', fontsize=12)
plt.ylabel('Cross-Validation R²', fontsize=12)
plt.title('Ridge vs Lasso: Cross-Validation Performance', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Best performance
best_ridge_score = max(ridge_scores)
best_lasso_score = max(lasso_scores)
print(f"Best Ridge CV R²: {best_ridge_score:.4f}")
print(f"Best Lasso CV R²: {best_lasso_score:.4f}")

if best_ridge_score > best_lasso_score:
    print("\nRidge wins on this dataset")
elif best_lasso_score > best_ridge_score:
    print("\nLasso wins on this dataset")
else:
    print("\nRidge and Lasso are tied")
```

---

## 9. Elastic Net: Best of Both Worlds

Can't decide between Ridge and Lasso? Why not both? Elastic Net combines L1 and L2 regularization to get the best of both worlds.

### 9.1 Combining L1 and L2

Elastic Net uses a mix of Ridge (L2) and Lasso (L1) penalties:

**Loss = MSE + α × [l1_ratio × Σ|coefficients| + (1 - l1_ratio) × Σ(coefficients²)]**

Two hyperparameters:
- **α (alpha):** Overall regularization strength (like Ridge and Lasso)
- **l1_ratio:** Mix between L1 and L2
  - l1_ratio = 0: Pure Ridge
  - l1_ratio = 1: Pure Lasso
  - l1_ratio = 0.5: Equal mix of both

Why combine them? Lasso can be unstable when features are highly correlated—it randomly picks one and zeros the others. Ridge keeps all correlated features but doesn't select. Elastic Net does feature selection (like Lasso) but more stably (like Ridge).

```{python}
from sklearn.linear_model import ElasticNet

# Fit Elastic Net with different l1_ratios
l1_ratios = [0.2, 0.5, 0.8]
alpha_en = 0.01

fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(features_ridge))
width = 0.2

# Also include Ridge and Lasso for comparison
models = {
    'Ridge (l1=0)': Ridge(alpha=alpha_en),
    'ElasticNet (l1=0.2)': ElasticNet(alpha=alpha_en, l1_ratio=0.2, max_iter=10000),
    'ElasticNet (l1=0.5)': ElasticNet(alpha=alpha_en, l1_ratio=0.5, max_iter=10000),
    'ElasticNet (l1=0.8)': ElasticNet(alpha=alpha_en, l1_ratio=0.8, max_iter=10000),
    'Lasso (l1=1)': Lasso(alpha=alpha_en, max_iter=10000)
}

for idx, (name, model) in enumerate(models.items()):
    model.fit(X_ridge_train_scaled, y_ridge_train)
    r2 = model.score(X_ridge_test_scaled, y_ridge_test)
    print(f"{name}: Test R² = {r2:.4f}")
    print(f"  Coefficients: {model.coef_}")
    print(f"  Non-zero coefficients: {np.sum(np.abs(model.coef_) > 0.0001)}")
    print()

# Visualize one Elastic Net model
model_en = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)
model_en.fit(X_ridge_train_scaled, y_ridge_train)

# Compare all three
fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(features_ridge))
width = 0.25

# Get a Ridge and Lasso with same alpha for fair comparison
model_ridge_comp = Ridge(alpha=0.01)
model_ridge_comp.fit(X_ridge_train_scaled, y_ridge_train)

model_lasso_comp = Lasso(alpha=0.01, max_iter=10000)
model_lasso_comp.fit(X_ridge_train_scaled, y_ridge_train)

ax.bar(x - width, model_ridge_comp.coef_, width, label='Ridge', alpha=0.8)
ax.bar(x, model_en.coef_, width, label='Elastic Net (l1_ratio=0.5)', alpha=0.8)
ax.bar(x + width, model_lasso_comp.coef_, width, label='Lasso', alpha=0.8)

ax.set_xlabel('Feature', fontsize=12)
ax.set_ylabel('Coefficient Value', fontsize=12)
ax.set_title('Ridge vs Elastic Net vs Lasso', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(features_ridge, rotation=45, ha='right')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
ax.axhline(0, color='black', linewidth=0.5)
plt.tight_layout()
plt.show()
```

Elastic Net sits between Ridge and Lasso. It zeros out some features (like Lasso) but keeps coefficients more stable (like Ridge).

### 9.2 When to Use Elastic Net

**Use Elastic Net when:**
- You have groups of correlated features
- You want feature selection but Lasso is too unstable
- You're not sure if Ridge or Lasso is better (hedge your bets)
- You have more features than observations (p > n)

**Real-world scenario:** You have 50 features measuring similar things (different weather stations, different survey questions, etc.). Lasso might randomly pick one from each group. Ridge keeps all 50. Elastic Net picks a few from each group—the best compromise.

**Practical advice:** If you're unsure, use Elastic Net with l1_ratio=0.5 and tune it with cross-validation. It's a safe default that adapts to your data.

```{python}
# Tune both alpha and l1_ratio with grid search
from sklearn.model_selection import GridSearchCV

param_grid = {
    'alpha': np.logspace(-3, 1, 20),
    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]
}

elastic_net = ElasticNet(max_iter=10000)
grid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='r2', n_jobs=-1)
grid_search.fit(X_ridge_train_scaled, y_ridge_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV R²: {grid_search.best_score_:.4f}")
print(f"Test R²: {grid_search.score(X_ridge_test_scaled, y_ridge_test):.4f}")

# Best model coefficients
best_model = grid_search.best_estimator_
print(f"\nBest model coefficients:")
for feature, coef in zip(features_ridge, best_model.coef_):
    if abs(coef) > 0.0001:
        print(f"  {feature}: {coef:.6f}")
    else:
        print(f"  {feature}: {coef:.6f} → ELIMINATED")
```

Elastic Net automatically found the best combination of Ridge and Lasso for your data. That's the power of combining regularization techniques.

---

## 10. Putting It All Together: A Complete Regression Analysis

You've learned all the pieces. Now let's put them together into a complete regression workflow that you can follow for any project.

### 10.1 The Diagnostic Workflow

Here's the process every regression analysis should follow:

**Step 1: Fit a baseline linear model**
**Step 2: Check assumptions with diagnostic plots**
**Step 3: Identify problems (non-linearity, heteroscedasticity, multicollinearity)**
**Step 4: Fix problems (transformations, polynomial features, regularization)**
**Step 5: Validate with cross-validation**
**Step 6: Interpret and communicate results**

Let's walk through this with a complete example:

```{python}
# Step 1: Fit baseline linear model
print("="*60)
print("STEP 1: Baseline Linear Regression")
print("="*60)

features_complete = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']
nyc_complete = nyc_census[features_complete + ['Income']].dropna()
X_complete = nyc_complete[features_complete].values
y_complete = nyc_complete['Income'].values

X_train_complete, X_test_complete, y_train_complete, y_test_complete = train_test_split(
    X_complete, y_complete, test_size=0.2, random_state=42
)

# Baseline model
model_baseline = LinearRegression()
model_baseline.fit(X_train_complete, y_train_complete)
y_pred_baseline = model_baseline.predict(X_test_complete)

print(f"Train R²: {model_baseline.score(X_train_complete, y_train_complete):.4f}")
print(f"Test R²: {model_baseline.score(X_test_complete, y_test_complete):.4f}")
print(f"Test RMSE: {np.sqrt(mean_squared_error(y_test_complete, y_pred_baseline)):.4f}")

# Step 2: Check assumptions with diagnostic plots
print("\n" + "="*60)
print("STEP 2: Diagnostic Plots")
print("="*60)

residuals_complete = y_test_complete - y_pred_baseline

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Residuals vs Fitted
axes[0, 0].scatter(y_pred_baseline, residuals_complete, alpha=0.5, s=10)
axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=2)
axes[0, 0].set_xlabel('Fitted Values')
axes[0, 0].set_ylabel('Residuals')
axes[0, 0].set_title('Residuals vs Fitted')
axes[0, 0].grid(True, alpha=0.3)

# Q-Q Plot
stats.probplot(residuals_complete, dist="norm", plot=axes[0, 1])
axes[0, 1].set_title('Q-Q Plot')
axes[0, 1].grid(True, alpha=0.3)

# Histogram of residuals
axes[1, 0].hist(residuals_complete, bins=50, edgecolor='black', alpha=0.7)
axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Residuals')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].set_title('Residual Distribution')
axes[1, 0].grid(True, alpha=0.3)

# Scale-Location Plot
standardized_resid = residuals_complete / residuals_complete.std()
axes[1, 1].scatter(y_pred_baseline, np.sqrt(np.abs(standardized_resid)), alpha=0.5, s=10)
axes[1, 1].set_xlabel('Fitted Values')
axes[1, 1].set_ylabel('√|Standardized Residuals|')
axes[1, 1].set_title('Scale-Location Plot')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("Observations:")
print("- Residuals show some heteroscedasticity (variance increases with fitted values)")
print("- Q-Q plot shows deviations at the tails (outliers)")
print("- Overall pattern suggests room for improvement")

# Step 3: Check for multicollinearity
print("\n" + "="*60)
print("STEP 3: Check Multicollinearity")
print("="*60)

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["Feature"] = features_complete
vif_data["VIF"] = [variance_inflation_factor(X_complete, i) for i in range(len(features_complete))]

print(vif_data.sort_values('VIF', ascending=False))
print("\nVIF < 5: No serious multicollinearity concerns")
```

See the workflow? Fit → diagnose → identify issues. Now let's fix them.

### 10.2 Model Selection Strategy

Based on diagnostics, try improvements systematically:

```{python}
# Step 4: Try different models
print("\n" + "="*60)
print("STEP 4: Model Comparison")
print("="*60)

# Scale features for regularization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_complete)
X_test_scaled = scaler.transform(X_test_complete)

# Dictionary to store results
results = []

# Model 1: Baseline Linear Regression
model1 = LinearRegression()
model1.fit(X_train_scaled, y_train_complete)
cv_score1 = cross_val_score(model1, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()
test_score1 = model1.score(X_test_scaled, y_test_complete)
results.append({
    'Model': 'Linear Regression',
    'CV R²': cv_score1,
    'Test R²': test_score1,
    'Features': len(features_complete)
})

# Model 2: Polynomial Features (degree 2)
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly_features.fit_transform(X_train_scaled)
X_test_poly = poly_features.transform(X_test_scaled)

model2 = LinearRegression()
model2.fit(X_train_poly, y_train_complete)
cv_score2 = cross_val_score(model2, X_train_poly, y_train_complete, cv=5, scoring='r2').mean()
test_score2 = model2.score(X_test_poly, y_test_complete)
results.append({
    'Model': 'Polynomial (degree=2)',
    'CV R²': cv_score2,
    'Test R²': test_score2,
    'Features': X_train_poly.shape[1]
})

# Model 3: Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_scaled, y_train_complete)
cv_score3 = cross_val_score(ridge, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()
test_score3 = ridge.score(X_test_scaled, y_test_complete)
results.append({
    'Model': 'Ridge (α=1.0)',
    'CV R²': cv_score3,
    'Test R²': test_score3,
    'Features': len(features_complete)
})

# Model 4: Lasso Regression
lasso = Lasso(alpha=0.01, max_iter=10000)
lasso.fit(X_train_scaled, y_train_complete)
cv_score4 = cross_val_score(lasso, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()
test_score4 = lasso.score(X_test_scaled, y_test_complete)
n_features_lasso = np.sum(np.abs(lasso.coef_) > 0.0001)
results.append({
    'Model': 'Lasso (α=0.01)',
    'CV R²': cv_score4,
    'Test R²': test_score4,
    'Features': f"{n_features_lasso} (selected)"
})

# Model 5: Elastic Net
elastic = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)
elastic.fit(X_train_scaled, y_train_complete)
cv_score5 = cross_val_score(elastic, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()
test_score5 = elastic.score(X_test_scaled, y_test_complete)
n_features_elastic = np.sum(np.abs(elastic.coef_) > 0.0001)
results.append({
    'Model': 'Elastic Net (α=0.01, l1=0.5)',
    'CV R²': cv_score5,
    'Test R²': test_score5,
    'Features': f"{n_features_elastic} (selected)"
})

# Display results
results_df = pd.DataFrame(results)
print("\nModel Comparison Results:")
print(results_df.to_string(index=False))

# Visualize
fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(results_df))
width = 0.35

ax.bar(x - width/2, results_df['CV R²'], width, label='CV R²', alpha=0.8)
ax.bar(x + width/2, results_df['Test R²'], width, label='Test R²', alpha=0.8)

ax.set_xlabel('Model', fontsize=12)
ax.set_ylabel('R²', fontsize=12)
ax.set_title('Model Performance Comparison', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# Best model
best_idx = results_df['CV R²'].argmax()
best_model_name = results_df.iloc[best_idx]['Model']
print(f"\n✓ Best model by CV: {best_model_name}")
```

The comparison shows which approach works best for your data. In this case, all models perform similarly, but Ridge and Elastic Net provide slightly better generalization.

### 10.3 Interpreting and Communicating Results

Once you've selected your best model, interpret the coefficients and communicate clearly:

```{python}
print("\n" + "="*60)
print("STEP 5: Interpret Final Model")
print("="*60)

# Use Ridge as our final model
final_model = Ridge(alpha=1.0)
final_model.fit(X_train_scaled, y_train_complete)

# Get coefficients
coef_df = pd.DataFrame({
    'Feature': features_complete,
    'Coefficient': final_model.coef_
}).sort_values('Coefficient', ascending=False)

print("\nFinal Model Coefficients (Ridge, α=1.0):")
print(coef_df.to_string(index=False))

print("\n" + "="*60)
print("Interpretation for Stakeholders:")
print("="*60)
print("""
Our regression model explains approximately 60% of the variance in median
household income across NYC census tracts. The model uses 6 key features:

Key findings:
1. **Income Per Capita** is the strongest predictor. Each $1,000 increase in
   per capita income corresponds to a significant increase in median household income.

2. **Professional Employment** has a positive effect. Census tracts with more
   professionals tend to have higher median household incomes.

3. **Poverty Rate** shows a strong negative relationship with income, as expected.

4. **Child Poverty** has an additional negative effect beyond general poverty,
   highlighting the economic challenges faced by families with children.

5. **Unemployment** shows a negative relationship with median household income,
   though this may be partially captured by the poverty variables.

6. **Total Population** has minimal effect on median household income at the
   census tract level.

The model performs consistently on both training and test data, suggesting
it generalizes well to new predictions.
""")
```

This is how you communicate results: translate coefficients into plain language, explain what matters, and acknowledge limitations.

::: {.callout-tip}
**Always present both statistical results (R², coefficients) and practical interpretation.** Stakeholders need to understand what the numbers mean for their decisions, not just that "the model has an R² of 0.60."
:::

The complete workflow: diagnose issues → try fixes systematically → select best model → interpret clearly. That's professional regression analysis.

---

## Summary

Linear regression is the foundation of machine learning, not because it's the most powerful model, but because understanding it deeply prepares you to understand everything else. This chapter covered the complete regression toolkit—from basic assumptions to advanced regularization techniques.

**Key Takeaways:**

1. **Linear regression makes four assumptions:** linearity, independence, homoscedasticity, and normality. Violate them, and your model might give terrible predictions even with high R².

2. **Residual plots are your diagnostic tool.** They reveal problems that metrics hide. A curved residual plot means non-linearity. A funnel means heteroscedasticity. Random scatter means you're good.

3. **Evaluation metrics serve different purposes.** MSE penalizes large errors heavily. MAE treats all errors equally. R² tells you variance explained. RMSE gives interpretable units. Use multiple metrics, not just one.

4. **Polynomial features capture non-linearity** while staying within linear regression. But high-degree polynomials overfit spectacularly. Always use validation data to choose the degree.

5. **Multicollinearity breaks coefficient interpretation** but doesn't hurt prediction accuracy. High VIF values warn you that coefficients are unstable. Regularization fixes this automatically.

6. **Ridge regression (L2) shrinks all coefficients** toward zero but never reaches exactly zero. It handles multicollinearity well and prevents overfitting. Always scale features first.

7. **Lasso regression (L1) does automatic feature selection** by setting some coefficients to exactly zero. Use it when you suspect many features are irrelevant.

8. **Elastic Net combines Ridge and Lasso.** It's more stable than Lasso with correlated features while still doing feature selection. When unsure, start with Elastic Net.

9. **The complete workflow matters:** Fit baseline → check diagnostics → identify problems → fix systematically → validate with cross-validation → interpret clearly. Skip steps, and you'll miss critical issues.

10. **Always communicate results in plain language.** Stakeholders don't care that "the coefficient for X1 is 0.437 with a p-value of 0.003." They care what that means for their decisions.

Regression modeling is both art and science. The science is in the diagnostics, metrics, and validation. The art is in knowing when assumptions matter, which fixes to try, and how to communicate findings. Master both, and you'll build models that actually work in the real world.

Use your brain. That's what it's there for.

---

## Practice Exercises

These exercises build progressively from understanding diagnostics to conducting complete regression analyses. Work through them to solidify your grasp of regression modeling.

### Exercise 1: Residual Analysis

**Task:** Load the NYC census dataset and fit a linear regression predicting median household income from just the `TotalPop` feature. Create a residuals vs. fitted values plot. Based on the pattern you observe:

a) Identify which regression assumption(s) are violated
b) Explain what the pattern tells you about the model's mistakes
c) Suggest two specific fixes that might improve the model
d) Implement one fix and show the improvement

**What you're learning:** How to read residual plots and diagnose problems.

### Exercise 2: Coefficient Interpretation

**Task:** Fit a multiple linear regression predicting median household income from `Professional`, `Poverty`, `Unemployment`, and `ChildPoverty`. For each coefficient:

a) Write the interpretation in plain English (e.g., "For every additional...")
b) Explain what "holding other features constant" means
c) Identify which coefficient is hardest to interpret and explain why
d) Calculate and report both R² and Adjusted R², then explain the difference

**What you're learning:** How to interpret and communicate regression results.

### Exercise 3: Polynomial Selection

**Task:** Using the `IncomePerCap` feature alone, fit polynomial regression models of degrees 1 through 8:

a) For each degree, calculate train MSE and test MSE
b) Create a plot showing both training and test MSE across all degrees
c) Identify the optimal polynomial degree and justify your choice
d) Visualize the polynomial fit for degrees 1, 3, and 8 on the same plot
e) Explain why degree 8 has lower training MSE but might perform worse in practice

**What you're learning:** The bias-variance tradeoff and how to prevent overfitting.

### Exercise 4: Multicollinearity Detection

**Task:** Using all numeric features in the NYC census dataset:

a) Compute and visualize the correlation matrix
b) Identify any feature pairs with correlation above 0.7
c) Calculate VIF for each feature
d) Identify features with VIF > 5 and explain what this means
e) Fit two models: one with all features, one removing high-VIF features. Compare coefficients and show which is more stable by refitting on a different random split.

**What you're learning:** How to detect and handle multicollinearity.

### Exercise 5: Ridge vs Lasso Comparison

**Task:** Using all features from the NYC census dataset, compare Ridge and Lasso regression:

a) For both Ridge and Lasso, find the optimal alpha using cross-validation
b) Plot the regularization paths for both methods (coefficients vs. alpha)
c) Create a table showing which features each method keeps (for optimal alpha)
d) Explain why Lasso eliminates some features while Ridge doesn't
e) Recommend which method you'd use for this dataset and why

**What you're learning:** The practical differences between L1 and L2 regularization.

### Exercise 6: Complete Regression Workflow

**Task:** Conduct a complete regression analysis on the NYC census dataset:

a) **Baseline:** Fit linear regression with all features. Report train/test R² and RMSE.
b) **Diagnostics:** Create residual plots (residuals vs fitted, Q-Q plot, scale-location). List any problems you identify.
c) **Multicollinearity:** Check VIF. Report any concerns.
d) **Model Improvement:** Try at least 3 different approaches (e.g., polynomial features, Ridge, Lasso, log transform). Use cross-validation to compare.
e) **Final Model:** Select your best model and justify the choice.
f) **Interpretation:** Write a 2-3 paragraph summary explaining your findings to a non-technical audience. Include the model's performance, key predictors, and limitations.

**What you're learning:** The complete end-to-end regression modeling workflow.

---

**Tips for Success:**
- Don't just run code—think about what each plot and metric is telling you
- When diagnostic plots show problems, try multiple fixes and compare results
- Always use train/validation/test splits or cross-validation properly
- Write your interpretations before looking at solutions
- Remember: a model that's slightly less accurate but easier to interpret is often more valuable in practice

Use your brain. That's what it's there for.

---

## Additional Resources

- [Regression Assumptions Explained](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/) - Detailed guide to assumptions
- [Interpreting Residual Plots](https://online.stat.psu.edu/stat462/node/117/) - Penn State course notes
- [Ridge vs Lasso](https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/) - Comprehensive comparison
- [Scikit-learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html) - Official documentation
- [Feature Scaling and Regularization](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/) - Why scaling matters
