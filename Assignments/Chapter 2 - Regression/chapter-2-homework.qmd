---
title: "Module 4 Homework: Regression Models"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
jupyter: python3
---

## Instructions

**Due Date:** [To be assigned by instructor]

This homework is divided into two parts. Part A should be completed **without AI assistance** to build your foundational understanding of regression models, diagnostics, and regularization. Part B should be completed **with AI assistance** (like Gemini CLI) to practice comprehensive model selection and diagnostic analysis at scale.

For all questions, submit:

1. Your code (in a `.py` file or Jupyter notebook)
2. All visualizations generated
3. Written answers to interpretation questions (can be in markdown or comments)
4. For Part B: Include the prompts you used with the AI assistant

### Datasets

You'll be working with three datasets:

**Dataset 1:** `real_estate.csv` - Real estate pricing data with columns:

- `price`: Sale price in dollars (TARGET)
- `sqft`: Square footage
- `bedrooms`: Number of bedrooms
- `bathrooms`: Number of bathrooms
- `age`: Age of property in years
- `distance_downtown`: Distance to downtown in miles
- `crime_rate`: Neighborhood crime rate
- `school_rating`: Local school rating (1-10)

**Dataset 2:** `advertising.csv` - Advertising budget and sales data with columns:

- `sales`: Product sales in thousands (TARGET)
- `tv`: TV advertising budget in thousands
- `radio`: Radio advertising budget in thousands
- `newspaper`: Newspaper advertising budget in thousands

**Dataset 3:** `employee_performance.csv` - Employee metrics with multicollinearity with columns:

- `performance_score`: Performance rating (TARGET)
- `years_experience`: Years of experience
- `training_hours`: Hours of training completed
- `projects_completed`: Number of projects completed
- `months_employed`: Months with company (highly correlated with years_experience)
- `certifications`: Number of certifications

---

## Part A: By Hand (No AI Assistance)

Complete questions 1-10 without using AI coding assistants. The goal is to build your foundational understanding of regression diagnostics and regularization.

### Question 1 (4 points)

Load the `advertising.csv` dataset and fit a Linear Regression model to predict `sales` using all three advertising channels.

a) Extract and display the coefficients for each feature

b) Interpret each coefficient in plain language (e.g., "For every additional thousand dollars spent on TV advertising, sales increase by...")

c) Which advertising channel has the strongest impact on sales?

d) Calculate R² for this model. What does this value tell you about the model's performance?

---

### Question 2 (5 points)

Continuing with the advertising dataset, manually calculate the following for your linear regression model:

a) Calculate the residuals (actual - predicted) for the first 5 data points

b) Compute the Mean Squared Error (MSE) manually from all residuals

c) Compute the Mean Absolute Error (MAE) manually from all residuals

d) Verify your calculations match sklearn's `mean_squared_error` and `mean_absolute_error`

e) Which metric (MSE or MAE) is more sensitive to outliers? Why?

---

### Question 3 (5 points)

Create diagnostic plots for the linear regression model from Question 1:

a) Create a scatter plot of residuals vs. fitted values (predictions)

b) Add a horizontal line at y=0

c) What pattern would you expect to see if the model assumptions are satisfied?

d) Do you observe any concerning patterns in your plot? What might they indicate?

e) Create a histogram of the residuals. Do they appear approximately normally distributed?

---

### Question 4 (5 points)

Load the `real_estate.csv` dataset. Calculate the correlation matrix for all numerical features.

a) Display the correlation matrix

b) Which pair of features has the highest correlation (excluding correlations with the target)?

c) What is multicollinearity and why is it problematic for linear regression?

d) Based on the correlation matrix, do you see evidence of multicollinearity in this dataset?

---

### Question 5 (5 points)

Using the `real_estate.csv` dataset, fit three models predicting `price`:

- **Model 1:** Linear Regression (no regularization)
- **Model 2:** Ridge Regression with alpha=1.0
- **Model 3:** Lasso Regression with alpha=1.0

For each model:

a) Fit the model using all features

b) Display the coefficients

c) Calculate R² on the test set (use 80/20 train/test split, random_state=42)

d) How do the coefficients differ between the three models? Which model has the smallest coefficient magnitudes?

---

### Question 6 (4 points)

Explain in your own words (3-4 sentences each):

a) What is the purpose of regularization in regression? When would you choose to use it?

b) What is the difference between Ridge (L2) and Lasso (L1) regularization?

c) Why might Lasso set some coefficients to exactly zero while Ridge does not?

d) In what situation would you prefer Lasso over Ridge?

---

### Question 7 (5 points)

Using the `advertising.csv` dataset, create polynomial features of degree 2 for the `tv` feature only.

a) Use `PolynomialFeatures(degree=2, include_bias=False)` to create polynomial features

b) Fit a Linear Regression model using the original features plus the polynomial features of TV

c) Compare R² with the original linear model (without polynomial features)

d) Did adding polynomial features improve performance? Why might this be the case?

e) Plot sales vs. TV budget, showing both the linear and polynomial model predictions

---

### Question 8 (4 points)

You fit a linear regression model and observe the following residual plot:

- For low fitted values: residuals are mostly positive
- For medium fitted values: residuals are mostly negative
- For high fitted values: residuals are mostly positive

a) What assumption of linear regression might be violated?

b) What does this pattern suggest about the relationship between features and target?

c) Suggest two approaches to address this issue.

---

### Question 9 (4 points)

Using the `employee_performance.csv` dataset, fit a Ridge regression model with alpha=10.0.

a) Split data into train/test (80/20, random_state=42)

b) Fit the model and calculate train R² and test R²

c) Now fit Ridge models with alpha values: [0.1, 1.0, 10.0, 100.0]

d) For each alpha, record the test R². Which alpha gives the best test performance?

e) What happens to the coefficients as alpha increases?

---

### Question 10 (4 points)

Write a function called `plot_residual_diagnostics` that:

- Takes a fitted model, X_test, and y_test as inputs
- Calculates residuals
- Creates two plots side by side: (1) residuals vs fitted values, (2) histogram of residuals
- Returns the residuals array

Test your function on a linear regression model fitted to the advertising dataset.

Based on the plots, do the residuals appear randomly distributed? Are they centered around zero?

---

## Part B: With AI Assistance

For questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you perform comprehensive regression analysis. The goal is to practice systematic model selection and diagnostic analysis.

**Important:** For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.

---

### Question 11 (5 points)

Use AI to perform a comprehensive grid search for Ridge regression on the `real_estate.csv` dataset.

Test alpha values: [0.001, 0.01, 0.1, 1, 10, 100, 1000]

Your code should:

- Perform grid search with cross-validation
- Plot test R² vs. alpha (use log scale for x-axis)
- Identify the optimal alpha value
- Show the coefficients for the best model

What happens to model performance as alpha increases? Is there a point where it gets worse?

**Deliverables:**

- Code
- Performance vs. alpha plot
- Best alpha and corresponding R²
- Written interpretation (3-4 sentences)
- Your AI prompt(s)

---

### Question 12 (6 points)

Prompt AI to create a comprehensive comparison of regularization techniques on the `employee_performance.csv` dataset (which has multicollinearity).

Compare:

- Linear Regression (no regularization)
- Ridge with optimal alpha
- Lasso with optimal alpha
- Elastic Net with optimal alpha

Your code should:

- Use GridSearchCV to find optimal alpha for each regularized method
- Create a bar plot comparing test R² across all methods
- Create a heatmap showing coefficient values for all methods
- Identify which features Lasso set to zero

Write a paragraph discussing how regularization helped with the multicollinearity problem.

**Deliverables:**

- Code
- Performance comparison plot
- Coefficient heatmap
- Written analysis (5-6 sentences)
- Your AI prompt(s)

---

### Question 13 (6 points)

Use AI to test polynomial regression of different degrees on the `advertising.csv` dataset using the `tv` feature.

Test polynomial degrees: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Your code should:

- Fit polynomial models for each degree
- Calculate train and test MSE for each
- Plot both MSE curves vs. polynomial degree
- Create a visualization showing the fitted curves for degrees [1, 3, 5, 8] overlaid on the actual data

At what degree does overfitting begin to occur? How can you tell?

**Deliverables:**

- Code
- MSE vs. degree plot
- Fitted curves visualization
- Written interpretation (4-5 sentences)
- Your AI prompt(s)

---

### Question 14 (6 points)

Prompt AI to generate a complete regression diagnostics report for a linear regression model on `real_estate.csv`.

The report should include:

- Residuals vs. Fitted values plot
- Residuals vs. each feature plot (to detect non-linearity with specific features)
- Histogram of residuals
- Scale-location plot (sqrt of standardized residuals vs. fitted)
- Summary statistics of residuals (mean, median, std, min, max)

Based on these diagnostics, are the linear regression assumptions satisfied? Which assumptions (if any) are violated?

**Deliverables:**

- Code for complete diagnostics
- All diagnostic plots
- Written assessment of assumptions (5-6 sentences)
- Your AI prompt(s)

---

### Question 15 (5 points)

Use AI to create a "coefficient path" visualization for Lasso regression.

Using the `real_estate.csv` dataset:

- Fit Lasso models with alpha values ranging from 0.001 to 1000 (log scale, 50 points)
- Track the coefficient values for each feature at each alpha
- Create a line plot showing how each coefficient changes with alpha
- Mark the point where each coefficient becomes zero

Which features are most important (last to be zeroed out)? Which are least important?

**Deliverables:**

- Code
- Coefficient path visualization
- Written interpretation (3-4 sentences)
- Your AI prompt(s)

---

### Question 16 (6 points)

Prompt AI to compare polynomial regression with and without regularization.

Using the `advertising.csv` dataset with TV feature:

- Fit polynomial models of degree 10 with:
  - No regularization (Linear Regression)
  - Ridge regularization (optimal alpha)
  - Lasso regularization (optimal alpha)
- Compare test MSE for all three
- Visualize the fitted curves for all three
- Show the coefficient values for all three

Does regularization help control overfitting in high-degree polynomials? How?

**Deliverables:**

- Code
- Performance comparison
- Fitted curves visualization
- Coefficient comparison
- Written analysis (4-5 sentences)
- Your AI prompt(s)

---

### Question 17 (5 points)

Use AI to investigate the impact of feature scaling on Ridge and Lasso regression.

Using the `real_estate.csv` dataset:

- Fit Ridge and Lasso models (alpha=1.0) with:
  - No scaling (raw features)
  - StandardScaler (z-score normalization)
  - MinMaxScaler (0-1 scaling)
- Compare test R² for all combinations
- Show how coefficients differ with different scaling approaches

Why does scaling matter for regularized regression but not for standard linear regression?

**Deliverables:**

- Code
- Performance comparison table
- Coefficient comparison
- Written explanation (3-4 sentences)
- Your AI prompt(s)

---

### Question 18 (6 points)

Prompt AI to create an automated feature selection pipeline using Lasso.

The pipeline should:

- Take a dataset and alpha value as input
- Fit Lasso regression
- Identify features with non-zero coefficients
- Refit a Linear Regression model using only selected features
- Compare performance with using all features
- Generate a report showing selected features and performance comparison

Test this on the `real_estate.csv` dataset with alpha=0.1.

How many features were selected? Did feature selection improve or hurt performance?

**Deliverables:**

- Pipeline code
- Feature selection results
- Performance comparison
- Written analysis (4-5 sentences)
- Your AI prompt(s)

---

### Question 19 (6 points)

Use AI to create a comprehensive model selection workflow that combines polynomial features with regularization.

Using the `advertising.csv` dataset:

- Test polynomial degrees [1, 2, 3, 4, 5]
- For each degree, test Ridge, Lasso, and Linear Regression
- Use cross-validation to evaluate each combination
- Create a heatmap showing performance (columns=method, rows=polynomial degree)
- Identify the best combination

Your code should:

- Automatically test all combinations
- Generate comprehensive comparison visualizations
- Select the best model based on cross-validation score
- Evaluate the best model on a held-out test set

**Deliverables:**

- Code for complete workflow
- Performance heatmap
- Best model identification
- Final test set results
- Written summary (5-6 sentences)
- Your AI prompt(s)

---

### Question 20 (7 points)

For this final question, ask AI to help you conduct a **complete regression analysis** from start to finish on the `real_estate.csv` dataset.

Your analysis should include:

1. **Exploratory Data Analysis:**
   - Summary statistics
   - Correlation analysis
   - Multicollinearity detection (VIF scores)

2. **Model Development:**
   - Train/validation/test split (60/20/20)
   - Fit multiple models: Linear, Ridge, Lasso, Polynomial (with regularization)
   - Hyperparameter tuning using validation set

3. **Model Diagnostics:**
   - Complete diagnostic plots for best model
   - Residual analysis
   - Assumption checking

4. **Model Selection:**
   - Compare all models on validation set
   - Select best model
   - Evaluate on test set (only once!)

5. **Interpretation:**
   - Feature importance
   - Coefficient interpretation
   - Business insights

Write a 1-page executive summary (300-400 words) that explains:

- Data characteristics and challenges (multicollinearity, etc.)
- Models tested and why
- How the best model was selected
- Performance on test set and what it means
- Key features driving predictions
- Recommendations and limitations

**Deliverables:**

- Complete analysis code (well-commented)
- All visualizations (EDA, diagnostics, comparisons)
- Executive summary
- Your AI prompt(s)
- Reflection: How did understanding diagnostics and regularization improve your modeling approach compared to just trying different models blindly?

---

## Submission Guidelines

Submit a ZIP file containing:

1. **Code files:** All `.py` files or Jupyter notebooks
2. **Visualizations folder:** All plots and charts generated
3. **Written responses:** A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used
4. **Data:** Include the datasets if you made any modifications

---

## Grading Rubric

**Part A: By Hand (45 points)**

- Code correctness and implementation: 25 points
- Proper understanding of diagnostics and regularization: 12 points
- Written interpretations and explanations: 8 points

**Part B: AI-Assisted (55 points)**

- Code functionality and correctness: 25 points
- Quality and specificity of AI prompts: 14 points
- Visualizations and diagnostic quality: 10 points
- Written interpretations and insights: 6 points

**Total: 100 points**

---

## Tips for Success

### For Part A:

- Draw residual plots carefully and look for patterns
- When computing metrics by hand, work with small subsets first to verify
- Regularization coefficients shrink toward zero—compare magnitudes across models
- Understanding assumptions is crucial—violating them can make predictions unreliable
- Polynomial features can capture non-linear relationships but risk overfitting

### For Part B:

- Request diagnostic plots that check all assumptions systematically
- Ask for log-scale plots when testing many alpha values
- Request coefficient path plots to understand feature selection
- Ask for side-by-side comparisons to see regularization effects clearly
- When testing polynomials, visualize the fitted curves to see overfitting

### General:

- Start early—comprehensive diagnostics take time to interpret
- Focus on understanding why regularization helps, not just applying it
- Use residual plots as your primary diagnostic tool
- Connect regularization back to bias-variance tradeoff from Module 3
- Use your brain. That's what it's there for.
