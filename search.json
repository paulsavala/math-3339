[
  {
    "objectID": "Claude.html",
    "href": "Claude.html",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "This course emphasizes conceptual understanding and practical application over implementation details. Students will develop the analytical thinking needed to select appropriate models, recognize when assumptions are violated, and critically evaluate model performance. While AI coding assistants are powerful tools, this course ensures students build the foundational knowledge to use them effectively and debug their output.\n\n\n\nWhen developing this course, Claude should follow the following guidelines:\n\nUtilize the existing folder Claude-planning by creating Markdown files to keep track of progress. Claude should keep one file per topic, and each file should have a clear title and a clear description of the topic. Each file should outline specifically what is being taught, which parts are done “by hand” (either programming or mathematically) and which parts are done with AI coding assistants. Claude should describe the in-class activities and homework which will enforce learning these topics.\nWhen designing a lesson and/or topic, Claude should refer back to the existing Claude-planning folder to ensure consistency and avoid redundancy. In addition, when possible, Claude should try to build on previous lessons and topics to create a cohesive and progressive learning experience. This should be transparent to the learner, with lessons refering back to previous lessons and topics whenever possible.\n\n\n\n\nWhen creating homework assignments or quizzes, Claude must follow the templates defined in the Templates/ folder:\n\n\nTemplate Location: Templates/homework-template.md\nKey Requirements:\n\nTwo-Part Structure:\n\nPart A: By Hand (40-45 points) - 10 questions without AI assistance to build foundational skills\nPart B: With AI Assistance (55-60 points) - 10 questions using AI coding assistants to scale analysis\nTotal: 100 points across 20 questions\n\nPart A Focus (By Hand):\n\nBasic implementation of techniques/models\nEvaluation and interpretation of results\nWriting helper functions to demonstrate understanding\nExplaining concepts in own words\nCreating simple visualizations\nQuestion range: 3-5 points each\n\nPart B Focus (AI-Assisted):\n\nHyperparameter tuning at scale\nComparing 5+ models/approaches simultaneously\nBuilding automation pipelines\nCross-validation and robustness testing\nFinal comprehensive analysis with executive summary\nQuestion range: 5-7 points each\nMUST require students to submit their AI prompts used\n\nRequired Sections:\n\nInstructions (including dataset descriptions)\nPart A questions (1-10)\nPart B questions (11-20)\nSubmission Guidelines\nGrading Rubric\nTips for Success (separate subsections for Part A, Part B, and General)\n\nRequired Deliverables for Part B:\n\nCode (AI-generated but student-verified)\nOutputs (visualizations, tables, results)\nWritten interpretation (student’s own analysis)\nThe prompt(s) used with AI assistant\n\nTips Section Must End With:\n\n“Use your brain. That’s what it’s there for.”\n\n\n\n\n\nTemplate Location: Templates/quiz-template.md\nKey Requirements:\n\nTwo-Section Structure:\n\nSection A: Conceptual Questions (~50% of points) - Testing understanding without a computer\nSection B: Code Writing (~50% of points) - Writing code by hand\nTotal: Typically 25-50 points\nTime: 30-45 minutes\n\nSection A Focus (Conceptual):\n\nScenario identification (classification vs regression, problem types)\nMetric interpretation and trade-off reasoning\nModel selection justification\nConceptual explanations\nProblem diagnosis\nQuestion range: 2-4 points each\n6-8 questions total\n\nSection B Focus (Code Writing):\n\nData manipulation (load, filter, split)\nModel instantiation with parameters\nFit/predict workflow\nMetric calculation\nSimple function writing\nQuestion range: 2-4 points each\n5-7 questions total\nMust include assumption section listing all imports\n\nRequired Elements:\n\nInstructions (time, format, note about partial credit)\nSection A questions\nAssumption section before Section B (listing imports and available variables)\nSection B questions\nGrading Rubric with statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\nCode Writing Philosophy:\n\nSyntax perfection NOT required\nFocus on logic and correct approach\nGrading: 70-80% for correct logic, 20-30% for syntax\nQuestions must be realistic to write by hand\n\n\n\n\n\nTemplate Location: Templates/textbook-chapter-template.md\nKey Requirements:\n\nStructure:\n\nFront matter (YAML with title, format settings)\nModule Resources section linking to homework and quiz\nIntroduction (3-5 engaging paragraphs)\n8-10 major sections covering core topics\nSummary (synthesizing key concepts)\nPractice Exercises (4-6 exercises)\nAdditional Resources\n\nCode Examples:\n\nInclude code every 2-3 paragraphs maximum\nUse executable Python code with proper imports\nBuild complexity gradually throughout the chapter\nExplain what code does and why it matters\nUse realistic datasets and examples\n\nImages and Visualizations:\n\nInclude 2-3 images/diagrams per chapter\nCreate images/ subfolder in module folder\nCreate placeholder-info.md describing needed images\nUse relative paths: ![Description](images/filename.png)\n\nWriting Style:\n\nConversational and casual tone\nUse probing questions rather than direct statements\nMaximum 2-3 paragraphs before showing code\nStart with intuition, then formalize\nConnect to what students already know\nEnd summary with: “Use your brain. That’s what it’s there for.”\n\nQuarto-Specific Requirements:\n\nUse {python} for executable code blocks (not standard markdown code blocks)\nInclude jupyter: python3 in YAML front matter\nSet code-fold: false to show all code by default\nUse callout boxes for tips, notes, and warnings:\n\n::: {.callout-tip} for helpful tips\n::: {.callout-note} for important information\n::: {.callout-warning} for cautions\n\n\n\n\n\n\nWhen asked to create homework, quiz, or textbook chapter for a module:\n\nRead the appropriate template from Templates/homework-template.md, Templates/quiz-template.md, or Templates/textbook-chapter-template.md\nReview the module-specific adaptations section in the template\nFollow the structure exactly - do not deviate from point distributions, section organization, or required elements\nRefer to content-overview.md to understand:\n\nCore topics for the module\nWhat should be done by-hand vs with AI\nLearning objectives\nAssessment focus\n\nCreate or update the Claude-planning file for the module first, then generate content\nEnsure content aligns with the “Topics Done By-Hand” and “Topics Done With AI” from content-overview.md\n\n\n\n\n\n\nLessons will be written as Quarto Markdown documents, with the programming language used being Python 3.12.\nStudents will use the Gemini CLI as the primary AI coding assistant. The primary model will be Flash 2.5, which means code generation skills are limited. Because of this, the text should encourage students to work on small components at a time, rather than prompting the model to build entire applications all at once.\nStudents will host their work on Github.\nPython packages used include:\n\nPandas\nNumPy\nSeaborn\nScikit-learn\nPyTorch\nStreamlit\nHuggingface\n\n\n\n\n\n\n\nThe professor writes in a casual, conversational manner that emphasizes understanding over formality. Key characteristics:\nTone and Voice:\n\nWrite as if talking directly to students\nUse probing questions to guide thinking rather than directly stating answers\nAvoid overly formal academic language\nBe direct and honest about challenges and complexities\nShow enthusiasm for the subject matter\n\nCommon Phrases and Patterns:\n\n“Let’s jump in” / “Let’s start with…”\n“Here’s the thing…” / “Here’s what’s happening…”\n“But wait—why does that work?”\n“Don’t worry about X right now, just…”\n“See what happened?” / “Notice what’s going on here?”\n“Use your brain. That’s what it’s there for.” (ending phrase)\n\nExample of the Professor’s Writing Style:\n\"Linear regression is perhaps the most important idea in machine learning. Once you understand the ins-and-outs of linear regression you can understand most other machine learning models as well. This is because the ideas developed for machine learning were first perfected on linear regression and then applied to other models. Let's jump in.\n\nLinear regression (LR) is what you have probably referred to as \"line of best fit\". It is a line meant to fit the data \"as well as possible\". I put that last phrase in quotes, because what exactly do we mean by a \"best fit\"? We will formulate this mathematically in the next section.\n\nWith that out of the way, we now turn to our next question: why do we care about linear regression? Linear regression is extremely important because it allows us to make predictions. Up until this point we have only explored and described the past by looking at datasets which (necessarily) had data about the past. However, the point of data science is largely to make predictions about the future using data from the past. This works because a line doesn't care what data we plug in. We can plug in data from the past in order to verify and explain past performance. But we can also plug in future numbers (dates, pricing changes, expected changes to our products, etc.) and see what the model returns.\n\nLet's start with a simple example. Don't worry about the code right now, just look at the graphs. We will work again with our data/boston.csv dataset, describing the median home value in various neighborhoods in Boston.\n\nIn blue is the actual measurements of poverty and home value for all neighborhoods. In red is the predicted value using the line of best fit. We can see that the regression line seems to fit the data fairly well, at least in all except the far left and right ends.\n\nOne interesting thing to note is that the regression line generally seems too high. For example, if we draw the same graph, but only keep poverty values between 5% and 20% we get the following:\n\nWhy is that? The reason is that the regression line is heavily affected by outliers. So the neighborhoods with low crime and high home value are throwing off the line and \"dragging it up.\" In general, you want the following four things to be true before using LR for making predictions:\n\n1. The data should be approximately linear\n\n2. The observations should be independent (so the crime rate and median home value in one neighborhood should be independent of other neighborhoods)\n\n3. The variance between the measurements should be approximately the same throughout the graph (the graph is more or less spread out the same amount in different areas)\n\n4. The points should be approximately normally distributed around the regression line.\n\nNone of these four are perfectly satisfied. However, that doesn't mean you can't use LR. It just means that you need to be careful when making predictions. Don't just make a regression line and say \"see, this predicts the future!\" Use your brain, that's what it's there for.\"\n\n\n\nConceptual Explanations:\n\nStart with intuition and concrete examples before formalizing\nUse analogies and real-world scenarios to ground abstract concepts\nAsk guiding questions that lead students to understanding\nBuild complexity gradually—simple cases first, then complications\nConnect new concepts to what students already know\nExplain the “why” before diving into the “how”\n\nCode Demonstrations:\n\nInclude code examples every 2-3 paragraphs maximum\nNever show code without explaining what it does and why it matters\nBuild code examples incrementally (don’t dump large blocks)\nUse realistic datasets that students can relate to\nComment code to explain reasoning, not just mechanics\nShow output or visualizations when relevant\nFollow this pattern:\n\nBrief intro to what you’re demonstrating\nShow the code\nExplain what happened and why\nConnect to the bigger picture\n\n\nCode Formatting for Quarto:\n\nUse {python} for executable code blocks: ```{python}\nUse regular markdown code blocks only for non-executable examples\nAdd #| error: true to code blocks that intentionally raise errors\nUse #| echo: true (default) to show code\nUse #| output: true to display output\n\nExamples and Datasets:\n\nUse consistent, realistic datasets throughout a chapter\nCalifornia housing data (from Module 1) is the primary dataset\nInclude real-world context for all examples\nMake examples build on each other when possible\nShow both successes and failures (what not to do)\n\nVisual Elements:\n\nInclude 2-3 images/diagrams per chapter minimum\nCreate images/ subfolder with placeholder-info.md\nUse callout boxes for important points:\n\n::: {.callout-tip} for helpful tips and best practices\n::: {.callout-note} for important information students must remember\n::: {.callout-warning} for common mistakes and cautions\n\nUse concrete visualizations to explain abstract concepts\n\nSection Flow:\n\nEach major section should have a clear narrative arc\nStart sections with context: why does this matter?\nEnd sections with synthesis: what did we learn?\nUse transitions to connect sections logically\nReference earlier content when building on it\nPreview upcoming content when it’s relevant\n\nPractical Examples:\n\nAlways ground theory in practice\nShow real scenarios where the concept matters\nInclude “what could go wrong” examples\nDemonstrate debugging and problem-solving approaches\nConnect to real data science workflows\n\nEnding Every Chapter:\n\nSummary must synthesize, not just list topics\nEmphasize key takeaways and connections\nAlways end with: “Use your brain. That’s what it’s there for.”\n\nWriting Theoretical Concepts:\nWhen explaining theoretical or conceptual material (like ML theory, loss functions, optimization, etc.):\n\nStart with the practical problem before introducing theory\n\nExample: “Models fail. They overfit. They underfit. Understanding what’s going on ‘under the hood’ helps you recognize these problems…”\nLead with why this matters in practice, then explain the theory\n\nUse concrete, visual examples first\n\nShow actual code and visualizations before equations\nLet students see the concept in action before formalizing it\nExample: Demonstrate gradient descent with a simple quadratic function and visualization before explaining the algorithm\n\nExplain concepts in plain English\n\n“Cross-entropy heavily penalizes confident wrong predictions”\n“The gradient is the slope of the loss with respect to parameters”\nAvoid jargon or explain it immediately when used\n\nUse analogies and metaphors\n\n“Think of it as navigating a landscape where height represents loss, and you’re trying to find the lowest valley”\n“Bias: fitting a straight line to clearly curved data”\n“Variance: fitting a wiggly line that goes through every single training point”\n\nBreak down mathematical concepts step-by-step\n\nIntroduce notation gradually\nExplain what each symbol means in context\nShow the formula, then explain it in words\nExample: Show cross-entropy formula, then explain “this heavily penalizes confident wrong predictions”\n\nUse callout boxes strategically\n\n::: {.callout-note} for connecting concepts (“Cross-entropy is just the log-likelihood for binary classification”)\n::: {.callout-warning} for common misconceptions or pitfalls\n::: {.callout-tip} for practical advice about when/how to use concepts\n\nConnect theory back to practice repeatedly\n\n“In practice, you rarely need to implement loss functions yourself—scikit-learn handles it”\n“Understanding what they’re doing helps you choose the right one and diagnose problems”\nAlways answer “why does this matter for my work?”\n\nUse contrasts and comparisons\n\n“MSE: Penalizes large errors heavily (good when outliers are costly)”\n“MAE: Treats all errors equally (good when outliers shouldn’t dominate)”\nSide-by-side visualizations showing different approaches\n\nEmphasize intuition over rigor\n\nGet the concept across first, worry about edge cases later\n“The idea is beautifully simple: calculate the gradient, take a step downhill, repeat”\nDon’t get bogged down in mathematical proofs\n\nUse progressive disclosure\n\nStart with 1D examples before moving to higher dimensions\n“In one dimension, the gradient is simply the derivative. In higher dimensions…”\nBuild from simple → complex gradually\n\nFrame concepts as tools, not just theory\n\n“Loss functions define what ‘good’ means”\n“The bias-variance tradeoff is your guide for choosing model complexity”\nPresent concepts as practical tools students will use\n\n\n\n\n\n\n\nWhen writing code examples in textbook chapters and assignments, follow these package-specific best practices to ensure code is clear, correct, and pedagogically sound.\n\n\nDataFrame Display:\n\nNever use print() for DataFrames - Instead, put the DataFrame (or df.head()) as the last line of the cell to use Jupyter’s rich display\nAlways use .head() to show DataFrames - This prevents overwhelming output and teaches good practices\n\n# BAD: Using print()\nprint(housing_df)\n\n# GOOD: Let Jupyter display it\nhousing_df.head()\nDataFrame Creation:\n\nNever subset data when creating DataFrames - Create the full DataFrame first, then display a subset\nThis prevents confusing students about what’s actually in the DataFrame\n\n# BAD: Subsetting during creation\ncomparison = pd.DataFrame({\n    'Actual': y_test.head(10),\n    'Predicted': predictions[:10]\n})\ncomparison\n\n# GOOD: Full DataFrame, then show subset\ncomparison = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': predictions\n})\ncomparison.head()\nColumn Selection:\n\nAlways demonstrate both single-column (Series) and multi-column (DataFrame) selection\nEmphasize that double brackets [[]] for multiple columns are just a list inside brackets\n\n# Single column (returns Series)\nhouse_values = housing_df['median_house_value']\n\n# Multiple columns (returns DataFrame)\nsubset = housing_df[['median_house_value', 'ocean_proximity']]\nChaining Operations:\n\nBreak complex chains across multiple lines for readability\nAdd comments explaining each step\n\n# GOOD: Clear, multi-line chaining\nresult = (housing_df\n          .dropna()  # Remove missing values\n          .groupby('ocean_proximity')  # Group by location\n          .agg({'median_house_value': 'mean'}))  # Calculate means\n\n\n\nTrain-Test Split:\n\nAlways set random_state for reproducibility\nAlways split data BEFORE any other operations\nUse meaningful variable names: X_train, X_test, y_train, y_test\n\n# GOOD: Always set random_state\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42\n)\nModel Workflow:\n\nAlways follow the same pattern: instantiate → fit → predict → evaluate\nShow the complete workflow in early examples\nUse consistent variable names: model, predictions, y_pred\n\n# The standard pattern\nmodel = LinearRegression()  # 1. Instantiate\nmodel.fit(X_train, y_train)  # 2. Fit\ny_pred = model.predict(X_test)  # 3. Predict\nscore = model.score(X_test, y_test)  # 4. Evaluate\nFeature Scaling:\n\nAlways fit scaler on training data only, then transform both train and test\nNever fit on the full dataset or test set\n\n# GOOD: Fit on train, transform both\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Only transform, don't fit\n\n# BAD: Fitting on test data\n# X_test_scaled = scaler.fit_transform(X_test)  # DON'T DO THIS\nModel Comparison:\n\nWhen comparing models, use consistent train/val/test splits\nStore results in a DataFrame for easy comparison\n\n# GOOD: Systematic comparison\nresults = []\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    results.append({'Model': name, 'Score': score})\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\nFigure Display:\n\nAlways use plt.show() to display figures in Quarto documents\nSet figure size explicitly with plt.figure(figsize=(width, height))\n\n# GOOD: Explicit figure size and show\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\nLabels and Titles:\n\nAlways include title, xlabel, and ylabel\nUse descriptive labels with units\nMake figures self-explanatory\n\nColor Usage:\n\nUse colorblind-friendly palettes: 'Set2', 'colorblind', 'husl'\nNever rely solely on red-green distinctions\n\n\n\n\nRandom State:\n\nAlways set np.random.seed() for reproducibility in examples\n\n# GOOD: Set seed for reproducibility\nnp.random.seed(42)\nrandom_values = np.random.randn(100)\nArray Creation:\n\nUse np.array() for small examples, but prefer Pandas for tabular data\nShow array shape and dtype when relevant\n\n\n\n\nVariable Naming:\n\nUse descriptive variable names: housing_df, X_train, y_pred\nFollow conventions: X (features), y (target), df (DataFrame)\nAvoid single letters except in mathematical contexts or brief examples\n\nComments:\n\nComment the “why”, not the “what”\nUse comments to explain reasoning and decisions\nAdd comments for code that students will modify or extend\n\n# GOOD: Explains reasoning\n# Use median instead of mean to avoid influence of outliers\nmedian_bedrooms = housing_df['total_bedrooms'].median()\n\n# BAD: States the obvious\n# Calculate median\nmedian_bedrooms = housing_df['total_bedrooms'].median()\nError Handling:\n\nUse #| error: true in Quarto code blocks that intentionally raise errors\nShow errors when teaching debugging or demonstrating common mistakes\n\n#| error: true\n# This will raise an error to demonstrate the problem\nassert 2 &gt; 3, \"This assertion will fail\"\n\n\n\nProgressive Complexity:\n\nStart with simplest working example\nAdd complexity gradually\nShow complete workflow first, then optimize\n\nReusable Patterns:\n\nEstablish patterns early and reuse them\nHighlight when you’re using a pattern seen before\nMake patterns explicit: “This is the same workflow we used for…”\n\nAvoid:\n\nMagic numbers without explanation\nUnexplained imports (always show imports)\nCopy-pasted code without variation\nExamples that don’t run or produce errors unintentionally\nDeprecated functions or outdated syntax"
  },
  {
    "objectID": "Claude.html#course-philosophy",
    "href": "Claude.html#course-philosophy",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "This course emphasizes conceptual understanding and practical application over implementation details. Students will develop the analytical thinking needed to select appropriate models, recognize when assumptions are violated, and critically evaluate model performance. While AI coding assistants are powerful tools, this course ensures students build the foundational knowledge to use them effectively and debug their output."
  },
  {
    "objectID": "Claude.html#agentic-planning",
    "href": "Claude.html#agentic-planning",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "When developing this course, Claude should follow the following guidelines:\n\nUtilize the existing folder Claude-planning by creating Markdown files to keep track of progress. Claude should keep one file per topic, and each file should have a clear title and a clear description of the topic. Each file should outline specifically what is being taught, which parts are done “by hand” (either programming or mathematically) and which parts are done with AI coding assistants. Claude should describe the in-class activities and homework which will enforce learning these topics.\nWhen designing a lesson and/or topic, Claude should refer back to the existing Claude-planning folder to ensure consistency and avoid redundancy. In addition, when possible, Claude should try to build on previous lessons and topics to create a cohesive and progressive learning experience. This should be transparent to the learner, with lessons refering back to previous lessons and topics whenever possible."
  },
  {
    "objectID": "Claude.html#assignment-templates",
    "href": "Claude.html#assignment-templates",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "When creating homework assignments or quizzes, Claude must follow the templates defined in the Templates/ folder:\n\n\nTemplate Location: Templates/homework-template.md\nKey Requirements:\n\nTwo-Part Structure:\n\nPart A: By Hand (40-45 points) - 10 questions without AI assistance to build foundational skills\nPart B: With AI Assistance (55-60 points) - 10 questions using AI coding assistants to scale analysis\nTotal: 100 points across 20 questions\n\nPart A Focus (By Hand):\n\nBasic implementation of techniques/models\nEvaluation and interpretation of results\nWriting helper functions to demonstrate understanding\nExplaining concepts in own words\nCreating simple visualizations\nQuestion range: 3-5 points each\n\nPart B Focus (AI-Assisted):\n\nHyperparameter tuning at scale\nComparing 5+ models/approaches simultaneously\nBuilding automation pipelines\nCross-validation and robustness testing\nFinal comprehensive analysis with executive summary\nQuestion range: 5-7 points each\nMUST require students to submit their AI prompts used\n\nRequired Sections:\n\nInstructions (including dataset descriptions)\nPart A questions (1-10)\nPart B questions (11-20)\nSubmission Guidelines\nGrading Rubric\nTips for Success (separate subsections for Part A, Part B, and General)\n\nRequired Deliverables for Part B:\n\nCode (AI-generated but student-verified)\nOutputs (visualizations, tables, results)\nWritten interpretation (student’s own analysis)\nThe prompt(s) used with AI assistant\n\nTips Section Must End With:\n\n“Use your brain. That’s what it’s there for.”\n\n\n\n\n\nTemplate Location: Templates/quiz-template.md\nKey Requirements:\n\nTwo-Section Structure:\n\nSection A: Conceptual Questions (~50% of points) - Testing understanding without a computer\nSection B: Code Writing (~50% of points) - Writing code by hand\nTotal: Typically 25-50 points\nTime: 30-45 minutes\n\nSection A Focus (Conceptual):\n\nScenario identification (classification vs regression, problem types)\nMetric interpretation and trade-off reasoning\nModel selection justification\nConceptual explanations\nProblem diagnosis\nQuestion range: 2-4 points each\n6-8 questions total\n\nSection B Focus (Code Writing):\n\nData manipulation (load, filter, split)\nModel instantiation with parameters\nFit/predict workflow\nMetric calculation\nSimple function writing\nQuestion range: 2-4 points each\n5-7 questions total\nMust include assumption section listing all imports\n\nRequired Elements:\n\nInstructions (time, format, note about partial credit)\nSection A questions\nAssumption section before Section B (listing imports and available variables)\nSection B questions\nGrading Rubric with statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\nCode Writing Philosophy:\n\nSyntax perfection NOT required\nFocus on logic and correct approach\nGrading: 70-80% for correct logic, 20-30% for syntax\nQuestions must be realistic to write by hand\n\n\n\n\n\nTemplate Location: Templates/textbook-chapter-template.md\nKey Requirements:\n\nStructure:\n\nFront matter (YAML with title, format settings)\nModule Resources section linking to homework and quiz\nIntroduction (3-5 engaging paragraphs)\n8-10 major sections covering core topics\nSummary (synthesizing key concepts)\nPractice Exercises (4-6 exercises)\nAdditional Resources\n\nCode Examples:\n\nInclude code every 2-3 paragraphs maximum\nUse executable Python code with proper imports\nBuild complexity gradually throughout the chapter\nExplain what code does and why it matters\nUse realistic datasets and examples\n\nImages and Visualizations:\n\nInclude 2-3 images/diagrams per chapter\nCreate images/ subfolder in module folder\nCreate placeholder-info.md describing needed images\nUse relative paths: ![Description](images/filename.png)\n\nWriting Style:\n\nConversational and casual tone\nUse probing questions rather than direct statements\nMaximum 2-3 paragraphs before showing code\nStart with intuition, then formalize\nConnect to what students already know\nEnd summary with: “Use your brain. That’s what it’s there for.”\n\nQuarto-Specific Requirements:\n\nUse {python} for executable code blocks (not standard markdown code blocks)\nInclude jupyter: python3 in YAML front matter\nSet code-fold: false to show all code by default\nUse callout boxes for tips, notes, and warnings:\n\n::: {.callout-tip} for helpful tips\n::: {.callout-note} for important information\n::: {.callout-warning} for cautions\n\n\n\n\n\n\nWhen asked to create homework, quiz, or textbook chapter for a module:\n\nRead the appropriate template from Templates/homework-template.md, Templates/quiz-template.md, or Templates/textbook-chapter-template.md\nReview the module-specific adaptations section in the template\nFollow the structure exactly - do not deviate from point distributions, section organization, or required elements\nRefer to content-overview.md to understand:\n\nCore topics for the module\nWhat should be done by-hand vs with AI\nLearning objectives\nAssessment focus\n\nCreate or update the Claude-planning file for the module first, then generate content\nEnsure content aligns with the “Topics Done By-Hand” and “Topics Done With AI” from content-overview.md"
  },
  {
    "objectID": "Claude.html#tech-stack",
    "href": "Claude.html#tech-stack",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "Lessons will be written as Quarto Markdown documents, with the programming language used being Python 3.12.\nStudents will use the Gemini CLI as the primary AI coding assistant. The primary model will be Flash 2.5, which means code generation skills are limited. Because of this, the text should encourage students to work on small components at a time, rather than prompting the model to build entire applications all at once.\nStudents will host their work on Github.\nPython packages used include:\n\nPandas\nNumPy\nSeaborn\nScikit-learn\nPyTorch\nStreamlit\nHuggingface"
  },
  {
    "objectID": "Claude.html#style-guidelines",
    "href": "Claude.html#style-guidelines",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "The professor writes in a casual, conversational manner that emphasizes understanding over formality. Key characteristics:\nTone and Voice:\n\nWrite as if talking directly to students\nUse probing questions to guide thinking rather than directly stating answers\nAvoid overly formal academic language\nBe direct and honest about challenges and complexities\nShow enthusiasm for the subject matter\n\nCommon Phrases and Patterns:\n\n“Let’s jump in” / “Let’s start with…”\n“Here’s the thing…” / “Here’s what’s happening…”\n“But wait—why does that work?”\n“Don’t worry about X right now, just…”\n“See what happened?” / “Notice what’s going on here?”\n“Use your brain. That’s what it’s there for.” (ending phrase)\n\nExample of the Professor’s Writing Style:\n\"Linear regression is perhaps the most important idea in machine learning. Once you understand the ins-and-outs of linear regression you can understand most other machine learning models as well. This is because the ideas developed for machine learning were first perfected on linear regression and then applied to other models. Let's jump in.\n\nLinear regression (LR) is what you have probably referred to as \"line of best fit\". It is a line meant to fit the data \"as well as possible\". I put that last phrase in quotes, because what exactly do we mean by a \"best fit\"? We will formulate this mathematically in the next section.\n\nWith that out of the way, we now turn to our next question: why do we care about linear regression? Linear regression is extremely important because it allows us to make predictions. Up until this point we have only explored and described the past by looking at datasets which (necessarily) had data about the past. However, the point of data science is largely to make predictions about the future using data from the past. This works because a line doesn't care what data we plug in. We can plug in data from the past in order to verify and explain past performance. But we can also plug in future numbers (dates, pricing changes, expected changes to our products, etc.) and see what the model returns.\n\nLet's start with a simple example. Don't worry about the code right now, just look at the graphs. We will work again with our data/boston.csv dataset, describing the median home value in various neighborhoods in Boston.\n\nIn blue is the actual measurements of poverty and home value for all neighborhoods. In red is the predicted value using the line of best fit. We can see that the regression line seems to fit the data fairly well, at least in all except the far left and right ends.\n\nOne interesting thing to note is that the regression line generally seems too high. For example, if we draw the same graph, but only keep poverty values between 5% and 20% we get the following:\n\nWhy is that? The reason is that the regression line is heavily affected by outliers. So the neighborhoods with low crime and high home value are throwing off the line and \"dragging it up.\" In general, you want the following four things to be true before using LR for making predictions:\n\n1. The data should be approximately linear\n\n2. The observations should be independent (so the crime rate and median home value in one neighborhood should be independent of other neighborhoods)\n\n3. The variance between the measurements should be approximately the same throughout the graph (the graph is more or less spread out the same amount in different areas)\n\n4. The points should be approximately normally distributed around the regression line.\n\nNone of these four are perfectly satisfied. However, that doesn't mean you can't use LR. It just means that you need to be careful when making predictions. Don't just make a regression line and say \"see, this predicts the future!\" Use your brain, that's what it's there for.\"\n\n\n\nConceptual Explanations:\n\nStart with intuition and concrete examples before formalizing\nUse analogies and real-world scenarios to ground abstract concepts\nAsk guiding questions that lead students to understanding\nBuild complexity gradually—simple cases first, then complications\nConnect new concepts to what students already know\nExplain the “why” before diving into the “how”\n\nCode Demonstrations:\n\nInclude code examples every 2-3 paragraphs maximum\nNever show code without explaining what it does and why it matters\nBuild code examples incrementally (don’t dump large blocks)\nUse realistic datasets that students can relate to\nComment code to explain reasoning, not just mechanics\nShow output or visualizations when relevant\nFollow this pattern:\n\nBrief intro to what you’re demonstrating\nShow the code\nExplain what happened and why\nConnect to the bigger picture\n\n\nCode Formatting for Quarto:\n\nUse {python} for executable code blocks: ```{python}\nUse regular markdown code blocks only for non-executable examples\nAdd #| error: true to code blocks that intentionally raise errors\nUse #| echo: true (default) to show code\nUse #| output: true to display output\n\nExamples and Datasets:\n\nUse consistent, realistic datasets throughout a chapter\nCalifornia housing data (from Module 1) is the primary dataset\nInclude real-world context for all examples\nMake examples build on each other when possible\nShow both successes and failures (what not to do)\n\nVisual Elements:\n\nInclude 2-3 images/diagrams per chapter minimum\nCreate images/ subfolder with placeholder-info.md\nUse callout boxes for important points:\n\n::: {.callout-tip} for helpful tips and best practices\n::: {.callout-note} for important information students must remember\n::: {.callout-warning} for common mistakes and cautions\n\nUse concrete visualizations to explain abstract concepts\n\nSection Flow:\n\nEach major section should have a clear narrative arc\nStart sections with context: why does this matter?\nEnd sections with synthesis: what did we learn?\nUse transitions to connect sections logically\nReference earlier content when building on it\nPreview upcoming content when it’s relevant\n\nPractical Examples:\n\nAlways ground theory in practice\nShow real scenarios where the concept matters\nInclude “what could go wrong” examples\nDemonstrate debugging and problem-solving approaches\nConnect to real data science workflows\n\nEnding Every Chapter:\n\nSummary must synthesize, not just list topics\nEmphasize key takeaways and connections\nAlways end with: “Use your brain. That’s what it’s there for.”\n\nWriting Theoretical Concepts:\nWhen explaining theoretical or conceptual material (like ML theory, loss functions, optimization, etc.):\n\nStart with the practical problem before introducing theory\n\nExample: “Models fail. They overfit. They underfit. Understanding what’s going on ‘under the hood’ helps you recognize these problems…”\nLead with why this matters in practice, then explain the theory\n\nUse concrete, visual examples first\n\nShow actual code and visualizations before equations\nLet students see the concept in action before formalizing it\nExample: Demonstrate gradient descent with a simple quadratic function and visualization before explaining the algorithm\n\nExplain concepts in plain English\n\n“Cross-entropy heavily penalizes confident wrong predictions”\n“The gradient is the slope of the loss with respect to parameters”\nAvoid jargon or explain it immediately when used\n\nUse analogies and metaphors\n\n“Think of it as navigating a landscape where height represents loss, and you’re trying to find the lowest valley”\n“Bias: fitting a straight line to clearly curved data”\n“Variance: fitting a wiggly line that goes through every single training point”\n\nBreak down mathematical concepts step-by-step\n\nIntroduce notation gradually\nExplain what each symbol means in context\nShow the formula, then explain it in words\nExample: Show cross-entropy formula, then explain “this heavily penalizes confident wrong predictions”\n\nUse callout boxes strategically\n\n::: {.callout-note} for connecting concepts (“Cross-entropy is just the log-likelihood for binary classification”)\n::: {.callout-warning} for common misconceptions or pitfalls\n::: {.callout-tip} for practical advice about when/how to use concepts\n\nConnect theory back to practice repeatedly\n\n“In practice, you rarely need to implement loss functions yourself—scikit-learn handles it”\n“Understanding what they’re doing helps you choose the right one and diagnose problems”\nAlways answer “why does this matter for my work?”\n\nUse contrasts and comparisons\n\n“MSE: Penalizes large errors heavily (good when outliers are costly)”\n“MAE: Treats all errors equally (good when outliers shouldn’t dominate)”\nSide-by-side visualizations showing different approaches\n\nEmphasize intuition over rigor\n\nGet the concept across first, worry about edge cases later\n“The idea is beautifully simple: calculate the gradient, take a step downhill, repeat”\nDon’t get bogged down in mathematical proofs\n\nUse progressive disclosure\n\nStart with 1D examples before moving to higher dimensions\n“In one dimension, the gradient is simply the derivative. In higher dimensions…”\nBuild from simple → complex gradually\n\nFrame concepts as tools, not just theory\n\n“Loss functions define what ‘good’ means”\n“The bias-variance tradeoff is your guide for choosing model complexity”\nPresent concepts as practical tools students will use"
  },
  {
    "objectID": "Claude.html#technical-guidelines",
    "href": "Claude.html#technical-guidelines",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "When writing code examples in textbook chapters and assignments, follow these package-specific best practices to ensure code is clear, correct, and pedagogically sound.\n\n\nDataFrame Display:\n\nNever use print() for DataFrames - Instead, put the DataFrame (or df.head()) as the last line of the cell to use Jupyter’s rich display\nAlways use .head() to show DataFrames - This prevents overwhelming output and teaches good practices\n\n# BAD: Using print()\nprint(housing_df)\n\n# GOOD: Let Jupyter display it\nhousing_df.head()\nDataFrame Creation:\n\nNever subset data when creating DataFrames - Create the full DataFrame first, then display a subset\nThis prevents confusing students about what’s actually in the DataFrame\n\n# BAD: Subsetting during creation\ncomparison = pd.DataFrame({\n    'Actual': y_test.head(10),\n    'Predicted': predictions[:10]\n})\ncomparison\n\n# GOOD: Full DataFrame, then show subset\ncomparison = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': predictions\n})\ncomparison.head()\nColumn Selection:\n\nAlways demonstrate both single-column (Series) and multi-column (DataFrame) selection\nEmphasize that double brackets [[]] for multiple columns are just a list inside brackets\n\n# Single column (returns Series)\nhouse_values = housing_df['median_house_value']\n\n# Multiple columns (returns DataFrame)\nsubset = housing_df[['median_house_value', 'ocean_proximity']]\nChaining Operations:\n\nBreak complex chains across multiple lines for readability\nAdd comments explaining each step\n\n# GOOD: Clear, multi-line chaining\nresult = (housing_df\n          .dropna()  # Remove missing values\n          .groupby('ocean_proximity')  # Group by location\n          .agg({'median_house_value': 'mean'}))  # Calculate means\n\n\n\nTrain-Test Split:\n\nAlways set random_state for reproducibility\nAlways split data BEFORE any other operations\nUse meaningful variable names: X_train, X_test, y_train, y_test\n\n# GOOD: Always set random_state\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42\n)\nModel Workflow:\n\nAlways follow the same pattern: instantiate → fit → predict → evaluate\nShow the complete workflow in early examples\nUse consistent variable names: model, predictions, y_pred\n\n# The standard pattern\nmodel = LinearRegression()  # 1. Instantiate\nmodel.fit(X_train, y_train)  # 2. Fit\ny_pred = model.predict(X_test)  # 3. Predict\nscore = model.score(X_test, y_test)  # 4. Evaluate\nFeature Scaling:\n\nAlways fit scaler on training data only, then transform both train and test\nNever fit on the full dataset or test set\n\n# GOOD: Fit on train, transform both\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Only transform, don't fit\n\n# BAD: Fitting on test data\n# X_test_scaled = scaler.fit_transform(X_test)  # DON'T DO THIS\nModel Comparison:\n\nWhen comparing models, use consistent train/val/test splits\nStore results in a DataFrame for easy comparison\n\n# GOOD: Systematic comparison\nresults = []\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    results.append({'Model': name, 'Score': score})\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\nFigure Display:\n\nAlways use plt.show() to display figures in Quarto documents\nSet figure size explicitly with plt.figure(figsize=(width, height))\n\n# GOOD: Explicit figure size and show\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\nLabels and Titles:\n\nAlways include title, xlabel, and ylabel\nUse descriptive labels with units\nMake figures self-explanatory\n\nColor Usage:\n\nUse colorblind-friendly palettes: 'Set2', 'colorblind', 'husl'\nNever rely solely on red-green distinctions\n\n\n\n\nRandom State:\n\nAlways set np.random.seed() for reproducibility in examples\n\n# GOOD: Set seed for reproducibility\nnp.random.seed(42)\nrandom_values = np.random.randn(100)\nArray Creation:\n\nUse np.array() for small examples, but prefer Pandas for tabular data\nShow array shape and dtype when relevant\n\n\n\n\nVariable Naming:\n\nUse descriptive variable names: housing_df, X_train, y_pred\nFollow conventions: X (features), y (target), df (DataFrame)\nAvoid single letters except in mathematical contexts or brief examples\n\nComments:\n\nComment the “why”, not the “what”\nUse comments to explain reasoning and decisions\nAdd comments for code that students will modify or extend\n\n# GOOD: Explains reasoning\n# Use median instead of mean to avoid influence of outliers\nmedian_bedrooms = housing_df['total_bedrooms'].median()\n\n# BAD: States the obvious\n# Calculate median\nmedian_bedrooms = housing_df['total_bedrooms'].median()\nError Handling:\n\nUse #| error: true in Quarto code blocks that intentionally raise errors\nShow errors when teaching debugging or demonstrating common mistakes\n\n#| error: true\n# This will raise an error to demonstrate the problem\nassert 2 &gt; 3, \"This assertion will fail\"\n\n\n\nProgressive Complexity:\n\nStart with simplest working example\nAdd complexity gradually\nShow complete workflow first, then optimize\n\nReusable Patterns:\n\nEstablish patterns early and reuse them\nHighlight when you’re using a pattern seen before\nMake patterns explicit: “This is the same workflow we used for…”\n\nAvoid:\n\nMagic numbers without explanation\nUnexplained imports (always show imports)\nCopy-pasted code without variation\nExamples that don’t run or produce errors unintentionally\nDeprecated functions or outdated syntax"
  },
  {
    "objectID": "Templates/homework-template.html",
    "href": "Templates/homework-template.html",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "All homework assignments should follow this two-part structure: - Part A: By Hand (40-45 points) - No AI assistance, 10 questions - Part B: With AI Assistance (55-60 points) - Using AI coding assistants, 10 questions\nTotal: 100 points across 20 questions\n\n\n\nPart A (40-45 points): - Questions should range from 3-5 points each - Simpler questions (identifying concepts, basic code): 3-4 points - More complex questions (writing functions, multi-step analysis): 4-5 points\nPart B (55-60 points): - Questions should range from 5-7 points each - Standard AI-assisted tasks: 5-6 points - Comprehensive final question with executive summary: 7 points\n\n\n\nPart A - By Hand: 1. Conceptual identification (3 points): Identify problem types, match concepts 2. Basic implementation (3-4 points): Load data, implement single model/technique 3. Evaluation and interpretation (4-5 points): Calculate metrics, create visualizations, interpret results 4. Function writing (4-5 points): Write reusable functions that demonstrate understanding 5. Explanation questions (4-5 points): Explain concepts in own words, justify decisions\nPart B - AI Assisted: 1. Hyperparameter tuning (5-6 points): Test multiple parameter combinations systematically 2. Large-scale comparison (5-6 points): Compare many models/approaches simultaneously 3. Automation (5-6 points): Create functions that automate repetitive tasks at scale 4. Feature engineering/exploration (5-6 points): Test multiple feature combinations or data transformations 5. Cross-validation/robustness (5-6 points): Evaluate across multiple splits or conditions 6. Comprehensive pipeline (6-7 points): End-to-end workflow combining multiple steps 7. Final integrated project (7 points): Complete analysis with executive summary\n\n\n\nGoals: - Build foundational skills that cannot be outsourced to AI - Ensure students understand the underlying concepts - Practice the basic workflow that AI will scale up in Part B - Develop intuition for when something is wrong\nTopics covered by-hand: - Basic library usage and syntax - Single-model implementation - Metric calculation and interpretation - Simple visualizations - Understanding what code does and why - Explaining concepts in own words - Recognizing patterns and problems\nTypical progression: 1. Conceptual warm-up (identify problem types, match definitions) 2. Basic data loading and exploration 3. Implement 2-4 different techniques/models individually 4. Evaluate and compare using appropriate metrics 5. Create basic visualizations 6. Write helper functions demonstrating understanding 7. Explain concepts and justify decisions\n\n\n\nGoals: - Scale up the techniques learned in Part A - Practice effective AI prompting - Learn to verify and understand AI-generated code - Tackle problems too large/tedious to do by hand - Build comprehensive pipelines and workflows\nAI assistance is used for: - Testing many hyperparameters systematically - Comparing 5+ models simultaneously - Creating comprehensive visualization suites - Automating repetitive tasks - Building end-to-end pipelines - Generating formatted reports - Cross-validation and robustness testing\nRequired deliverables for each question: - The code (AI-generated but verified by student) - Outputs (visualizations, tables, results) - Written interpretation (student’s own analysis) - The prompt(s) used with AI assistant\nTypical progression: 1. Hyperparameter tuning for single model type 2. Compare multiple models at scale 3. Explore impact of different choices (features, scaling, etc.) 4. Build automation functions/pipelines 5. Perform robust evaluation (cross-validation, multiple conditions) 6. Comprehensive integrated analysis with executive summary\n\n\n\nProvide 1-2 datasets per assignment: - Should be realistic and somewhat messy (matching real-world data) - Include both numerical and categorical features - Should align with module topics - Clearly specify which dataset to use for which questions - Include complete column descriptions\nExample dataset formats: - Module 1 (EDA): housing_data.csv - Module 2 (ML Models): employee_salaries.csv (regression) + customer_churn.csv (classification) - Module 4 (Regression): Include features with multicollinearity, outliers - Module 5 (Classification): Include class imbalance, multiple classes\n\n\n\nInstructions: - Clear, direct, and specific - Use numbered lists for multi-part questions - Bold key terms and concepts - Specify exact deliverables expected\nQuestions: - Start with action verbs: “Calculate”, “Create”, “Explain”, “Compare”, “Write” - Include context and reasoning prompts: “Why?”, “How does X affect Y?”, “What do you notice?” - For Part B: explicitly request AI prompts as deliverables\nTips sections: - Casual but helpful tone - Practical, actionable advice - Separate tips for Part A, Part B, and General - End with: “Use your brain. That’s what it’s there for.”\n\n\n\nIMPORTANT - List Rendering: - Lists preceded by bold text, headings, or other content MUST have a blank line before the list starts - Without the blank line, Quarto will not render the list properly\nCorrect formatting:\n**Deliverables:**\n\n- Code\n- Visualizations\n- Written interpretation\nIncorrect formatting (will not render as list):\n**Deliverables:**\n- Code\n- Visualizations\n- Written interpretation\nApply this rule to: - Deliverables sections in Part B questions - Dataset column descriptions - Multi-part question items (a, b, c) - Submission guidelines - Tips sections - Any list following text, bold text, or headings\n\n\n\nPart A (40-45 points): - Code correctness and functionality: ~55% - Proper implementation of techniques: ~25% - Written interpretations and explanations: ~20%\nPart B (55-60 points): - Code functionality and correctness: ~45% - Quality and specificity of AI prompts: ~25% - Visualizations and comparative analysis: ~20% - Written interpretations and insights: ~10%\n\n\n\nEvery homework should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions section with:\n\nDue date (to be assigned)\nTwo-part structure explanation\nSubmission requirements (code, visualizations, written answers, AI prompts)\nDataset descriptions\n\nPart A: By Hand (10 questions, 40-45 points)\nPart B: With AI Assistance (10 questions, 55-60 points)\nSubmission Guidelines (ZIP file contents)\nGrading Rubric (detailed breakdown)\nTips for Success (Part A, Part B, General)\n\n\n\n\n\n\n---\ntitle: \"Module [X] Homework: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Instructions\n\n**Due Date:** [To be assigned by instructor]\n\nThis homework is divided into two parts. Part A should be completed **without AI assistance** to build your foundational [skills/understanding of topic]. Part B should be completed **with AI assistance** (like Gemini CLI) to practice [scaling your work/tackling more comprehensive tasks].\n\nFor all questions, submit:\n\n1. Your code (in a `.py` file or Jupyter notebook)\n2. All visualizations generated\n3. Written answers to interpretation questions (can be in markdown or comments)\n4. For Part B: Include the prompts you used with the AI assistant\n\n### Dataset(s)\n\n[Provide 1-2 dataset descriptions with column explanations]\n\n**[Dataset Name 1]:** `filename.csv` contains [description] with columns:\n\n- `column1`: Description (note if TARGET variable)\n- `column2`: Description\n- [etc.]\n\n---\n\n## Part A: By Hand (No AI Assistance)\n\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational [skills in X].\n\n### Question 1 ([3-5] points)\n\n[Question text with clear instructions]\n\n[Optional: Multi-part with a), b), c)]\n\n---\n\n### Question 2 ([3-5] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 3-10...]\n\n---\n\n## Part B: With AI Assistance\n\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you write and scale your [analysis/code/experiments]. The goal is to practice [specific AI-assisted skills].\n\n**Important:** For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n---\n\n### Question 11 ([5-7] points)\n\n[Question text]\n\n**Deliverables:**\n\n- Code\n- [Visualizations/Tables/Results]\n- Written interpretation ([X] sentences)\n- Your AI prompt(s)\n\n---\n\n### Question 12 ([5-7] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 12-19...]\n\n---\n\n### Question 20 (7 points)\n\n[Comprehensive final question requiring integration of multiple concepts]\n\nWrite a [length] executive summary ([word count] words) that explains:\n\n- [Point 1]\n- [Point 2]\n- [Point 3]\n- [etc.]\n\n**Deliverables:**\n\n- Complete [workflow/analysis] code (well-commented)\n- All visualizations\n- Executive summary\n- Your AI prompt(s)\n- Reflection: [Specific reflection question about AI-assisted vs by-hand work]\n\n---\n\n## Submission Guidelines\n\nSubmit a ZIP file containing:\n\n1. **Code files:** All `.py` files or Jupyter notebooks\n2. **Visualizations folder:** All plots and charts generated\n3. **Written responses:** A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\n4. **Data:** Include your processed datasets if you created any modifications\n\n---\n\n## Grading Rubric\n\n**Part A: By Hand ([40-45] points)**\n\n- Code correctness and functionality: [~22-25] points\n- Proper implementation of [techniques/models]: [~10-11] points\n- Written interpretations and explanations: [~8-10] points\n\n**Part B: AI-Assisted ([55-60] points)**\n\n- Code functionality and correctness: [~25-27] points\n- Quality and specificity of AI prompts: [~14-15] points\n- Visualizations and comparative analysis: [~11-12] points\n- Written interpretations and insights: [~5-6] points\n\n**Total: 100 points**\n\n---\n\n## Tips for Success\n\n### For Part A:\n\n- [3-5 specific, actionable tips relevant to the module]\n- Test your code on small examples first\n- Use meaningful variable names\n- Comment your code to explain your reasoning\n\n### For Part B:\n\n- Write specific, detailed prompts\n- If the AI's first attempt isn't quite right, refine your prompt\n- Always review and test the AI-generated code\n- Don't just accept AI output—understand what it's doing\n- Include context in your prompts (e.g., \"I'm working with a [dataset] with columns...\")\n\n### General:\n\n- Start early—[module-specific reason]\n- Your interpretations are just as important as your code\n- If you find something interesting in the data, explore it further!\n- Use your brain. That's what it's there for.\n\n\n\n\n\n\n\nFocus: Data exploration, visualization, testing\nBy-hand: Basic Pandas, Seaborn, simple statistics\nAI-assisted: Comprehensive profiling, many visualizations, complete test suites\n\n\n\n\n\nFocus: Model implementation, comparison, evaluation\nBy-hand: Instantiate/fit/predict workflow, basic metrics\nAI-assisted: Hyperparameter tuning, comparing 5+ models, cross-validation\n\n\n\n\n\nFocus: Loss functions, optimization, bias-variance tradeoff\nBy-hand: Implement simple loss functions, understand concepts\nAI-assisted: Compare many loss functions, visualize optimization landscapes\n\n\n\n\n\nFocus: Linear regression variants, regularization, diagnostics\nBy-hand: Fit linear/ridge/lasso, residual analysis, interpret coefficients\nAI-assisted: Grid search over many alphas, polynomial feature testing, comprehensive diagnostics\n\n\n\n\n\nFocus: Multiple classifiers, evaluation metrics, class imbalance\nBy-hand: Implement classifiers, confusion matrices, metric calculation\nAI-assisted: Hyperparameter tuning, handling imbalance strategies, comprehensive metric comparison\n\n\n\n\n\nFocus: Network architecture, PyTorch, training loops\nBy-hand: Read PyTorch code, understand architecture, plot training curves\nAI-assisted: Write training loops, architecture variations, extensive hyperparameter tuning\n\n\n\n\n\nFocus: Hugging Face, fine-tuning, transfer learning\nBy-hand: Load and use pretrained models, basic inference\nAI-assisted: Fine-tuning pipelines, comparing many pretrained models, end-to-end applications"
  },
  {
    "objectID": "Templates/homework-template.html#template-guidelines",
    "href": "Templates/homework-template.html#template-guidelines",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "All homework assignments should follow this two-part structure: - Part A: By Hand (40-45 points) - No AI assistance, 10 questions - Part B: With AI Assistance (55-60 points) - Using AI coding assistants, 10 questions\nTotal: 100 points across 20 questions\n\n\n\nPart A (40-45 points): - Questions should range from 3-5 points each - Simpler questions (identifying concepts, basic code): 3-4 points - More complex questions (writing functions, multi-step analysis): 4-5 points\nPart B (55-60 points): - Questions should range from 5-7 points each - Standard AI-assisted tasks: 5-6 points - Comprehensive final question with executive summary: 7 points\n\n\n\nPart A - By Hand: 1. Conceptual identification (3 points): Identify problem types, match concepts 2. Basic implementation (3-4 points): Load data, implement single model/technique 3. Evaluation and interpretation (4-5 points): Calculate metrics, create visualizations, interpret results 4. Function writing (4-5 points): Write reusable functions that demonstrate understanding 5. Explanation questions (4-5 points): Explain concepts in own words, justify decisions\nPart B - AI Assisted: 1. Hyperparameter tuning (5-6 points): Test multiple parameter combinations systematically 2. Large-scale comparison (5-6 points): Compare many models/approaches simultaneously 3. Automation (5-6 points): Create functions that automate repetitive tasks at scale 4. Feature engineering/exploration (5-6 points): Test multiple feature combinations or data transformations 5. Cross-validation/robustness (5-6 points): Evaluate across multiple splits or conditions 6. Comprehensive pipeline (6-7 points): End-to-end workflow combining multiple steps 7. Final integrated project (7 points): Complete analysis with executive summary\n\n\n\nGoals: - Build foundational skills that cannot be outsourced to AI - Ensure students understand the underlying concepts - Practice the basic workflow that AI will scale up in Part B - Develop intuition for when something is wrong\nTopics covered by-hand: - Basic library usage and syntax - Single-model implementation - Metric calculation and interpretation - Simple visualizations - Understanding what code does and why - Explaining concepts in own words - Recognizing patterns and problems\nTypical progression: 1. Conceptual warm-up (identify problem types, match definitions) 2. Basic data loading and exploration 3. Implement 2-4 different techniques/models individually 4. Evaluate and compare using appropriate metrics 5. Create basic visualizations 6. Write helper functions demonstrating understanding 7. Explain concepts and justify decisions\n\n\n\nGoals: - Scale up the techniques learned in Part A - Practice effective AI prompting - Learn to verify and understand AI-generated code - Tackle problems too large/tedious to do by hand - Build comprehensive pipelines and workflows\nAI assistance is used for: - Testing many hyperparameters systematically - Comparing 5+ models simultaneously - Creating comprehensive visualization suites - Automating repetitive tasks - Building end-to-end pipelines - Generating formatted reports - Cross-validation and robustness testing\nRequired deliverables for each question: - The code (AI-generated but verified by student) - Outputs (visualizations, tables, results) - Written interpretation (student’s own analysis) - The prompt(s) used with AI assistant\nTypical progression: 1. Hyperparameter tuning for single model type 2. Compare multiple models at scale 3. Explore impact of different choices (features, scaling, etc.) 4. Build automation functions/pipelines 5. Perform robust evaluation (cross-validation, multiple conditions) 6. Comprehensive integrated analysis with executive summary\n\n\n\nProvide 1-2 datasets per assignment: - Should be realistic and somewhat messy (matching real-world data) - Include both numerical and categorical features - Should align with module topics - Clearly specify which dataset to use for which questions - Include complete column descriptions\nExample dataset formats: - Module 1 (EDA): housing_data.csv - Module 2 (ML Models): employee_salaries.csv (regression) + customer_churn.csv (classification) - Module 4 (Regression): Include features with multicollinearity, outliers - Module 5 (Classification): Include class imbalance, multiple classes\n\n\n\nInstructions: - Clear, direct, and specific - Use numbered lists for multi-part questions - Bold key terms and concepts - Specify exact deliverables expected\nQuestions: - Start with action verbs: “Calculate”, “Create”, “Explain”, “Compare”, “Write” - Include context and reasoning prompts: “Why?”, “How does X affect Y?”, “What do you notice?” - For Part B: explicitly request AI prompts as deliverables\nTips sections: - Casual but helpful tone - Practical, actionable advice - Separate tips for Part A, Part B, and General - End with: “Use your brain. That’s what it’s there for.”\n\n\n\nIMPORTANT - List Rendering: - Lists preceded by bold text, headings, or other content MUST have a blank line before the list starts - Without the blank line, Quarto will not render the list properly\nCorrect formatting:\n**Deliverables:**\n\n- Code\n- Visualizations\n- Written interpretation\nIncorrect formatting (will not render as list):\n**Deliverables:**\n- Code\n- Visualizations\n- Written interpretation\nApply this rule to: - Deliverables sections in Part B questions - Dataset column descriptions - Multi-part question items (a, b, c) - Submission guidelines - Tips sections - Any list following text, bold text, or headings\n\n\n\nPart A (40-45 points): - Code correctness and functionality: ~55% - Proper implementation of techniques: ~25% - Written interpretations and explanations: ~20%\nPart B (55-60 points): - Code functionality and correctness: ~45% - Quality and specificity of AI prompts: ~25% - Visualizations and comparative analysis: ~20% - Written interpretations and insights: ~10%\n\n\n\nEvery homework should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions section with:\n\nDue date (to be assigned)\nTwo-part structure explanation\nSubmission requirements (code, visualizations, written answers, AI prompts)\nDataset descriptions\n\nPart A: By Hand (10 questions, 40-45 points)\nPart B: With AI Assistance (10 questions, 55-60 points)\nSubmission Guidelines (ZIP file contents)\nGrading Rubric (detailed breakdown)\nTips for Success (Part A, Part B, General)"
  },
  {
    "objectID": "Templates/homework-template.html#homework-template-structure",
    "href": "Templates/homework-template.html#homework-template-structure",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "---\ntitle: \"Module [X] Homework: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Instructions\n\n**Due Date:** [To be assigned by instructor]\n\nThis homework is divided into two parts. Part A should be completed **without AI assistance** to build your foundational [skills/understanding of topic]. Part B should be completed **with AI assistance** (like Gemini CLI) to practice [scaling your work/tackling more comprehensive tasks].\n\nFor all questions, submit:\n\n1. Your code (in a `.py` file or Jupyter notebook)\n2. All visualizations generated\n3. Written answers to interpretation questions (can be in markdown or comments)\n4. For Part B: Include the prompts you used with the AI assistant\n\n### Dataset(s)\n\n[Provide 1-2 dataset descriptions with column explanations]\n\n**[Dataset Name 1]:** `filename.csv` contains [description] with columns:\n\n- `column1`: Description (note if TARGET variable)\n- `column2`: Description\n- [etc.]\n\n---\n\n## Part A: By Hand (No AI Assistance)\n\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational [skills in X].\n\n### Question 1 ([3-5] points)\n\n[Question text with clear instructions]\n\n[Optional: Multi-part with a), b), c)]\n\n---\n\n### Question 2 ([3-5] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 3-10...]\n\n---\n\n## Part B: With AI Assistance\n\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you write and scale your [analysis/code/experiments]. The goal is to practice [specific AI-assisted skills].\n\n**Important:** For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n---\n\n### Question 11 ([5-7] points)\n\n[Question text]\n\n**Deliverables:**\n\n- Code\n- [Visualizations/Tables/Results]\n- Written interpretation ([X] sentences)\n- Your AI prompt(s)\n\n---\n\n### Question 12 ([5-7] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 12-19...]\n\n---\n\n### Question 20 (7 points)\n\n[Comprehensive final question requiring integration of multiple concepts]\n\nWrite a [length] executive summary ([word count] words) that explains:\n\n- [Point 1]\n- [Point 2]\n- [Point 3]\n- [etc.]\n\n**Deliverables:**\n\n- Complete [workflow/analysis] code (well-commented)\n- All visualizations\n- Executive summary\n- Your AI prompt(s)\n- Reflection: [Specific reflection question about AI-assisted vs by-hand work]\n\n---\n\n## Submission Guidelines\n\nSubmit a ZIP file containing:\n\n1. **Code files:** All `.py` files or Jupyter notebooks\n2. **Visualizations folder:** All plots and charts generated\n3. **Written responses:** A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\n4. **Data:** Include your processed datasets if you created any modifications\n\n---\n\n## Grading Rubric\n\n**Part A: By Hand ([40-45] points)**\n\n- Code correctness and functionality: [~22-25] points\n- Proper implementation of [techniques/models]: [~10-11] points\n- Written interpretations and explanations: [~8-10] points\n\n**Part B: AI-Assisted ([55-60] points)**\n\n- Code functionality and correctness: [~25-27] points\n- Quality and specificity of AI prompts: [~14-15] points\n- Visualizations and comparative analysis: [~11-12] points\n- Written interpretations and insights: [~5-6] points\n\n**Total: 100 points**\n\n---\n\n## Tips for Success\n\n### For Part A:\n\n- [3-5 specific, actionable tips relevant to the module]\n- Test your code on small examples first\n- Use meaningful variable names\n- Comment your code to explain your reasoning\n\n### For Part B:\n\n- Write specific, detailed prompts\n- If the AI's first attempt isn't quite right, refine your prompt\n- Always review and test the AI-generated code\n- Don't just accept AI output—understand what it's doing\n- Include context in your prompts (e.g., \"I'm working with a [dataset] with columns...\")\n\n### General:\n\n- Start early—[module-specific reason]\n- Your interpretations are just as important as your code\n- If you find something interesting in the data, explore it further!\n- Use your brain. That's what it's there for."
  },
  {
    "objectID": "Templates/homework-template.html#module-specific-adaptations",
    "href": "Templates/homework-template.html#module-specific-adaptations",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "Focus: Data exploration, visualization, testing\nBy-hand: Basic Pandas, Seaborn, simple statistics\nAI-assisted: Comprehensive profiling, many visualizations, complete test suites\n\n\n\n\n\nFocus: Model implementation, comparison, evaluation\nBy-hand: Instantiate/fit/predict workflow, basic metrics\nAI-assisted: Hyperparameter tuning, comparing 5+ models, cross-validation\n\n\n\n\n\nFocus: Loss functions, optimization, bias-variance tradeoff\nBy-hand: Implement simple loss functions, understand concepts\nAI-assisted: Compare many loss functions, visualize optimization landscapes\n\n\n\n\n\nFocus: Linear regression variants, regularization, diagnostics\nBy-hand: Fit linear/ridge/lasso, residual analysis, interpret coefficients\nAI-assisted: Grid search over many alphas, polynomial feature testing, comprehensive diagnostics\n\n\n\n\n\nFocus: Multiple classifiers, evaluation metrics, class imbalance\nBy-hand: Implement classifiers, confusion matrices, metric calculation\nAI-assisted: Hyperparameter tuning, handling imbalance strategies, comprehensive metric comparison\n\n\n\n\n\nFocus: Network architecture, PyTorch, training loops\nBy-hand: Read PyTorch code, understand architecture, plot training curves\nAI-assisted: Write training loops, architecture variations, extensive hyperparameter tuning\n\n\n\n\n\nFocus: Hugging Face, fine-tuning, transfer learning\nBy-hand: Load and use pretrained models, basic inference\nAI-assisted: Fine-tuning pipelines, comparing many pretrained models, end-to-end applications"
  },
  {
    "objectID": "Templates/quiz-template.html",
    "href": "Templates/quiz-template.html",
    "title": "Quiz Template",
    "section": "",
    "text": "All quizzes should follow this two-section structure: - Section A: Conceptual Questions (approximately 50% of points) - Testing understanding of concepts, trade-offs, and decision-making - Section B: Code Writing (approximately 50% of points) - Testing ability to write code by hand\nTotal: Typically 25-50 points (adjustable based on chapter complexity)\nTime: 30-45 minutes\n\n\n\nPoints should reflect: - Complexity of the concept or code required - Number of parts in the question - Level of reasoning needed\nTypical ranges: - Simple recall/identification: 2 points - Moderate explanation/interpretation: 3 points - Complex reasoning/multi-part questions: 3-4 points - Code writing tasks: 2-4 points depending on complexity\n\n\n\nPurpose: - Test conceptual understanding without a computer - Assess ability to reason about trade-offs and make decisions - Evaluate recognition of when techniques are appropriate/inappropriate - Check understanding of metrics and their interpretations\nQuestion Types:\n\nScenario identification (2-3 points)\n\nGiven scenarios, identify problem type/appropriate technique\nExample: “Is this classification or regression?”\nMultiple scenarios in one question\n\nMetric interpretation (3 points)\n\nGiven metric values, interpret meaning\nCompare models based on different metrics\nExplain which metric to prioritize in a business context\n\nTrade-off reasoning (3-4 points)\n\nUnderstand precision vs recall, bias vs variance, etc.\nExplain when to choose one approach over another\nJustify decisions based on context\n\nConceptual explanation (2-3 points)\n\nDefine terms in own words\nExplain why something works or fails\nDescribe relationships between concepts\n\nProblem diagnosis (3-4 points)\n\nGiven symptoms (overfitting, poor performance, etc.), identify cause\nSuggest solutions to problems\nRecognize when assumptions are violated\n\nMatching questions (2-3 points)\n\nMatch models to their characteristics\nMatch metrics to their uses\nMatch problems to solutions\n\nScenario-based reasoning (3-4 points)\n\nMulti-part questions about a specific scenario\nRequires applying multiple concepts\nTests understanding of workflow and decision-making\n\n\nTypical distribution: 6-8 questions totaling 45-55% of quiz points\n\n\n\nPurpose: - Test ability to write basic code without computer/IDE - Ensure students know fundamental syntax and workflows - Assess understanding of what code does, not just copying - Check recall of essential library functions\nImportant Notes: - Syntax perfection is NOT required - Focus on logic and correct approach - Minor syntax errors minimally penalized - Students should demonstrate understanding of the workflow\nQuestion Types:\n\nData manipulation (2-3 points)\n\nLoad data, select columns, filter rows\nCreate train/test splits\nBasic Pandas operations\n\nModel instantiation (2-3 points)\n\nCreate model objects with correct parameters\nExample: LinearRegression(), DecisionTreeClassifier(max_depth=5)\n\nModel workflow (3-4 points)\n\nComplete fit/predict workflow\nMay include model creation, fitting, and prediction in sequence\n\nMetric calculation (2-3 points)\n\nCalculate metrics using sklearn functions\nExample: accuracy_score(y_test, predictions)\n\nVisualization code (3-4 points)\n\nCreate basic plots (scatter, histogram, bar)\nAdd labels and titles\nMay be more challenging due to syntax details\n\nFunction writing (4-5 points)\n\nWrite simple functions that take inputs and return outputs\nDemonstrate logic and workflow understanding\nExample: comparison function, evaluation function\n\n\nTypical distribution: 5-7 questions totaling 45-55% of quiz points\n\n\n\nSection A (Conceptual): - Full credit: Demonstrates clear understanding - Partial credit: Shows correct reasoning even if incomplete - Focus on understanding, not perfect wording\nSection B (Code): - Correct logic and approach: 70-80% of points - Syntax and details: 20-30% of points - Minor syntax errors minimally penalized - Must show understanding of workflow\nInclude statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\n\n\nEvery quiz should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions with:\n\nTime limit (30-45 minutes)\nFormat: In-class, by-hand (no computer)\nGeneral instruction about writing neatly\nNote about partial credit\n\nSection A: Conceptual Questions (~50% of points, 6-8 questions)\nSection B: Code Writing (~50% of points, 5-7 questions)\nGrading Rubric (breakdown by section with note about syntax)\n\n\n\n\nAlways include for Section B:\nFor questions X-Y, assume you have already imported:\n```python\n[relevant imports for the chapter]\nAlso assume you have [description of available data/variables].\n\n**Example for ML chapter:**\n```markdown\nFor questions 6-12, assume you have already imported:\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nAlso assume you have a DataFrame called df with features and a target column.\n\n### Writing Style\n\n**Instructions:**\n- Clear and direct\n- Specify \"by hand\" and \"no computer\" explicitly\n- Set expectations about syntax\n\n**Questions:**\n- Start with clear action verbs\n- Provide sufficient context\n- Multi-part questions should use a), b), c)\n- Ask \"why\" and \"explain your reasoning\" frequently\n\n**Conceptual questions:**\n- Present realistic scenarios\n- Require application of knowledge, not just recall\n- Include business/practical context when relevant\n\n**Code questions:**\n- Be specific about what to create\n- Specify variable names if relevant\n- Break complex tasks into steps\n\n### Formatting Guidelines for Quarto Markdown\n\n**IMPORTANT - List Rendering:**\n- Lists preceded by bold text, headings, or other content **MUST** have a blank line before the list starts\n- Without the blank line, Quarto will not render the list properly\n\n**Correct formatting:**\n```markdown\n**Models:** Logistic Regression, Decision Tree, k-Nearest Neighbors\n\n**Advantages:**\n\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\n3. Provides probability estimates and handles binary classification well\nIncorrect formatting (will not render as list):\n**Advantages:**\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\nApply this rule to: - Multi-part questions (a, b, c) - Matching question options - Scenario lists - Any list following text, bold text, or headings\n\n\n\n\n\n---\ntitle: \"Chapter [X] Quiz: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    code-fold: false\n    theme: cosmo\n---\n\n## Instructions\n\n**Time:** 30-45 minutes\n**Format:** In-class, by-hand (no computer)\n\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn't perfect.\n\n---\n\n## Section A: Conceptual Questions\n\n### Question 1 ([2-4] points)\n\n[Conceptual question with clear scenario/context]\n\n[If multi-part: **a)**, **b)**, **c)**]\n\n---\n\n### Question 2 ([2-4] points)\n\n[Continue pattern for 6-8 questions total]\n\n---\n\n[Questions 3-8...]\n\n---\n\n## Section B: Code Writing\n\nFor questions [X]-[Y], assume you have already imported:\n```python\n[relevant imports]\nAlso assume you have [description of available data/variables].\n\n\n\nWrite code to [specific task].\n\n\n\n\n[Continue pattern for 5-7 questions total]\n\n[Remaining code questions…]\n\n\n\n\n\nSection A: Conceptual Questions ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[Category 3]: [points]\n[etc.]\n\nSection B: Code Writing ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[etc.]\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: [sum] points\n\n---\n\n## Chapter-Specific Adaptations\n\n### Chapter 1 (EDA):\n**Conceptual focus:**\n- When to use different visualizations\n- Data quality issues and solutions\n- Interpretation of distributions and patterns\n- AI prompting strategies\n- Purpose of different types of tests\n\n**Code focus:**\n- Loading and filtering data (Pandas)\n- Creating visualizations (Seaborn)\n- Basic statistics\n- Writing simple test functions\n\n**Imports:**\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nConceptual focus: - Classification vs regression identification - Model selection justification - Metric interpretation (R², MSE, accuracy, precision, recall, F1) - Understanding overfitting - Hyperparameter concepts - Scaling importance\nCode focus: - Train/test split - Model instantiation with parameters - Fit/predict workflow - Metric calculation - Simple comparison functions\nImports:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n\n\n\nConceptual focus: - Loss function selection and interpretation - Bias-variance tradeoff - Optimization concepts (gradient descent) - Overfitting vs underfitting diagnosis - Training/validation/test set methodology\nCode focus: - Implementing simple loss functions - Creating train/val/test splits - Comparing models on different metrics - Plotting learning curves\n\n\n\nConceptual focus: - Linear regression assumptions - When regularization helps - Interpreting coefficients - Residual analysis - Multicollinearity problems - Choosing between Ridge/Lasso/Elastic Net\nCode focus: - Fitting linear regression variants - Computing residuals - Polynomial feature creation - Regularized regression with alpha parameter - Diagnostic plotting\n\n\n\nConceptual focus: - Choosing appropriate evaluation metrics - Precision-recall tradeoffs - Class imbalance handling - Decision boundary interpretation - Feature importance understanding - When to use different classifiers\nCode focus: - Multiple classifier implementation - Confusion matrix creation - ROC curve plotting - Handling class weights - Feature importance extraction\n\n\n\nConceptual focus: - When neural networks are appropriate - Activation function selection - Architecture design reasoning - Interpreting training curves - Overfitting diagnosis - Regularization techniques (dropout, batch norm)\nCode focus: - Reading PyTorch model definitions - Understanding layer structures - Creating DataLoaders - Interpreting training loop code - Plotting loss curves\n\n\n\nConceptual focus: - When to use pretrained vs train from scratch - Transfer learning concepts - Fine-tuning vs feature extraction - Model selection from Hugging Face - Understanding model cards - Trade-offs between model size and accuracy\nCode focus: - Loading pretrained models - Basic inference code - Understanding tokenization - Data format preparation for pretrained models - Interpreting model outputs\n\n\n\n\n\nBefore finalizing a quiz, verify:\n\nTotal points add up correctly (typically 25-50)\nConceptual and code sections are roughly balanced (±5 points)\nTime estimate is reasonable (30-45 min)\nAll necessary imports are listed for code section\nQuestions progress from simpler to more complex\nMulti-part questions use consistent formatting (a, b, c)\nConceptual questions require reasoning, not just recall\nCode questions are realistic to write by hand\nGrading rubric is clear and complete\nNote about syntax errors is included\nQuestions align with module learning objectives\nScenarios are realistic and relatable"
  },
  {
    "objectID": "Templates/quiz-template.html#template-guidelines",
    "href": "Templates/quiz-template.html#template-guidelines",
    "title": "Quiz Template",
    "section": "",
    "text": "All quizzes should follow this two-section structure: - Section A: Conceptual Questions (approximately 50% of points) - Testing understanding of concepts, trade-offs, and decision-making - Section B: Code Writing (approximately 50% of points) - Testing ability to write code by hand\nTotal: Typically 25-50 points (adjustable based on chapter complexity)\nTime: 30-45 minutes\n\n\n\nPoints should reflect: - Complexity of the concept or code required - Number of parts in the question - Level of reasoning needed\nTypical ranges: - Simple recall/identification: 2 points - Moderate explanation/interpretation: 3 points - Complex reasoning/multi-part questions: 3-4 points - Code writing tasks: 2-4 points depending on complexity\n\n\n\nPurpose: - Test conceptual understanding without a computer - Assess ability to reason about trade-offs and make decisions - Evaluate recognition of when techniques are appropriate/inappropriate - Check understanding of metrics and their interpretations\nQuestion Types:\n\nScenario identification (2-3 points)\n\nGiven scenarios, identify problem type/appropriate technique\nExample: “Is this classification or regression?”\nMultiple scenarios in one question\n\nMetric interpretation (3 points)\n\nGiven metric values, interpret meaning\nCompare models based on different metrics\nExplain which metric to prioritize in a business context\n\nTrade-off reasoning (3-4 points)\n\nUnderstand precision vs recall, bias vs variance, etc.\nExplain when to choose one approach over another\nJustify decisions based on context\n\nConceptual explanation (2-3 points)\n\nDefine terms in own words\nExplain why something works or fails\nDescribe relationships between concepts\n\nProblem diagnosis (3-4 points)\n\nGiven symptoms (overfitting, poor performance, etc.), identify cause\nSuggest solutions to problems\nRecognize when assumptions are violated\n\nMatching questions (2-3 points)\n\nMatch models to their characteristics\nMatch metrics to their uses\nMatch problems to solutions\n\nScenario-based reasoning (3-4 points)\n\nMulti-part questions about a specific scenario\nRequires applying multiple concepts\nTests understanding of workflow and decision-making\n\n\nTypical distribution: 6-8 questions totaling 45-55% of quiz points\n\n\n\nPurpose: - Test ability to write basic code without computer/IDE - Ensure students know fundamental syntax and workflows - Assess understanding of what code does, not just copying - Check recall of essential library functions\nImportant Notes: - Syntax perfection is NOT required - Focus on logic and correct approach - Minor syntax errors minimally penalized - Students should demonstrate understanding of the workflow\nQuestion Types:\n\nData manipulation (2-3 points)\n\nLoad data, select columns, filter rows\nCreate train/test splits\nBasic Pandas operations\n\nModel instantiation (2-3 points)\n\nCreate model objects with correct parameters\nExample: LinearRegression(), DecisionTreeClassifier(max_depth=5)\n\nModel workflow (3-4 points)\n\nComplete fit/predict workflow\nMay include model creation, fitting, and prediction in sequence\n\nMetric calculation (2-3 points)\n\nCalculate metrics using sklearn functions\nExample: accuracy_score(y_test, predictions)\n\nVisualization code (3-4 points)\n\nCreate basic plots (scatter, histogram, bar)\nAdd labels and titles\nMay be more challenging due to syntax details\n\nFunction writing (4-5 points)\n\nWrite simple functions that take inputs and return outputs\nDemonstrate logic and workflow understanding\nExample: comparison function, evaluation function\n\n\nTypical distribution: 5-7 questions totaling 45-55% of quiz points\n\n\n\nSection A (Conceptual): - Full credit: Demonstrates clear understanding - Partial credit: Shows correct reasoning even if incomplete - Focus on understanding, not perfect wording\nSection B (Code): - Correct logic and approach: 70-80% of points - Syntax and details: 20-30% of points - Minor syntax errors minimally penalized - Must show understanding of workflow\nInclude statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\n\n\nEvery quiz should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions with:\n\nTime limit (30-45 minutes)\nFormat: In-class, by-hand (no computer)\nGeneral instruction about writing neatly\nNote about partial credit\n\nSection A: Conceptual Questions (~50% of points, 6-8 questions)\nSection B: Code Writing (~50% of points, 5-7 questions)\nGrading Rubric (breakdown by section with note about syntax)\n\n\n\n\nAlways include for Section B:\nFor questions X-Y, assume you have already imported:\n```python\n[relevant imports for the chapter]\nAlso assume you have [description of available data/variables].\n\n**Example for ML chapter:**\n```markdown\nFor questions 6-12, assume you have already imported:\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nAlso assume you have a DataFrame called df with features and a target column.\n\n### Writing Style\n\n**Instructions:**\n- Clear and direct\n- Specify \"by hand\" and \"no computer\" explicitly\n- Set expectations about syntax\n\n**Questions:**\n- Start with clear action verbs\n- Provide sufficient context\n- Multi-part questions should use a), b), c)\n- Ask \"why\" and \"explain your reasoning\" frequently\n\n**Conceptual questions:**\n- Present realistic scenarios\n- Require application of knowledge, not just recall\n- Include business/practical context when relevant\n\n**Code questions:**\n- Be specific about what to create\n- Specify variable names if relevant\n- Break complex tasks into steps\n\n### Formatting Guidelines for Quarto Markdown\n\n**IMPORTANT - List Rendering:**\n- Lists preceded by bold text, headings, or other content **MUST** have a blank line before the list starts\n- Without the blank line, Quarto will not render the list properly\n\n**Correct formatting:**\n```markdown\n**Models:** Logistic Regression, Decision Tree, k-Nearest Neighbors\n\n**Advantages:**\n\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\n3. Provides probability estimates and handles binary classification well\nIncorrect formatting (will not render as list):\n**Advantages:**\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\nApply this rule to: - Multi-part questions (a, b, c) - Matching question options - Scenario lists - Any list following text, bold text, or headings"
  },
  {
    "objectID": "Templates/quiz-template.html#quiz-template-structure",
    "href": "Templates/quiz-template.html#quiz-template-structure",
    "title": "Quiz Template",
    "section": "",
    "text": "---\ntitle: \"Chapter [X] Quiz: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    code-fold: false\n    theme: cosmo\n---\n\n## Instructions\n\n**Time:** 30-45 minutes\n**Format:** In-class, by-hand (no computer)\n\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn't perfect.\n\n---\n\n## Section A: Conceptual Questions\n\n### Question 1 ([2-4] points)\n\n[Conceptual question with clear scenario/context]\n\n[If multi-part: **a)**, **b)**, **c)**]\n\n---\n\n### Question 2 ([2-4] points)\n\n[Continue pattern for 6-8 questions total]\n\n---\n\n[Questions 3-8...]\n\n---\n\n## Section B: Code Writing\n\nFor questions [X]-[Y], assume you have already imported:\n```python\n[relevant imports]\nAlso assume you have [description of available data/variables].\n\n\n\nWrite code to [specific task].\n\n\n\n\n[Continue pattern for 5-7 questions total]\n\n[Remaining code questions…]"
  },
  {
    "objectID": "Templates/quiz-template.html#grading-rubric",
    "href": "Templates/quiz-template.html#grading-rubric",
    "title": "Quiz Template",
    "section": "",
    "text": "Section A: Conceptual Questions ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[Category 3]: [points]\n[etc.]\n\nSection B: Code Writing ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[etc.]\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: [sum] points\n\n---\n\n## Chapter-Specific Adaptations\n\n### Chapter 1 (EDA):\n**Conceptual focus:**\n- When to use different visualizations\n- Data quality issues and solutions\n- Interpretation of distributions and patterns\n- AI prompting strategies\n- Purpose of different types of tests\n\n**Code focus:**\n- Loading and filtering data (Pandas)\n- Creating visualizations (Seaborn)\n- Basic statistics\n- Writing simple test functions\n\n**Imports:**\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nConceptual focus: - Classification vs regression identification - Model selection justification - Metric interpretation (R², MSE, accuracy, precision, recall, F1) - Understanding overfitting - Hyperparameter concepts - Scaling importance\nCode focus: - Train/test split - Model instantiation with parameters - Fit/predict workflow - Metric calculation - Simple comparison functions\nImports:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n\n\n\nConceptual focus: - Loss function selection and interpretation - Bias-variance tradeoff - Optimization concepts (gradient descent) - Overfitting vs underfitting diagnosis - Training/validation/test set methodology\nCode focus: - Implementing simple loss functions - Creating train/val/test splits - Comparing models on different metrics - Plotting learning curves\n\n\n\nConceptual focus: - Linear regression assumptions - When regularization helps - Interpreting coefficients - Residual analysis - Multicollinearity problems - Choosing between Ridge/Lasso/Elastic Net\nCode focus: - Fitting linear regression variants - Computing residuals - Polynomial feature creation - Regularized regression with alpha parameter - Diagnostic plotting\n\n\n\nConceptual focus: - Choosing appropriate evaluation metrics - Precision-recall tradeoffs - Class imbalance handling - Decision boundary interpretation - Feature importance understanding - When to use different classifiers\nCode focus: - Multiple classifier implementation - Confusion matrix creation - ROC curve plotting - Handling class weights - Feature importance extraction\n\n\n\nConceptual focus: - When neural networks are appropriate - Activation function selection - Architecture design reasoning - Interpreting training curves - Overfitting diagnosis - Regularization techniques (dropout, batch norm)\nCode focus: - Reading PyTorch model definitions - Understanding layer structures - Creating DataLoaders - Interpreting training loop code - Plotting loss curves\n\n\n\nConceptual focus: - When to use pretrained vs train from scratch - Transfer learning concepts - Fine-tuning vs feature extraction - Model selection from Hugging Face - Understanding model cards - Trade-offs between model size and accuracy\nCode focus: - Loading pretrained models - Basic inference code - Understanding tokenization - Data format preparation for pretrained models - Interpreting model outputs"
  },
  {
    "objectID": "Templates/quiz-template.html#quality-checklist",
    "href": "Templates/quiz-template.html#quality-checklist",
    "title": "Quiz Template",
    "section": "",
    "text": "Before finalizing a quiz, verify:\n\nTotal points add up correctly (typically 25-50)\nConceptual and code sections are roughly balanced (±5 points)\nTime estimate is reasonable (30-45 min)\nAll necessary imports are listed for code section\nQuestions progress from simpler to more complex\nMulti-part questions use consistent formatting (a, b, c)\nConceptual questions require reasoning, not just recall\nCode questions are realistic to write by hand\nGrading rubric is clear and complete\nNote about syntax errors is included\nQuestions align with module learning objectives\nScenarios are realistic and relatable"
  },
  {
    "objectID": "Projects/mini-project-1/mini-project-1.html",
    "href": "Projects/mini-project-1/mini-project-1.html",
    "title": "Mini Project 1: EDA",
    "section": "",
    "text": "Mini project 1 is a one week project where you take a dataset and research questions, and perform exploratory data analysis (EDA). The goal of this project is for you to get hands-on practice working with an AI agent to do EDA, and quickly turning around a project. Note that in this project you will not be building any machine learning models. You simply want to use the EDA process to understand your data."
  },
  {
    "objectID": "Projects/mini-project-1/mini-project-1.html#overview",
    "href": "Projects/mini-project-1/mini-project-1.html#overview",
    "title": "Mini Project 1: EDA",
    "section": "",
    "text": "Mini project 1 is a one week project where you take a dataset and research questions, and perform exploratory data analysis (EDA). The goal of this project is for you to get hands-on practice working with an AI agent to do EDA, and quickly turning around a project. Note that in this project you will not be building any machine learning models. You simply want to use the EDA process to understand your data."
  },
  {
    "objectID": "Projects/mini-project-1/mini-project-1.html#datasets-and-research-questions",
    "href": "Projects/mini-project-1/mini-project-1.html#datasets-and-research-questions",
    "title": "Mini Project 1: EDA",
    "section": "Datasets and research questions",
    "text": "Datasets and research questions\n\nEarthquakes\n\nDataset\nResearch questions:\n\nExplain the various metrics associated with earthquakes (depth, gap, time-travel residuals, etc.), and what they tell us about an earthquake.\nHow does the number and intensity of earthquakes vary geographically?\nHow has the number, intensity and location of earthquakes changed over time?\n\n\n\n\nCO2 and Greenhouse Gas Emissions\n\nDataset\nResearch questions:\n\nDiscuss the key metrics from this dataset related to emissions.\nAcross the globe, how have emissions changed over time?\nHow are emissions distributed geographically?\nFrom the 1900s to now, how can you describe the geographic trends in emissions? Which countries are improving, which are worsening? What else do you see?\n\n\n\n\nGlobal Gender Inequality\n\nDataset\nResearch questions:\n\nExplain the various metrics associated with gender (in)equality. What are they, are how do they relate to the perception of (in)equality”?\nHow have various measures of gender equality changed over time? Geographically?\nHow would you describe the trends in gender (in)equality? Which countries are improving? Which metrics are improving quickly, and which are stagnating? What else do you see?\n\n\n\n\nNBA\n\nDataset\nResearch questions:\n\nHow can you describe the similarities and differences between teams in the NBA seasons covered? What helps us understand the teams who are winning, the middling teams, and the teams who are struggling?\nTo what extent does team history (e.g. record of championships, new teams vs old teams, etc) have an effect of performance? Do top teams continue to attract top talent?\nHow can we understand individual players? That is, what sorts of players are fundamental? What attributes do we see from the top players? How much are they playing? What is their mixture of offense and defense like? What stats are key?\n\n\n\n\nFormula 1\n\nDataset\nResearch questions:\n\nThis dataset includes a number of files across all sorts of topics, including individual drivers, teams, pit stops, tracks, etc. Quickly decide on a few key research questions and get them approved by your instructor.\nFor all research questions, be sure to start with an overview of all key terms and metrics, and their relevance to performance (e.g. what is a pit stop, and why does it matter).\n\n\n\n\nCity of Austin Revenue Budget\n\nDataset\nResearch quetsions:\n\nThis data shows the City of Austin’s budget for revenue. Start by explaining key terms and metrics.\nHow can we understand the distribution of revenue for the City of Austin?\nHow is the distribution of revenue changing across time, both at a macro level, and on a per-department level? What does this help us learn about the priorities for the City of Austin?\n\n\n\n\nNYC Taxi Trips\n\nDataset\nBorough/zones dataset\nResearch questions:\n\nHow is taxi data distributed throughout the city and the five boroughs?\nWhat factors leads to higher tips?\nHow can we understand the spatial and/or temporal distribution of taxi trips?\n\n\n\n\nSan Francisco Building Permits\n\nDataset\nResearch questions:\n\nHow do permit status relate to the other factors in this dataset? For example, what permit status get issued the most, and what leads to them being completed vs withdrawn vs expired, etc?\nHow does location relate to all the other factors present?\nHow does time of year relate to all the other factors present?"
  },
  {
    "objectID": "Projects/mini-project-1/mini-project-1.html#deliverables",
    "href": "Projects/mini-project-1/mini-project-1.html#deliverables",
    "title": "Mini Project 1: EDA",
    "section": "Deliverables",
    "text": "Deliverables\n\nA ~10 minute presentation giving an overview of your data, addressing your research questions, and finishing with a conclusion\nA LinkedIn article explaining how you utilized agents/LLMs to do your EDA process"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html",
    "href": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html",
    "title": "Chapter 2.5: Polynomial Regression",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Create polynomial features and select the optimal degree to avoid overfitting."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html#quick-recap",
    "title": "Chapter 2.5: Polynomial Regression",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nPolynomial features turn X into [X, X², X³, …] to capture curves\nHigher degree = more flexible, but risk of overfitting\nOverfitting: High training score, low validation score\nUse validation set to choose the best degree\n\n\n# Load diamonds dataset\ndiamonds = sns.load_dataset('diamonds')\n\n# Use a subset for faster training\ndiamonds_sample = diamonds.sample(n=5000, random_state=42)\n\n# We'll predict price from carat (single feature for visualization)\n...\n\n# Three-way split: 60% train, 20% val, 20% test\n..."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html#practice",
    "href": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html#practice",
    "title": "Chapter 2.5: Polynomial Regression",
    "section": "Practice",
    "text": "Practice\n\n1. Create degree-2 polynomial features from carat\n\n# Step 1: Create PolynomialFeatures with degree=2\n...\n\n# Step 2: Fit on training data and transform train, val, test\n...\n\n\n\n2. Fit linear regression on polynomial features, calculate train and val R²\n\n# Step 1: Fit LinearRegression on polynomial features\n...\n\n# Step 2: Calculate R² on train and validation\n...\n\n\n\n3. Repeat for degree 3, 4, 5\n\n# Degree 3\n# Step 1: Create PolynomialFeatures(degree=3)\n\n\n# Step 2: Transform data\n\n\n# Step 3: Fit model and calculate scores\n\n\n\n# Degree 4\n\n\n\n# Degree 5\n\n\n\n\n4. Create a table showing degree, train R², val R²\n\n# Create a DataFrame from the results which shows the polynomial degree, as well as the training and validation R² values\n...\n\n\n\n5. Which degree shows the largest train-val gap (overfitting)?\n\n# Visualize the results\n\nYour observation: Which degree has the largest gap between train and val R²? This is a sign of overfitting.\n(Write your answer here)\n\n\n6. Which degree would you choose and why?\n\n# Pick the model with the highest validation R²\n\nYour recommendation: Which degree would you choose for this model? Explain your reasoning.\n(Write your answer here - consider both validation performance AND simplicity)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html#visualize-the-polynomial-fits",
    "href": "In-class activities/Chapter 2/2.5 - Polynomial Regression.html#visualize-the-polynomial-fits",
    "title": "Chapter 2.5: Polynomial Regression",
    "section": "Visualize the polynomial fits",
    "text": "Visualize the polynomial fits\n\n# Create a graph which shows the data points and the fitted polynomial overlaid on top of it"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html",
    "href": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html",
    "title": "Chapter 2.8 and 2.9: Elastic Net",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Understand when Elastic Net combines the benefits of Ridge and Lasso."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html#quick-recap",
    "title": "Chapter 2.8 and 2.9: Elastic Net",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nElastic Net = L1 (Lasso) + L2 (Ridge) combined\nl1_ratio controls the blend:\n\nl1_ratio = 0 → Pure Ridge\nl1_ratio = 1 → Pure Lasso\nl1_ratio = 0.5 → Equal mix\n\nWhen to use: Features are correlated AND you want feature selection\n\n\n# Load and prepare Auto MPG data (same as previous activity)\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\ncolumns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']\ndf = pd.read_csv(url, delim_whitespace=True, names=columns, na_values='?')\ndf = df.dropna().drop('car_name', axis=1)\n\n# Set X to all columns except the target (\"mpg\"), and y equal to just the target column\n...\n\n# Train/test split the data\n..."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html#practice",
    "href": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html#practice",
    "title": "Chapter 2.8 and 2.9: Elastic Net",
    "section": "Practice",
    "text": "Practice\n\n1. Fit ElasticNet with l1_ratio=0.5, alpha=1\n\n# Step 1: Create ElasticNet with l1_ratio=0.5 and alpha=1\n...\n\n# Step 2: Fit on scaled training data\n...\n\n# Step 3: Display coefficients\n...\n\n\n\n2. How many coefficients are zero? Compare to Lasso\n\n# Fit Lasso for comparison\n...\n\n# How many coefficients did ElasticNet shrink to zero, and how many did Lasso shrink to zero?\n...\n\nYour observation: Does Elastic Net eliminate as many features as Lasso?\n(Write your answer here)\n\n\n3. Fit with l1_ratio=0.1 (more Ridge-like) - fewer zeros?\n\n# Step 1: Create ElasticNet with l1_ratio=0.1\n...\n\n# Step 2: Fit and count zeros\n...\n\n# Step 3: Print R² results\n...\n\n\n\n4. Fit with l1_ratio=0.9 (more Lasso-like) - more zeros?\n\n# Step 1: Create ElasticNet with l1_ratio=0.9\n...\n\n# Step 2: Fit and count zeros\n...\n\n# Step 3: Print R² results\n...\n\nYour observation: Summarize the relationship between l1_ratio and number of zero coefficients:\n\n\n\nl1_ratio\nZero Coefficients\nR²\n\n\n\n\n0.1 (Ridge-like)\n\n\n\n\n0.5 (balanced)\n\n\n\n\n0.9 (Lasso-like)\n\n\n\n\n\n(Fill in the table with your results)\n\n\n5. Use ElasticNetCV to find optimal parameters\n\n# ElasticNetCV tunes both alpha and l1_ratio. Pick a few l1 ratios between 0 and 1.\n...\n\n# Print out which alpha and l1 ratio were best, and the best overall test R²\n...\n\n\n# What features still remain (coefficients didn't shrink to zero)?\n...\n\n\n\n6. Compare R² scores: OLS vs Ridge vs Lasso vs Elastic Net\n\n# Fit all models with CV-tuned parameters\nfrom sklearn.linear_model import RidgeCV, LassoCV\n\n# Fit a linear regression model (no regularization)\n...\n\n# Fit a Ridge CV model\n...\n\n# Fit a Lasso CV model\n...\n\n# Compare all models, which is best?\n...\n\nYour recommendation: Based on this comparison, which model would you choose for this dataset? Consider: - Performance (R²) - Simplicity (number of features) - Interpretability\n(Write your recommendation here)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html#summary-when-to-use-each-method",
    "href": "In-class activities/Chapter 2/2.8-2.9 - Elastic Net.html#summary-when-to-use-each-method",
    "title": "Chapter 2.8 and 2.9: Elastic Net",
    "section": "Summary: When to Use Each Method",
    "text": "Summary: When to Use Each Method\n\n\n\n\n\n\n\n\nMethod\nBest When…\nKey Characteristic\n\n\n\n\nOLS\nFeatures are independent, no overfitting concern\nNo regularization\n\n\nRidge\nFeatures are correlated, want to keep all features\nShrinks but keeps all\n\n\nLasso\nWant automatic feature selection, sparse model\nSets some to zero\n\n\nElastic Net\nFeatures are correlated AND want selection\nBest of both worlds"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.1 - Train Test Split.html",
    "href": "In-class activities/Chapter 2/2.1 - Train Test Split.html",
    "title": "Chapter 2.1: Train/Test Split",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Practice proper data splitting and understand why the validation set matters."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.1 - Train Test Split.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.1 - Train Test Split.html#quick-recap",
    "title": "Chapter 2.1: Train/Test Split",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nTraining set: Model learns from this data\nValidation set: Used to tune hyperparameters and compare models\nTest set: Final evaluation only - touched ONCE at the very end\n\nWhy three sets? If you tune on test data, you’re giving the model the answers to the test.\n\n# For this activity, we'll use a simpler dataset - California Housing\nfrom sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing(as_frame=True)\ndf = housing.frame\ndf.head()\n\n\n# Look at the data, check for missing values, check for weird data, etc.\n\n\n# Set the X (MedHouseVal) and y (everything else)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.1 - Train Test Split.html#practice",
    "href": "In-class activities/Chapter 2/2.1 - Train Test Split.html#practice",
    "title": "Chapter 2.1: Train/Test Split",
    "section": "Practice",
    "text": "Practice\n\n1. Split data into train (60%), validation (20%), test (20%)\nYou’ll need to call train_test_split twice: 1. First split: 80% train+val, 20% test 2. Second split: Split the 80% into 75% train, 25% val (which gives 60%/20% overall)\n\n# Step 1: Split into train (80%) and test (20%)\n\n\n\n# Step 2: Split training data into into train (75% of 80% = 60%) and validation (25% of 80% = 20%)\n\n\n\n2. Assert that no data was lost in the split\nThe total number of samples across all three sets should equal the original dataset size.\n\n# Step 1: Calculate total samples across train, val, and test\n\n\n# Step 2: Assert it equals len(X)\n\n\n# Step 3: Print all sizes to verify everything looks correct\n\n\n\n3. Use AI - Fit LinearRegression on train, predict on val AND test\n\n# Step 1: Create and fit a LinearRegression model on training data\n\n\n# Step 2: Calculate R² score on validation set\n\n\n# Step 3: Calculate R² score on test set\n\n\n\n4. Use AI - Compare val score vs test score - are they similar?\nIf the validation and test scores are similar, that’s a good sign - it means the validation set is giving us a reliable estimate of how the model will perform on unseen data.\n\n# Print both scores and calculate the difference\n\n\n# Are they within 0.02 of each other? That's pretty good!\n\nYour interpretation: Are the validation and test scores similar? What does this tell you?\n(Write your answer here)\n\n\n5. Use AI - Use cross_val_score with 5 folds on training data\nCross-validation gives you multiple estimates of model performance, which is more reliable than a single validation score.\n\n# Step 1: Create a new LinearRegression model\n# Step 2: Use cross_val_score with cv=5 on the TRAINING data (not all data!)\n\n\n# Print all 5 scores\n\n\n\n6. Use AI - What’s the mean and std of cross-validation scores?\n\n# Calculate mean and standard deviation of the CV scores\n\nYour interpretation: How does the CV mean compare to your single validation score from earlier? Is the standard deviation large or small?\n(Write your answer here)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.1 - Train Test Split.html#discussion-question",
    "href": "In-class activities/Chapter 2/2.1 - Train Test Split.html#discussion-question",
    "title": "Chapter 2.1: Train/Test Split",
    "section": "Discussion Question",
    "text": "Discussion Question\nWhy shouldn’t you tune hyperparameters on the test set? What would happen if you did?\n(Discuss with a neighbor)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.7 - Regularization.html",
    "href": "In-class activities/Chapter 2/2.7 - Regularization.html",
    "title": "Chapter 2.7: Ridge and Lasso Regularization",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Compare Ridge vs Lasso, understand coefficient shrinkage and feature selection."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.7 - Regularization.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.7 - Regularization.html#quick-recap",
    "title": "Chapter 2.7: Ridge and Lasso Regularization",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nRidge (L2): Shrinks all coefficients toward zero, but never exactly zero\nLasso (L1): Can shrink coefficients to exactly zero → automatic feature selection\nAlpha: Controls regularization strength (higher = more shrinkage)\nImportant: Always scale features before regularization!\n\n\n# Load Auto MPG dataset\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\ncolumns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']\ndf = pd.read_csv(url, delim_whitespace=True, names=columns, na_values='?')\n\n# Drop rows with missing values and car_name (not numeric)\ndf = df.dropna()\ndf = df.drop('car_name', axis=1)\n\ndf.head()\n\n\n# Set X to all columns except the target (\"mpg\"), and y equal to just the target column\n...\n\n# Train/test split the data\n..."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.7 - Regularization.html#practice",
    "href": "In-class activities/Chapter 2/2.7 - Regularization.html#practice",
    "title": "Chapter 2.7: Ridge and Lasso Regularization",
    "section": "Practice",
    "text": "Practice\n\n1. Scale features using StandardScaler (fit on train, transform both)\n\n# Step 1: Create StandardScaler\n...\n\n# Step 2: Fit on training data ONLY, then transform both train and test\n...\n\n# Verify scaling worked (mean should be ~0, std should be ~1 for train)\n...\n\n\n\n2. Fit LinearRegression, note the coefficients\n\n# Step 1: Fit LinearRegression on scaled data\n...\n\n# Step 2: Display coefficients\n...\n\n\n\n3. Fit Ridge(alpha=1), compare coefficients - are they smaller?\n\n# Step 1: Create and fit Ridge with alpha=1\n...\n\n# Step 2: Add Ridge coefficients to the comparison DataFrame\n...\n\nYour observation: Are the Ridge coefficients smaller (closer to 0) than OLS? Which features shrank the most?\n(Write your answer here)\n\n\n4. Fit Lasso(alpha=1), which coefficients are exactly 0?\n\n# Step 1: Create and fit Lasso with alpha=1\n...\n\n# Step 2: Add Lasso coefficients to the comparison DataFrame\n...\n\n\n\n5. Fit Lasso with alpha=0.1, 0.5, 1, 2 - how do zero coefficients change?\n\n# Compare Lasso with different alpha values\n...\n\nYour observation: As alpha increases, what happens to the number of zero coefficients? What happens to R²?\n(Write your answer here)\n\n\n6. Use RidgeCV to find optimal alpha\n\n# Step 1: Create RidgeCV with a range of alphas\n...\n\n# Step 2: Fit on training data\n...\n\n# Step 3: Get the best alpha\n...\n\n\n\n7. Use LassoCV to find optimal alpha\n\n# Step 1: Create LassoCV (it automatically chooses alphas)\n...\n\n# Step 2: Fit on training data\n...\n\n# Step 3: Get the best alpha and see which features remain\n...\n\n\n\n8. Which regularization method would you choose for this data and why?\n\n# Compare all models to see which is best\n...\n\nYour recommendation: Based on the results, which method would you choose? Consider: - Performance (R² scores) - Interpretability (how many features?) - Simplicity\n(Write your recommendation and reasoning here)"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html",
    "href": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html",
    "title": "Chapter 1.6 and 1.7: Iterative EDA",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Practice the iterative, question-driven nature of EDA and document your findings."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#loading-the-data",
    "href": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#loading-the-data",
    "title": "Chapter 1.6 and 1.7: Iterative EDA",
    "section": "Loading the Data",
    "text": "Loading the Data\nWe’ll use the Palmer Penguins dataset, which contains measurements of penguins from three different species on islands in Antarctica. Each row is one penguin, with measurements like bill length, flipper length, and body mass.\n\n# Load the penguins dataset from Seaborn\npenguins = sns.load_dataset('penguins')\n\n\n# View your data, check for missing values, check for weird outliers/values\n...\n\n\n# Make a few graphs to better understand your data\n...\n\n\n# What are all the unique values in the \"species\" column?\n..."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#the-iterative-mindset",
    "href": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#the-iterative-mindset",
    "title": "Chapter 1.6 and 1.7: Iterative EDA",
    "section": "The Iterative Mindset",
    "text": "The Iterative Mindset\nEDA isn’t a checklist. It’s more like a conversation with your data:\n\nAsk a question → “What’s the average bill length?”\nAnswer it → Compute the statistic or make a plot\nNotice something interesting → “Hmm, that’s higher than I expected…”\nAsk a follow-up question → “Does it vary by species?”\nRepeat\n\nLet’s walk through an example of this process."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#worked-example-following-a-thread",
    "href": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#worked-example-following-a-thread",
    "title": "Chapter 1.6 and 1.7: Iterative EDA",
    "section": "Worked Example: Following a Thread",
    "text": "Worked Example: Following a Thread\nStarting question: What’s the average bill length?\n\n# Answer: What's the average bill length?\n...\n\nWhat do you see:\nFollow-up: We have multiple species, maybe it differs by species?\n\n# Answer: Average bill length by species (use groupby)\n...\n\n\n# Make an appropriate graph to visualize distribution (not just average) of bill length by species\nsns.boxplot(data=penguins, x='species', y='bill_length_mm')\nplt.title('Bill Length by Species')\nplt.ylabel('Bill Length (mm)')\nplt.show()\n\nWhat do you see:\nAnother follow-up: Is this true for other measurements too?\n\n# Check body mass by species\nsns.boxplot(data=penguins, x='species', y='body_mass_g')\nplt.title('Body Mass by Species')\nplt.ylabel('Body mass (g)')\nplt.show()\n\nNow Gentoo is the biggest! The pattern is different for body mass than for bill length.\nInteresting follow-up: Bigger body mass sometimes (but not always?) seems to also have a longer bill. Are these related?\n\nsns.scatterplot(data=penguins, x='body_mass_g', y='bill_length_mm')\nplt.xlabel('Body Mass (g)')\nplt.ylabel('Bill Length (mm)')\nplt.title('Body mass vs Bill length')\nplt.show()\n\nThere is a relationship, but there’s also some weird clustering (e.g. in the top-left quadrant). Could it differ by species?\n\nsns.scatterplot(data=penguins, x='body_mass_g', y='bill_length_mm', hue='species')\nplt.xlabel('Body Mass (g)')\nplt.ylabel('Bill Length (mm)')\nplt.title('Body mass vs Bill length')\nplt.show()\n\nThat’s quite a clear-cut relationship!\n\nDocumenting What We Found\nAs you explore, document your findings. Here’s what we discovered:\nFinding 1: Bill length varies significantly by species. - Adelie: ~39mm (shortest) - Chinstrap: ~49mm (longest) - Gentoo: ~47mm\nFinding 2: Body mass has a different pattern. - Gentoo penguins are the heaviest (~5000g) - Adelie and Chinstrap are similar (~3700g)\nFinding 3: Body mass and bill length have a clear positive, linear relationship. - For Adelie and Gentoo penguins, the relationship basically looks the same, just that Gentoo are heavier. - For Chinstrap penguins, the relationshp is a little weaker, and compared to other penguins, for the same body mass they have significantly longer bills.\nQuestion for later: Why does Gentoo have a different pattern? Is it related to their environment or diet?"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#your-turn-explore-a-question",
    "href": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#your-turn-explore-a-question",
    "title": "Chapter 1.6 and 1.7: Iterative EDA",
    "section": "Your Turn: Explore a Question",
    "text": "Your Turn: Explore a Question\nNow it’s your turn to follow a thread of inquiry. Choose ONE of these starting questions:\n\nDo larger penguins have longer bills? (Is there a relationship between body mass and bill length?)\nDoes body mass differ by island? (Are penguins on some islands bigger?)\nIs there a relationship between flipper length and bill depth?\n\nFor your chosen question, follow these steps:\n\nStep 1: Answer Your Starting Question\nCompute a relevant statistic or create a visualization to answer your question.\n\n# Your code here - answer your starting question\n\n\n\nStep 2: Document What You Found\nWrite 1-2 sentences about what you discovered. What pattern do you see? Does anything surprise you?\nYour finding:\n(Write your finding here)\n\n\nStep 3: Ask a Follow-up Question\nBased on what you found, what’s the natural next question? Write it down.\nYour follow-up question:\n(Write your question here)\n\n\nStep 4: Investigate Your Follow-up Question\n\n# Your code here - investigate your follow-up question\n\n\n\nStep 5: Document Your Final Finding\nWhat did you learn from this exploration? Write a brief summary (2-3 sentences).\nYour summary:\n(Write your summary here)"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#wrap-up-discussion",
    "href": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#wrap-up-discussion",
    "title": "Chapter 1.6 and 1.7: Iterative EDA",
    "section": "Wrap-up Discussion",
    "text": "Wrap-up Discussion\nTurn to a neighbor and share: 1. Which question did you start with? 2. What did you find? 3. What follow-up question did you ask?\nNotice that everyone’s path was different - and that’s okay! EDA is about following your curiosity."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#key-takeaways",
    "href": "In-class activities/Chapter 1/1.6-1.7 - Iterative EDA.html#key-takeaways",
    "title": "Chapter 1.6 and 1.7: Iterative EDA",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nEDA is iterative: Each answer leads to new questions\nDocument as you go: Write down findings and decisions while they’re fresh\nThere’s no “right” path: Different analysts will explore different directions\nFollow your curiosity: When something surprises you, dig deeper\n\nThe best EDA happens when you stay curious and keep asking “why?” and “what if?”"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.1 - Setup and Prompting.html",
    "href": "In-class activities/Chapter 1/1.1 - Setup and Prompting.html",
    "title": "Setup checklist",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nWork through these items with your instructor."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.1 - Setup and Prompting.html#prompting-practice",
    "href": "In-class activities/Chapter 1/1.1 - Setup and Prompting.html#prompting-practice",
    "title": "Setup checklist",
    "section": "Prompting practice",
    "text": "Prompting practice\nSuppose you wanted to make a tic-tac-toe game. You want to use Cursor’s Agent (or another LLM) to help you create it. Discuss the following questions:\n\nWhat information should you be giving the LLM? Is it enough to say “make tic tac toe”? If not, what else would you include? Make a comprehensive list.\nYou should always start complex projects like this with a planning prompt. This is a prompt telling the LLM that you are just planning right now, you don’t want to actually write any code yet. Write out your planning prompt.\nAfter it writes the plan, look over it! Are you happy with it? Are there changes that should be made? Did you think of a new feature you want to implement? Don’t be afraid to go back and forth with your LLM to refine your plan.\nOnce you have a plan, give it back to the LLM, and ask it to create the steps that it will go through in creating this game. Again, we don’t want it to write any code yet! All we want is a detailed plan.\nOnce your plan is complete, ask your LLM to do the first step (from the steps determined in what you just did). Test it out! Does it run? Does anything work? Be as critical as possible to make sure things are how you want. Once you’ve got that completed, move on to the next step."
  },
  {
    "objectID": "Assignments/Chapter 1 - EDA/chapter-1-homework.html",
    "href": "Assignments/Chapter 1 - EDA/chapter-1-homework.html",
    "title": "Chapter 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "",
    "text": "This homework is meant to be completed with AI assistance (such as an in-browser LLM like ChatGPT, and/or using a CLI like Claude Code) to practice scaling your data exploration work.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nThe prompts you used with the AI assistant(s)\n\n\n\nYou’ll be working with the California Housing Dataset, which contains information about California census block groups. You should load it using sklearn.datasets.fetch_california_housing and then create a Pandas DataFrame with the feature columns plus the target.\nData Set Characteristics:\n\nNumber of Instances: 20640\nNumber of Attributes: 8 numeric, predictive attributes and the target\n\nAttribute Information:\n\nMedInc: Median income in block group\nHouseAge: Median house age in block group\nAveRooms: Average number of rooms per household\nAveBedrms: Average number of bedrooms per household\nPopulation: Block group population\nAveOccup: Average number of household members\nLatitude: Block group latitude\nLongitude: Block group longitude\n\nTarget Variable:\n\nMedHouseVal: Median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n\nNotes:\n\nThis dataset comes from the StatLib repository: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\nThis dataset was derived from the 1990 U.S. census and uses one row per census block group.\nA block group typically has a population of 600 to 3,000 people.\nSince rooms/bedrooms are averages per household, some values can be surprisingly large.\nMissing Attribute Values: None\n\nUse the following code to load the dataset into a DataFrame named df:\n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\nhousing = fetch_california_housing(as_frame=True)\ndf = housing.frame\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422"
  },
  {
    "objectID": "Assignments/Chapter 1 - EDA/chapter-1-homework.html#instructions",
    "href": "Assignments/Chapter 1 - EDA/chapter-1-homework.html#instructions",
    "title": "Chapter 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "",
    "text": "This homework is meant to be completed with AI assistance (such as an in-browser LLM like ChatGPT, and/or using a CLI like Claude Code) to practice scaling your data exploration work.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nThe prompts you used with the AI assistant(s)\n\n\n\nYou’ll be working with the California Housing Dataset, which contains information about California census block groups. You should load it using sklearn.datasets.fetch_california_housing and then create a Pandas DataFrame with the feature columns plus the target.\nData Set Characteristics:\n\nNumber of Instances: 20640\nNumber of Attributes: 8 numeric, predictive attributes and the target\n\nAttribute Information:\n\nMedInc: Median income in block group\nHouseAge: Median house age in block group\nAveRooms: Average number of rooms per household\nAveBedrms: Average number of bedrooms per household\nPopulation: Block group population\nAveOccup: Average number of household members\nLatitude: Block group latitude\nLongitude: Block group longitude\n\nTarget Variable:\n\nMedHouseVal: Median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n\nNotes:\n\nThis dataset comes from the StatLib repository: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\nThis dataset was derived from the 1990 U.S. census and uses one row per census block group.\nA block group typically has a population of 600 to 3,000 people.\nSince rooms/bedrooms are averages per household, some values can be surprisingly large.\nMissing Attribute Values: None\n\nUse the following code to load the dataset into a DataFrame named df:\n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\nhousing = fetch_california_housing(as_frame=True)\ndf = housing.frame\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422"
  },
  {
    "objectID": "Assignments/Chapter 1 - EDA/chapter-1-homework.html#questions",
    "href": "Assignments/Chapter 1 - EDA/chapter-1-homework.html#questions",
    "title": "Chapter 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Questions",
    "text": "Questions\n\nQuestion 0:\n\nWhich LLM assistant(s) did you use? Be specific. For example, “I used ChatGPT in the browser”, or “I used Claude Code in the terminal”.\n\n\n\nQuestion 1 (5 points)\nUse AI to create a comprehensive data profile that includes:\n\nSummary statistics for ALL numerical columns (including the target)\nA report on missing values (which columns, how many, what percentage)\nIdentification of potential outliers in numerical columns (using percentile method: values below 1st or above 99th percentile)\n\nDeliverables:\n\nThe prompt(s) you used\nThe code generated\nThe output/report\n\n\n\n\nQuestion 2 (5 points)\nPrompt the AI to create a function that generates a complete set of univariate visualizations for the dataset. This should include:\n\nHistograms for all numerical columns (including the target)\nAll plots should be saved to a folder called univariate_plots/\nEach plot should have proper labels and titles\n\nRun the function and submit all generated plots along with your code.\nDeliverables:\n\nThe prompt(s) you used\nThe code\nAll generated visualizations\n\n\n\n\nQuestion 3 (5 points)\nUse AI to create a pair plot (using sns.pairplot()) showing relationships between key numerical variables: MedHouseVal, MedInc, HouseAge, and AveRooms. Color the points by binned MedHouseVal (for example, low/medium/high using pd.qcut).\nBased on the pair plot, answer:\n\nWhich pair of variables shows the strongest visual relationship?\nAre there any variables that don’t seem to relate to MedHouseVal?\nDo you notice any differences across the low/medium/high value groups?\n\nDeliverables:\n\nThe prompt(s) you used\nThe pair plot visualization\nYour written interpretation (3-4 sentences)\n\n\n\n\nQuestion 4 (6 points)\nPrompt the AI to create a function that compares MedHouseVal distributions across different subgroups. Specifically:\n\nCompare MedHouseVal by binned MedInc (e.g., low/medium/high)\nCompare MedHouseVal by binned HouseAge (e.g., 0-10, 10-30, 30+)\nCompare MedHouseVal by latitude region (e.g., North/Central/South using cutoffs)\nGenerate both statistical summaries AND visualizations (box plots or violin plots)\n\nRun this function and write 2-3 paragraphs interpreting the results. What insights did you gain about what drives housing values?\nDeliverables:\n\nThe prompt(s) you used\nThe code\nAll visualizations\nYour written interpretation\n\n\n\n\nQuestion 5 (6 points)\nAsk the AI to help you identify potential outliers in the MedHouseVal column. Your approach should:\n\nDefine outliers as values below the 1st percentile or above the 99th percentile\nCreate visualizations showing the distribution with outliers highlighted\nDisplay summary statistics for the full dataset and for data excluding outliers\nInvestigate the outliers: are they data errors or legitimate expensive/cheap houses?\n\nHow many potential outliers did you identify? Should they be removed, or do they represent legitimate data points? Justify your answer.\nDeliverables:\n\nThe prompt(s) you used\nThe code\nVisualizations showing outliers\nWritten analysis (4-5 sentences)\n\n\n\n\nQuestion 6 (6 points)\nUse AI to create a comprehensive test suite for the housing dataset. Your test suite should include at least 5 unit tests that check:\n\nData types are correct\nNo negative values in columns where they don’t make sense (MedInc, AveRooms, AveBedrms, Population, AveOccup)\nReasonable ranges for geographic coordinates (Latitude between 32 and 42, Longitude between -125 and -114)\nReasonable ranges for HouseAge (e.g., 0 to 100)\nNo duplicate rows\n\nRun all tests and report which (if any) failed.\nDeliverables:\n\nThe prompt(s) you used\nThe complete test suite code\nTest results\n\n\n\n\nQuestion 7 (5 points)\nPrompt the AI to create multiple scatter plots exploring the relationship between MedHouseVal and other numerical variables. You should generate:\n\nMedHouseVal vs. MedInc (colored by Latitude)\nMedHouseVal vs. HouseAge (colored by binned HouseAge)\nMedHouseVal vs. AveRooms (with a trend line)\nMedHouseVal vs. AveOccup (colored by Longitude)\n\nFor each plot, write 1-2 sentences describing what you observe.\nDeliverables:\n\nThe prompt(s) you used\nAll scatter plots\nYour written observations\n\n\n\n\nQuestion 8 (6 points)\nUse AI to create a function that systematically explores how MedHouseVal relates to engineered categorical variables created from numeric columns. For each engineered categorical variable, the function should:\n\nCreate a bar plot showing average MedHouseVal by category\nGenerate a box plot showing the distribution of MedHouseVal across categories\nCalculate and display summary statistics (mean, median, count) for each category\nSave all plots to a folder called categorical_analysis/\n\nUse at least three engineered categories such as:\n\nMedInc binned into low/medium/high\nHouseAge binned into 3-4 groups\nLatitude binned into North/Central/South\n\nRun this function and write a paragraph identifying which engineered categories seem most important in determining house values.\nDeliverables:\n\nThe prompt(s) you used\nThe function code\nAll generated visualizations\nYour written analysis\n\n\n\n\nQuestion 9 (6 points)\nUse AI to create a function that performs a complete exploratory data analysis for ANY numerical column in the dataset. The function should:\n\nTake a DataFrame and column name as inputs\nGenerate: histogram, box plot, and summary statistics\nIdentify potential outliers using percentiles (values below 1st or above 99th percentile)\nCreate a markdown-formatted report with findings\nSave all plots to a specified folder\n\nRun this function on three different columns: MedHouseVal, MedInc, and AveRooms.\nDeliverables:\n\nThe prompt(s) you used\nThe function code\nAll generated visualizations\nThe markdown reports for each column\n\n\n\n\nQuestion 10 (7 points)\nFor this final question, ask the AI to help you create a comprehensive EDA report. This should be a multi-page visualization that includes:\n\nOverall dataset summary (rows, columns, missing values)\nDistribution plots for key numerical variables\nCorrelation analysis\nMedHouseVal analysis by different engineered categorical variables\nAny interesting patterns or anomalies you discovered\n\nWrite a 1-page executive summary (300-400 words) that you would present to a stakeholder who doesn’t know anything about data science. Explain:\n\nWhat the dataset contains\nKey findings from your analysis\nWhat factors seem most important in determining house values\nAny data quality issues or limitations\nRecommendations for next steps\n\nDeliverables:\n\nThe prompt(s) you used\nAll visualizations in your EDA report\nYour executive summary\nReflection: How did using AI change your approach to this comprehensive analysis compared to doing it by hand?"
  },
  {
    "objectID": "Assignments/Chapter 1 - EDA/chapter-1-homework.html#submission-guidelines",
    "href": "Assignments/Chapter 1 - EDA/chapter-1-homework.html#submission-guidelines",
    "title": "Chapter 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nSubmit a ZIP file containing one folder for each problem. Within each folder you should have one file for each of the following:\n\nCode files: All .py files or Jupyter notebooks\nVisualizations folder: All plots and charts generated\nWritten responses: A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\nData: Include your cleaned/modified datasets if you created any"
  },
  {
    "objectID": "Assignments/Chapter 3 - Classification/chapter-3-homework.html",
    "href": "Assignments/Chapter 3 - Classification/chapter-3-homework.html",
    "title": "Chapter 3 Homework: Classification Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is meant to be completed with AI assistance (like Gemini CLI) to practice classification workflows, diagnostics, and model comparison at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nThe prompts you used with the AI assistant(s)\n\n\n\nYou’ll be working with three datasets:\nDataset 1: titanic.csv - Titanic passenger survival data with columns:\n\nSurvived: Whether passenger survived (0=No, 1=Yes) (TARGET)\nPclass: Passenger class (1, 2, or 3)\nSex: Passenger sex\nAge: Passenger age\nSibSp: Number of siblings/spouses aboard\nParch: Number of parents/children aboard\nFare: Ticket fare\nEmbarked: Port of embarkation (C, Q, S)\n\nLoad this data using the data hosted here: https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv.\nDataset 2: spotify.csv - Spotify song data with columns:\n\npopularity: Song popularity score (0-100)\ndanceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo: Audio features\n\nYou will create a binary target: viral_hit = popularity &gt; 80.\nDownload the Spotify data from here: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset\nDataset 3: isp_customers.csv - Internet service provider customer data with columns:\n\nChurn: Whether customer churned (TARGET)\ngender, SeniorCitizen, Partner, Dependents: Demographics\ntenure: Months with company\nPhoneService, InternetService, Contract: Service details\nMonthlyCharges, TotalCharges: Billing information\n\nDownload the ISP customer data from here: https://www.kaggle.com/datasets/mehmetsabrikunt/internet-service-churn.\n\n\n\n\nLoad the titanic.csv dataset and prepare it for classification:\n\nDrop rows with missing values in Age, Fare, and Survived\nCreate feature matrix X using: Pclass, Age, SibSp, Parch, Fare\nCreate target vector y using Survived\nSplit into train/test sets (80/20, random_state=42)\nHow many passengers are in each class (survived vs not survived) in the training set? Is this balanced or imbalanced?\n\n\n\n\n\nFit a Logistic Regression model on the Titanic data:\n\nFit the model on X_train and y_train\nGet predictions on X_test\nCreate and display a confusion matrix\nCalculate accuracy, precision, recall, and F1 score manually from the confusion matrix values\nVerify your calculations match sklearn’s metric functions\n\n\n\n\n\nInterpret the logistic regression results:\n\nExtract the coefficients and create a DataFrame showing feature names and their coefficients\nCalculate the odds ratio for each feature (exp of coefficient)\nWhich feature has the strongest positive association with survival? Strongest negative?\nInterpret the Pclass coefficient in plain language: “For each unit increase in passenger class…”\nWhy should we be careful interpreting these coefficients?\n\n\n\n\n\nFit a Decision Tree classifier on the Titanic data:\n\nFit a tree with max_depth=3\nCalculate training and test accuracy\nExtract and plot feature importances as a horizontal bar chart\nWhich feature does the decision tree consider most important? Does this match the logistic regression coefficients?\n\n\n\n\n\nInvestigate the bias-variance tradeoff with decision trees:\n\nFit decision trees with max_depth values: [1, 2, 3, 5, 7, 10, 15, 20, None]\nFor each, calculate training and test accuracy\nPlot both accuracies vs. max_depth on the same graph\nAt what depth does overfitting become apparent?\nWhat max_depth would you choose for this dataset? Why?\n\n\n\n\n\nFit a Random Forest classifier:\n\nFit with n_estimators=100, max_depth=5, random_state=42\nCompare its test accuracy to the single decision tree with max_depth=5\nExtract feature importances from both models and compare them\nWhy might the feature importances differ between a single tree and a random forest?\n\n\n\n\n\nFit a k-Nearest Neighbors classifier:\n\nScale the features using StandardScaler (fit on train, transform both)\nFit k-NN with k=5 on the scaled data\nCalculate test accuracy\nTest k values [1, 3, 5, 7, 10, 15, 20] and plot test accuracy vs. k\nWhat k value gives the best performance? What happens at very small and very large k?\n\n\n\n\n\nCreate an ROC curve for the logistic regression model:\n\nGet probability predictions for the positive class\nCalculate FPR and TPR using roc_curve\nCalculate the AUC score\nPlot the ROC curve with the diagonal reference line and display AUC in the legend\n\n\n\n\n\nCompare multiple classifiers:\n\nFit Logistic Regression, Decision Tree (depth=5), Random Forest (100 trees), and k-NN (k=5, scaled)\nCalculate test accuracy for each\nCreate a bar chart comparing accuracies\nGet ROC curves for all models that provide predict_proba and plot them on the same axes\nBased on both accuracy and AUC, which model performs best?\n\n\n\n\n\nExplain in your own words (3-4 sentences each):\n\nWhen would you choose precision as your primary metric? Give a real-world example.\nWhen would you choose recall as your primary metric? Give a real-world example.\nWhat is the F1 score and when is it useful?\nWhy is accuracy often misleading for imbalanced datasets?\n\n\n\n\nUse AI to perform comprehensive hyperparameter tuning for a Decision Tree on the Titanic dataset.\nTest combinations of: - max_depth: [2, 3, 5, 7, 10, 15] - min_samples_split: [2, 5, 10, 20] - min_samples_leaf: [1, 2, 5, 10]\nYour code should:\n\nUse GridSearchCV with 5-fold cross-validation\nFind the best hyperparameter combination\nDisplay the top 5 parameter combinations\nFit the best model and evaluate on test set\n\nDeliverables:\n\nCode\nBest parameters and test accuracy\nWritten interpretation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to create a comprehensive model comparison on the Titanic dataset.\nCompare these classifiers: - Logistic Regression - Decision Tree (tuned) - Random Forest (tuned) - SVM with RBF kernel - k-NN (tuned)\nYour code should:\n\nUse GridSearchCV to tune each model\nCreate a comparison table with accuracy, precision, recall, F1, and AUC\nPlot all ROC curves on the same graph\nCreate a heatmap of the comparison metrics\n\nWrite a paragraph recommending which model to use and why.\nDeliverables:\n\nCode\nComparison table and visualizations\nROC curve comparison plot\nWritten recommendation (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\n\nUse AI to analyze class imbalance in the Spotify dataset.\nCreate a binary classification target: viral_hit = popularity &gt; 80\nYour code should:\n\nShow the class distribution\nFit a baseline model (Logistic Regression) without handling imbalance\nApply three strategies: class_weight=‘balanced’, SMOTE, random undersampling\nCompare performance using precision, recall, F1, and AUC for each strategy\nCreate confusion matrices for each approach\n\nWhich strategy works best for identifying viral hits?\nDeliverables:\n\nCode\nClass distribution visualization\nPerformance comparison table\nConfusion matrices for each approach\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to visualize decision boundaries for different classifiers.\nUsing the Titanic dataset with only Age and Fare as features:\n\nCreate decision boundary plots for:\n\nLogistic Regression\nDecision Tree (depth=3)\nDecision Tree (depth=10)\nRandom Forest\nSVM (linear kernel)\nSVM (RBF kernel)\nk-NN (k=3)\nk-NN (k=20)\n\n\nArrange all 8 plots in a 2x4 grid with consistent axis limits and color schemes.\nHow do the decision boundaries differ? Which creates the most complex boundaries?\nDeliverables:\n\nCode\nDecision boundary visualization (2x4 grid)\nWritten comparison (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\n\nUse AI to analyze feature importance across different models.\nUsing the Titanic dataset (all numeric features):\n\nExtract feature importances from:\n\nLogistic Regression (absolute coefficients)\nDecision Tree\nRandom Forest\nPermutation importance for k-NN and SVM\n\nCreate a heatmap showing feature importance rankings across all models\n\nWhich features are consistently important? Which differ between models?\nDeliverables:\n\nCode\nFeature importance heatmap\nWritten analysis (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to perform cross-dataset model evaluation.\nTrain models on the Titanic dataset and ISP customer dataset (predicting churn):\n\nFor each dataset, tune and fit: Logistic Regression, Random Forest, k-NN\nCompare how well each model type performs across the two datasets\nCreate a comparison table showing model performance on each dataset\n\nDo the same model types perform best on both datasets? Why or why not?\nDeliverables:\n\nCode for both datasets\nCross-dataset comparison table\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to create an ensemble voting classifier.\nUsing the Titanic dataset:\n\nCreate a VotingClassifier combining:\n\nLogistic Regression\nRandom Forest\nSVM (with probability=True)\n\nUse both ‘hard’ and ‘soft’ voting\nCompare the ensemble to individual models\n\nDoes the ensemble outperform the best individual model?\nDeliverables:\n\nCode\nPerformance comparison table\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\n\nUse AI to analyze k-NN with different distance metrics.\nUsing the ISP customer dataset (with a mix of categorical and numerical features):\n\nTest distance metrics: euclidean, manhattan, cosine\nFor categorical features, use appropriate encoding\nCompare performance across metrics\nAnalyze which metric works best for this mixed-type data\n\nHow does the choice of distance metric affect performance?\nDeliverables:\n\nCode\nPerformance comparison table\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\n\nFor this final question, ask AI to help you conduct a complete classification analysis from start to finish on the ISP customer dataset (predicting churn).\nYour analysis should include:\n\nExploratory Data Analysis:\n\nSummary statistics\nClass distribution\nFeature distributions by target class\nCorrelation analysis\n\nData Preparation:\n\nHandle missing values\nEncode categorical variables\nFeature scaling (where appropriate)\nTrain/validation/test split (60/20/20)\n\nModel Development:\n\nFit multiple models: Logistic Regression, Decision Tree, Random Forest, SVM, k-NN\nHyperparameter tuning using validation set\nHandle class imbalance appropriately\n\nModel Evaluation:\n\nCompare all models using appropriate metrics (given potential imbalance)\nROC curves for all models\nSelect best model based on business context (what matters more: finding churners or not falsely flagging loyal customers?)\n\nFinal Model:\n\nEvaluate on test set (only once!)\nFeature importance analysis\nCreate confusion matrix with business interpretation\n\n\nWrite a 1-page executive summary (300-400 words) that explains:\n\nThe business problem (customer churn prediction)\nData characteristics and challenges\nModels tested and evaluation approach\nHow the best model was selected\nPerformance on test set\nKey features predicting churn\nBusiness recommendations\n\nDeliverables:\n\nComplete analysis code (well-commented)\nAll visualizations (EDA, ROC curves, confusion matrices)\nExecutive summary\nYour AI prompt(s)\nReflection: What classification-specific challenges did you encounter that you wouldn’t face in regression?"
  },
  {
    "objectID": "Assignments/Chapter 3 - Classification/chapter-3-homework.html#instructions",
    "href": "Assignments/Chapter 3 - Classification/chapter-3-homework.html#instructions",
    "title": "Chapter 3 Homework: Classification Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is meant to be completed with AI assistance (like Gemini CLI) to practice classification workflows, diagnostics, and model comparison at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nThe prompts you used with the AI assistant(s)\n\n\n\nYou’ll be working with three datasets:\nDataset 1: titanic.csv - Titanic passenger survival data with columns:\n\nSurvived: Whether passenger survived (0=No, 1=Yes) (TARGET)\nPclass: Passenger class (1, 2, or 3)\nSex: Passenger sex\nAge: Passenger age\nSibSp: Number of siblings/spouses aboard\nParch: Number of parents/children aboard\nFare: Ticket fare\nEmbarked: Port of embarkation (C, Q, S)\n\nLoad this data using the data hosted here: https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv.\nDataset 2: spotify.csv - Spotify song data with columns:\n\npopularity: Song popularity score (0-100)\ndanceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo: Audio features\n\nYou will create a binary target: viral_hit = popularity &gt; 80.\nDownload the Spotify data from here: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset\nDataset 3: isp_customers.csv - Internet service provider customer data with columns:\n\nChurn: Whether customer churned (TARGET)\ngender, SeniorCitizen, Partner, Dependents: Demographics\ntenure: Months with company\nPhoneService, InternetService, Contract: Service details\nMonthlyCharges, TotalCharges: Billing information\n\nDownload the ISP customer data from here: https://www.kaggle.com/datasets/mehmetsabrikunt/internet-service-churn.\n\n\n\n\nLoad the titanic.csv dataset and prepare it for classification:\n\nDrop rows with missing values in Age, Fare, and Survived\nCreate feature matrix X using: Pclass, Age, SibSp, Parch, Fare\nCreate target vector y using Survived\nSplit into train/test sets (80/20, random_state=42)\nHow many passengers are in each class (survived vs not survived) in the training set? Is this balanced or imbalanced?\n\n\n\n\n\nFit a Logistic Regression model on the Titanic data:\n\nFit the model on X_train and y_train\nGet predictions on X_test\nCreate and display a confusion matrix\nCalculate accuracy, precision, recall, and F1 score manually from the confusion matrix values\nVerify your calculations match sklearn’s metric functions\n\n\n\n\n\nInterpret the logistic regression results:\n\nExtract the coefficients and create a DataFrame showing feature names and their coefficients\nCalculate the odds ratio for each feature (exp of coefficient)\nWhich feature has the strongest positive association with survival? Strongest negative?\nInterpret the Pclass coefficient in plain language: “For each unit increase in passenger class…”\nWhy should we be careful interpreting these coefficients?\n\n\n\n\n\nFit a Decision Tree classifier on the Titanic data:\n\nFit a tree with max_depth=3\nCalculate training and test accuracy\nExtract and plot feature importances as a horizontal bar chart\nWhich feature does the decision tree consider most important? Does this match the logistic regression coefficients?\n\n\n\n\n\nInvestigate the bias-variance tradeoff with decision trees:\n\nFit decision trees with max_depth values: [1, 2, 3, 5, 7, 10, 15, 20, None]\nFor each, calculate training and test accuracy\nPlot both accuracies vs. max_depth on the same graph\nAt what depth does overfitting become apparent?\nWhat max_depth would you choose for this dataset? Why?\n\n\n\n\n\nFit a Random Forest classifier:\n\nFit with n_estimators=100, max_depth=5, random_state=42\nCompare its test accuracy to the single decision tree with max_depth=5\nExtract feature importances from both models and compare them\nWhy might the feature importances differ between a single tree and a random forest?\n\n\n\n\n\nFit a k-Nearest Neighbors classifier:\n\nScale the features using StandardScaler (fit on train, transform both)\nFit k-NN with k=5 on the scaled data\nCalculate test accuracy\nTest k values [1, 3, 5, 7, 10, 15, 20] and plot test accuracy vs. k\nWhat k value gives the best performance? What happens at very small and very large k?\n\n\n\n\n\nCreate an ROC curve for the logistic regression model:\n\nGet probability predictions for the positive class\nCalculate FPR and TPR using roc_curve\nCalculate the AUC score\nPlot the ROC curve with the diagonal reference line and display AUC in the legend\n\n\n\n\n\nCompare multiple classifiers:\n\nFit Logistic Regression, Decision Tree (depth=5), Random Forest (100 trees), and k-NN (k=5, scaled)\nCalculate test accuracy for each\nCreate a bar chart comparing accuracies\nGet ROC curves for all models that provide predict_proba and plot them on the same axes\nBased on both accuracy and AUC, which model performs best?\n\n\n\n\n\nExplain in your own words (3-4 sentences each):\n\nWhen would you choose precision as your primary metric? Give a real-world example.\nWhen would you choose recall as your primary metric? Give a real-world example.\nWhat is the F1 score and when is it useful?\nWhy is accuracy often misleading for imbalanced datasets?\n\n\n\n\nUse AI to perform comprehensive hyperparameter tuning for a Decision Tree on the Titanic dataset.\nTest combinations of: - max_depth: [2, 3, 5, 7, 10, 15] - min_samples_split: [2, 5, 10, 20] - min_samples_leaf: [1, 2, 5, 10]\nYour code should:\n\nUse GridSearchCV with 5-fold cross-validation\nFind the best hyperparameter combination\nDisplay the top 5 parameter combinations\nFit the best model and evaluate on test set\n\nDeliverables:\n\nCode\nBest parameters and test accuracy\nWritten interpretation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to create a comprehensive model comparison on the Titanic dataset.\nCompare these classifiers: - Logistic Regression - Decision Tree (tuned) - Random Forest (tuned) - SVM with RBF kernel - k-NN (tuned)\nYour code should:\n\nUse GridSearchCV to tune each model\nCreate a comparison table with accuracy, precision, recall, F1, and AUC\nPlot all ROC curves on the same graph\nCreate a heatmap of the comparison metrics\n\nWrite a paragraph recommending which model to use and why.\nDeliverables:\n\nCode\nComparison table and visualizations\nROC curve comparison plot\nWritten recommendation (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\n\nUse AI to analyze class imbalance in the Spotify dataset.\nCreate a binary classification target: viral_hit = popularity &gt; 80\nYour code should:\n\nShow the class distribution\nFit a baseline model (Logistic Regression) without handling imbalance\nApply three strategies: class_weight=‘balanced’, SMOTE, random undersampling\nCompare performance using precision, recall, F1, and AUC for each strategy\nCreate confusion matrices for each approach\n\nWhich strategy works best for identifying viral hits?\nDeliverables:\n\nCode\nClass distribution visualization\nPerformance comparison table\nConfusion matrices for each approach\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to visualize decision boundaries for different classifiers.\nUsing the Titanic dataset with only Age and Fare as features:\n\nCreate decision boundary plots for:\n\nLogistic Regression\nDecision Tree (depth=3)\nDecision Tree (depth=10)\nRandom Forest\nSVM (linear kernel)\nSVM (RBF kernel)\nk-NN (k=3)\nk-NN (k=20)\n\n\nArrange all 8 plots in a 2x4 grid with consistent axis limits and color schemes.\nHow do the decision boundaries differ? Which creates the most complex boundaries?\nDeliverables:\n\nCode\nDecision boundary visualization (2x4 grid)\nWritten comparison (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\n\nUse AI to analyze feature importance across different models.\nUsing the Titanic dataset (all numeric features):\n\nExtract feature importances from:\n\nLogistic Regression (absolute coefficients)\nDecision Tree\nRandom Forest\nPermutation importance for k-NN and SVM\n\nCreate a heatmap showing feature importance rankings across all models\n\nWhich features are consistently important? Which differ between models?\nDeliverables:\n\nCode\nFeature importance heatmap\nWritten analysis (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to perform cross-dataset model evaluation.\nTrain models on the Titanic dataset and ISP customer dataset (predicting churn):\n\nFor each dataset, tune and fit: Logistic Regression, Random Forest, k-NN\nCompare how well each model type performs across the two datasets\nCreate a comparison table showing model performance on each dataset\n\nDo the same model types perform best on both datasets? Why or why not?\nDeliverables:\n\nCode for both datasets\nCross-dataset comparison table\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\n\nPrompt AI to create an ensemble voting classifier.\nUsing the Titanic dataset:\n\nCreate a VotingClassifier combining:\n\nLogistic Regression\nRandom Forest\nSVM (with probability=True)\n\nUse both ‘hard’ and ‘soft’ voting\nCompare the ensemble to individual models\n\nDoes the ensemble outperform the best individual model?\nDeliverables:\n\nCode\nPerformance comparison table\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\n\nUse AI to analyze k-NN with different distance metrics.\nUsing the ISP customer dataset (with a mix of categorical and numerical features):\n\nTest distance metrics: euclidean, manhattan, cosine\nFor categorical features, use appropriate encoding\nCompare performance across metrics\nAnalyze which metric works best for this mixed-type data\n\nHow does the choice of distance metric affect performance?\nDeliverables:\n\nCode\nPerformance comparison table\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\n\nFor this final question, ask AI to help you conduct a complete classification analysis from start to finish on the ISP customer dataset (predicting churn).\nYour analysis should include:\n\nExploratory Data Analysis:\n\nSummary statistics\nClass distribution\nFeature distributions by target class\nCorrelation analysis\n\nData Preparation:\n\nHandle missing values\nEncode categorical variables\nFeature scaling (where appropriate)\nTrain/validation/test split (60/20/20)\n\nModel Development:\n\nFit multiple models: Logistic Regression, Decision Tree, Random Forest, SVM, k-NN\nHyperparameter tuning using validation set\nHandle class imbalance appropriately\n\nModel Evaluation:\n\nCompare all models using appropriate metrics (given potential imbalance)\nROC curves for all models\nSelect best model based on business context (what matters more: finding churners or not falsely flagging loyal customers?)\n\nFinal Model:\n\nEvaluate on test set (only once!)\nFeature importance analysis\nCreate confusion matrix with business interpretation\n\n\nWrite a 1-page executive summary (300-400 words) that explains:\n\nThe business problem (customer churn prediction)\nData characteristics and challenges\nModels tested and evaluation approach\nHow the best model was selected\nPerformance on test set\nKey features predicting churn\nBusiness recommendations\n\nDeliverables:\n\nComplete analysis code (well-commented)\nAll visualizations (EDA, ROC curves, confusion matrices)\nExecutive summary\nYour AI prompt(s)\nReflection: What classification-specific challenges did you encounter that you wouldn’t face in regression?"
  },
  {
    "objectID": "Assignments/Chapter 3 - Classification/chapter-3-homework.html#submission-guidelines",
    "href": "Assignments/Chapter 3 - Classification/chapter-3-homework.html#submission-guidelines",
    "title": "Chapter 3 Homework: Classification Models",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nSubmit a ZIP file containing:\n\nCode files: All .py files or Jupyter notebooks\nVisualizations folder: All plots and charts generated\nWritten responses: A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\nData: Include the datasets if you made any modifications"
  },
  {
    "objectID": "Assignments/Chapter 3 - Classification/chapter-3-homework.html#tips-for-success",
    "href": "Assignments/Chapter 3 - Classification/chapter-3-homework.html#tips-for-success",
    "title": "Chapter 3 Homework: Classification Models",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nGeneral:\n\nAlways check class balance before fitting models\nPrecision = “of those I predicted positive, how many were correct?”\nRecall = “of those that were actually positive, how many did I find?”\nDecision tree depth controls overfitting—start shallow\nk-NN requires scaling; trees do not\nWhen comparing models, use multiple metrics, not just accuracy\nAsk AI to test many hyperparameter combinations systematically\nWhen dealing with imbalance, always compare multiple strategies\nROC/AUC is great for model comparison but doesn’t tell the whole story\nFeature importance from different models may disagree—that’s okay\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#required-images",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#required-images",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Required Images",
    "text": "Required Images\n\n1. llm-extraction-workflow.png\nLocation in chapter: Section 1.4 “The LLM Advantage: Zero-Shot Extraction”\nDescription: A flowchart showing the complete LLM extraction workflow: - Input: Unstructured text (review, ticket, post) - Step 1: Write prompt + include text - Step 2: Call LLM API - Step 3: Receive JSON response - Step 4: Parse and validate - Step 5: Convert to DataFrame - Step 6: Feed to ML model - Output: Predictions\nUse boxes and arrows to show flow from left to right. Include icons or visual indicators for each step (text icon, API icon, JSON icon, table icon, model icon).\nPurpose: Give students the big picture of how LLM extraction fits into an ML pipeline\nSuggested tools: Draw.io, Lucidchart, PowerPoint, or Figma\n\n\n\n2. prompt-comparison.png\nLocation in chapter: Section 3.1 “The Anatomy of a Good Extraction Prompt”\nDescription: Side-by-side comparison showing:\nLeft side - BAD PROMPT:\nTell me about this review:\n[review text]\nResponse: Vague, unstructured answer\nRight side - GOOD PROMPT:\nExtract sentiment from this review.\nAnswer with: positive, negative, or neutral\n\nReview: [review text]\n\nSentiment:\nResponse: “positive”\nUse red ✗ for bad prompt, green ✓ for good prompt. Highlight the key differences (specificity, format, clarity).\nPurpose: Visually demonstrate what makes a good extraction prompt\nSuggested tools: Screenshot and annotate, or create in PowerPoint/Keynote\n\n\n\n3. cost-comparison-chart.png\nLocation in chapter: Section 6.1-6.2 “Understanding Token-Based Pricing” and “Calculating Extraction Costs”\nDescription: Bar chart comparing costs for processing 10,000 reviews across different LLMs: - X-axis: LLM model (GPT-3.5-turbo, GPT-4, Claude Sonnet, Gemini) - Y-axis: Total cost in dollars (logarithmic scale) - Bars showing relative costs with actual dollar amounts labeled\nExample values: - GPT-3.5: $0.50 - GPT-4: $5.00 - Claude: $2.50 - Gemini: $0.40\nUse color coding: green for cheapest, yellow for moderate, red for expensive.\nPurpose: Make the dramatic cost differences between models immediately visible\nSuggested approach: Create with matplotlib/seaborn or Excel/Google Sheets\n\n\n\n4. quality-vs-cost.png\nLocation in chapter: Section 6.3 “When to Use Which Model”\nDescription: Scatter plot showing the quality-cost tradeoff: - X-axis: Extraction quality/accuracy (70% - 95%) - Y-axis: Cost per extraction (logarithmic scale, $0.0001 - $0.01) - Points for different approaches: - Regex/Keywords: Low cost, moderate quality (bottom left) - GPT-3.5: Low-moderate cost, good quality (middle) - GPT-4: High cost, high quality (top right) - Traditional ML: Low cost (once trained), good quality (bottom middle-right)\nAdd labels for each point. Draw a “sweet spot” circle around GPT-3.5 area.\nPurpose: Help students visualize the tradeoff between cost and quality\nSuggested approach: matplotlib scatter plot with annotations\n\n\n\n5. extraction-validation.png\nLocation in chapter: Section 7.1 “Measuring Extraction Accuracy”\nDescription: Visual example showing correct vs incorrect extractions:\nTop section - Correct Extractions (✓): - Review: “Love this product!” → Extracted: “positive” → Ground truth: “positive” - Review: “Terrible quality.” → Extracted: “negative” → Ground truth: “negative”\nBottom section - Incorrect Extractions (✗): - Review: “It’s not bad.” → Extracted: “negative” → Ground truth: “positive” - Annotation: “Missed double negative” - Review: “I expected amazing quality…” → Extracted: “positive” → Ground truth: “negative” - Annotation: “Only read first part”\nUse green checkmarks for correct, red X’s for incorrect. Add brief explanations for why errors occurred.\nPurpose: Show students what to look for when validating extractions\nSuggested tools: PowerPoint, Google Slides, or design tool\n\n\n\n6. when-to-use-llms.png\nLocation in chapter: Section 8.1 “Decision Framework”\nDescription: Decision tree flowchart for choosing extraction method:\nStart: “Need to extract information from text” ↓ Question 1: “Is it simple pattern matching (emails, phone numbers)?” → YES: Use Regex → NO: Continue\nQuestion 2: “Do you have 1000+ labeled examples?” → YES: Train Traditional ML → NO: Continue\nQuestion 3: “Need to understand context/nuance?” → YES: Use LLM → NO: Try Regex/Keywords first\nUse diamond shapes for questions, rectangles for decisions. Color-code paths (green for simple solutions, yellow for moderate complexity, blue for LLM).\nPurpose: Give students a practical decision-making framework\nSuggested tools: Draw.io, Lucidchart, or flowchart software"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#image-style-guidelines",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#image-style-guidelines",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Image Style Guidelines",
    "text": "Image Style Guidelines\nAll images should follow these guidelines:\n\nResolution: At least 1200px wide for diagrams, 1000px wide for plots\nFormat: PNG with transparent background where appropriate\nColors: Use colorblind-friendly palettes\n\nBlue (#3498db) for primary elements\nOrange (#e67e22) for secondary elements\nGreen (#2ecc71) for success/correct elements\nRed (#e74c3c) for errors/warnings\n\nFonts: Use clear, readable fonts (Arial, Helvetica, or similar)\nStyle: Professional but approachable—match the textbook’s conversational tone\nLabels: All axes, boxes, and elements should be clearly labeled\nSize: Large enough text that it’s readable when embedded in the document"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#priority-order",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#priority-order",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Priority Order",
    "text": "Priority Order\nIf time is limited, create images in this priority order:\n\nllm-extraction-workflow.png - Most important for understanding the overall process\nprompt-comparison.png - Critical for teaching effective prompting\ncost-comparison-chart.png - Essential for understanding cost implications\nwhen-to-use-llms.png - Practical decision-making tool\nquality-vs-cost.png - Helps with model selection\nextraction-validation.png - Nice to have for quality control section"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#notes",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Notes",
    "text": "Notes\n\nCost comparison chart can be generated directly from Python using matplotlib\nQuality vs cost scatter plot should also be generated programmatically\nThe workflow diagram and decision tree need to be created manually with diagramming tools\nPrompt comparison can be screenshots from actual code/API calls with annotations added\nUse consistent color schemes across all images for professional appearance\nConsider creating a Python script to generate the data visualization plots for consistency"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-resources",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-resources",
    "title": "Chapter 2: Regression Models",
    "section": "Chapter Resources",
    "text": "Chapter Resources\nRelated Assignments:\n\nChapter 2 Homework"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-intro",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-intro",
    "title": "Chapter 2: Regression Models",
    "section": "Introduction",
    "text": "Introduction\nLinear regression is perhaps the most important algorithm in machine learning. Not because it’s the most powerful—it’s not. Not because it always works—it doesn’t. Linear regression matters because understanding it deeply gives you the foundation to understand almost everything else in machine learning.\nHere’s the thing: most machine learning is just sophisticated regression. Neural networks? Stacked regressions with non-linear activation functions. Logistic regression? Linear regression passed through a special function. Even tree-based methods are essentially breaking the data into regions and fitting constants (which are just simple regressions) in each region.\nBut linear regression has a dirty secret: it only works well when certain assumptions are met. Violate those assumptions, and your beautiful model with a high R² might be making terrible predictions. This is where many beginners get burned. They fit a model, see a nice R² value, and think they’re done. Then they deploy it to production and watch it fail spectacularly.\nThis chapter is about becoming a regression expert. Not just someone who can call LinearRegression().fit(), but someone who knows when regression will work, when it won’t, and how to fix it when it breaks. We’ll cover the assumptions behind linear regression, how to check if they’re violated, what to do when they are, and how regularization techniques can save you when things get messy.\n\n\n\n\n\n\nTip\n\n\n\nYou may assume that linear regression is only useful when your data has a linear relationship. But this is not true! Even data with complex relationships can be modeled using linear regression with the right features, transformations, and regularization techniques.\n\n\nBy the end of this chapter, you’ll understand:\n\nWhat assumptions linear regression makes and why they matter\nHow to diagnose problems using residual plots\nWhen and how to use polynomial features for non-linear relationships\nHow multicollinearity breaks coefficient interpretation\nHow Ridge, Lasso, and Elastic Net regularization fix overfitting and multicollinearity\nHow to choose between different regression approaches\n\nLet’s jump in."
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-1",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-1",
    "title": "Chapter 2: Regression Models",
    "section": "1. Train/Validation/Test Methodology",
    "text": "1. Train/Validation/Test Methodology\nBefore we dive into specific regression models, we need to establish a crucial foundation: how to properly split and evaluate our data. Understanding train/validation/test methodology is essential for building models that actually generalize to new data.\n\n1.1 Why Three Sets?\nYou’ve seen train/test splits. But proper ML methodology requires three sets. Here’s why:\n\nTraining set: Used to fit the model (learn parameters)\nValidation set: Used to tune hyperparameters and select models\nTest set: Used ONCE at the very end to get an unbiased estimate of performance\n\n\n\n\n\n\n\nNote\n\n\n\nBefore you freak out at the huge block of code below, slow down! You’re not expected to write code like this from scratch. Instead, what you should do is go through, look at the code comments (everything after the hashtag #), and get a rough feeling for what it’s doing. In practice, an LLM would write this code for you based on your prompt.\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Load California housing data for demonstration\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Select features and target\nX = housing_df[['median_income', 'housing_median_age', 'total_rooms', 'population']]\ny = housing_df['median_house_value']\n\n# Remove missing values\nX_clean = X.dropna()\ny_clean = y[X_clean.index]\n\n# Proper three-way split\ndef train_val_test_split(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=1):\n    \"\"\"Split data into train, validation, and test sets\"\"\"\n\n    # First split: separate test set\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    # Second split: separate train and validation\n    # val_size needs to be recalculated relative to remaining data\n    val_size_adjusted = val_size / (train_size + val_size)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n    )\n\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\n# Split the data\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X_clean, y_clean)\n\nprint(\"Dataset sizes (California housing):\")\nprint(f\"Training: {len(X_train)} samples ({len(X_train)/len(X_clean)*100:.0f}%)\")\nprint(f\"Validation: {len(X_val)} samples ({len(X_val)/len(X_clean)*100:.0f}%)\")\nprint(f\"Test: {len(X_test)} samples ({len(X_test)/len(X_clean)*100:.0f}%)\")\nprint(f\"Total: {len(X_clean)} samples\")\n\nDataset sizes (California housing):\nTraining: 12384 samples (60%)\nValidation: 4128 samples (20%)\nTest: 4128 samples (20%)\nTotal: 20640 samples\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNever use the test set for model selection or hyperparameter tuning. If you evaluate multiple models on the test set and pick the best, you’ve contaminated it. The test set should be used exactly once, at the very end, to get an honest estimate of how your chosen model will perform on new data.\n\n\n\n\n1.2 Cross-Validation: Getting More Reliable Estimates\nA single train/validation split can be unlucky—maybe the validation set happens to be easy or hard. Cross-validation solves this by using multiple validation sets.\nIn cross-validation, we split the training data into multiple “folds”. The model is fit on some folds and validated on the remaining fold. This process is repeated multiple times, with each fold serving as the validation set once. The final cross-validation score is the average of all validation scores.\nThe benefits of this are that:\n\nWe can better understand the variability of our predictions, because we can see how much the model’s performance changes when we use different validation sets.\nWe can keep our test set “clean”, and only use it once at the very end.\n\nLet’s implement this in code. Here we’ll use cross-validation to find the optimal number of neighbors for a KNN regression model. We’ll choose between 2, 5, 10, 20, 30, 40, and 50 neighbors. Rather than doing a single fit on the training set and then validating on the validation set, we’ll use cross-validation. Doing so allows us to only use the test set once at the very end, and also to better understand the variability of our predictions, which we’ll show as error bars.\n\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\n# Use cross-validation to find the optimal number of neighbors\n# Test different values of n_neighbors\nn_neighbors_options = [2, 5, 10, 20, 30, 40, 50]\n\nresults = []\nfor n in n_neighbors_options:\n    # Create model with this value of n_neighbors\n    model = KNeighborsRegressor(n_neighbors=n)\n\n    # Perform 5-fold cross-validation\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5,\n                                 scoring='neg_mean_squared_error')\n\n    # Convert to MSE (scores are negative)\n    mse_scores = -cv_scores\n    mean_mse = mse_scores.mean()\n    std_mse = mse_scores.std()\n\n    results.append({\n        'n_neighbors': n,\n        'mean_mse': mean_mse,\n        'std_mse': std_mse\n    })\n\n# Find the best n_neighbors\nbest_result = min(results, key=lambda x: x['mean_mse'])\n\n# Visualize the results\nplt.figure(figsize=(10, 6))\nn_values = [r['n_neighbors'] for r in results]\nmean_mses = [r['mean_mse'] for r in results]\nstd_mses = [r['std_mse'] for r in results]\n\nplt.errorbar(n_values, mean_mses, yerr=std_mses, marker='o', linewidth=2,\n             capsize=5, capthick=2, markersize=8, alpha=0.75)\nplt.xlabel('Number of Neighbors', fontsize=12)\nplt.ylabel('Mean Squared Error', fontsize=12)\nplt.title('Cross-Validation: Finding Optimal n_neighbors', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nWe see that 10 neighbors appears optimal, as it has the lowest mean squared error with reasonably low variation. This means that it generalizes well to new data.\nCross-validation gives you:\n\nMore reliable estimates: Averages over multiple validation sets\nUncertainty quantification: Standard deviation tells you how variable performance is\nBetter use of data: Every example gets to be in the validation set once\n\n\n\n\n\n\n\nTip\n\n\n\nUse cross-validation for model selection when you have limited data. For very large datasets, a single train/val/test split is often sufficient and much faster.\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nExplain why a train-test-validation split is key for producing reliable findings, and the role of each of the three sets (training, testing, validation)\nExplain what cross validation is, and when it’s beneficial"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-2",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-2",
    "title": "Chapter 2: Regression Models",
    "section": "2. Linear Regression",
    "text": "2. Linear Regression\n\n2.1 The Line of Best Fit\nYou’ve probably heard linear regression described as “finding the line of best fit.” But what does “best” actually mean? Best according to what criteria?\nThe answer: best means the line that minimizes the sum of squared errors. For each data point, we calculate how far the prediction is from the actual value (the error), square it, and add up all these squared errors. We can either minimize this sum, or minimize the average, they’re equivalent. The line that makes this value as small as possible is our “best fit” line. The equation is\n\\[\n\\frac{1}{n}\\displaystyle\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nThis is called mean squared error (MSE), which is the loss function for linear regression. The goal of linear regression is to find the line that minimizes this loss.\nLet’s see this in action with the NYC census data:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Load NYC census data\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Filter to complete cases for income prediction\nnyc_clean = nyc_census[['IncomePerCap', 'Income']].dropna()\n\n# Let's predict median household income from per-capita income\n# Start with just one feature to visualize easily\nX = nyc_clean[['IncomePerCap']].values  # Income per capita\ny = nyc_clean['Income'].values  # Median household income\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit linear regression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\n# Visualize the fit\nplt.figure(figsize=(10, 6))\nplt.scatter(X_train, y_train, alpha=0.5, s=20, label='Training Data')\nplt.plot(sorted(X_train.flatten()), model.predict(sorted(X_train.reshape(-1, 1))),\n         'r-', linewidth=2, label='Best Fit Line')\nplt.xlabel('Income Per Capita ($)', fontsize=12)\nplt.ylabel('Median Household Income ($)', fontsize=12)\nplt.title('Linear Regression: Per Capita Income vs Household Income', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Slope (coefficient): {model.coef_[0]:.4f}\")\nprint(f\"Intercept: {model.intercept_:.4f}\")\nprint(f\"Training R²: {model.score(X_train, y_train):.4f}\")\nprint(f\"Test R²: {model.score(X_test, y_test):.4f}\")\n\n\n\n\n\n\n\n\nSlope (coefficient): 0.9894\nIntercept: 27540.5059\nTraining R²: 0.6878\nTest R²: 0.7202\n\n\nThe regression line tries to find the balance that minimizes total squared error across all points. The slope and y-intercept are learned (machine learning) such that the sum of squared residuals, or MSE, is as small as possible.\n\n\n\n\n\n\nNote\n\n\n\nYou are not expected to write code like this from hand. However, you are expected to understand it! You will be given quiz questions with blocks of code like the code above, and asked questions about it, which you must answer without the help of an LLM.\n\n\n\n\n2.2 The Assumptions Behind Linear Regression\nLinear regression makes four key assumptions. Violate them, and your model might give you terrible predictions even if the R² looks good.\nThe Four Assumptions:\n\nLinearity: The relationship between X and y is actually linear. If it’s curved, a straight line won’t fit well (however, we’ll handle those cases later in this chapter).\nIndependence: Each observation is independent. One house’s price doesn’t directly affect another’s. (This can be violated with time series or spatial data.)\nHomoscedasticity: The “spread” of points around the line should be roughly the same everywhere. If errors are tiny for low X values but huge for high X values, that’s a problem. (The fancy term means “constant variance.”)\nNormality: The residuals (errors) are normally distributed. This matters more for statistical inference than prediction.\n\nWhat happens when you violate these?\n\nViolate linearity: Your predictions will be systematically wrong in certain ranges\nViolate independence: Your confidence intervals and p-values become unreliable\nViolate homoscedasticity: Some predictions are much more uncertain than others (but you won’t know which ones!)\nViolate normality: Your confidence intervals might be wrong, but predictions can still be okay\n\nWe’ll learn how to check these assumptions using diagnostic plots in Section 3.\n\n\n2.3 Interpreting Coefficients\nCoefficients tell you the relationship between features and the target. Let’s extract and interpret them:\n\n# Fit a model with multiple features\nfeatures = ['TotalPop', 'Professional', 'Poverty', 'Unemployment']\nnyc_multi = nyc_census[features + ['Income']].dropna()\nX_multi = nyc_multi[features].values\ny_multi = nyc_multi['Income'].values\n\nX_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n    X_multi, y_multi, test_size=0.2, random_state=42\n)\n\n# Fit model\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_train_multi, y_train_multi)\n\n# Display coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': model_multi.coef_\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"Regression Coefficients:\")\nprint(coef_df)\nprint(f\"\\nIntercept: {model_multi.intercept_:.4f}\")\n\nRegression Coefficients:\n        Feature  Coefficient\n1  Professional   777.977464\n0      TotalPop    -0.184356\n3  Unemployment  -191.478521\n2       Poverty -1036.991744\n\nIntercept: 53635.7278\n\n\nHow to interpret these:\n\nProfessional coefficient: For every 1% increase in the percentage of professionals, median household income changes by the coefficient amount, holding all other features constant.\nPoverty coefficient: For every 1% increase in the poverty rate, median household income changes by the coefficient amount (likely negative), holding all other features constant.\nIntercept: The predicted median household income when all features are zero. Often not meaningful in practice (what census tract has zero population or zero unemployment?).\n\nNotice the key phrase: “holding all other features constant.” That’s crucial. It means the coefficient shows the isolated effect of that one feature, assuming everything else stays the same. Change one feature while keeping others fixed, and the coefficient tells you how much the prediction changes.\n\n\n\n\n\n\nWarning\n\n\n\nCoefficient interpretation breaks down when features are highly correlated (multicollinearity). We’ll address this in Section 5 when we discuss multicollinearity.\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nGiven LLM-written linear regression code, explain what each step is doing (loading the function, cleaning the data, creating X/y sets, etc)\nList and explain all four assumptions behind linear regression\nInterpret the coefficients from a linear regression model"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-3",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-3",
    "title": "Chapter 2: Regression Models",
    "section": "3. Regression Evaluation Metrics",
    "text": "3. Regression Evaluation Metrics\nHow do you know if your regression model is any good? You need metrics. R² is popular, but it can be misleading. Let’s look at the most important metrics, what they mean, and when to use each one.\n\n3.1 Mean Squared Error (MSE)\nMSE is the average of squared errors. Remember, an error (or residual) is just actual value minus predicted value. Square those, average them, and you have MSE.\nWhy does MSE matter? It directly reflects what linear regression minimizes during training. When you fit a linear regression, you’re literally finding the line that gives you the smallest possible MSE on the training data.\nBut here’s the catch: squaring errors means big errors get penalized heavily. If one prediction is off by 100 and another is off by 10, the first error contributes 10,000 to MSE while the second contributes only 100. That’s 100 times worse, not 10 times worse. This makes MSE sensitive to outliers.\n\n\n3.2 Mean Absolute Error (MAE)\nMAE is simpler: just the average of the absolute values of errors. No squaring involved.\nMAE vs MSE: What’s the difference?\nMAE treats all errors proportionally. An error of 100 is exactly 10 times worse than an error of 10. With MSE, that error of 100 is 100 times worse.\n\n\n\n\n\n\nTip\n\n\n\nWhile MSE does not preserve units (since it squares the errors), you can convert back to the original units by taking a square root, such as\n\\[\n\\text{RMSE} = \\sqrt{\\text{MSE}} = \\displaystyle\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\]\nThis is called “root mean squared error”, or RMSE. It’s in the same units as the target variable, making it more interpretable.\n\n\n\n\n\n\n\n\nTip\n\n\n\nUnless there’s an extremely compelling reason to do otherwise, you should always start with MSE or RMSE as your metric. It’s the most natural metric for linear regression, and it’s what linear regression optimizes.\nHowever, it’s often beneficial to also compute MAE, as it’s more interpretable and less sensitive to outliers. It’s typical to report both values, as they each have their own strengths.\n\n\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Calculate MAE and MSE\nmse_sklearn = mean_squared_error(y_test, y_pred_test)\nmae = mean_absolute_error(y_test, y_pred_test)\n\n# Calculate Root Mean Squared Error (RMSE) for comparison\nrmse = np.sqrt(mse_sklearn)\n\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\n\nprint(f\"\\nInterpretation:\")\nprint(f\"- On average, predictions are off by ${mae:.0f} (MAE)\")\nprint(f\"- MAE is lower than RMSE because MSE penalizes large errors more heavily\")\n\nMAE: 12478.0785\nRMSE: 15670.5818\n\nInterpretation:\n- On average, predictions are off by $12478 (MAE)\n- MAE is lower than RMSE because MSE penalizes large errors more heavily\n\n\nMAE is lower than RMSE (square root of MSE). That’s typical—RMSE is always at least as large as MAE, and the gap tells you something about outliers. A big gap means you have some really bad predictions pulling up the RMSE.\n\n\n3.3 R² (R-Squared)\nR² tells you the proportion of variance in the target variable that your model explains. It ranges from 0 to 1, where:\n\nR² = 1: Perfect predictions (your model explains 100% of the variance)\nR² = 0: Your model is no better than just predicting the mean every time\nR² &lt; 0: Your model is worse than predicting the mean (yes, this is possible!)\n\nThe formula is: R² = 1 - (Sum of Squared Residuals / Total Sum of Squares)\nBut what does that actually mean? Think of it this way: imagine you know nothing about the features and just predict the mean house value for every house. That’s your baseline. R² tells you how much better (or worse!) your model is than that naive baseline.\n\nfrom sklearn.metrics import r2_score\n\n# Calculate R² manually to understand it\n# Total Sum of Squares: how far each point is from the mean\ny_mean = y_test.mean()\ntotal_ss = ((y_test - y_mean) ** 2).sum()\n\n# Residual Sum of Squares: how far predictions are from actual values\nresidual_ss = ((y_test - y_pred_test) ** 2).sum()\n\n# R² = 1 - (residual variation / total variation)\nr2_manual = 1 - (residual_ss / total_ss)\n\n# Compare to sklearn\nr2_sklearn = r2_score(y_test, y_pred_test)\n\nprint(f\"R² (manual): {r2_manual:.4f}\")\nprint(f\"R² (sklearn): {r2_sklearn:.4f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"The model explains {r2_sklearn*100:.2f}% of the variance in house prices\")\nprint(f\"That means {(1-r2_sklearn)*100:.2f}% of the variance is unexplained\")\n\n# Visualize what R² means\nfig = plt.figure(figsize=(10, 6))\n\n# Baseline plot\nplt.scatter(X_test, y_test, alpha=0.3, s=10, label='Actual Data')\nplt.axhline(y_mean, color='red', linestyle='--', linewidth=2, label=f'Baseline: Predict Mean = {y_mean:.2f}')\n\n# Regression line\nplt.plot(X_test, y_pred_test, 'b-', linewidth=2, label=f'Regression Line (R² = {r2_sklearn:.4f})')\n\nplt.xlabel('Median Income (in $10,000s)', fontsize=12)\nplt.ylabel('Median House Value (in $100,000s)', fontsize=12)\nplt.title('Baseline Model (R² = 0)', fontsize=14)\n\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nR² (manual): 0.7202\nR² (sklearn): 0.7202\n\nInterpretation:\nThe model explains 72.02% of the variance in house prices\nThat means 27.98% of the variance is unexplained\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how our regression line is a significantly better fit than the baseline line. That leads to an R² value significantly above zero.\n\n\nR² is intuitive and easy to explain to non-technical audiences. But it has problems:\nProblem 1: R² always increases when you add features, even if those features are random noise. Add 100 random columns to your data, and R² will go up, even though the model hasn’t actually improved.\nProblem 2: R² doesn’t tell you if your predictions are biased. R² tells you how close to a line your actual vs predicted values are. What’s one easy way to get a straight line? Just predict the same y-value every single time! Congrats, you have a perfectly straight line, and thus have an R² of 100%! R² doesn’t tell you if your predictions are biased. You could have a high R² but still systematically overpredict or underpredict in certain ranges.\nProblem 3: R² can be negative on test data if your model is terrible. That’s a good warning sign, actually.\n\n\n3.4 Adjusted R²\nAdjusted R² fixes the “adding features always increases R²” problem. It penalizes you for adding features that don’t actually help.\nThe formula adjusts R² based on the number of features (p) and number of observations (n):\nAdjusted R² = \\(1 - \\displaystyle\\biggl[\\frac{(1 - R^2)(n - 1)}{(n - p - 1)}\\biggr]\\)\nAdd a useless feature? Regular R² might go from 0.75 to 0.751. But adjusted R² might drop from 0.75 to 0.748 because the penalty for adding a feature outweighs the tiny improvement.\nWhen to use Adjusted R²: - When comparing models with different numbers of features - When you’re worried about overfitting - When presenting results to stakeholders (it’s more honest)\nWhen regular R² is fine: - When comparing models with the same number of features - When you’re more concerned about prediction accuracy than interpretation\nReal-World Example: When Many Features Mislead\nLet’s see this with real census data from Staten Island. We’ll predict income using many demographic features:\n\n# Load NYC census tract data\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Filter to just Staten Island\nstaten_island_data = nyc_census[nyc_census['Borough'] == 'Staten Island'].copy()\n\n# Remove rows with missing income (our target)\nstaten_island_data = staten_island_data.dropna(subset=['Income'])\n\n# Select features (exclude identifiers and the target)\nfeature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',\n                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',\n                'ChildPoverty', 'Professional', 'Service', 'Office',\n                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',\n                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',\n                'Unemployment']\n\n# Prepare data (drop rows with any missing values)\nstaten_island_clean = staten_island_data[feature_cols + ['Income']].dropna()\n\nX_si = staten_island_clean[feature_cols].values\ny_si = staten_island_clean['Income'].values\n\nprint(f\"\\n{len(staten_island_clean)} observations\")\nprint(f\"Number of features: {len(feature_cols)}\")\nprint(f\"Ratio of features to observations (p/n): {len(feature_cols)/len(staten_island_clean):.3f}\")\n\n# Split data\nX_train_si, X_test_si, y_train_si, y_test_si = train_test_split(\n    X_si, y_si, test_size=0.3, random_state=42\n)\n\n# Fit model\nmodel_si = LinearRegression()\nmodel_si.fit(X_train_si, y_train_si)\n\n# Calculate metrics\ntrain_r2_si = model_si.score(X_train_si, y_train_si)\ntest_r2_si = model_si.score(X_test_si, y_test_si)\n\n# Calculate Adjusted R² for training set\nn_train = len(X_train_si)\np_si = X_train_si.shape[1]\nadj_r2_train_si = 1 - ((1 - train_r2_si) * (n_train - 1) / (n_train - p_si - 1))\n\n# Calculate Adjusted R² for test set\nn_test = len(X_test_si)\nadj_r2_test_si = 1 - ((1 - test_r2_si) * (n_test - 1) / (n_test - p_si - 1))\n\n# Visualize the comparison\nmetrics_si = pd.DataFrame({\n    'Metric': ['R²', 'Adjusted R²', 'R²', 'Adjusted R²'],\n    'Dataset': ['Training', 'Training', 'Test', 'Test'],\n    'Value': [train_r2_si, adj_r2_train_si, test_r2_si, adj_r2_test_si]\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx_pos = [0, 1, 3, 4]\ncolors = ['skyblue', 'lightcoral', 'skyblue', 'lightcoral']\nbars = ax.bar(x_pos, metrics_si['Value'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n\n# Add value labels on bars (handle negative values)\nfor i, (pos, val) in enumerate(zip(x_pos, metrics_si['Value'])):\n    y_offset = 0.02 if val &gt;= 0 else -0.05\n    ax.text(pos, val + y_offset, f'{val:.3f}', ha='center', fontsize=10, weight='bold')\n\nax.set_ylabel('Score', fontsize=12)\nax.set_title(f'R² vs Adjusted R² with {p_si} Features', fontsize=14)\nax.set_xticks(x_pos)\nax.set_xticklabels(['R²\\n\\nTraining', 'Adj R²\\n\\nTraining', 'R²\\n\\nTest', 'Adj R²\\n\\nTest'], fontsize=11)\n# Set y-limits to accommodate negative adjusted R² values\ny_min = min(metrics_si['Value'].min() - 0.1, -0.1)\nax.set_ylim(y_min, 1.1)\nax.axhline(y=0, color='black', linewidth=0.8, linestyle='-')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n107 observations\nNumber of features: 30\nRatio of features to observations (p/n): 0.280\n\n\n\n\n\n\n\n\n\nSee the problem? With 30 features and a small number of Staten Island census tracts, the picture is alarming:\n\nAdjusted R² can be negative - When test adjusted R² is negative, the model performs worse than just predicting the mean after accounting for model complexity. The penalty for using so many features overwhelms any predictive value. Remember, this is possible because the model learned from the training data, but is evaluated on the test data. What was learned from the training data is leading the model astray on the test data.\nTest R² drops significantly from training - the model doesn’t generalize well\nThe high p/n ratio is the culprit - too many features relative to observations\n\nThis is a realistic scenario where you might look at training R² and think “Great, decent fit!” but adjusted R² and test performance reveal the truth: the model is overfitted and unreliable.\n\n\n\n\n\n\nWarning\n\n\n\nRule of thumb: When your p/n ratio exceeds 0.10 (1 feature per 10 observations), be very skeptical of regular R². Always check Adjusted R² and test set performance. Better yet, use cross-validation to get a more honest assessment.\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nExplain what MSE and MAE are, and how they penalize a model differently from one another\nExplain how to interpret \\(R^2\\) values, and explain the three common problems associated with \\(R^2\\)\nExplain how adjusted \\(R^2\\) differs from adjusted \\(R^2\\), and what problems it attempts to solve"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-4",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-4",
    "title": "Chapter 2: Regression Models",
    "section": "4. Residual Analysis: Your Diagnostic Tool",
    "text": "4. Residual Analysis: Your Diagnostic Tool\nMetrics like R² and MSE tell you how much error your model makes. But residual plots tell you where and why the model is making mistakes. This is where you catch problems before they bite you in production.\n\n4.1 What Are Residuals?\nA residual is simple: residual = actual value - predicted value.\nIf your model predicts a house is worth $250,000 but it’s actually worth $300,000, the residual is $50,000. Positive residual means you underpredicted. Negative residual means you overpredicted.\nWhy do residuals matter more than just looking at errors? Because the pattern of residuals reveals whether your model’s assumptions are violated. Random residuals? Great. Systematic patterns? Problem.\n\n# Load NYC census tract data (all boroughs this time)\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Remove rows with missing income (our target)\nnyc_census = nyc_census.dropna(subset=['Income'])\n\n# Select features (exclude identifiers and the target)\nfeature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',\n                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',\n                'ChildPoverty', 'Professional', 'Service', 'Office',\n                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',\n                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',\n                'Unemployment']\n\n# Prepare data (drop rows with any missing values)\nnyc_census = nyc_census[feature_cols + ['Income']].dropna()\n\nX_nyc = nyc_census[feature_cols].values\ny_nyc = nyc_census['Income'].values\n\n# Split data\nX_train_nyc, X_test_nyc, y_train_nyc, y_test_nyc = train_test_split(\n    X_nyc, y_nyc, test_size=0.3, random_state=1\n)\n\n# Fit model\nmodel_nyc = LinearRegression()\nmodel_nyc.fit(X_train_nyc, y_train_nyc)\n\n# Calculate residuals from our simple model\ny_pred_test = model_nyc.predict(X_test_nyc)\nresiduals = y_test_nyc - y_pred_test\n\n# Create a DataFrame for easy viewing\nresidual_df = pd.DataFrame({\n    'Actual': y_test_nyc.flatten(),\n    'Predicted': y_pred_test.flatten(),\n    'Residual': residuals.flatten()\n})\n\nresidual_df.head()\n\n\n\n\n\n\n\n\nActual\nPredicted\nResidual\n\n\n\n\n0\n87708.0\n89123.902250\n-1415.902250\n\n\n1\n21577.0\n20985.111768\n591.888232\n\n\n2\n51595.0\n59802.080768\n-8207.080768\n\n\n3\n102625.0\n72263.008289\n30361.991711\n\n\n4\n51045.0\n58308.513690\n-7263.513690\n\n\n\n\n\n\n\nNotice that the residuals all fluctuate around zero? That’s expected for linear regression—the errors cancel out on average. But that doesn’t mean the errors are acceptable! We need to look at the pattern.\n\n\n4.2 Residuals vs. Fitted Values Plot\nThis is the single most important diagnostic plot you’ll make. It plots residuals (y-axis) against predicted values (x-axis).\nWhat you want to see: Random scatter around zero. No patterns, no trends, just noise.\nWhat you don’t want to see: Curves, funnels, or systematic patterns. These indicate problems.\n\n# Create the residuals vs fitted plot\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred_test, residuals, alpha=0.5, s=20)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.xlabel('Fitted Values (Predictions)', fontsize=12)\nplt.ylabel('Residuals', fontsize=12)\nplt.title('Residuals vs. Fitted Values', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Also show distribution of residuals\nplt.figure(figsize=(10, 5))\nplt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Residual', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.title('Distribution of Residuals', fontsize=14)\nplt.axvline(0, color='red', linestyle='--', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee what’s happening? The residuals are somewhat randomly scattered, but there’s two small issues:\n\nThe model tends to underpredict (positive residuals) for very low predictions.\nThe model tends to overpredict (negative residuals) for very high predictions.\n\nThis suggests the relationship might not be perfectly linear. However, despite this, this shows a reasonably good fit overall, with normally distributed residuals centered around zero.\n\n\n4.3 What Patterns Tell You\nLet’s look at specific patterns and what they mean:\nPattern 1: Curved Residuals (Non-Linearity)\nIf residuals form a U-shape or inverted U-shape, your relationship isn’t linear. The model systematically underpredicts in some regions and overpredicts in others.\nFix: Try polynomial features, log transforms, or use a non-linear model.\nPattern 2: Funnel Shape (Heteroscedasticity)\nIf residuals spread out as predictions increase (or decrease), you have non-constant variance. Maybe small houses have predictable prices, but mansion prices vary wildly.\nFix: Transform the target variable (log, square root), use weighted regression, or use a model that handles heteroscedasticity.\nPattern 3: Outliers\nPoints far from zero are outliers. A few outliers are normal. Many outliers mean something’s wrong with your model or data.\nFix: Investigate outliers (data errors? special cases?), consider robust regression, or use regularization.\nLet’s demonstrate these patterns with synthetic data, comparing good vs. bad residual plots:\n\n# Create three datasets: good fit, non-linear, and heteroscedastic\nnp.random.seed(42)\n\n# GOOD: Linear relationship with constant variance\nX_good = np.linspace(0, 10, 200).reshape(-1, 1)\ny_good = 5 * X_good.flatten() + np.random.normal(0, 5, 200)\n\nmodel_good = LinearRegression()\nmodel_good.fit(X_good, y_good)\ny_pred_good = model_good.predict(X_good)\nresiduals_good = y_good - y_pred_good\n\n# BAD: Non-linear relationship\nX_nonlinear = np.linspace(0, 10, 200).reshape(-1, 1)\ny_nonlinear = 2 * X_nonlinear.flatten()**2 + np.random.normal(0, 10, 200)\n\nmodel_nonlinear = LinearRegression()\nmodel_nonlinear.fit(X_nonlinear, y_nonlinear)\ny_pred_nonlinear = model_nonlinear.predict(X_nonlinear)\nresiduals_nonlinear = y_nonlinear - y_pred_nonlinear\n\n# BAD: Heteroscedasticity (funnel pattern)\nX_hetero = np.linspace(1, 10, 200).reshape(-1, 1)\ny_hetero = 5 * X_hetero.flatten() + np.random.normal(0, X_hetero.flatten(), 200)\n\nmodel_hetero = LinearRegression()\nmodel_hetero.fit(X_hetero, y_hetero)\ny_pred_hetero = model_hetero.predict(X_hetero)\nresiduals_hetero = y_hetero - y_pred_hetero\n\n# Create comparison plot\nfig, axes = plt.subplots(3, 2, figsize=(14, 15))\n\n# Row 1: GOOD - Linear data with constant variance\naxes[0, 0].scatter(X_good, y_good, alpha=0.5, s=10, color='green')\naxes[0, 0].plot(X_good, y_pred_good, 'r-', linewidth=2)\naxes[0, 0].set_title('GOOD: Linear Data, Constant Variance', fontsize=12, fontweight='bold', color='green')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('y')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].scatter(y_pred_good, residuals_good, alpha=0.5, s=10, color='green')\naxes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[0, 1].set_title('✓ Random Scatter (Good!)', fontsize=12, fontweight='bold', color='green')\naxes[0, 1].set_xlabel('Fitted Values')\naxes[0, 1].set_ylabel('Residuals')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Row 2: BAD - Non-linearity\naxes[1, 0].scatter(X_nonlinear, y_nonlinear, alpha=0.5, s=10, color='orange')\naxes[1, 0].plot(X_nonlinear, y_pred_nonlinear, 'r-', linewidth=2)\naxes[1, 0].set_title('BAD: Non-Linear Data with Linear Fit', fontsize=12, fontweight='bold', color='orange')\naxes[1, 0].set_xlabel('X')\naxes[1, 0].set_ylabel('y')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].scatter(y_pred_nonlinear, residuals_nonlinear, alpha=0.5, s=10, color='orange')\naxes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[1, 1].set_title('✗ Curved Pattern (Bad!)', fontsize=12, fontweight='bold', color='orange')\naxes[1, 1].set_xlabel('Fitted Values')\naxes[1, 1].set_ylabel('Residuals')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Row 3: BAD - Heteroscedasticity\naxes[2, 0].scatter(X_hetero, y_hetero, alpha=0.5, s=10, color='red')\naxes[2, 0].plot(X_hetero, y_pred_hetero, 'r-', linewidth=2)\naxes[2, 0].set_title('BAD: Data with Increasing Variance', fontsize=12, fontweight='bold', color='red')\naxes[2, 0].set_xlabel('X')\naxes[2, 0].set_ylabel('y')\naxes[2, 0].grid(True, alpha=0.3)\n\naxes[2, 1].scatter(y_pred_hetero, residuals_hetero, alpha=0.5, s=10, color='red')\naxes[2, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[2, 1].set_title('✗ Funnel Pattern (Bad!)', fontsize=12, fontweight='bold', color='red')\naxes[2, 1].set_xlabel('Fitted Values')\naxes[2, 1].set_ylabel('Residuals')\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Compare the three residual plots (right column):\")\nprint(\"  TOP (Green): Random scatter around zero = GOOD\")\nprint(\"  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)\")\nprint(\"  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)\")\n\n\n\n\n\n\n\n\nCompare the three residual plots (right column):\n  TOP (Green): Random scatter around zero = GOOD\n  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)\n  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)\n\n\nSee the difference?\n\nTop row (GOOD): Residuals are randomly scattered around zero with constant spread. This is what you want!\nMiddle row (BAD - Curved): Residuals show a clear U-shaped pattern—the model systematically underpredicts at the extremes and overpredicts in the middle.\nBottom row (BAD - Funnel): Residuals fan out as predictions increase—variance is not constant.\n\n\n\n\n\n\n\nWarning\n\n\n\nA high R² doesn’t mean your assumptions are met! You can have R² = 0.95 but still have terrible residual patterns that indicate the model will fail on new data.\n\n\nThese plots are your early warning system. Learn to read them, and you’ll catch problems before they become disasters.\n\n\n4.4 What to Do When Assumptions Fail\nOkay, you’ve identified a problem in your residual plots. Now what?\nProblem: Non-Linearity (curved residual plot)\nSolutions:\n\nPolynomial features: Add X², X³, etc. (covered in Section 4)\nTransform features: Try log(X), √X, or 1/X\nTransform target: Try log(y) or √y\nUse a non-linear model: Tree-based models, neural networks, etc.\n\nProblem: Heteroscedasticity (funnel residual plot)\nSolutions:\n\nTransform target variable: log(y) often stabilizes variance\nWeighted least squares: Give less weight to high-variance observations\nUse robust standard errors: Adjust your confidence intervals\nJust accept it: If you only care about predictions (not inference), heteroscedasticity matters less\n\nProblem: Outliers\nSolutions:\n\nInvestigate: Are they data errors? Real but unusual observations?\nRemove them: Only if justified (e.g., data entry errors)\nUse robust regression: Huber regression, RANSAC, etc.\nUse regularization: Ridge and Lasso reduce outlier influence (covered in Sections 6-7)\n\n\n\n\n\n\n\nTip\n\n\n\nStart with the simplest fix first. A log transformation of the target variable often fixes multiple problems at once: non-linearity and heteroscedasticity.\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nCalculate residuals given the true values and the predicted values\nInterpret a residuals vs fitted plot to understand your models predictions\nInterpret a histogram of residuals to understand your models predictions\nInterpret the three common patterns in residual plots (section 4.3)"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-5",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-5",
    "title": "Chapter 2: Regression Models",
    "section": "5. Polynomial Regression: Handling Non-Linearity",
    "text": "5. Polynomial Regression: Handling Non-Linearity\nWhat do you do when your residual plot shows a clear curve? The relationship isn’t linear, so a straight line won’t work. This is where polynomial regression saves you.\n\n5.1 When Linear Isn’t Enough\nReal relationships are rarely perfectly linear. Temperature and ice cream sales? Definitely curved (sales don’t go negative when it’s cold, and they plateau when it’s hot).\nPolynomial regression lets you fit curves while still using linear regression. The trick? Create new features that are powers of your original features: X², X³, etc. Then fit a linear model to these polynomial features.\nHere’s the key insight: polynomial regression is still linear regression. It’s linear in the coefficients, even though the relationship with X is non-linear. The model is y = β₀ + β₁X + β₂X² + β₃X³, which is a linear combination of the features [X, X², X³].\n\n\n5.2 Creating Polynomial Features\nLet’s see this in action. We’ll create polynomial features and fit them to data with a clear non-linear relationship.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Use data with a non-linear relationship\n# Let's explore Unemployment vs Income (often a non-linear relationship)\nnyc_poly = nyc_census[['Unemployment', 'Income']].dropna()\n\n# Filter to only unemployment under 30% (ignore outliers)\nnyc_poly = nyc_poly[nyc_poly['Unemployment'] &lt; 30]\n\nX_unemp = nyc_poly[['Unemployment']].values\ny_income_poly = nyc_poly['Income'].values\n\n# Split the data\nX_unemp_train, X_unemp_test, y_income_poly_train, y_income_poly_test = train_test_split(\n    X_unemp, y_income_poly, test_size=0.2, random_state=42\n)\n\n# First, try linear regression\nmodel_linear_unemp = LinearRegression()\nmodel_linear_unemp.fit(X_unemp_train, y_income_poly_train)\ny_pred_linear_unemp = model_linear_unemp.predict(X_unemp_test)\n\nprint(\"Linear Model:\")\nprint(f\"R² = {model_linear_unemp.score(X_unemp_test, y_income_poly_test):.4f}\")\n\n# Now try polynomial regression (degree 2)\npoly_2 = PolynomialFeatures(degree=2, include_bias=False)\nX_unemp_train_poly2 = poly_2.fit_transform(X_unemp_train)\nX_unemp_test_poly2 = poly_2.transform(X_unemp_test)\n\nprint(f\"\\nOriginal features shape: {X_unemp_train.shape}\")\nprint(f\"Polynomial features shape: {X_unemp_train_poly2.shape}\")\nprint(f\"\\nFeature names: {poly_2.get_feature_names_out(['Unemployment'])}\")\n\nmodel_poly2 = LinearRegression()\nmodel_poly2.fit(X_unemp_train_poly2, y_income_poly_train)\ny_pred_poly2 = model_poly2.predict(X_unemp_test_poly2)\n\nprint(\"\\nDegree-2 Polynomial Model:\")\nprint(f\"R² = {model_poly2.score(X_unemp_test_poly2, y_income_poly_test):.4f}\")\nprint(f\"Coefficients: {model_poly2.coef_}\")\n\n# Visualize the difference\nplt.figure(figsize=(10, 6))\n# Sort for smooth plotting\nsort_idx = np.argsort(X_unemp_test.flatten())\nX_test_sorted = X_unemp_test.flatten()[sort_idx]\ny_pred_linear_sorted = y_pred_linear_unemp.flatten()[sort_idx]\ny_pred_poly2_sorted = y_pred_poly2.flatten()[sort_idx]\n\nplt.scatter(X_unemp_test, y_income_poly_test, alpha=0.3, s=10, label='Actual Data')\nplt.plot(X_test_sorted, y_pred_linear_sorted, 'r-', linewidth=2, label='Linear Model')\nplt.plot(X_test_sorted, y_pred_poly2_sorted, 'g-', linewidth=2, label='Degree-2 Polynomial')\nplt.xlabel('Unemployment Rate (%)', fontsize=12)\nplt.ylabel('Median Household Income ($)', fontsize=12)\nplt.title('Linear vs Polynomial Regression', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nLinear Model:\nR² = 0.2180\n\nOriginal features shape: (1670, 1)\nPolynomial features shape: (1670, 2)\n\nFeature names: ['Unemployment' 'Unemployment^2']\n\nDegree-2 Polynomial Model:\nR² = 0.2339\nCoefficients: [-5763.42735409   114.23513525]\n\n\n\n\n\n\n\n\n\nSee what happened? PolynomialFeatures(degree=2) created a new feature: Unemployment². The linear model then fits: y = β₀ + β₁(Unemployment) + β₂(Unemployment²). This gives us a parabola, which can capture curvature that a straight line can’t.\n\n\n\n\n\n\nTip\n\n\n\nHere we’re using the PolynomialFeatures pipeline from scikit-learn. However, you can also just square (or cube, or whatever you want) the column, such as df['unemp_poly2'] = df['unemp']**2.\n\n\n\n\n5.3 Choosing Polynomial Degree\nHow do you know what degree to use? Degree 1 is linear. Degree 2 adds curvature. Degree 3 adds an S-curve. Higher degrees add more wiggles.\nThe problem: Higher degree doesn’t always mean better. You can overfit spectacularly with high-degree polynomials.\nThe solution: Try multiple degrees and use a validation set (or cross-validation) to pick the best one.\n\n# Try polynomials of degree 1 through 10\ndegrees = range(1, 11)\ntrain_scores = []\ntest_scores = []\n\nfor degree in degrees:\n    # Create polynomial features\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_unemp_train)\n    X_test_poly = poly.transform(X_unemp_test)\n\n    # Fit model\n    model = LinearRegression()\n    model.fit(X_train_poly, y_income_poly_train)\n\n    # Score\n    train_score = model.score(X_train_poly, y_income_poly_train)\n    test_score = model.score(X_test_poly, y_income_poly_test)\n\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n\n    print(f\"Degree {degree}: Train R² = {train_score:.4f}, Test R² = {test_score:.4f}\")\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, train_scores, 'o-', linewidth=2, label='Training R²')\nplt.plot(degrees, test_scores, 's-', linewidth=2, label='Test R²')\nplt.xlabel('Polynomial Degree', fontsize=12)\nplt.ylabel('R²', fontsize=12)\nplt.title('Model Performance vs Polynomial Degree', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(degrees)\nplt.show()\n\n# Find optimal degree\noptimal_degree = degrees[np.argmax(test_scores)]\nprint(f\"\\nOptimal polynomial degree: {optimal_degree} (Test R² = {max(test_scores):.4f})\")\n\nDegree 1: Train R² = 0.2718, Test R² = 0.2180\nDegree 2: Train R² = 0.2890, Test R² = 0.2339\nDegree 3: Train R² = 0.2904, Test R² = 0.2329\nDegree 4: Train R² = 0.2904, Test R² = 0.2317\nDegree 5: Train R² = 0.2923, Test R² = 0.2294\nDegree 6: Train R² = 0.2953, Test R² = 0.2225\nDegree 7: Train R² = 0.2955, Test R² = 0.2185\nDegree 8: Train R² = 0.2965, Test R² = 0.2265\nDegree 9: Train R² = 0.2960, Test R² = 0.2290\nDegree 10: Train R² = 0.2969, Test R² = 0.2255\n\n\n\n\n\n\n\n\n\n\nOptimal polynomial degree: 2 (Test R² = 0.2339)\n\n\nSee that? Training R² keeps increasing with degree—the model just memorizes the training data. But test R² peaks and then starts decreasing. That’s overfitting. The model gets so wiggly it fits training noise instead of the true pattern.\n\n\n\n\n\n\nWarning\n\n\n\nNever choose polynomial degree based on training performance alone! Always use validation data or cross-validation. Otherwise, you’ll pick a high degree that overfits.\n\n\n\n\n5.4 The Overfitting Risk\nLet’s visualize what high-degree polynomials do. They create absurd wiggles that fit every bump in the training data but fail on new data.\n\n# Compare degree 2 vs degree 10\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Create small dataset to exaggerate overfitting\nnp.random.seed(42)\nX_small = np.linspace(0, 10, 30).reshape(-1, 1)\ny_small = 2 * X_small.flatten() + np.random.normal(0, 5, 30)\n\n# Degree 2\npoly_2_small = PolynomialFeatures(degree=2, include_bias=False)\nX_small_poly2 = poly_2_small.fit_transform(X_small)\nmodel_poly2_small = LinearRegression()\nmodel_poly2_small.fit(X_small_poly2, y_small)\n\n# Create smooth line for plotting\nX_plot = np.linspace(0, 10, 200).reshape(-1, 1)\nX_plot_poly2 = poly_2_small.transform(X_plot)\ny_plot_poly2 = model_poly2_small.predict(X_plot_poly2)\n\naxes[0].scatter(X_small, y_small, s=50, label='Training Data')\naxes[0].plot(X_plot, y_plot_poly2, 'r-', linewidth=2, label='Degree 2')\naxes[0].set_xlabel('X', fontsize=12)\naxes[0].set_ylabel('y', fontsize=12)\naxes[0].set_title('Degree 2: Reasonable Fit', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Degree 10\npoly_10_small = PolynomialFeatures(degree=10, include_bias=False)\nX_small_poly10 = poly_10_small.fit_transform(X_small)\nmodel_poly10_small = LinearRegression()\nmodel_poly10_small.fit(X_small_poly10, y_small)\n\nX_plot_poly10 = poly_10_small.transform(X_plot)\ny_plot_poly10 = model_poly10_small.predict(X_plot_poly10)\n\naxes[1].scatter(X_small, y_small, s=50, label='Training Data')\naxes[1].plot(X_plot, y_plot_poly10, 'r-', linewidth=2, label='Degree 10')\naxes[1].set_xlabel('X', fontsize=12)\naxes[1].set_ylabel('y', fontsize=12)\naxes[1].set_title('Degree 10: Overfitting!', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim(axes[0].get_ylim())  # Same y-axis for comparison\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Degree 2 - Training R²: {model_poly2_small.score(X_small_poly2, y_small):.4f}\")\nprint(f\"Degree 10 - Training R²: {model_poly10_small.score(X_small_poly10, y_small):.4f}\")\n\n\n\n\n\n\n\n\nDegree 2 - Training R²: 0.5739\nDegree 10 - Training R²: 0.6720\n\n\nLook at that degree-10 model! It wiggles wildly to pass through training points. Training R² is higher, but the model is useless for prediction. It learned noise, not signal.\nThe Extreme Case: More Features Than Observations (p &gt; n)\nNow let’s see what happens when you push this to the limit: more features than data points. This creates an overdetermined system where you can get perfect training fit (R² = 1.0) that means absolutely nothing.\n\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Load our Bronx census data again\nbronx_subset = nyc_census[nyc_census['Borough'] == 'Bronx'].copy()\nbronx_subset = bronx_subset.dropna(subset=['Income'])\n\n# Select just a few features to start\nbase_features = ['TotalPop', 'IncomePerCap', 'Poverty', 'Professional', 'Unemployment']\nbronx_tiny = bronx_subset[base_features + ['Income']].dropna()\n\n# Take only 30 census tracts (small sample)\nbronx_tiny_sample = bronx_tiny.sample(n=30, random_state=42)\n\nX_tiny = bronx_tiny_sample[base_features].values\ny_tiny = bronx_tiny_sample['Income'].values\n\nprint(f\"Starting with: {len(bronx_tiny_sample)} observations, {len(base_features)} features\")\n\n# Now create polynomial features to blow up the feature count\npoly_extreme = PolynomialFeatures(degree=4, include_bias=False)\nX_tiny_poly = poly_extreme.fit_transform(X_tiny)\n\nprint(f\"After polynomial expansion (degree 4): {X_tiny_poly.shape[0]} observations, {X_tiny_poly.shape[1]} features\")\nprint(f\"Features &gt; Observations: {X_tiny_poly.shape[1] &gt; X_tiny_poly.shape[0]}\")\n\n# Hold out just ONE observation for \"testing\"\nX_train_tiny = X_tiny_poly[:-1]\ny_train_tiny = y_tiny[:-1]\nX_test_tiny = X_tiny_poly[-1:]\ny_test_tiny = y_tiny[-1:]\n\nprint(f\"\\nTraining: {X_train_tiny.shape[0]} observations, {X_train_tiny.shape[1]} features\")\nprint(f\"p &gt; n? {X_train_tiny.shape[1] &gt; X_train_tiny.shape[0]}\")\n\n# Fit the model\nmodel_extreme = LinearRegression()\nmodel_extreme.fit(X_train_tiny, y_train_tiny)\n\n# Check performance\ntrain_r2_extreme = model_extreme.score(X_train_tiny, y_train_tiny)\ny_pred_train_extreme = model_extreme.predict(X_train_tiny)\ny_pred_test_extreme = model_extreme.predict(X_test_tiny)\n\ntrain_mse_extreme = mean_squared_error(y_train_tiny, y_pred_train_extreme)\ntest_error_extreme = abs(y_test_tiny[0] - y_pred_test_extreme[0])\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RESULTS: {X_train_tiny.shape[1]} features, {X_train_tiny.shape[0]} observations\")\nprint(f\"{'='*60}\")\nprint(f\"Training R²: {train_r2_extreme:.10f}\")\nprint(f\"Training MSE: {train_mse_extreme:.10f}\")\nprint(f\"\\nActual income (held-out): ${y_test_tiny[0]:,.0f}\")\nprint(f\"Predicted income: ${y_pred_test_extreme[0]:,.0f}\")\nprint(f\"Prediction error: ${test_error_extreme:,.0f}\")\nprint(f\"Prediction error (%): {100 * test_error_extreme / y_test_tiny[0]:.1f}%\")\n\n# Visualize the \"perfect\" fit\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Training \"perfection\"\naxes[0].scatter(y_train_tiny, y_pred_train_extreme, s=50, alpha=0.7)\naxes[0].plot([y_train_tiny.min(), y_train_tiny.max()],\n             [y_train_tiny.min(), y_train_tiny.max()],\n             'r--', linewidth=2, label='Perfect Prediction')\naxes[0].set_xlabel('Actual Income', fontsize=12)\naxes[0].set_ylabel('Predicted Income', fontsize=12)\naxes[0].set_title(f'Training: R² = {train_r2_extreme:.6f}', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Right: Test disaster\naxes[1].scatter([y_test_tiny[0]], [y_pred_test_extreme[0]], s=200, c='red',\n                marker='X', label='Test Prediction', zorder=3)\naxes[1].plot([y_train_tiny.min(), y_train_tiny.max()],\n             [y_train_tiny.min(), y_train_tiny.max()],\n             'r--', linewidth=2, label='Perfect Prediction')\naxes[1].set_xlabel('Actual Income', fontsize=12)\naxes[1].set_ylabel('Predicted Income', fontsize=12)\naxes[1].set_title(f'Test: Error = ${test_error_extreme:,.0f}', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nStarting with: 30 observations, 5 features\nAfter polynomial expansion (degree 4): 30 observations, 125 features\nFeatures &gt; Observations: True\n\nTraining: 29 observations, 125 features\np &gt; n? True\n\n============================================================\nRESULTS: 125 features, 29 observations\n============================================================\nTraining R²: 1.0000000000\nTraining MSE: 0.0000000000\n\nActual income (held-out): $52,344\nPredicted income: $167,147\nPrediction error: $114,803\nPrediction error (%): 219.3%\n\n\n\n\n\n\n\n\n\nWhat just happened?\nWhen p &gt; n (more features than observations), the system becomes underdetermined. The model can perfectly fit every training point because it has enough degrees of freedom to pass through all of them. R² = 1.0 is guaranteed mathematically—but it’s meaningless!\nThis is like drawing a curve through 5 points with a 10th-degree polynomial. You have so much flexibility that you can hit every point exactly. But the curve between points? Pure nonsense.\n\n\n\n\n\n\nWarning\n\n\n\nDANGER: The Perfect R² Trap\nIf you ever see R² = 1.0000 (or extremely close) on training data, be immediately suspicious:\n\nCheck if p ≥ n (features ≥ observations)\nCheck if you accidentally included the target variable as a feature\nCheck for data leakage (future information in features)\nCheck if you have duplicate rows\n\nA “perfect” fit is almost never real. It’s almost always a problem.\n\n\nReal-world implications:\nThis happens more often than you think:\n\nSmall datasets: Medical studies with 50 patients but 200 genetic markers\nHigh-dimensional data: Images, text, genomics where features vastly outnumber samples\nTime series: Predicting tomorrow with 100 technical indicators but only 30 days of data\n\nThe solution? Regularization (covered in the next sections) or getting more data. Never trust a model where p/n &gt; 1.0, and be very cautious when p/n &gt; 0.3.\nKey takeaways:\n\nPolynomial features let you model non-linear relationships with linear regression\nChoose degree using validation data, not training data\nLower degree often generalizes better than higher degree\nWhen p ≥ n, you get perfect training fit but meaningless predictions\nRegularization (covered next) can help control overfitting in polynomial models\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nUnderstand what polynomial features contribute to a linear regression model\nUnderstand what it means to be “linear regression” when using terms like \\(X^2\\), \\(X^3\\), etc.\nUnderstanding linear regression code using polynmomial regression\nModifying existing linear regression code to change polynomial regression features (e.g. adding additional terms, changing the variable being transformed, etc)\nInterpreting model performance vs polynomial degree graphs to pick an optimal degree\nUnderstand and explain the problems arising for picking too high of a polynomial degree (overfitting)\nUnderstand and explain why picking a degree higher than the number of data points can lead to “perfect” predictions which don’t generalize"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-6",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-6",
    "title": "Chapter 2: Regression Models",
    "section": "6. Multicollinearity: When Features Are Too Similar",
    "text": "6. Multicollinearity: When Features Are Too Similar\nImagine trying to predict house prices using both “square footage” and “square meters” as separate features. They contain basically the same information! This creates multicollinearity, and it breaks coefficient interpretation in sneaky ways.\n\n6.1 What Is Multicollinearity?\nMulticollinearity means your features are highly correlated with each other. One feature can be predicted fairly well from others.\nWhy is this a problem? Linear regression tries to isolate the effect of each feature while “holding others constant.” But if two features move together, you can’t hold one constant while changing the other—they’re tied together!\nThe result: coefficient estimates become unstable. Add or remove one observation, and coefficients swing wildly. Even worse, a feature that’s clearly important might show up with a tiny coefficient (or even the wrong sign!) because its effect is “stolen” by correlated features.\nLet’s create an example to see this:\n\n# Create dataset with multicollinearity\nnp.random.seed(42)\nn = 1000\n\n# X1 is random\nX1 = np.random.normal(0, 1, n)\n\n# X2 is highly correlated with X1\nX2 = X1 + np.random.normal(0, 0.1, n)  # Almost identical to X1\n\n# X3 is independent\nX3 = np.random.normal(0, 1, n)\n\n# True relationship: y = 5*X1 + 0*X2 + 3*X3 + noise\n# Note: X2 has NO effect, but it's correlated with X1\ny_multi = 5*X1 + 3*X3 + np.random.normal(0, 1, n)\n\n# Create DataFrame\ndf_multi = pd.DataFrame({\n    'X1': X1,\n    'X2': X2,\n    'X3': X3,\n    'y': y_multi\n})\n\n# Check correlation matrix\nprint(\"Correlation Matrix:\")\nprint(df_multi.corr())\n\n# Visualize correlations\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_multi.corr(), annot=True, cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Feature Correlation Matrix', fontsize=14)\nplt.show()\n\nCorrelation Matrix:\n          X1        X2        X3         y\nX1  1.000000  0.994818  0.022129  0.843973\nX2  0.994818  1.000000  0.020966  0.838088\nX3  0.022129  0.020966  1.000000  0.525535\ny   0.843973  0.838088  0.525535  1.000000\n\n\n\n\n\n\n\n\n\nSee that? X1 and X2 have a correlation of about 0.995. They’re nearly identical. Now watch what happens when we fit a regression:\n\n# Fit model with all features\nX_multi = df_multi[['X1', 'X2', 'X3']].values\ny_multi_target = df_multi['y'].values\n\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_multi, y_multi_target)\n\nprint(\"Coefficients with multicollinearity:\")\nfor i, name in enumerate(['X1', 'X2', 'X3']):\n    print(f\"  {name}: {model_multi.coef_[i]:.4f}\")\n\nprint(f\"\\nRemember: True coefficients are X1=5, X2=0, X3=3\")\nprint(\"But the model can't tell X1 and X2 apart!\")\n\n# Fit model with only X1 and X3 (no multicollinearity)\nX_clean = df_multi[['X1', 'X3']].values\nmodel_clean = LinearRegression()\nmodel_clean.fit(X_clean, y_multi_target)\n\nprint(\"\\nCoefficients without multicollinearity:\")\nfor i, name in enumerate(['X1', 'X3']):\n    print(f\"  {name}: {model_clean.coef_[i]:.4f}\")\n\nprint(\"\\nMuch closer to the true values!\")\n\nCoefficients with multicollinearity:\n  X1: 5.5507\n  X2: -0.5675\n  X3: 3.0223\n\nRemember: True coefficients are X1=5, X2=0, X3=3\nBut the model can't tell X1 and X2 apart!\n\nCoefficients without multicollinearity:\n  X1: 4.9855\n  X3: 3.0229\n\nMuch closer to the true values!\n\n\n\n\n6.2 Why It’s a Problem\nLet me be blunt: multicollinearity doesn’t hurt prediction accuracy. Your R² will be fine. Your predictions will be fine. So why do we care?\nProblem 1: Coefficient interpretation becomes meaningless\nIf X1 and X2 are highly correlated, the model might give X1 a coefficient of 10 and X2 a coefficient of -5. Or it might do the opposite: X1 = -5, X2 = 10. Or X1 = 2.5, X2 = 2.5. All three give similar predictions! But which feature is “really” important? You can’t tell.\n\n\n\n\n\n\nNote\n\n\n\nOne of the main benefits of linear regression models is their interpretability. You can look at the coefficients and read off what they tell you. If you lose that, then you might as well not use a linear model!\n\n\nProblem 2: Coefficient instability\nSmall changes in the data cause huge swings in coefficients. Add 10 new observations? Coefficients might flip signs. This makes the model untrustworthy for understanding relationships.\nProblem 3: Hard to select features\nIf X1 and X2 are correlated, dropping one might barely hurt performance, but dropping both kills it. This makes feature selection confusing.\nWhen to care:\n\nYou need to interpret coefficients (e.g., explaining to stakeholders)\nYou want to identify the “most important” features\nYou’re making causal claims\n\n\n\n6.3 Detecting Multicollinearity\nMethod 1: Correlation Matrix\nThe simplest approach. Look for correlations close to ±1 (say, above 0.8 or 0.9).\n\n# Use NYC census data\nfeatures_for_vif = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']\nnyc_vif = nyc_census[features_for_vif].dropna()\nX_vif = nyc_vif\n\n# Correlation matrix\ncorr_matrix = X_vif.corr()\n\n# Visualize\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, fmt='.3f')\nplt.title('Feature Correlation Matrix - NYC Census Data', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Identify high correlations\nhigh_corr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if abs(corr_matrix.iloc[i, j]) &gt; 0.7:\n            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\nprint(\"\\nHighly correlated feature pairs (|correlation| &gt; 0.7):\")\nfor feat1, feat2, corr in high_corr_pairs:\n    print(f\"  {feat1} & {feat2}: {corr:.3f}\")\n\n\n\n\n\n\n\n\n\nHighly correlated feature pairs (|correlation| &gt; 0.7):\n  Professional & IncomePerCap: 0.782\n  Poverty & ChildPoverty: 0.911\n\n\nMethod 2: Variance Inflation Factor (VIF)\nVIF measures how much the variance of a coefficient is “inflated” due to multicollinearity.\nVIF interpretation:\n\nVIF = 1: No multicollinearity\nVIF = 1-5: Moderate multicollinearity (usually okay)\nVIF = 5-10: High multicollinearity (concerning)\nVIF &gt; 10: Severe multicollinearity (definitely a problem)\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = features_for_vif\nvif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(features_for_vif))]\n\n# Visualize VIF\nplt.figure(figsize=(10, 6))\nplt.barh(vif_data[\"Feature\"], vif_data[\"VIF\"])\nplt.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='VIF = 5 (Moderate)')\nplt.axvline(x=10, color='red', linestyle='--', linewidth=2, label='VIF = 10 (Severe)')\nplt.xlabel('VIF', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Variance Inflation Factors', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3, axis='x')\nplt.show()\n\n\n\n\n\n\n\n\nSee features with high VIF? Those are the ones tangled up with others. The solution? Remove one of the correlated features, combine them, or use regularization (which we’ll cover next).\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nExplain what multicollinearity is, and why it can cause issues with predictions in linear regression models\nInterpret feature correlation matrices to find highly correlated features\nInterpret VIF values to find highly correlated features"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-7",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-7",
    "title": "Chapter 2: Regression Models",
    "section": "7. Ridge Regression: L2 Regularization",
    "text": "7. Ridge Regression: L2 Regularization\nMulticollinearity makes coefficients unstable. Polynomial features risk overfitting. The solution to both? Regularization. It’s one of the most important ideas in machine learning.\n\n7.1 The Regularization Idea\nHere’s the core insight: penalize large coefficients. Instead of just minimizing error, also minimize the size of coefficients. The model has to balance two goals:\n\nFit the training data well (low error)\nKeep coefficients small (low complexity)\n\nWhy does this help? Large coefficients make the model sensitive to small changes in features. By shrinking coefficients, you make the model more stable. We want to discourage the model from making wild swings in predictions from very small changes in the data. Imagine if you had a coefficient of 10,000,000. Then a one unit change in x would add ten million to your predicted value. That’s almost certainly not desirable.\nYes, you’re intentionally introducing bias (the model won’t fit training data perfectly). But you reduce variance (the model generalizes better to new data).\nThis is the famous bias-variance tradeoff. A little bias for a lot less variance is usually a great deal.\n\n\n7.2 How Ridge Works\nRidge regression adds a penalty term to the loss function:\n\\[\n\\text{Loss} = \\text{MSE} + \\alpha \\displaystyle\\sum\\text{coefficients}^2\n\\]\nThat second term is the L2 penalty (sum of squared coefficients). The α (alpha) parameter controls how much you penalize:\n\nα = 0: No penalty, just regular linear regression\nSmall α: Light penalty, coefficients shrink a little\nLarge α: Heavy penalty, coefficients shrink toward zero (but never reach exactly zero)\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that we want as small a loss as possible. The goal of machine learning algorithms are to find the parameters (e.g. coefficients of the regression model) that minimize the loss function. Regularization adds the coefficients themselves (squared) to the loss function, meaning the algorithm is penalized for choosing large coefficients.\n\n\nLet’s see it in action using our synthetic data with extreme multicollinearity from section 5.1. Remember: X1 and X2 are nearly identical (correlation ≈ 0.995), and the true model is y = 5X1 + 3X3.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\n# Use the multicollinearity data we created earlier\n# X1 and X2 are highly correlated; true coefficients are X1=5, X2=0, X3=3\nX_ridge = df_multi[['X1', 'X2', 'X3']].values\ny_ridge = df_multi['y'].values\n\n# Split data\nX_ridge_train, X_ridge_test, y_ridge_train, y_ridge_test = train_test_split(\n    X_ridge, y_ridge, test_size=0.2, random_state=42\n)\n\n# IMPORTANT: Always scale features before regularization!\n# Features with larger scales get penalized more\nscaler = StandardScaler()\nX_ridge_train_scaled = scaler.fit_transform(X_ridge_train)\nX_ridge_test_scaled = scaler.transform(X_ridge_test)\n\n# Fit regular linear regression\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_ridge_train_scaled, y_ridge_train)\nprint(\"Linear Regression (no regularization):\")\nprint(f\"  Train R²: {model_lr.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lr.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"  Coefficients: {model_lr.coef_}\")\nprint(f\"  Coefficient magnitudes: {np.abs(model_lr.coef_).sum():.4f}\")\nprint(f\"\\n  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)\")\nprint(f\"  True coefficients: X1=5, X2=0, X3=3\")\nprint(f\"  But with multicollinearity, coefficients are unstable!\")\n\n# Fit Ridge with alpha=1\nmodel_ridge = Ridge(alpha=1.0)\nmodel_ridge.fit(X_ridge_train_scaled, y_ridge_train)\nprint(\"\\nRidge Regression (alpha=1.0):\")\nprint(f\"  Train R²: {model_ridge.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_ridge.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"  Coefficients: {model_ridge.coef_}\")\nprint(f\"  Coefficient magnitudes: {np.abs(model_ridge.coef_).sum():.4f}\")\nprint(f\"\\n  Ridge distributes the X1 effect across X1 and X2 more evenly\")\nprint(f\"  Coefficients are more stable!\")\n\n# Visualize coefficient shrinkage\nfeature_names = ['X1', 'X2', 'X3']\ncoef_comparison = pd.DataFrame({\n    'Feature': feature_names,\n    'Linear Regression': model_lr.coef_,\n    'Ridge (α=1)': model_ridge.coef_,\n    'True Value': [5, 0, 3]\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\nax.bar(x - width, coef_comparison['True Value'], width, label='True Coefficients', alpha=0.8, color='green')\nax.bar(x + width, coef_comparison['Ridge (α=1)'], width, label='Ridge (α=1)', alpha=0.8, color='red')\nax.bar(x, coef_comparison['Linear Regression'], width, label='Linear Regression', alpha=0.8, color='blue')\nax.set_xlabel('Feature', fontsize=12)\nax.set_ylabel('Coefficient Value', fontsize=12)\nax.set_title('Ridge Handles Multicollinearity Better Than OLS', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(feature_names)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\nLinear Regression (no regularization):\n  Train R²: 0.9700\n  Test R²: 0.9665\n  Coefficients: [ 5.71502205 -0.82461178  3.00187963]\n  Coefficient magnitudes: 9.5415\n\n  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)\n  True coefficients: X1=5, X2=0, X3=3\n  But with multicollinearity, coefficients are unstable!\n\nRidge Regression (alpha=1.0):\n  Train R²: 0.9699\n  Test R²: 0.9670\n  Coefficients: [ 5.07662499 -0.1891781   2.99844018]\n  Coefficient magnitudes: 8.2642\n\n  Ridge distributes the X1 effect across X1 and X2 more evenly\n  Coefficients are more stable!\n\n\n\n\n\n\n\n\n\nSee what happened? With regular linear regression, the coefficients for X1 and X2 were far from the true values. Ridge regression distributes the effect more evenly between the correlated features. While Ridge coefficients still aren’t perfect (X2 should be 0), they’re much more stable and reasonable.\n\n\n\n\n\n\nWarning\n\n\n\nAlways scale features before using Ridge! Features with large scales get penalized more heavily than features with small scales. Standardizing (mean=0, std=1) ensures all features are treated equally. Imagine if I wrote your salary in pennies. Each year you get a raise, and the raise would look enormous to the model!\n\n\n\n\n7.3 Choosing Alpha\nHow do you pick \\(\\alpha\\)? Try many values and use cross-validation to see which generalizes best. Let’s continue with our synthetic multicollinearity data:\n\nfrom sklearn.model_selection import cross_val_score\n\n# Try many alpha values\nalphas = np.logspace(-2, 2, 50)  # From 0.01 to 100\ntrain_scores_ridge = []\ntest_scores_ridge = []\ncv_scores_ridge = []\ncoef_sum_ridge = []\n\nfor alpha in alphas:\n    model = Ridge(alpha=alpha)\n\n    # Train score\n    model.fit(X_ridge_train_scaled, y_ridge_train)\n    train_scores_ridge.append(model.score(X_ridge_train_scaled, y_ridge_train))\n\n    # Test score\n    test_scores_ridge.append(model.score(X_ridge_test_scaled, y_ridge_test))\n\n    # Cross-validation score (5-fold)\n    cv_score = cross_val_score(model, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    cv_scores_ridge.append(cv_score)\n\n    # Track total coefficient magnitude\n    coef_sum_ridge.append(np.abs(model.coef_).sum())\n\n# Plot results\nfig = plt.figure(figsize=(8, 5))\n\n# Plot 1: R² scores\nplt.plot(alphas, train_scores_ridge, label='Training R²', linewidth=2)\nplt.plot(alphas, test_scores_ridge, label='Test R²', linewidth=2)\nplt.plot(alphas, cv_scores_ridge, label='CV R² (5-fold)', linewidth=2, linestyle='--')\nplt.xscale('log')\nplt.xlabel('Alpha (log scale)', fontsize=12)\nplt.ylabel('R²', fontsize=12)\nplt.title('Ridge Regression: Choosing Alpha', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\noptimal_alpha = alphas[np.argmax(cv_scores_ridge)]\nplt.axvline(optimal_alpha, color='red', linestyle=':', linewidth=2, label='Optimal Alpha')\n\nplt.tight_layout()\nplt.show()\n\n# Best alpha\nprint(f\"Optimal alpha (by cross-validation): {optimal_alpha:.4f}\")\nprint(f\"Best CV R²: {max(cv_scores_ridge):.4f}\")\n\n\n\n\n\n\n\n\nOptimal alpha (by cross-validation): 0.0100\nBest CV R²: 0.9692\n\n\nAs \\(\\alpha\\) increases:\n\nTraining R² decreases (more bias, less fit to training data), however…\nTest/CV R² are still high, and even increase for a while (multicollinearity handled), then sharply decreases (too much bias)\n\nThe optimal \\(\\alpha\\) is where test/CV performance peaks. For this synthetic data with severe multicollinearity, Ridge helps stabilize coefficients even with small α values.\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nUnderstand mathematically (at a high level) how ridge regression penalizes models for picking very large coefficients\nUnderstand the role of \\(\\alpha\\) in regularization (how larger or smaller \\(\\alpha\\) values change the penalty)\nInterpret a graph showing \\(\\alpha\\) versus \\(R^2\\) to determine the optimal \\(\\alpha\\)"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-8",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-8",
    "title": "Chapter 2: Regression Models",
    "section": "8. Lasso Regression: L1 Regularization",
    "text": "8. Lasso Regression: L1 Regularization\nRidge is great, but it never says “this feature is useless.” Lasso does. It performs automatic feature selection by setting some coefficients to exactly zero.\n\n8.1 How Lasso Differs from Ridge\nLasso uses L1 regularization instead of L2:\n\\[\n\\text{Loss} = \\text{MSE} + \\alpha \\sum |\\text{coefficients}|\n\\]\nThe difference? Instead of squaring coefficients (Ridge), we take their absolute value (Lasso). This small change has a huge impact: Lasso can set coefficients to exactly zero.\nWhy? It’s geometry. The L1 penalty creates “corners” where the optimal solution often has some coefficients at exactly zero. Ridge’s L2 penalty is smooth, so coefficients approach zero but never arrive.\n\n\n8.2 Lasso for Feature Selection\nLasso is feature selection built into the regression. As α increases, Lasso zeroes out features one by one, keeping only the most important.\n\nfrom sklearn.linear_model import Lasso\n\nalphas_path = np.logspace(-2, 2, 100)\n\n# Fit Lasso with alpha=0.01\nmodel_lasso = Lasso(alpha=0.01, max_iter=10000)\nmodel_lasso.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(\"Lasso Regression (alpha=0.01):\")\nprint(f\"  Train R²: {model_lasso.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lasso.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"\\nCoefficients:\")\nfor feature, coef in zip(feature_names, model_lasso.coef_):\n    if abs(coef) &lt; 0.0001:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED!\")\n    else:\n        print(f\"  {feature}: {coef:.6f}\")\n\n# Compare with larger alpha\nmodel_lasso_strong = Lasso(alpha=0.1, max_iter=10000)\nmodel_lasso_strong.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(\"\\nLasso Regression (alpha=0.1):\")\nprint(f\"  Train R²: {model_lasso_strong.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lasso_strong.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"\\nCoefficients:\")\nfor feature, coef in zip(feature_names, model_lasso_strong.coef_):\n    if abs(coef) &lt; 0.0001:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED!\")\n    else:\n        print(f\"  {feature}: {coef:.6f}\")\n\n# Visualize feature selection\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\nax.bar(x - width, model_lr.coef_, width, label='Linear Regression', alpha=0.8)\nax.bar(x, model_lasso.coef_, width, label='Lasso (α=0.01)', alpha=0.8)\nax.bar(x + width, model_lasso_strong.coef_, width, label='Lasso (α=0.1)', alpha=0.8)\nax.set_xlabel('Feature', fontsize=12)\nax.set_ylabel('Coefficient Value', fontsize=12)\nax.set_title('Lasso Feature Selection', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(feature_names)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\nLasso Regression (alpha=0.01):\n  Train R²: 0.9698\n  Test R²: 0.9671\n\nCoefficients:\n  X1: 4.884947\n  X2: 0.000000 → ELIMINATED!\n  X3: 2.992327\n\nLasso Regression (alpha=0.1):\n  Train R²: 0.9692\n  Test R²: 0.9680\n\nCoefficients:\n  X1: 4.797478\n  X2: 0.000000 → ELIMINATED!\n  X3: 2.904858\n\n\n\n\n\n\n\n\n\nSee how Lasso zeroed out some features completely? That’s automatic feature selection. Larger α means more aggressive selection.\n\n\n8.3 Visualizing the Regularization Path\nThe “regularization path” shows how coefficients shrink as α increases. For Lasso, coefficients hit zero and stay there.\n\n# Track coefficients across alpha values for both Lasso and Ridge\nalphas_lasso = np.logspace(-3, 1, 100)  # From 0.001 to 10\ncoefs_lasso = []\ncoefs_ridge_comparison = []\n\nfor alpha in alphas_lasso:\n    # Lasso\n    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n    lasso_model.fit(X_ridge_train_scaled, y_ridge_train)\n    coefs_lasso.append(lasso_model.coef_)\n\n    # Ridge (for comparison)\n    ridge_model = Ridge(alpha=alpha)\n    ridge_model.fit(X_ridge_train_scaled, y_ridge_train)\n    coefs_ridge_comparison.append(ridge_model.coef_)\n\ncoefs_lasso = np.array(coefs_lasso)\ncoefs_ridge_comparison = np.array(coefs_ridge_comparison)\n\n# Plot Lasso regularization path\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lasso path\ncolors = ['blue', 'red', 'green']\nfor i, feature in enumerate(feature_names):\n    axes[0].plot(alphas_lasso, coefs_lasso[:, i], label=feature, linewidth=2, color=colors[i])\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Alpha (log scale)', fontsize=12)\naxes[0].set_ylabel('Coefficient Value', fontsize=12)\naxes[0].set_title('Lasso Regularization Path', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n\n# Ridge path (for comparison)\nfor i, feature in enumerate(feature_names):\n    axes[1].plot(alphas_lasso, coefs_ridge_comparison[:, i], label=feature, linewidth=2, color=colors[i])\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Alpha (log scale)', fontsize=12)\naxes[1].set_ylabel('Coefficient Value', fontsize=12)\naxes[1].set_title('Ridge Regularization Path', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSee the difference? Lasso coefficients hit zero abruptly and stay there (left plot). Ridge coefficients smoothly approach zero but never reach it (right plot).\n\n\n\n\n\n\nNote\n\n\n\nLasso is more aggressive than Ridge. It makes hard decisions: “this feature matters” or “this feature doesn’t.” Ridge says “this feature matters a little less.” Choose based on whether you want sparse models (Lasso) or stable coefficients (Ridge).\n\n\n\n\n8.4 When to Use Lasso vs. Ridge\nUse Lasso when:\n\nYou suspect many features are irrelevant\nYou want a sparse model (fewer features)\nYou need to explain which features matter most\nInterpretability is critical\n\nUse Ridge when:\n\nYou think most features contribute something\nYou have multicollinearity and want stable coefficients\nYou’re okay with keeping all features\nYou prioritize prediction over interpretation\n\nThe truth? Try both and use cross-validation to decide. Sometimes Ridge wins. Sometimes Lasso wins. Sometimes they’re tied.\n\n# Compare Ridge vs Lasso performance\nalphas_compare = np.logspace(-3, 2, 50)\nridge_scores = []\nlasso_scores = []\n\nfor alpha in alphas_compare:\n    # Ridge\n    ridge = Ridge(alpha=alpha)\n    ridge_cv = cross_val_score(ridge, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    ridge_scores.append(ridge_cv)\n\n    # Lasso\n    lasso = Lasso(alpha=alpha, max_iter=10000)\n    lasso_cv = cross_val_score(lasso, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    lasso_scores.append(lasso_cv)\n\n# Plot comparison\nplt.figure(figsize=(10, 6))\nplt.plot(alphas_compare, ridge_scores, 'o-', label='Ridge', linewidth=2)\nplt.plot(alphas_compare, lasso_scores, 's-', label='Lasso', linewidth=2)\nplt.xscale('log')\nplt.xlabel('Alpha (log scale)', fontsize=12)\nplt.ylabel('Cross-Validation R²', fontsize=12)\nplt.title('Ridge vs Lasso: Cross-Validation Performance', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Best performance\nbest_ridge_score = max(ridge_scores)\nbest_lasso_score = max(lasso_scores)\nprint(f\"Best Ridge CV R²: {best_ridge_score:.4f}\")\nprint(f\"Best Lasso CV R²: {best_lasso_score:.4f}\")\n\n\n\n\n\n\n\n\nBest Ridge CV R²: 0.9692\nBest Lasso CV R²: 0.9692\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere we see that as \\(\\alpha\\) increased, ridge was relatively unaffectd, but lasso quickly had poor prediction. Why do you think that is? Compare this with the regularization path plots above, can you explain what’s going on here?\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nUnderstand how lasso regularization differs from ridge regularization\nUnderstand what lasso does to coefficients in linear regression models\nUnderstand when to use lasso vs ridge regularization"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-9",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-9",
    "title": "Chapter 2: Regression Models",
    "section": "9. Elastic Net: Best of Both Worlds",
    "text": "9. Elastic Net: Best of Both Worlds\nCan’t decide between Ridge and Lasso? Why not both? Elastic Net combines L1 and L2 regularization to get the best of both worlds.\n\n9.1 Combining L1 and L2\nElastic Net uses a mix of Ridge (L2) and Lasso (L1) penalties: \\[\n\\text{Loss} = \\text{MSE} + \\alpha \\left[\n    \\omega \\sum |\\beta_j| + (1 - \\omega) \\sum \\beta_j^2 \\right]\n\\]\nTwo hyperparameters:\n\nα (alpha): Overall regularization strength (like Ridge and Lasso)\n\\(\\omega\\) = l1_ratio: Mix between L1 and L2\n\n\\(\\omega\\) = 0: Pure Ridge\n\\(\\omega\\) = 1: Pure Lasso\n\\(\\omega\\) = 0.5: Equal mix of both\n\n\nWhy combine them? Lasso can be unstable when features are highly correlated—it randomly picks one and zeros the others. Ridge keeps all correlated features but doesn’t select. Elastic Net does feature selection (like Lasso) but more stably (like Ridge).\n\nfrom sklearn.linear_model import ElasticNet\n\n# Fit Elastic Net with different l1_ratios\nl1_ratios = [0.2, 0.5, 0.8]\nalpha_en = 0.01\n\n# Also include Ridge and Lasso for comparison\nmodels = {\n    'Ridge (l1=0)': Ridge(alpha=alpha_en),\n    'ElasticNet (l1=0.2)': ElasticNet(alpha=alpha_en, l1_ratio=0.2, max_iter=10000),\n    'ElasticNet (l1=0.5)': ElasticNet(alpha=alpha_en, l1_ratio=0.5, max_iter=10000),\n    'ElasticNet (l1=0.8)': ElasticNet(alpha=alpha_en, l1_ratio=0.8, max_iter=10000),\n    'Lasso (l1=1)': Lasso(alpha=alpha_en, max_iter=10000)\n}\n\nfor idx, (name, model) in enumerate(models.items()):\n    model.fit(X_ridge_train_scaled, y_ridge_train)\n    r2 = model.score(X_ridge_test_scaled, y_ridge_test)\n    print(f\"{name}: Test R² = {r2:.4f}\")\n    print(f\"  Coefficients: {model.coef_}\")\n    print(f\"  Non-zero coefficients: {np.sum(np.abs(model.coef_) &gt; 0.0001)}\")\n    print()\n\n# Visualize one Elastic Net model\nmodel_en = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)\nmodel_en.fit(X_ridge_train_scaled, y_ridge_train)\n\n# Compare all three\nfig = plt.figure(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\n\n# Get a Ridge and Lasso with same alpha for fair comparison\nmodel_ridge_comp = Ridge(alpha=0.01)\nmodel_ridge_comp.fit(X_ridge_train_scaled, y_ridge_train)\n\nmodel_lasso_comp = Lasso(alpha=0.01, max_iter=10000)\nmodel_lasso_comp.fit(X_ridge_train_scaled, y_ridge_train)\n\nplt.bar(x - width, model_ridge_comp.coef_, width, label='Ridge', alpha=0.8)\nplt.bar(x, model_en.coef_, width, label='Elastic Net (l1_ratio=0.5)', alpha=0.8)\nplt.bar(x + width, model_lasso_comp.coef_, width, label='Lasso', alpha=0.8)\n\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Coefficient Value', fontsize=12)\nplt.title('Ridge vs Elastic Net vs Lasso', fontsize=14)\nplt.xticks(x, feature_names)\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\nRidge (l1=0): Test R² = 0.9665\n  Coefficients: [ 5.70711611 -0.81673551  3.00184572]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.2): Test R² = 0.9673\n  Coefficients: [3.73186865 1.1378942  2.97734663]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.5): Test R² = 0.9673\n  Coefficients: [4.118586   0.75533971 2.98299579]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.8): Test R² = 0.9672\n  Coefficients: [4.82987896 0.04824256 2.98856424]\n  Non-zero coefficients: 3\n\nLasso (l1=1): Test R² = 0.9671\n  Coefficients: [4.88494701 0.         2.99232692]\n  Non-zero coefficients: 2\n\n\n\n\n\n\n\n\n\n\nElastic Net sits between Ridge and Lasso. It zeros out some features (like Lasso) but keeps coefficients more stable (like Ridge).\n\n\n9.2 When to Use Elastic Net\nUse Elastic Net when:\n\nYou have groups of correlated features\nYou want feature selection but Lasso is too unstable\nYou’re not sure if Ridge or Lasso is better (hedge your bets)\nYou have more features than observations (p &gt; n)\n\nReal-world scenario: You have 50 features measuring similar things (different weather stations, different survey questions, etc.). Lasso might randomly pick one from each group. Ridge keeps all 50. Elastic Net picks a few from each group—the best compromise.\nPractical advice: If you’re unsure, use Elastic Net with l1_ratio=0.5 and tune it with cross-validation. It’s a safe default that adapts to your data.\n\n# Tune both alpha and l1_ratio with grid search\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'alpha': np.logspace(-3, 1, 20),\n    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n}\n\nelastic_net = ElasticNet(max_iter=10000)\ngrid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='r2', n_jobs=-1)\ngrid_search.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV R²: {grid_search.best_score_:.4f}\")\nprint(f\"Test R²: {grid_search.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\n\n# Best model coefficients\nbest_model = grid_search.best_estimator_\nprint(f\"\\nBest model coefficients:\")\nfor feature, coef in zip(feature_names, best_model.coef_):\n    if abs(coef) &gt; 0.0001:\n        print(f\"  {feature}: {coef:.6f}\")\n    else:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED\")\n\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"folder\",\"base64_name\" for automatic cleanup: unknown resource type (\"L3Zhci9mb2xkZXJzL3pmLzdkMnBkbWxzN3lsZDk2Z2owMmR6Yl9naDAwMDBnbi9UL2pvYmxpYl9tZW1tYXBwaW5nX2ZvbGRlcl85OTA4M180OTQ0ODM3YTBiZDk0ODhiYWQxZGE1YzFiMTFhN2FlYl9mYjcxODE2YzliNmI0MTgwYjMxNjMyZDJkMGNhZDY4Mw==\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtM3JfMnA4OHI=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtZDlzOHU3OHA=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMta2NuenA5djQ=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtZWNhaGt6ODU=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtZHRlc3o4eHg=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtcTVoNml6cDc=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"folder\",\"base64_name\" for automatic cleanup: unknown resource type (\"L3Zhci9mb2xkZXJzL3pmLzdkMnBkbWxzN3lsZDk2Z2owMmR6Yl9naDAwMDBnbi9UL2pvYmxpYl9tZW1tYXBwaW5nX2ZvbGRlcl85OTA4M180OTQ0ODM3YTBiZDk0ODhiYWQxZGE1YzFiMTFhN2FlYl8xMWM4YTMwMTQ1OTM0MmZkYTFiODFhZDZlMmVkYWMzOA==\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMteDNlaWo5Ym0=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtc2Y3ejlrNno=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtdm9jYnR6N2w=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtaHFiNm5uY3Y=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtaml1c2w1em0=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtc2Y2bnQ2YTU=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtNHBhZGxzMDU=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\nTraceback (most recent call last):\n\n  File \"/Users/paulsavala/Projects/math-3339-instructor/venv/lib/python3.14/site-packages/joblib/externals/loky/backend/resource_tracker.py\", line 297, in main\n\n    raise ValueError(\n\n    ...&lt;4 lines&gt;...\n\n    )\n\nValueError: Cannot register \"REGISTER\",\"rtype\":\"semlock\",\"base64_name\" for automatic cleanup: unknown resource type (\"L2xva3ktOTkwODMtcnIyMmRlbzA=\"}). Resource type should be one of the following: ['noop', 'folder', 'file', 'semlock']\n\n\n\n\nBest parameters: {'alpha': np.float64(0.001), 'l1_ratio': 0.9}\nBest CV R²: 0.9691\nTest R²: 0.9667\n\nBest model coefficients:\n  X1: 5.474694\n  X2: -0.584535\n  X3: 3.000771\n\n\nElastic Net automatically found the best combination of Ridge and Lasso for your data. That’s the power of combining regularization techniques.\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nUnderstand how elastic net combines both ridge and lasso regularization\nUnderstand when to use elastic net regularization"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-10",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-10",
    "title": "Chapter 2: Regression Models",
    "section": "10. Putting It All Together: A Complete Regression Analysis",
    "text": "10. Putting It All Together: A Complete Regression Analysis\nYou’ve learned all the pieces. Now let’s put them together into a complete regression workflow that you can follow for any project.\n\n10.1 The Diagnostic Workflow\nHere’s the process every regression analysis should follow:\n\nStep 1: Fit a baseline linear model\nStep 2: Check assumptions with diagnostic plots\nStep 3: Identify problems (non-linearity, heteroscedasticity, multicollinearity)\nStep 4: Fix problems (transformations, polynomial features, regularization)\nStep 5: Validate with cross-validation\nStep 6: Interpret and communicate results\n\nLet’s walk through this with a complete example:\n\nfrom scipy import stats\n\n# Step 1: Fit baseline linear model\nprint(\"=\"*60)\nprint(\"STEP 1: Baseline Linear Regression\")\nprint(\"=\"*60)\n\nfeatures_complete = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']\nnyc_complete = nyc_census[features_complete + ['Income']].dropna()\nX_complete = nyc_complete[features_complete].values\ny_complete = nyc_complete['Income'].values\n\nX_train_complete, X_test_complete, y_train_complete, y_test_complete = train_test_split(\n    X_complete, y_complete, test_size=0.2, random_state=42\n)\n\n# Baseline model\nmodel_baseline = LinearRegression()\nmodel_baseline.fit(X_train_complete, y_train_complete)\ny_pred_baseline = model_baseline.predict(X_test_complete)\n\nprint(f\"Train R²: {model_baseline.score(X_train_complete, y_train_complete):.4f}\")\nprint(f\"Test R²: {model_baseline.score(X_test_complete, y_test_complete):.4f}\")\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test_complete, y_pred_baseline)):.4f}\")\n\n# Step 2: Check assumptions with diagnostic plots\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: Diagnostic Plots\")\nprint(\"=\"*60)\n\nresiduals_complete = y_test_complete - y_pred_baseline\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Residuals vs Fitted\naxes[0].scatter(y_pred_baseline, residuals_complete, alpha=0.5, s=10)\naxes[0].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Fitted Values')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('Residuals vs Fitted')\naxes[0].grid(True, alpha=0.3)\n\n# Histogram of residuals\naxes[1].hist(residuals_complete, bins=50, edgecolor='black', alpha=0.7)\naxes[1].axvline(0, color='red', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Residuals')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Residual Distribution')\naxes[1].grid(True, alpha=0.3)\n\n# Scale-Location Plot\nstandardized_resid = residuals_complete / residuals_complete.std()\naxes[2].scatter(y_pred_baseline, np.sqrt(np.abs(standardized_resid)), alpha=0.5, s=10)\naxes[2].set_xlabel('Fitted Values')\naxes[2].set_ylabel('√|Standardized Residuals|')\naxes[2].set_title('Scale-Location Plot')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Observations:\")\nprint(\"- Residuals show some heteroscedasticity (variance increases with fitted values)\")\nprint(\"- Overall pattern suggests room for improvement\")\n\n# Step 3: Check for multicollinearity\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 3: Check Multicollinearity\")\nprint(\"=\"*60)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = features_complete\nvif_data[\"VIF\"] = [variance_inflation_factor(X_complete, i) for i in range(len(features_complete))]\n\nprint(vif_data.sort_values('VIF', ascending=False))\nprint(\"\\nVIF &lt; 5: No serious multicollinearity concerns\")\n\n============================================================\nSTEP 1: Baseline Linear Regression\n============================================================\nTrain R²: 0.8347\nTest R²: 0.8406\nTest RMSE: 11405.3004\n\n============================================================\nSTEP 2: Diagnostic Plots\n============================================================\n\n\n\n\n\n\n\n\n\nObservations:\n- Residuals show some heteroscedasticity (variance increases with fitted values)\n- Overall pattern suggests room for improvement\n\n============================================================\nSTEP 3: Check Multicollinearity\n============================================================\n        Feature        VIF\n2       Poverty  21.903183\n5  ChildPoverty  16.856925\n1  Professional   9.947219\n4  IncomePerCap   7.250800\n3  Unemployment   4.843157\n0      TotalPop   4.334884\n\nVIF &lt; 5: No serious multicollinearity concerns\n\n\nSee the workflow? Fit → diagnose → identify issues. Now let’s fix them.\n\n\n10.2 Model Selection Strategy\nBased on diagnostics, try improvements systematically:\n\n# Step 4: Try different models\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 4: Model Comparison\")\nprint(\"=\"*60)\n\n# Scale features for regularization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_complete)\nX_test_scaled = scaler.transform(X_test_complete)\n\n# Dictionary to store results\nresults = []\n\n# Model 1: Baseline Linear Regression\nmodel1 = LinearRegression()\nmodel1.fit(X_train_scaled, y_train_complete)\ncv_score1 = cross_val_score(model1, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score1 = model1.score(X_test_scaled, y_test_complete)\nresults.append({\n    'Model': 'Linear Regression',\n    'CV R²': cv_score1,\n    'Test R²': test_score1,\n    'Features': len(features_complete)\n})\n\n# Model 2: Polynomial Features (degree 2)\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = poly_features.fit_transform(X_train_scaled)\nX_test_poly = poly_features.transform(X_test_scaled)\n\nmodel2 = LinearRegression()\nmodel2.fit(X_train_poly, y_train_complete)\ncv_score2 = cross_val_score(model2, X_train_poly, y_train_complete, cv=5, scoring='r2').mean()\ntest_score2 = model2.score(X_test_poly, y_test_complete)\nresults.append({\n    'Model': 'Polynomial (degree=2)',\n    'CV R²': cv_score2,\n    'Test R²': test_score2,\n    'Features': X_train_poly.shape[1]\n})\n\n# Model 3: Ridge Regression\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train_complete)\ncv_score3 = cross_val_score(ridge, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score3 = ridge.score(X_test_scaled, y_test_complete)\nresults.append({\n    'Model': 'Ridge (α=1.0)',\n    'CV R²': cv_score3,\n    'Test R²': test_score3,\n    'Features': len(features_complete)\n})\n\n# Model 4: Lasso Regression\nlasso = Lasso(alpha=0.01, max_iter=10000)\nlasso.fit(X_train_scaled, y_train_complete)\ncv_score4 = cross_val_score(lasso, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score4 = lasso.score(X_test_scaled, y_test_complete)\nn_features_lasso = np.sum(np.abs(lasso.coef_) &gt; 0.0001)\nresults.append({\n    'Model': 'Lasso (α=0.01)',\n    'CV R²': cv_score4,\n    'Test R²': test_score4,\n    'Features': f\"{n_features_lasso} (selected)\"\n})\n\n# Model 5: Elastic Net\nelastic = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)\nelastic.fit(X_train_scaled, y_train_complete)\ncv_score5 = cross_val_score(elastic, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score5 = elastic.score(X_test_scaled, y_test_complete)\nn_features_elastic = np.sum(np.abs(elastic.coef_) &gt; 0.0001)\nresults.append({\n    'Model': 'Elastic Net (α=0.01, l1=0.5)',\n    'CV R²': cv_score5,\n    'Test R²': test_score5,\n    'Features': f\"{n_features_elastic} (selected)\"\n})\n\n# Display results\nresults_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison Results:\")\nprint(results_df.to_string(index=False))\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(results_df))\nwidth = 0.35\n\nax.bar(x - width/2, results_df['CV R²'], width, label='CV R²', alpha=0.8)\nax.bar(x + width/2, results_df['Test R²'], width, label='Test R²', alpha=0.8)\n\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('R²', fontsize=12)\nax.set_title('Model Performance Comparison', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Best model\nbest_idx = results_df['CV R²'].argmax()\nbest_model_name = results_df.iloc[best_idx]['Model']\nprint(f\"\\n✓ Best model by CV: {best_model_name}\")\n\n\n============================================================\nSTEP 4: Model Comparison\n============================================================\n\nModel Comparison Results:\n                       Model    CV R²  Test R²     Features\n           Linear Regression 0.832510 0.840628            6\n       Polynomial (degree=2) 0.845916 0.856639           27\n               Ridge (α=1.0) 0.832520 0.840618            6\n              Lasso (α=0.01) 0.832510 0.840628 6 (selected)\nElastic Net (α=0.01, l1=0.5) 0.832527 0.840470 6 (selected)\n\n\n\n\n\n\n\n\n\n\n✓ Best model by CV: Polynomial (degree=2)\n\n\nThe comparison shows which approach works best for your data. In this case, all models perform similarly, but Ridge and Elastic Net provide slightly better generalization.\n\n\n10.3 Interpreting and Communicating Results\nOnce you’ve selected your best model, interpret the coefficients and communicate clearly:\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 5: Interpret Final Model\")\nprint(\"=\"*60)\n\n# Use Ridge as our final model\nfinal_model = Ridge(alpha=1.0)\nfinal_model.fit(X_train_scaled, y_train_complete)\n\n# Get coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features_complete,\n    'Coefficient': final_model.coef_\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"\\nFinal Model Coefficients (Ridge, α=1.0):\")\nprint(coef_df.to_string(index=False))\n\n\n============================================================\nSTEP 5: Interpret Final Model\n============================================================\n\nFinal Model Coefficients (Ridge, α=1.0):\n     Feature   Coefficient\nIncomePerCap  15857.063355\nProfessional   2913.318816\nChildPoverty   1977.037962\nUnemployment   -882.530764\n    TotalPop  -1494.961708\n     Poverty -12904.379677\n\n\nInterpretation for Stakeholders: Our regression model explains approximately 60% of the variance in median household income across NYC census tracts. The model uses 6 key features:\nKey findings: 1. Income Per Capita is the strongest predictor. Each $1,000 increase in per capita income corresponds to a significant increase in median household income.\n\nProfessional Employment has a positive effect. Census tracts with more professionals tend to have higher median household incomes.\nPoverty Rate shows a strong negative relationship with income, as expected.\nChild Poverty has an additional negative effect beyond general poverty, highlighting the economic challenges faced by families with children.\nUnemployment shows a negative relationship with median household income, though this may be partially captured by the poverty variables.\nTotal Population has minimal effect on median household income at the census tract level.\n\nThe model performs consistently on both training and test data, suggesting it generalizes well to new predictions.\n\n\n\n\n\n\nTip\n\n\n\nThis is how you communicate results: translate coefficients into plain language, explain what matters, and acknowledge limitations.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways present both statistical results (R², coefficients) and practical interpretation. Stakeholders need to understand what the numbers mean for their decisions, not just that “the model has an R² of 0.60.”\n\n\nThe complete workflow: diagnose issues → try fixes systematically → select best model → interpret clearly. That’s professional regression analysis."
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-summary",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-summary",
    "title": "Chapter 2: Regression Models",
    "section": "Summary",
    "text": "Summary\nLinear regression is the foundation of machine learning, not because it’s the most powerful model, but because understanding it deeply prepares you to understand everything else. This chapter covered the complete regression toolkit—from basic assumptions to advanced regularization techniques.\nKey Takeaways:\n\nLinear regression makes four assumptions: linearity, independence, homoscedasticity, and normality. Violate them, and your model might give terrible predictions even with high R².\nResidual plots are your diagnostic tool. They reveal problems that metrics hide. A curved residual plot means non-linearity. A funnel means heteroscedasticity. Random scatter means you’re good.\nEvaluation metrics serve different purposes. MSE penalizes large errors heavily. MAE treats all errors equally. R² tells you variance explained. RMSE gives interpretable units. Use multiple metrics, not just one.\nPolynomial features capture non-linearity while staying within linear regression. But high-degree polynomials overfit spectacularly. Always use validation data to choose the degree.\nMulticollinearity breaks coefficient interpretation but doesn’t hurt prediction accuracy. High VIF values warn you that coefficients are unstable. Regularization fixes this automatically.\nRidge regression (L2) shrinks all coefficients toward zero but never reaches exactly zero. It handles multicollinearity well and prevents overfitting. Always scale features first.\nLasso regression (L1) does automatic feature selection by setting some coefficients to exactly zero. Use it when you suspect many features are irrelevant.\nElastic Net combines Ridge and Lasso. It’s more stable than Lasso with correlated features while still doing feature selection. When unsure, start with Elastic Net.\nThe complete workflow matters: Fit baseline → check diagnostics → identify problems → fix systematically → validate with cross-validation → interpret clearly. Skip steps, and you’ll miss critical issues.\nAlways communicate results in plain language. Stakeholders don’t care that “the coefficient for X1 is 0.437 with a p-value of 0.003.” They care what that means for their decisions.\n\nRegression modeling is both art and science. The science is in the diagnostics, metrics, and validation. The art is in knowing when assumptions matter, which fixes to try, and how to communicate findings. Master both, and you’ll build models that actually work in the real world.\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-practice",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-practice",
    "title": "Chapter 2: Regression Models",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nThese exercises build progressively from understanding diagnostics to conducting complete regression analyses. Work through them to solidify your grasp of regression modeling.\n\nExercise 1: Residual Analysis\nTask: Load the NYC census dataset and fit a linear regression predicting median household income from just the TotalPop feature. Create a residuals vs. fitted values plot. Based on the pattern you observe:\n\nIdentify which regression assumption(s) are violated\nExplain what the pattern tells you about the model’s mistakes\nSuggest two specific fixes that might improve the model\nImplement one fix and show the improvement\n\nWhat you’re learning: How to read residual plots and diagnose problems.\n\n\nExercise 2: Coefficient Interpretation\nTask: Fit a multiple linear regression predicting median household income from Professional, Poverty, Unemployment, and ChildPoverty. For each coefficient:\n\nWrite the interpretation in plain English (e.g., “For every additional…”)\nExplain what “holding other features constant” means\nIdentify which coefficient is hardest to interpret and explain why\nCalculate and report both R² and Adjusted R², then explain the difference\n\nWhat you’re learning: How to interpret and communicate regression results.\n\n\nExercise 3: Polynomial Selection\nTask: Using the IncomePerCap feature alone, fit polynomial regression models of degrees 1 through 8:\n\nFor each degree, calculate train MSE and test MSE\nCreate a plot showing both training and test MSE across all degrees\nIdentify the optimal polynomial degree and justify your choice\nVisualize the polynomial fit for degrees 1, 3, and 8 on the same plot\nExplain why degree 8 has lower training MSE but might perform worse in practice\n\nWhat you’re learning: The bias-variance tradeoff and how to prevent overfitting.\n\n\nExercise 4: Multicollinearity Detection\nTask: Using all numeric features in the NYC census dataset:\n\nCompute and visualize the correlation matrix\nIdentify any feature pairs with correlation above 0.7\nCalculate VIF for each feature\nIdentify features with VIF &gt; 5 and explain what this means\nFit two models: one with all features, one removing high-VIF features. Compare coefficients and show which is more stable by refitting on a different random split.\n\nWhat you’re learning: How to detect and handle multicollinearity.\n\n\nExercise 5: Ridge vs Lasso Comparison\nTask: Using all features from the NYC census dataset, compare Ridge and Lasso regression:\n\nFor both Ridge and Lasso, find the optimal alpha using cross-validation\nPlot the regularization paths for both methods (coefficients vs. alpha)\nCreate a table showing which features each method keeps (for optimal alpha)\nExplain why Lasso eliminates some features while Ridge doesn’t\nRecommend which method you’d use for this dataset and why\n\nWhat you’re learning: The practical differences between L1 and L2 regularization.\n\n\nExercise 6: Complete Regression Workflow\nTask: Conduct a complete regression analysis on the NYC census dataset:\n\nBaseline: Fit linear regression with all features. Report train/test R² and RMSE.\nDiagnostics: Create residual plots (residuals vs fitted, histogram of residuals, scale-location). List any problems you identify.\nMulticollinearity: Check VIF. Report any concerns.\nModel Improvement: Try at least 3 different approaches (e.g., polynomial features, Ridge, Lasso, log transform). Use cross-validation to compare.\nFinal Model: Select your best model and justify the choice.\nInterpretation: Write a 2-3 paragraph summary explaining your findings to a non-technical audience. Include the model’s performance, key predictors, and limitations.\n\nWhat you’re learning: The complete end-to-end regression modeling workflow.\n\nTips for Success:\n\nDon’t just run code—think about what each plot and metric is telling you\nWhen diagnostic plots show problems, try multiple fixes and compare results\nAlways use train/validation/test splits or cross-validation properly\nWrite your interpretations before looking at solutions\nRemember: a model that’s slightly less accurate but easier to interpret is often more valuable in practice\n\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-additional-resources",
    "href": "Textbook/Chapter-2-Regression/chapter-2-regression.html#ch2-additional-resources",
    "title": "Chapter 2: Regression Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRegression Assumptions Explained - Detailed guide to assumptions\nInterpreting Residual Plots - Penn State course notes\nRidge vs Lasso - Comprehensive comparison\nScikit-learn Linear Models - Official documentation\nFeature Scaling and Regularization - Why scaling matters"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-resources",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-resources",
    "title": "Chapter 3: Classification Models",
    "section": "Chapter Resources",
    "text": "Chapter Resources\nRelated Assignments:\n\nChapter 3 Homework"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-intro",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-intro",
    "title": "Chapter 3: Classification Models",
    "section": "Introduction",
    "text": "Introduction\nYou’ve spent weeks learning regression—predicting continuous values like house prices or income. But what happens when you need to predict categories instead? Will this customer churn or stay? Is this email spam or legitimate? Does this patient have the disease or not?\nThis is classification, and it’s everywhere. Classification powers the spam filter in your email, the fraud detection on your credit card, the recommendation systems suggesting what you should watch next, and the medical diagnostics helping doctors identify diseases. If you’ve ever wondered “will this happen or not?” or “which category does this belong to?”—that’s a classification problem.\nJust like with regression, there isn’t one “best” classification algorithm. Logistic regression is fast and interpretable. Decision trees are easy to explain to non-technical stakeholders. Random forests are robust and powerful. Support vector machines handle high-dimensional data elegantly. k-Nearest neighbors is beautifully simple but computationally expensive. Each has strengths, weaknesses, and situations where it shines.\nAnd the evaluation is completely different from regression. You can’t use R² or MSE. Instead, you’ll need to navigate confusion matrices, ROC curves, precision-recall tradeoffs, and decide whether false positives or false negatives are more costly for your specific problem. A model that’s 99% accurate might be completely useless if you’re trying to detect a rare disease.\nThis chapter will teach you not just how to fit classification models, but how to think like a data scientist choosing between them. You’ll learn:\n\nHow five major classification algorithms work and when to use each\nHow to interpret confusion matrices and choose the right metrics\nThe precision-recall tradeoff and why it matters\nHow to handle imbalanced datasets (the most common real-world scenario)\nHow to visualize decision boundaries to understand what your model is actually doing\nHow to use ROC curves to compare model performance\n\nLet’s jump in."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-1",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-1",
    "title": "Chapter 3: Classification Models",
    "section": "1. Machine Learning Paradigms: Supervised vs Unsupervised Learning",
    "text": "1. Machine Learning Paradigms: Supervised vs Unsupervised Learning\nBefore we dive into specific classification algorithms, let’s step back and understand a fundamental distinction in machine learning: supervised versus unsupervised learning. Understanding this paradigm will help you recognize when classification is the right approach and when other techniques might be more appropriate.\n\n1.1 Supervised Learning: Learning from Labels\nSupervised learning is what we’ve been doing throughout this course: you give the model input data (features) and the correct answers (labels/targets), and it learns to predict the right answer for new inputs.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load Spotify data (supervised classification example)\nspotify_df = pd.read_csv('../data/spotify.csv')\n\n# We have features (X) and a target (y)\n# Let's predict whether a song will be popular (binary classification)\nX = spotify_df[['danceability', 'energy', 'valence', 'tempo']]\ny = (spotify_df['popularity'] &gt; 50).astype(int)  # 1 if popular, 0 if not\n\nprint(\"Supervised learning setup:\")\nprint(f\"Features (X) shape: {X.shape}\")\nprint(f\"Target (y) shape: {y.shape}\")\nprint(f\"Class distribution: {y.value_counts().to_dict()}\")\nprint(\"\\nFirst few rows of features:\")\n\nX.head()\n\nSupervised learning setup:\nFeatures (X) shape: (42663, 4)\nTarget (y) shape: (42663,)\nClass distribution: {0: 33578, 1: 9085}\n\nFirst few rows of features:\n\n\n\n\n\n\n\n\n\ndanceability\nenergy\nvalence\ntempo\n\n\n\n\n0\n0.792\n0.7310\n0.8380\n113.007\n\n\n1\n0.418\n0.3280\n0.6800\n164.315\n\n\n2\n0.199\n0.0957\n0.0391\n77.722\n\n\n3\n0.862\n0.5210\n0.3730\n154.983\n\n\n4\n0.168\n0.1960\n0.0554\n83.898\n\n\n\n\n\n\n\n\nprint(\"\\nCorresponding targets (1 = popular, 0 = not popular):\")\ny.head()\n\n\nCorresponding targets (1 = popular, 0 = not popular):\n\n\n0    0\n1    0\n2    0\n3    1\n4    0\nName: popularity, dtype: int64\n\n\nThe key to supervised learning is that we have labels. We know what the correct answer is for each training example. The model learns by comparing its predictions to the true labels and adjusting to get closer.\n\n\n\n\n\n\nNote\n\n\n\nBoth regression (from Chapter 2) and classification are supervised learning tasks. The difference is that regression predicts continuous values while classification predicts discrete categories. But both require labeled training data.\n\n\n\n\n1.2 Unsupervised Learning: Finding Patterns Without Labels\nUnsupervised learning is different: you only have input data (X), no labels (y). The model’s job is to find patterns, structure, or groupings in the data on its own.\nFor example, we can use unsupervised learning to find natural groupings of flowers based on their measurements. We don’t tell the model what the groupings should be—it discovers them on its own. This is what clustering algorithms like KMeans do.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Using Iris dataset, but ignoring the labels (species)\n# Can we find natural groupings of flowers based on their measurements?\niris = load_iris(as_frame=True)\niris_df = iris.frame\n\nX_unlabeled = iris_df[['petal length (cm)', 'petal width (cm)']]\n\n# K-Means clustering: find groups in the data\nkmeans = KMeans(n_clusters=3, random_state=1, n_init=10)\nclusters = kmeans.fit_predict(X_unlabeled)\n\n# Visualize the clusters\nplt.figure(figsize=(10, 6))\nplt.scatter(X_unlabeled['petal length (cm)'], X_unlabeled['petal width (cm)'],\n            c=clusters, cmap='viridis', alpha=0.6)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c='red', marker='X', s=200, edgecolors='black', label='Centroids')\nplt.xlabel('Petal Length (cm)')\nplt.ylabel('Petal Width (cm)')\nplt.title('K-Means Clustering: Discovering Flower Groups (Unsupervised)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice what happened: we didn’t tell the model which flowers were which species. We just said “find 3 groups” and it discovered natural clusters based on petal measurements alone.\nCommon unsupervised learning tasks include:\n\nClustering: Grouping similar items together (customer segmentation, document clustering)\nDimensionality Reduction: Reducing many features to a few key ones (PCA, t-SNE for visualization)\nAnomaly Detection: Finding unusual examples (fraud detection, manufacturing defects)\n\n\n\n1.3 When to Use Each Paradigm\nUse supervised learning when:\n\nYou have labeled data (you know the correct answers)\nYou want to predict something specific\nYou can define success (accuracy, error rate, etc.)\n\nUse unsupervised learning when:\n\nYou don’t have labels (or getting labels is too expensive)\nYou want to explore data structure\nYou’re looking for patterns you don’t know exist yet\n\n\n\n\n\n\n\nNote\n\n\n\nMost real-world applications use supervised learning, because prediction is usually the goal. Unsupervised learning is powerful for exploration and preprocessing, but harder to evaluate (how do you know if clusters are “good”?). Unsupervised learning is often used as an intermediate step to generate new features, which are then used in a supervised learning model.\n\n\nFor the rest of this chapter, we’ll focus on supervised classification, since that’s where you’ll spend most of your time as a data scientist."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-2",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-2",
    "title": "Chapter 3: Classification Models",
    "section": "2. Classification vs Regression: What’s Different?",
    "text": "2. Classification vs Regression: What’s Different?\n\n2.1 The Fundamental Difference\nIn regression, we predict a continuous value. In classification, we predict a category. Seems simple, but this fundamental difference changes everything about how we build, train, and evaluate models.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Let's load a classification dataset - Titanic survival\n# This is a classic binary classification problem: survived or didn't survive\ntitanic = pd.read_csv('../data/titanic.csv')\n\n# Look at the target variable\nprint(\"Target variable (Survived) value counts:\")\nprint(titanic['Survived'].value_counts())\nprint(f\"\\nProportion survived: {titanic['Survived'].mean():.2%}\")\n\nTarget variable (Survived) value counts:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nProportion survived: 38.38%\n\n\nInstead of predicting a number on a continuous scale, we’re predicting one of two discrete outcomes: 0 (didn’t survive) or 1 (survived). This is binary classification—the most common type.\n\n\n2.2 Types of Classification Problems\nBinary Classification: Two possible outcomes (yes/no, spam/ham, fraud/legitimate)\n\nTitanic survival\nEmail spam detection\nLoan default prediction\nDisease diagnosis\n\nMulti-class Classification: More than two categories\n\nIris flower species (setosa, versicolor, virginica)\nHandwritten digit recognition (0-9)\nCustomer segment classification\nImage classification (cat, dog, bird, etc.)\n\nMost of this chapter focuses on binary classification since it’s simpler to visualize and understand. But the techniques extend naturally to multi-class problems.\n\n\n2.3 Why We Can’t Use Linear Regression\nYou might be tempted to use linear regression for classification. Just predict the category as a number, right? Let’s see why that breaks:\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nimport seaborn as sns\n\n# Prepare simple features\ntitanic_clean = titanic[['Age', 'Survived', 'Pclass', 'Fare']].dropna()\nX = titanic_clean[['Age', 'Pclass', 'Fare']].values\ny = titanic_clean['Survived'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Try linear regression (WRONG!)\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\nlinear_pred = linear_model.predict(X_test)\n\nprint('Predictions:')\nprint(linear_pred[:5])\n\nprint('Classes:')\nprint(y_test[:5])\n\nPredictions:\n[0.33588326 0.70372944 0.4655869  0.77416664 0.0881573 ]\nClasses:\n[0 1 1 1 0]\n\n\nSee the problem? Linear regression gives us predictions like 0.73 or -0.15 or 1.42. But we need 0 or 1! We could threshold at 0.5, but linear regression makes no guarantee that predictions will be between 0 and 1. It’s using the wrong tool for the job.\nClassification models are designed to output probabilities (values between 0 and 1) or direct class predictions. That’s why we need specialized algorithms."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-3",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-3",
    "title": "Chapter 3: Classification Models",
    "section": "3. Logistic Regression: The Foundation",
    "text": "3. Logistic Regression: The Foundation\n\n3.1 From Linear to Logistic\nLogistic regression might sound like a regression technique, but don’t be fooled—it’s pure classification. The name comes from its history: it takes linear regression and transforms it to work for classification.\nHere’s the key insight: instead of predicting the outcome directly, logistic regression predicts the probability of the positive class. It does this by taking a linear combination of features (just like linear regression) and passing it through the sigmoid function:\n\\[\n\\text{probability} = \\frac{1}{1 + e^{-z}}\n\\]\nwhere \\(z\\) is the linear combination: \\(z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\)\nThe sigmoid function has a beautiful property: it squashes any real number into the range (0, 1), making it perfect for probabilities.\n\n# Visualize the sigmoid function\nz = np.linspace(-10, 10, 100)\nsigmoid = 1 / (1 + np.exp(-z))\n\nplt.figure(figsize=(10, 6))\nplt.plot(z, sigmoid, 'b-', linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Decision threshold (0.5)')\nplt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\nplt.xlabel('z (linear combination of features)', fontsize=12)\nplt.ylabel('Probability of positive class', fontsize=12)\nplt.title('The Sigmoid Function: Turning Linear into Probability', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWhen \\(z = 0\\), the probability is exactly 0.5. As \\(z\\) increases, the probability approaches 1. As \\(z\\) decreases, the probability approaches 0. The sigmoid smoothly transitions between these extremes.\n\n\n3.2 Fitting Logistic Regression\nLet’s fit a logistic regression model to predict Titanic survival:\n\n# Fit logistic regression\nlog_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_model.fit(X_train, y_train)\n\n# Get predictions - both probabilities and classes\ny_pred_proba = log_model.predict_proba(X_test)[:, 1]  # Probability of class 1\ny_pred_class = log_model.predict(X_test)  # Predicted class (0 or 1)\n\n# Show the difference\ncomparison = pd.DataFrame({\n    'True': y_test,\n    'Prob_Survived': y_pred_proba,\n    'Predicted': y_pred_class\n})\n\ncomparison.head(10)\n\n\n\n\n\n\n\n\nTrue\nProb_Survived\nPredicted\n\n\n\n\n0\n0\n0.302603\n0\n\n\n1\n1\n0.741984\n1\n\n\n2\n1\n0.459317\n0\n\n\n3\n1\n0.791706\n1\n\n\n4\n0\n0.118771\n0\n\n\n5\n1\n0.707625\n1\n\n\n6\n1\n0.183988\n0\n\n\n7\n1\n0.202158\n0\n\n\n8\n0\n0.217621\n0\n\n\n9\n0\n0.261196\n0\n\n\n\n\n\n\n\nNotice the two types of predictions:\n\nProbabilities (from predict_proba): Values between 0 and 1 representing confidence\nClasses (from predict): Hard 0/1 decisions using a threshold (default 0.5)\n\nIf the probability is above 0.5, we predict class 1 (survived). Otherwise, class 0 (didn’t survive). But you can adjust this threshold based on your problem—more on that later.\nHow can we evaluate this model’s performance? One simple way is to ask about the average probability for each different true class (i.e. average probabibility of survival for those who actually survived vs. those who didn’t).\n\n# Calculate average probability for each true class\navg_prob_survived = y_pred_proba[y_test == 1].mean()\navg_prob_not_survived = y_pred_proba[y_test == 0].mean()\n\nprint(f\"Average probability for those who survived: {avg_prob_survived:.3f}\")\nprint(f\"Average probability for those who didn't survive: {avg_prob_not_survived:.3f}\")\n\nAverage probability for those who survived: 0.505\nAverage probability for those who didn't survive: 0.339\n\n\nThis is good, but it fails to capture the variability in predictions. A better approach would be to look at the distribution of probabilities for each class.\nLet’s visualize this:\n\nimport seaborn as sns\nimport pandas as pd\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=pd.DataFrame({'prob': y_pred_proba[y_test == 1], 'class': 'Survived'}), x='prob', alpha=0.7, label='Survived', bins=20, color='blue', stat='density')\nsns.histplot(data=pd.DataFrame({'prob': y_pred_proba[y_test == 0], 'class': 'Did not survive'}), x='prob', alpha=0.7, label='Did not survive', bins=20, color='red', stat='density')\nplt.xlabel('Predicted Probability of Survival')\nplt.ylabel('Density')\nplt.title('Distribution of Predicted Probabilities by True Outcome')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe see that there are more people who survived with higher predicted probability of survival, and more people who died with lower predicted probability. This shows that our model is somewhat calibrated - it tends to give higher probabilities to those who actually survived and lower probabilities to those who didn’t.\nWe’ll learn more advanced techniques later, but this is a good starting point.\n\n\n3.3 Interpreting Coefficients\nJust like linear regression, logistic regression has coefficients. But the interpretation is different. In particular, we’re primarily concerned with the odds ratio of a coefficient, which is calculated as \\(e^{\\beta}\\) where \\(\\beta\\) is the coefficient. This tells us how the odds of the outcome change for a one-unit increase in the predictor.\n\n# Let's use more features to make interpretation interesting\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ntitanic_features = titanic[features + ['Survived']].dropna()\n\nX_full = titanic_features[features].values\ny_full = titanic_features['Survived'].values\n\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n    X_full, y_full, test_size=0.2, random_state=42\n)\n\n# Fit the model\nlog_model_full = LogisticRegression(random_state=42, max_iter=1000)\nlog_model_full.fit(X_train_full, y_train_full)\n\n# Display coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': log_model_full.coef_[0],\n    'Odds_Ratio': np.exp(log_model_full.coef_[0])\n})\n\ncoef_df\n\n\n\n\n\n\n\n\nFeature\nCoefficient\nOdds_Ratio\n\n\n\n\n0\nPclass\n-1.129795\n0.323100\n\n\n1\nAge\n-0.051874\n0.949448\n\n\n2\nSibSp\n-0.295579\n0.744101\n\n\n3\nParch\n0.241379\n1.273004\n\n\n4\nFare\n0.003802\n1.003810\n\n\n\n\n\n\n\nOdds ratio measures how much the odds of the outcome change for a one-unit increase in the predictor. An odds ratio greater than 1 indicates a positive association with the outcome, while an odds ratio less than 1 indicates a negative association.\nFor example, if Fare has an odds ratio of 1.5, it means that for every one-unit increase in fare, the odds of survival increase by 50%. If Pclass has an odds ratio of 0.7, it means that for every one-unit increase in class (higher class number), the odds of survival decrease by 30%. Here we see that:\n\nIncreasing passenger class by 1 (e.g. 3rd class -&gt; 2nd class) decreased the predicted probability of survival by about 68% \\((1-0.323 = 0.677)\\).\nIncreasing the age by 1 year slightly decreased (~5%) the predicted probability of survival.\nIncreasing the number of parents/children in the family (Parch) increased the predicted probability of survival by a whopping 27%!\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful about collinearity! Remember that last chapter we discussed how collinearity can lead to non-interpretable coefficients. Since logistic regression is simply performing linear regression under the hood, all those same caveats apply here. For example, it seems weird to say that people in higher passenger classes are less likely to survive. What’s probably going on is collinearity between Pclass and Fare, since both are essentially measuring the same thing (how much the ticket cost)."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-4",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-4",
    "title": "Chapter 3: Classification Models",
    "section": "4. The Confusion Matrix: Understanding Errors",
    "text": "4. The Confusion Matrix: Understanding Errors\n\n4.1 What Is a Confusion Matrix?\nWhen you make predictions, you’ll make mistakes. The confusion matrix breaks down exactly what kinds of mistakes you’re making. It’s a 2×2 table for binary classification:\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Get predictions\ny_pred = log_model_full.predict(X_test_full)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test_full, y_pred)\n\n# Visualize it\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Not Survived', 'Survived'],\n            yticklabels=['Not Survived', 'Survived'])\nplt.title('Confusion Matrix', fontsize=14)\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.show()\n\nprint(f\"\\nTrue Negatives (TN): {cm[0, 0]} people died and we predicted they would die\")\nprint(f\"False Positives (FP): {cm[0, 1]} people survived but we predicted they would die\")\nprint(f\"False Negatives (FN): {cm[1, 0]} people died but we predicted they would survive\")\nprint(f\"True Positives (TP): {cm[1, 1]} people survived and we predicted they would survive\")\n\n\n\n\n\n\n\n\n\nTrue Negatives (TN): 74 people died and we predicted they would die\nFalse Positives (FP): 13 people survived but we predicted they would die\nFalse Negatives (FN): 30 people died but we predicted they would survive\nTrue Positives (TP): 26 people survived and we predicted they would survive\n\n\nThe four quadrants:\n\nTrue Negatives (TN): Correctly predicted “didn’t survive”\nFalse Positives (FP): Predicted “survived” but actually didn’t (Type I error)\nFalse Negatives (FN): Predicted “didn’t survive” but actually did (Type II error)\nTrue Positives (TP): Correctly predicted “survived”\n\nHere we have shown the confusion matrix with the actual counts from our logistic regression model, such as 74 people, 13 people, etc. However, we’re often more interested in understanding how well our model predicted the correct values. For example, of the people who actually survived, what percentage did we predict will and won’t survive? In other words, we’re normalizing along the rows so that each rows adds up to 100%.\n\n# Calculate row-wise percentages (normalize by actual positives)\nrow_sums = cm.sum(axis=1)\nrecall_precision = cm.astype(float) / row_sums.reshape(-1, 1)\n\n# Display as a confusion matrix, with numbers formatted as percentages\nplt.figure(figsize=(8, 6))\nsns.heatmap(recall_precision, annot=True, fmt='.0%', cmap='Blues', cbar=False,\n            xticklabels=['Not Survived', 'Survived'],\n            yticklabels=['Not Survived', 'Survived'])\nplt.title('Normalized Confusion Matrix (Row-wise)', fontsize=14)\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nHere we see that, of the people who actually died, 87% were correctly predicted as having died (True Negative rate), while 13% were incorrectly predicted as having survived (False Positive rate). Similarly, of the people who actually survived, 92% were correctly predicted as having survived (True Positive rate), while 8% were incorrectly predicted as having died (False Negative rate).\n\n\n\n\n\n\nTip\n\n\n\nDisplaying the data normalized relative to the true values is typically more useful than looking at the raw numbers. When converting to percentages we see that our model did quite well predicting people who will die, but very poorly predicting people who will survive. This suggests our model may be biased towards predicting death, which could be important to consider for real-world applications.\n\n\n\n\n4.2 Computing Metrics from the Confusion Matrix\nAll the important classification metrics come directly from these four numbers:\nAccuracy: What percentage of all predictions were correct? \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\n\naccuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\nprint(f\"Accuracy: {accuracy:.3f}\")\n\nAccuracy: 0.699\n\n\nPrecision: When you predict positive, how often are you right? \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\n\nprecision = cm[1, 1] / (cm[1, 1] + cm[0, 1])\nprint(f\"Precision: {precision:.3f}\")\n\nPrecision: 0.667\n\n\nRecall (Sensitivity): Of all actual positives, how many did you find? \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\n\nrecall = cm[1, 1] / (cm[1, 1] + cm[1, 0])\nprint(f\"Recall: {recall:.3f}\")\n\nRecall: 0.464\n\n\nF1 Score: Harmonic mean of precision and recall \\[\n\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\nf1 = 2 * (precision * recall) / (precision + recall)\nprint(f\"F1 Score: {f1:.3f}\")\n\nF1 Score: 0.547\n\n\n\n\n4.3 When Each Metric Matters\nDifferent problems care about different metrics:\nUse Accuracy when:\n\nClasses are balanced\nFalse positives and false negatives are equally costly\nExample: Predicting coin flips (50/50 balanced, no asymmetric cost)\n\nUse Precision when:\n\nFalse positives are very costly\nYou want to be confident when you predict positive\nExample: Spam detection (marking legitimate email as spam is very annoying, but one or two spam emails slipping through is acceptable)\n\nUse Recall when:\n\nFalse negatives are very costly\nYou want to catch all positive cases, even if it means some false alarms\nExample: Disease screening (missing a sick patient with a life-threatening condition is much worse than telling a healthy patient that they are sick)\n\nUse F1 Score when:\n\nYou want a balance between precision and recall\nClasses are imbalanced\nExample: Fraud detection (imbalanced, and both FP and FN have significant consequences)\n\n\n\n\n\n\n\nNote\n\n\n\nThere’s almost always a tradeoff between precision and recall. Increase one, and the other goes down. You need to decide which matters more for your specific problem.\n\n\nLet’s see the full classification report:\n\nprint(classification_report(y_test_full, y_pred,\n                          target_names=['Not Survived', 'Survived']))\n\n              precision    recall  f1-score   support\n\nNot Survived       0.71      0.85      0.77        87\n    Survived       0.67      0.46      0.55        56\n\n    accuracy                           0.70       143\n   macro avg       0.69      0.66      0.66       143\nweighted avg       0.69      0.70      0.69       143\n\n\n\nThis report shows precision, recall, and F1 for both classes, plus overall accuracy."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-5",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-5",
    "title": "Chapter 3: Classification Models",
    "section": "5. Decision Trees: Interpretable Non-Linear Classifiers",
    "text": "5. Decision Trees: Interpretable Non-Linear Classifiers\n\n5.1 How Decision Trees Work\nDecision trees make predictions by asking a series of yes/no questions about the features. They split the data recursively based on feature values, creating a tree structure.\nHere’s the beautiful part: decision trees are incredibly interpretable. You can literally draw out the decision-making process and explain it to anyone.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n# Fit a simple decision tree\ntree_model = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_model.fit(X_train_full, y_train_full)\n\n# Visualize the tree\nplt.figure(figsize=(20, 10))\nplot_tree(tree_model, feature_names=features, class_names=['Not Survived', 'Survived'],\n          filled=True, fontsize=10, rounded=True)\nplt.title('Decision Tree for Titanic Survival', fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\nEach box shows:\n\nThe question being asked (e.g., “Fare &lt;= 26.27?”)\nThe Gini impurity (measure of how mixed the classes are, discussed in the next section)\nThe number of samples reaching this node\nThe class distribution (number of zeros and ones)\nThe predicted class\n\n\n\n5.2 Splitting Criteria: Gini vs Entropy\nDecision trees decide where to split by maximizing information gain. The most common criteria is the Gini impurity.\nGini Impurity: Measures how often a randomly chosen element would be incorrectly classified \\[\n\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2\n\\]\nwhere \\(p_i\\) is the proportion of samples in class \\(i\\).\n\n\n\n\n\n\nNote\n\n\n\nAnother criteria you’ll sometimes see is the Entropy, which is a measure of the amount of “disorder” or uncertainty. It is defined as \\[\n\\text{Entropy} = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n\\] The actual values of Gini and entropy are often extremely similar, and are measuring similar things. Therefore, we’ll focus on Gini impurity in this textbook.\n\n\nBut what exactly is Gini computing? Let’s look at the tree we just created. In the first node, we see that there are 308 samples with class 0 (survived) and 154 samples with class 1 (did not survive). The Gini impurity is computed as follows: \\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=1}^2 p_i^2 = 1 - \\left(\\frac{308}{462}\\right)^2 - \\left(\\frac{154}{462}\\right)^2 = \\frac{4}{9} = 0.444\n\\]\nNotice that this is exactly the Gini imprutiy stated in that node in the tree.\nIn general, how should we think about the value for Gini impurity?\nImagine that, in one of the nodes, there were 100 samples, and all of the samples were people who survived. Then the Gini impurity would be\n\\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{100}{100}\\right)^2 - \\left(\\frac{0}{100}\\right)^2 = 0\n\\]\nSimilarly, if all of the samples were people who did not survive, the Gini impurity would be\n\\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{0}{100}\\right)^2 - \\left(\\frac{100}{100}\\right)^2 = 0\n\\]\nOn the other hand, suppose there were a 50-50 split of the samples, with 50 samples of each class. Then the Gini impurity would be\n\\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{50}{100}\\right)^2 - \\left(\\frac{50}{100}\\right)^2 = \\frac{1}{2} = 0.5\n\\]\nNote how Gini impurity is small at the extremes (all samples have the same class), and bigger when the classes are more balanced. We can see this in the graph below, where the x-axis represents the proportion of class 1, and the y-axis represents the Gini impurity.\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0, 1, 100, endpoint=True)\ngini = 1 - (p**2 + (1-p)**2)\n\ngini_df = pd.DataFrame({\n    'p': p,\n    'gini': gini\n})\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='p', y='gini', data=gini_df)\nplt.xlabel('Proportion of Class 1')\nplt.ylabel('Gini Impurity')\nplt.title('Gini Impurity in a Binary Classification Problem')\nplt.show()\n\n\n\n\n\n\n\n\nSo, how is Gini impurity actually used to determine the splitting in decision trees? Our goal is to choose a split (i.e. a question) that best splits the data. For example, if we wanted to determine how well students will do in a class, asking whether they own a dog is a valid question, but won’t give me any additional information. On the other hand, if we ask if they regularly attend tutoring, we will get a much better idea of their performance.\nIn decision trees we use Gini impurity by calculating the Gini impurity for each possible split, and then choosing the split that has the smallest Gini impurity. This is because a smaller Gini impurity means that the split is better at separating the classes.\n\n\n5.3 Controlling Tree Depth: The Overfitting Problem\nTrees have a dangerous tendency: if you let them grow without limits, they’ll memorize the training data.\n\n# Compare different tree depths\ndepths = [1, 3, 5, 10, 20, None]  # None means no limit\ntrain_scores = []\ntest_scores = []\n\nfor depth in depths:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train_full, y_train_full)\n    train_scores.append(tree.score(X_train_full, y_train_full))\n    test_scores.append(tree.score(X_test_full, y_test_full))\n\n# Plot\ndepth_labels = [str(d) if d is not None else 'Unlimited' for d in depths]\nplt.figure(figsize=(10, 6))\nplt.plot(depth_labels, train_scores, 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(depth_labels, test_scores, 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('Maximum Tree Depth', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('Tree Depth vs Performance: The Overfitting Story', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nClassic overfitting! Training accuracy keeps increasing with depth, but test accuracy peaks and then plateaus or even decreases. The tree is memorizing noise in the training data.\nWe can see this concretely by printing out one of the decision paths in the deep trees:\n\n# Print one of the decision paths in the deep trees\ntree = DecisionTreeClassifier(max_depth=20, random_state=42)\ntree.fit(X_train_full, y_train_full)\n\nsample_id = 0\nsingle_sample = X_test_full[sample_id].reshape(1, -1) # Reshape for single sample input\n\n# Get the decision path for the single sample\nnode_indicator = tree.decision_path(single_sample)\nnode_indices = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n\n# Get tree structure information for interpreting the path\nchildren_left = tree.tree_.children_left\nchildren_right = tree.tree_.children_right\nfeature = tree.tree_.feature\nthreshold = tree.tree_.threshold\nfeature_names = features\n\nprint(f\"Decision path for sample {sample_id}:\")\nfor node_id in node_indices:\n    if children_left[node_id] != children_right[node_id]:  # Check if it's a split node\n        feature_index = feature[node_id]\n        threshold_value = threshold[node_id]\n        sample_feature_value = single_sample[0, feature_index]\n\n        if sample_feature_value &lt;= threshold_value:\n            decision = f\"{feature_names[feature_index]} ({sample_feature_value:.2f}) &lt;= {threshold_value:.2f}\"\n        else:\n            decision = f\"{feature_names[feature_index]} ({sample_feature_value:.2f}) &gt; {threshold_value:.2f}\"\n        print(f\"  Node {node_id}: Split on {decision}\")\n    else:  # It's a leaf node\n        print(f\"  Node {node_id}: Leaf node reached (prediction: {tree.predict(single_sample)[0]})\")\n\nDecision path for sample 0:\n  Node 0: Split on Fare (13.00) &lt;= 52.28\n  Node 1: Split on Age (42.00) &gt; 5.50\n  Node 11: Split on Pclass (2.00) &lt;= 2.50\n  Node 12: Split on Age (42.00) &lt;= 42.50\n  Node 13: Split on Fare (13.00) &gt; 12.31\n  Node 23: Split on Pclass (2.00) &gt; 1.50\n  Node 43: Split on Age (42.00) &gt; 15.00\n  Node 45: Split on Age (42.00) &gt; 39.50\n  Node 103: Split on Fare (13.00) &lt;= 26.50\n  Node 104: Leaf node reached (prediction: 1)\n\n\nWe start by asking reasonable seeming questions, but then asking more and more specific questions that seem unnecessary. For example, we ask if the persons age is:\n\nOver 5.5\nLess than 42.5\nOver 15\nOver 39.5\n\nPutting these together, we’ve asked if the person is between the ages of 39.5 and 42.5. That’s unnecessarily specific; do we really believe that a person being exactly 40 or 41 years old is so important? Instead, what’s happening is that our model is overfitting to the training data, and the results don’t generalize to the test data.\nCommon hyperparameters to control overfitting:\n\nmax_depth: Maximum depth of the tree\nmin_samples_split: Minimum samples required to split a node\nmin_samples_leaf: Minimum samples required at a leaf node\nmax_features: Number of features to consider for each split\n\n\n\n5.4 Feature Importance\nTrees can tell you which features are most important for making predictions:\n\n# Get feature importances\ntree_final = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_final.fit(X_train_full, y_train_full)\n\nimportance_df = pd.DataFrame({\n    'Feature': features,\n    'Importance': tree_final.feature_importances_\n}).sort_values('Importance', ascending=False)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance from Decision Tree', fontsize=14)\nplt.gca().invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\n\nFeature importance represents how much each feature contributes to reducing impurity across all splits. Higher values mean more important features.\n\n\n\n\n\n\nNote\n\n\n\nFeature importance can be calculated a number of different ways. One way is to take a column, randomly shuffle the values, and see how the impurity changes. If the impurity decreases, then the feature is important. This is called permutation feature importance.\nFor DecisionTreeClassifier, feature importance is calculated using impurity decrease. The impurity decrease is the difference in impurity before and after a split. The higher the impurity decrease, the more important the feature. For example, if a feature can be used to end up with nodes with small impurity, that means that feature can split the data into groups where one class is highly represented. Recall from above that impurity is a measure of how mixed up the classes are in a node. So if a feature can be used to end up with nodes with small impurity, that means that feature can split the data into groups where one class is highly represented. This is why the feature is important.\n\n\n\n\n\n\n\n\nTip\n\n\n\nDifferent models calculate feature importance differently. However, they’re all trying to answer the same question: how much does this feature matter in terms of making predictions?\nDon’t treat feature importance like a gold standard. It’s a tool for understanding your model, but it’s not a substitute for domain knowledge. Instead, you can often use feature importance to double-check your own understanding of the problem. If you believe a feature should be important, but it’s not showing up in the feature importance, then why not? Is it a problem with the model/data, or a problem with your understanding?"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-6",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-6",
    "title": "Chapter 3: Classification Models",
    "section": "6. Random Forests: Ensemble Power",
    "text": "6. Random Forests: Ensemble Power\n\n6.1 Why Ensembles Work\nHere’s a powerful idea: what if instead of training one tree, we trained many trees and let them vote?\nRandom Forests are one of the most successful examples. The key insight: many weak learners can combine to create a strong learner.\n\n\n\n\n\n\nNote\n\n\n\nEnsemble learning is the idea of combining many weak learners to create a strong learner. The key insight is that many weak learners can combine to create a strong learner.\nBy a weak learner we typically mean a very simple model, such as a decision tree with a depth of two. By a strong learner we typically mean a more complex model, such as a decision tree with a depth of ten.\nThe idea is that one large complex model may overfit to the training data, but many small simple models can combine to create a strong learner that generalizes well to the test data.\nRandom forests are one example of ensemble models, but we’ll learn more. Ensemble models are typically the gold standard in classical machine learning.\n\n\nHow Random Forests work:\n\nCreate many decision trees (e.g., 100 trees)\nFor each tree:\n\nSample a random subset of the data (bootstrapping)\nAt each split, only consider a random subset of features\n\nMake predictions by majority vote (classification), or by averaging probabilities (regression)\n\nThe randomness in both samples and features ensures that trees are different from each other. After all, if we took all rows and all columns, each weak learner would likely be exactly the same. By sampling a random subset of the data and features, we ensure that each tree is different from the others.\nWhen weak learners disagree, it’s often because they’re focusing on different aspects of the data. When they agree, you can be more confident.\n\n\n6.2 Fitting a Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Fit random forest\nrf_model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\nrf_model.fit(X_train_full, y_train_full)\n\n# Compare to single tree\ntree_comparison = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_comparison.fit(X_train_full, y_train_full)\n\nprint(\"Single Decision Tree:\")\nprint(f\"  Training Accuracy: {tree_comparison.score(X_train_full, y_train_full):.3f}\")\nprint(f\"  Test Accuracy: {tree_comparison.score(X_test_full, y_test_full):.3f}\")\n\nprint(\"\\nRandom Forest (50 trees):\")\nprint(f\"  Training Accuracy: {rf_model.score(X_train_full, y_train_full):.3f}\")\nprint(f\"  Test Accuracy: {rf_model.score(X_test_full, y_test_full):.3f}\")\n\nSingle Decision Tree:\n  Training Accuracy: 0.765\n  Test Accuracy: 0.678\n\nRandom Forest (50 trees):\n  Training Accuracy: 0.788\n  Test Accuracy: 0.699\n\n\nRandom forests typically achieve better generalization than single trees. The ensemble reduces overfitting through diversity.\n\n\n\n\n\n\nTip\n\n\n\nRandom forests are most useful for complex data with many columns, and with complex relationships between columns. For simple data with few columns, a single decision tree is often sufficient.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou may feel like “if one tree is good, then more trees are better!” This is true to an extent, but it comes with a tradeoff. When you train an ensemble model such as a random forest, you’re training many models. Each model takes time to train, and each model uses memory and compute time. Imagine training a single tree which takes ten seconds to train.\nNow imagine training 50 trees, which would take 500 seconds to train, or nearly ten minutes! Combine this with hyperparameter tuning such as through a grid search, and you could easily be looking at an hour or more of training time. If the improvement in performance is large this may be worth it. But if your data is simple enough to get by with a simpler model, you can save hours of compute by going with a simpler model.\n\n\n\n\n6.3 Feature Importance in Random Forests\nRandom forests also provide feature importances, but they’re generally more reliable than single trees because they average across many trees:\n\n# Get feature importances from both models\ntree_importance = pd.DataFrame({\n    'Feature': features,\n    'Decision Tree': tree_comparison.feature_importances_,\n    'Random Forest': rf_model.feature_importances_\n})\n\n# Melt the dataframe for plotting\nimportance_melted = tree_importance.melt(\n    id_vars='Feature',\n    var_name='Model',\n    value_name='Importance'\n)\n\n# Sort by average importance across both models\navg_importance = tree_importance.set_index('Feature').mean(axis=1).sort_values(ascending=False)\nimportance_melted['Feature'] = pd.Categorical(\n    importance_melted['Feature'],\n    categories=avg_importance.index,\n    ordered=True\n)\n\n# Create dodged bar chart\nplt.figure(figsize=(10, 6))\nsns.barplot(data=importance_melted, x='Importance', y='Feature', hue='Model', palette='Set2')\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance: Decision Tree vs Random Forest', fontsize=14)\nplt.legend(title='Model', fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRandom forest importances tend to be more stable because they’re averaged across many trees with different random subsets of data and features.\n\n\n6.4 Hyperparameter Tuning\nRandom forests have several important hyperparameters:\n\n# Test different numbers of trees\nn_trees_list = [25, 50, 75, 100, 150, 200]\nscores = []\n\nfor n_trees in n_trees_list:\n    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n    rf.fit(X_train_full, y_train_full)\n    scores.append(rf.score(X_test_full, y_test_full))\n\nplt.figure(figsize=(10, 6))\nplt.plot(n_trees_list, scores, 'o-', linewidth=2, markersize=8)\nplt.xlabel('Number of Trees', fontsize=12)\nplt.ylabel('Test Accuracy', fontsize=12)\nplt.title('Random Forest Performance vs Number of Trees', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nPerformance typically improves with more trees, but you get diminishing returns. After a certain point (often 100-500 trees), adding more trees barely helps (or even leads to overfitting)but makes training slower. Use hyperparameter tuning to find the right number of trees. A typically starting point is 50 to 100 trees.\n\n\n\n\n\n\nTip\n\n\n\nRandom forests are often a great default choice for classification. They’re robust, handle non-linear relationships, require minimal hyperparameter tuning, and rarely overfit badly. When in doubt, try a random forest!"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-7",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-7",
    "title": "Chapter 3: Classification Models",
    "section": "7. Support Vector Machines: Maximum Margin Classifiers",
    "text": "7. Support Vector Machines: Maximum Margin Classifiers\n\n7.1 The Margin Concept\nSupport Vector Machines (SVMs) have a beautiful geometric intuition: find the decision boundary that maximizes the distance to the nearest points from each class.\nThink of it like this: if you’re drawing a line to separate two groups of points, you want it to be as far as possible from both groups. This gives you more confidence that future points will be classified correctly.\nThe “support vectors” are the points closest to the decision boundary—these are the critical points that define where the boundary goes.\n\nfrom sklearn.svm import SVC\n\n# Fit SVM with linear kernel\nsvm_linear = SVC(kernel='linear', random_state=42)\nsvm_linear.fit(X_train_full, y_train_full)\n\nprint(f\"SVM Linear Kernel Accuracy: {svm_linear.score(X_test_full, y_test_full):.3f}\")\nprint(f\"Number of support vectors: {len(svm_linear.support_)}\")\n\nSVM Linear Kernel Accuracy: 0.678\nNumber of support vectors: 374\n\n\n\n\n7.2 The Kernel Trick\nHere’s where SVMs get really powerful: the kernel trick. By using different kernel functions, SVMs can create non-linear decision boundaries while still solving a linear problem in a higher-dimensional space.\nCommon kernels:\n\nLinear: Creates straight boundaries (like logistic regression)\nRBF (Radial Basis Function): Creates circular/curved boundaries\nPolynomial: Creates polynomial curves as boundaries\n\n\n# Compare different kernels\nkernels = ['linear', 'rbf', 'poly']\nsvm_models = {}\n\nfor kernel in kernels:\n    svm = SVC(kernel=kernel, random_state=42)\n    svm.fit(X_train_full, y_train_full)\n    svm_models[kernel] = svm\n    print(f\"{kernel:8s} kernel - Test Accuracy: {svm.score(X_test_full, y_test_full):.3f}\")\n\nlinear   kernel - Test Accuracy: 0.678\nrbf      kernel - Test Accuracy: 0.615\npoly     kernel - Test Accuracy: 0.629\n\n\n\n\n6.3 Visualizing SVM Decision Boundaries\nLet’s see how different kernels create different boundaries. We’ll use California housing data, where different regions of the state have vastly different housing prices—a perfect example of non-linear geographic clustering:\n\n# Load California housing data\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\ncalifornia = fetch_california_housing(as_frame=True)\nca_df = california.frame\n\n# Create binary classification: expensive (&gt;$3) vs affordable (&lt;=$3) houses\n# Median house value is in $100,000s, so 3 = $300,000\nca_df['expensive'] = (ca_df['MedHouseVal'] &gt; 3.0).astype(int)\n\n# Use longitude and median house value for visualization\n# Different parts of California have very different price patterns\nX_viz = ca_df[['Latitude', 'MedHouseVal']].values\ny_viz = ca_df['expensive'].values\n\n# Sample for faster visualization (full dataset is 20k+ points)\nnp.random.seed(42)\nsample_idx = np.random.choice(len(X_viz), size=2000, replace=False)\nX_viz_raw = X_viz[sample_idx]\ny_viz = y_viz[sample_idx]\n\n# Scale features for SVM (IMPORTANT: SVMs are sensitive to feature scales)\nscaler = StandardScaler()\nX_viz = scaler.fit_transform(X_viz_raw)\n\nexpensive = y_viz == 1\n\n# Create a mesh for plotting decision boundaries (in scaled space)\nlon_range = np.linspace(X_viz[:, 0].min(), X_viz[:, 0].max(), 100)\nprice_range = np.linspace(X_viz[:, 1].min(), X_viz[:, 1].max(), 100)\nlon_mesh, price_mesh = np.meshgrid(lon_range, price_range)\nmesh_points = np.c_[lon_mesh.ravel(), price_mesh.ravel()]\n\n# Fit SVMs with different kernels on scaled data\nsvm_linear_viz = SVC(kernel='linear', random_state=42)\nsvm_rbf_viz = SVC(kernel='rbf', gamma='auto', random_state=42)\n\nsvm_linear_viz.fit(X_viz, y_viz)\nsvm_rbf_viz.fit(X_viz, y_viz)\n\n# Create predictions on mesh\nmesh_linear = svm_linear_viz.predict(mesh_points).reshape(lon_mesh.shape)\nmesh_rbf = svm_rbf_viz.predict(mesh_points).reshape(lon_mesh.shape)\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor ax, mesh_pred, title in zip(axes, [mesh_linear, mesh_rbf],\n                                 ['SVM - Linear Kernel', 'SVM - RBF Kernel']):\n    ax.contourf(lon_mesh, price_mesh, mesh_pred, levels=1, cmap='RdYlGn', alpha=0.4)\n    ax.scatter(X_viz[expensive, 0], X_viz[expensive, 1], c='darkgreen', marker='o',\n               s=20, edgecolors='black', alpha=0.5, label='Expensive (&gt;$300k)')\n    ax.scatter(X_viz[~expensive, 0], X_viz[~expensive, 1], c='darkred', marker='x',\n               s=20, alpha=0.5, label='Affordable (≤$300k)')\n    ax.set_xlabel('Latitude', fontsize=12)\n    ax.set_ylabel('Median House Value ($100k)', fontsize=12)\n    ax.set_title(title, fontsize=14)\n    ax.legend()\n    ax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSee the difference? The linear kernel creates a straight boundary—it tries to separate expensive from affordable homes with a single line. But the RBF kernel creates smooth, curved boundaries that adapt to the geographic clustering of housing prices.\nNotice how the RBF kernel captures the reality that certain geographic regions (coastal areas, Bay Area) command higher prices regardless of other factors. The curved decision boundary wraps around these high-value clusters much more naturally than a straight line ever could.\n\n\n7.4 When to Use SVMs\nStrengths:\n\nEffective in high-dimensional spaces\nMemory efficient (only stores support vectors)\nFlexible with different kernels\nWorks well with clear margin of separation\n\nWeaknesses:\n\nSlow to train on large datasets (doesn’t scale well beyond ~10,000 samples)\nRequires feature scaling (sensitive to feature magnitudes)\nChoosing the right kernel and hyperparameters can be tricky\nLess interpretable than trees or logistic regression\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways scale your features before using SVMs! They’re very sensitive to feature magnitudes. Use StandardScaler or MinMaxScaler from scikit-learn."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-8",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-8",
    "title": "Chapter 3: Classification Models",
    "section": "8. k-Nearest Neighbors: Simple but Powerful",
    "text": "8. k-Nearest Neighbors: Simple but Powerful\n\n8.1 The k-NN Algorithm\nk-Nearest Neighbors might be the simplest classification algorithm: to classify a new point, find the k closest training points and let them vote.\nThat’s it. No training phase. No learning parameters. Just store the data and compute distances when you need to make predictions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Fit k-NN with k=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_full, y_train_full)\n\nprint(f\"k-NN (k=5) Test Accuracy: {knn.score(X_test_full, y_test_full):.3f}\")\n\nk-NN (k=5) Test Accuracy: 0.650\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKNN models sound like they should be simple. You simply find the k closest training points and let them vote. But in practice, they can be devilishly complex. For example, how do you measure “closeness”? In some data that may be simple, such as closeness in position or time. But what about data on students? What do we mean by the “closest students”? Closest in age? Same/similar major? Same/similar year in college? Are all of these equally important? Is one more important than another? How about closeness in courses taken? When should we consider two courses “close”?\nAll of these questions are enormously important in building effective KNN models, but they don’t have easy answers. KNN models, more than many others, require extensive testing to determine what works best.\n\n\n\n\n8.2 Choosing k: The Bias-Variance Tradeoff Again\nThe value of k controls the bias-variance tradeoff:\n\nSmall k (e.g., k=1): Very flexible, low bias, high variance (overfitting)\nLarge k (e.g., k=100): Smoother boundaries, high bias, low variance (underfitting)\n\n\n# Test different values of k\nk_values = [1, 3, 5, 10, 20, 50, 100]\ntrain_scores_knn = []\ntest_scores_knn = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_full, y_train_full)\n    train_scores_knn.append(knn.score(X_train_full, y_train_full))\n    test_scores_knn.append(knn.score(X_test_full, y_test_full))\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, train_scores_knn, 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(k_values, test_scores_knn, 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('k (number of neighbors)', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('k-NN: Choosing k', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nNotice the pattern: small k gives high training accuracy but may overfit. Moderate k (often 3-10) tends to work best, but hyperparameter tuning is needed to determine the best choice.\n\n\n8.3 Distance Metrics: How Do We Measure “Closeness”?\nThe entire k-NN algorithm hinges on one question: how do you measure which points are “closest”? This isn’t just a technical detail—it fundamentally changes how your model behaves. Scikit-learn supports several distance metrics, and choosing the right one can dramatically affect performance.\nThe most common distance metrics:\n\nEuclidean distance (default): The straight-line distance between two points \\[d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\]\nManhattan distance: Sum of absolute differences (think of it as the number of city blocks between two locations, you can only walk horizontally or vertically) \\[d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\\]\nMinkowski distance: A mixture between Euclidean and Manhattan (p=1 is Manhattan, p=2 is Euclidean, but you can set p to any positive real number) \\[d(x, y) = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}\\]\nCosine distance: Measures angle between vectors (ignores the length of the vectors, and only cares about how far apart the directions they point are) \\[d(x, y) = 1 - \\frac{x \\cdot y}{||x|| \\cdot ||y||}\\]\nHamming distance: Checks whether two values are equal or not, and computes the average number of features where two samples are equal \\[d(x, y) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{1}(x_i \\neq y_i)\\] where \\(\\mathbb{1}(x_i \\neq y_i)\\) equals 1 if \\(x_i \\neq y_i\\) and 0 otherwise.\n\nLet’s see how different metrics perform on our Titanic data:\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Test different distance metrics\nmetrics = ['euclidean', 'manhattan', 'minkowski', 'cosine', 'hamming']\nmetric_scores = {}\n\nfor metric in metrics:\n    # Some metrics need additional parameters\n    if metric == 'minkowski':\n        knn = KNeighborsClassifier(n_neighbors=5, metric=metric, p=3)\n    else:\n        knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n\n    knn.fit(X_train_full, y_train_full)\n    train_score = knn.score(X_train_full, y_train_full)\n    test_score = knn.score(X_test_full, y_test_full)\n    metric_scores[metric] = {'train': train_score, 'test': test_score}\n\n    # print(f\"{metric:12s} - Train: {train_score:.3f}, Test: {test_score:.3f}\")\n\n# Plot the metric and train/test score\nplt.figure(figsize=(10, 6))\nplt.plot(metrics, [scores['train'] for scores in metric_scores.values()], 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(metrics, [scores['test'] for scores in metric_scores.values()], 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('Distance Metric', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('k-NN: Choosing Distance Metric', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nDifferent metrics can give meaningfully different results! But which should you use?\nWhen to use each metric:\nEuclidean distance (the default) works well when:\n\nAll features have similar scales and units\nYou care about the actual geometric distance\nFeatures are continuous numeric values\nExample: Geographic coordinates (latitude/longitude), physical measurements\n\nManhattan distance works well when:\n\nFeatures represent different units that shouldn’t be combined quadratically\nYou have grid-like data (think city blocks, not “as the crow flies”)\nYou want to reduce the influence of outliers (no squaring!)\nExample: Recommender systems, routing/navigation problems\n\nCosine distance works well when:\n\nYou care about direction/orientation, not magnitude\nData is high-dimensional and sparse\nFeature scales vary wildly\nExample: Text data (word counts), recommendation systems with user ratings\n\nHamming distance works well when:\n\nYou have categorical or binary features\nAll features are equally important (no scaling needed)\nYou want to count how many features differ, not by how much\nExample: DNA sequences, binary feature vectors, categorical data (after one-hot encoding)\n\n\n\n\n\n\n\nNote\n\n\n\nHamming distance treats all feature differences equally. If Feature A differs by 0.1 and Feature B differs by 10, Hamming sees both as “different.” It’s perfect for categorical data where “different is different” regardless of magnitude, but not ideal for continuous numeric features where the size of the difference matters.\n\n\nLet’s visualize how these different metrics create different neighborhoods. We’ll use a simple 2D example:\n\nfrom sklearn.neighbors import NearestNeighbors\n\n# Create a simple 2D point to query\nquery_point = np.array([[2.0, 3.0]])\n\n# Create some sample points\nnp.random.seed(42)\nsample_points = np.random.rand(20, 2) * 5\n\n# Find 5 nearest neighbors using different metrics\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor ax, metric in zip(axes, ['euclidean', 'manhattan', 'cosine']):\n    # Find neighbors\n    nbrs = NearestNeighbors(n_neighbors=5, metric=metric)\n    nbrs.fit(sample_points)\n    distances, indices = nbrs.kneighbors(query_point)\n\n    # Plot\n    ax.scatter(sample_points[:, 0], sample_points[:, 1], c='lightgray',\n               s=100, alpha=0.6, label='Other points')\n    ax.scatter(sample_points[indices[0], 0], sample_points[indices[0], 1],\n               c='blue', s=100, edgecolors='black', linewidth=2, label='5 nearest neighbors')\n    ax.scatter(query_point[0, 0], query_point[0, 1], c='red', s=200,\n               marker='*', edgecolors='black', linewidth=2, label='Query point')\n\n    # Draw lines to nearest neighbors\n    for idx in indices[0]:\n        ax.plot([query_point[0, 0], sample_points[idx, 0]],\n                [query_point[0, 1], sample_points[idx, 1]],\n                'b--', alpha=0.3, linewidth=1)\n\n    ax.set_title(f'{metric.capitalize()} Distance', fontsize=14)\n    ax.set_xlabel('Feature 1', fontsize=12)\n    ax.set_ylabel('Feature 2', fontsize=12)\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSee how the same query point has different nearest neighbors depending on the metric? Euclidean forms circular neighborhoods, Manhattan forms diamond-shaped neighborhoods, and cosine focuses on angular similarity.\n\n\n8.4 Mixing Metrics: Different Features Need Different Distances\nHere’s a critical insight that’s often overlooked: real datasets have different types of features, and each type needs its own distance metric.\nThink about internet service provider (ISP) customer data and predicting churn:\n\nInternet Service: Categorical (DSL, Fiber optic, No internet). We want to know if two customers have the same service type—not treat “DSL” as somehow numerically between “No internet” and “Fiber optic”\nContract type: Categorical (Month-to-month, One year, Two year). Either the same or different.\nGender: Categorical (Male, Female). Same or different.\nMonthly Charges: Continuous numeric variable. A customer paying $50/month is more similar to one paying $55 than to one paying $100.\nTenure: Continuous numeric variable. The actual difference in months matters.\n\nThe problem? When you call KNeighborsClassifier(metric='euclidean'), it treats ALL features the same way! It computes Euclidean distance on internet service type and contract (treating categorical values as if they were numbers) just like it does on monthly charges and tenure.\nThe solution: Create a custom distance metric that treats different feature types appropriately.\nFor example, you could define a custom distance function that:\n\nUses Hamming distance (equality check) for categorical features (Internet Service, Contract, Gender)\nUses Euclidean distance for continuous features (Monthly Charges, Tenure)\n\nThese can be difficult to implement by hand, so working together with an AI coding assistant is the way to go.\n\n\n\n\n\n\nNote\n\n\n\nWhy does this matter?\nImagine comparing two ISP customers: - Customer A: DSL, Month-to-month contract, Male, $50/month, 12 months tenure - Customer B: Fiber optic, Month-to-month contract, Male, $52/month, 14 months tenure\nWith pure Euclidean distance, if internet service is encoded as DSL=0 and Fiber optic=1, the difference in service type contributes “1” to the distance calculation, just like a $1/month price difference. But internet service type is categorical! Having DSL vs Fiber optic is a fundamental categorical difference—not a numeric one.\nWith the mixed metric, we recognize that internet service differs (Hamming distance = 1), contract and gender are the same (Hamming distance = 0 for each), and then we properly compute Euclidean distance for the continuous features (monthly charges, tenure) where the magnitude of difference actually matters.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen building custom distance metrics:\n\nIdentify feature types first: Which are categorical? Which are continuous?\nScale continuous features: Use StandardScaler before computing distances\nDon’t scale categorical features: They represent discrete categories, not magnitudes\nTest your metric: Does it give sensible distances for sample pairs?\nWeight carefully: You might want to weight categorical and continuous distances differently\n\nThe custom metric approach requires more work, but it’s often worth it for datasets with mixed feature types!\n\n\n\n\n\n\n\n\nTip\n\n\n\nHow to choose a distance metric:\n\nStart with Euclidean (the default) - it works well in most cases\nTry Hamming if you have categorical features that have no obvious ordering\nTry Manhattan if you have outliers or features on very different scales\nTry Cosine if your data is high-dimensional or sparse (like text data)\nUse cross-validation to compare metrics on your specific dataset\nAlways scale your features before using distance-based methods!\n\nThe “best” metric depends on your data and problem. Don’t just accept the default—experiment and use validation performance to guide your choice.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFeature scaling is critical for k-NN! If one feature ranges from 0-1 and another ranges from 0-1000, the second feature will dominate the distance calculation. Always use StandardScaler or MinMaxScaler before fitting k-NN models.\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n\n\n\n8.5 When to Use k-NN\nStrengths:\n\nSimple to understand and implement\nNo training phase (though this is also a weakness)\nNaturally handles multi-class problems\nCan capture complex patterns\n\nWeaknesses:\n\nSlow prediction (has to compute distances to all training points)\nMemory intensive (stores all training data)\nRequires feature scaling\nChoosing k can be tricky\nDetermining appropriate distance metric can be complex"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-9",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-9",
    "title": "Chapter 3: Classification Models",
    "section": "9. ROC Curves and AUC: Comparing Models",
    "text": "9. ROC Curves and AUC: Comparing Models\n\n9.1 The ROC Curve\nSo far we’ve been using a fixed threshold (0.5) to convert probabilities to class predictions. But what if we tried different thresholds?\nThe ROC curve (Receiver Operating Characteristic) shows model performance across all possible thresholds. It plots:\n\nTrue Positive Rate (TPR) = Recall = TP / (TP + FN)\nFalse Positive Rate (FPR) = FP / (FP + TN)\n\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Get probability predictions from logistic regression\ny_proba_log = log_model_full.predict_proba(X_test_full)[:, 1]\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_full, y_proba_log)\n\n# Compute AUC (Area Under the Curve)\nauc = roc_auc_score(y_test_full, y_proba_log)\n\n# Plot\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Logistic Regression (AUC = {auc:.3f})')\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier (AUC = 0.5)')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate (Recall)', fontsize=12)\nplt.title('ROC Curve', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nInterpreting the ROC curve:\n\nThe diagonal line represents a random classifier (flip a coin)\nThe closer your curve sticks to the top-left corner, the better\nAUC (Area Under the Curve) summarizes performance in one number\nAUC = 1.0: Perfect classifier\nAUC = 0.5: Random guessing\nAUC &lt; 0.5: Worse than random (you’re predicting backwards!)\n\n\n\n\n\n\n\nNote\n\n\n\nWhy is an ROC curve “sticking to the top-left corner” a good thing? The top-left corner means we have essentially zero false positives, and high true positives.\n\n\n\n\n9.2 Comparing Multiple Models with ROC\nLet’s compare all our models on the same ROC plot:\n\n# Get probabilities from all models\nmodels_for_roc = {\n    'Logistic Regression': log_model_full,\n    'Decision Tree': tree_final,\n    'Random Forest': rf_model,\n    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42).fit(X_train_full, y_train_full),\n    'k-NN': knn\n}\n\nplt.figure(figsize=(10, 8))\n\nfor name, model in models_for_roc.items():\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test_full)[:, 1]\n    else:\n        # SVM without probability=True would fail here\n        y_proba = model.predict_proba(X_test_full)[:, 1]\n\n    fpr, tpr, _ = roc_curve(y_test_full, y_proba)\n    auc = roc_auc_score(y_test_full, y_proba)\n    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves: Model Comparison', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nThis visualization makes it easy to compare models at a glance. The model with the highest AUC is typically performing best across all thresholds.\n\n\n9.3 When to Use ROC/AUC\nUse ROC/AUC when:\n\nYou want threshold-independent evaluation\nClasses are relatively balanced\nYou care about ranking (who’s more likely to be positive?)\n\nDon’t use ROC/AUC when:\n\nClasses are severely imbalanced\nYou have a specific threshold constraint\nYou care more about absolute performance at one threshold"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-10",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-10",
    "title": "Chapter 3: Classification Models",
    "section": "11. Comparing All Models: A Practical Guide",
    "text": "11. Comparing All Models: A Practical Guide\n\n11.1 Model Selection Framework\nWith so many classification algorithms, how do you choose? Here’s a practical framework:\nStart with logistic regression if:\n\nYou need interpretability (coefficients matter)\nYou want fast training and prediction\nYou suspect linear decision boundaries\nYou have limited data\n\nUse decision trees if:\n\nYou need maximum interpretability (show the tree to stakeholders)\nFeatures are on different scales (trees don’t need scaling)\nYou have non-linear relationships\nYou’re okay with potential overfitting\n\nUse random forests if:\n\nYou want robust performance without much tuning\nYou have enough data (hundreds or thousands of samples)\nYou don’t need interpretability\nYou want feature importance estimates\n\nUse SVMs if:\n\nYou have high-dimensional data (many features)\nYou have clear margin of separation\nYou’re willing to spend time tuning hyperparameters\nDataset is not too large (&lt; 10,000 samples)\n\nUse k-NN if:\n\nYou have small datasets\nYou don’t need fast predictions\nYou have low-to-moderate dimensions\nDecision boundaries are very irregular\n\n\n\n11.2 Complete Model Comparison\nLet’s do a comprehensive comparison:\n\nfrom sklearn.model_selection import cross_val_score\n\n# Define models\ncomparison_models = {\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Compare on Titanic data using cross-validation\nresults = []\n\nfor name, model in comparison_models.items():\n    # Cross-validation scores\n    cv_scores = cross_val_score(model, X_full, y_full, cv=5, scoring='accuracy')\n\n    # Fit and evaluate\n    model.fit(X_train_full, y_train_full)\n    y_pred_comp = model.predict(X_test_full)\n\n    # Compute metrics\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n    results.append({\n        'Model': name,\n        'CV Accuracy (mean)': cv_scores.mean(),\n        'CV Accuracy (std)': cv_scores.std(),\n        'Test Accuracy': accuracy_score(y_test_full, y_pred_comp),\n        'Precision': precision_score(y_test_full, y_pred_comp),\n        'Recall': recall_score(y_test_full, y_pred_comp),\n        'F1 Score': f1_score(y_test_full, y_pred_comp)\n    })\n\nresults_df = pd.DataFrame(results).set_index('Model')\nresults_df\n\n\n\n\n\n\n\n\nCV Accuracy (mean)\nCV Accuracy (std)\nTest Accuracy\nPrecision\nRecall\nF1 Score\n\n\nModel\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n0.689077\n0.042540\n0.699301\n0.666667\n0.464286\n0.547368\n\n\nDecision Tree\n0.715719\n0.041416\n0.678322\n0.619048\n0.464286\n0.530612\n\n\nRandom Forest\n0.703103\n0.037517\n0.664336\n0.600000\n0.428571\n0.500000\n\n\nSVM (RBF)\n0.666699\n0.072339\n0.615385\n0.513514\n0.339286\n0.408602\n\n\nk-NN (k=5)\n0.659795\n0.059218\n0.650350\n0.565217\n0.464286\n0.509804\n\n\n\n\n\n\n\n\n\n11.3 Visualizing Model Performance\n\n# Plot comparison\nmetrics = ['Test Accuracy', 'Precision', 'Recall', 'F1 Score']\nresults_df[metrics].plot(kind='bar', figsize=(12, 6), rot=45)\nplt.ylabel('Score', fontsize=12)\nplt.title('Model Comparison Across Metrics', fontsize=14)\nplt.legend(loc='lower right')\nplt.ylim(0.5, 1.0)\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n11.4 The Importance of Context\nThere’s no single “best” model. The right choice depends on:\n\nProblem requirements: Speed? Interpretability? Accuracy?\nData characteristics: Size? Dimensionality? Imbalance?\nComputational resources: Training time? Prediction time? Memory?\nBusiness context: Cost of errors? Regulatory requirements?\n\nA model with 90% accuracy might be useless if it misses the 10% that actually matters. A model with 75% accuracy might be perfect if it catches the critical cases."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-summary",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-summary",
    "title": "Chapter 3: Classification Models",
    "section": "Summary",
    "text": "Summary\nYou’ve learned the fundamentals of classification and explored five major approaches. Let’s recap the key insights.\nClassification is fundamentally different from regression. You’re predicting categories, not continuous values. This changes everything: the algorithms, the evaluation metrics, the challenges you’ll face. Linear regression is the wrong tool. You need classifiers designed to output probabilities or discrete predictions.\nEach algorithm has a sweet spot. Logistic regression for speed and interpretability with linear boundaries. Decision trees for maximum explainability. Random forests for robust performance without much tuning. SVMs for high-dimensional data with clear margins. k-NN for small datasets with complex local patterns. There’s no universal best—context matters.\nThe confusion matrix is your diagnostic tool. True positives, false positives, true negatives, false negatives—these four numbers tell you exactly where your model succeeds and fails. Every metric (accuracy, precision, recall, F1) derives from them. Master confusion matrices and you can navigate any classification problem.\nAccuracy alone is almost always insufficient. Especially with imbalanced data, accuracy can be completely misleading. You need to understand precision (when I predict positive, am I usually right?) and recall (of all actual positives, how many do I catch?). The tradeoff between them depends on your specific problem’s costs. Medical diagnosis? Maximize recall. Spam detection? Maybe maximize precision. There’s no one-size-fits-all answer.\nROC curves let you compare models across all thresholds. Instead of committing to 0.5 as your decision threshold, ROC curves show performance across all possible thresholds. AUC summarizes this in one number. Higher is better.\nClass imbalance is the norm, not the exception. Fraud detection, disease diagnosis, rare event prediction—most interesting real-world problems have imbalanced classes. Naive models will just predict the majority class and claim victory with high accuracy. You need to detect imbalance (check value counts!), use appropriate metrics (forget accuracy, use F1 or AUC), and handle it properly (class weights, SMOTE, or other resampling techniques).\nVisualization helps build intuition. Decision boundaries, ROC curves, confusion matrix heatmaps—these aren’t just pretty pictures. They help you understand what your model is actually doing. A model might have great accuracy but terrible decision boundaries. Visualization helps you see problems that metrics alone might hide.\nClassification is a core data science skill. You’ll use it constantly: predicting customer churn, detecting fraud, diagnosing diseases, filtering spam, recommending products, identifying images. The algorithms you’ve learned here are the foundation. Master them, understand their tradeoffs, and you’ll be equipped to tackle real classification problems.\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-practice",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-practice",
    "title": "Chapter 3: Classification Models",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nBuild and Compare Classifiers: Using the Titanic dataset (or another binary classification dataset), fit all five classifier types (Logistic Regression, Decision Tree, Random Forest, SVM, k-NN). Create confusion matrices for each and compare their precision, recall, and F1 scores. Which performs best? Why do you think that is?\nROC Curve Comparison: Using the same dataset from Exercise 1, plot ROC curves for all five models on the same figure. Which model has the highest AUC? Does this match the model with the best accuracy? Why or why not?\nHyperparameter Tuning: Take a Decision Tree classifier and experiment with different values of max_depth (try 1, 3, 5, 10, 20, None). Plot training and test accuracy vs depth. At what depth does overfitting become apparent? How can you tell?\nFeature Importance Analysis: Fit a Random Forest on a classification dataset with multiple features. Extract and visualize feature importances. Which features are most predictive? Now remove the top feature and retrain. How much does performance drop?\nClass Imbalance Challenge: Create an imbalanced dataset (90% class 0, 10% class 1) using make_classification. Fit a naive logistic regression and check its confusion matrix. Then try three approaches to handle the imbalance: class weights, random oversampling, and SMOTE. Which works best? Use F1 score to compare.\nThreshold Tuning: Using logistic regression with predict_proba(), manually try different classification thresholds (0.3, 0.5, 0.7, 0.9). For each threshold, compute precision and recall. Plot precision vs recall as you vary the threshold. Explain the tradeoff you observe."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-additional-resources",
    "href": "Textbook/Chapter-3-Classification/chapter-3-classification.html#ch3-additional-resources",
    "title": "Chapter 3: Classification Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nScikit-learn Classification Documentation - Official docs for all classifiers\nUnderstanding the ROC Curve - Google’s ML crash course on ROC/AUC\nImbalanced-learn Documentation - Handling imbalanced datasets with Python\nConfusion Matrix Guide - Clear explanation of TP, FP, TN, FN\nPrecision vs Recall - When to optimize for which metric\nRandom Forests Explained - Original paper by Leo Breiman (creator of random forests)\nSVM Visualization - StatQuest video explaining SVMs visually"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-resources",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-resources",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Chapter Resources",
    "text": "Chapter Resources\nRelated Assignments:\n\nChapter 1 Homework"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-intro",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-intro",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is EDA?\nYou can’t build effective models if you don’t understand your data. Exploratory Data Analysis (EDA) is the foundation of data science - it’s where you explore and understand your data before building models. In this first chapter we’ll learn the tools and techniques needed to do effective EDA.\n\n\nAI Coding Assistants\nThroughout this course we’ll learn both how to do work by hand, and how to use AI tools to scale our work. In today’s landscape, LLMs are exceptionally powerful, and can act as a “calculator for words”. However, it’s important to remember that LLMs are tools, not replacements for understanding.\nGiven the stochastic (unpredictable, non-deterministic) nature of LLMs, it’s important to learn how to work with them effectively as partners, rather than human replacements. It’s also important to learn how to do certain things by hand, such as calculating averages, making quick graphs, and selecting subsets of your data to analyze. If you’re reliant on LLMs to do every time step for you, you’ll take significantly longer to build models, and you’ll also be less likely to catch errors in your work. By learning to work with LLMs, instead of treating LLMs as a replacement for your own work, you’ll be able to build models more quickly and with greater accuracy."
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-1",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-1",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "1. Getting Started with AI Coding Assistants",
    "text": "1. Getting Started with AI Coding Assistants\n\n1.1 Why Use AI for Coding?\nImagine the following situation: You have a data set with five columns of data, one of which is a target variable you want to predict. For example, you might be trying to predict the price of a house based on its size, number of bedrooms, number of bathrooms, and location. You want to build a model to predict the price of a house based on these features. This data is small enough that you could do the EDA by hand, including:\n\nMaking graphs for each column\nCalculating summary statistics\nChecking for missing values\nChecking for outliers\nChecking for data quality issues\n\nHowever, what if your data had three hundred columns? Now you move well beyond what you could do by hand, and what could be analyzed by a human. This is where AI coding assistants come in.\nThis is exactly how we’ll use LLMs in this class. We’ll first learn the topics by hand at a foundational level and a small scale. Then, we’ll learn how to effectively use AI to scale our work.\n\n\n\n\n\n\nNote\n\n\n\nAI won’t take your job. Someone who knows how to use AI more effectively than you do will take your job. Be that person.\n\n\n\n\n1.2 Writing Effective Prompts\nLearning to write good prompts is like learning to communicate clearly with a colleague. You wouldn’t walk up to a coworker and say “make graph” and expect them to know exactly what you want. The same goes for AI coding assistants.\nHere are the key principles for effective prompting:\n1. Be specific about what you want\nDon’t just ask for “a visualization” when you know you want a histogram. Tell the AI exactly what type of output you’re looking for.\n2. Provide context\nLet the AI know what data you’re working with, what columns exist, and what you’re trying to accomplish. The more context you provide, the better the results.\n3. Start simple, then iterate\nDon’t try to get everything perfect in one prompt. Start with a basic request, see what you get, and refine from there. This is much faster than trying to write the “perfect” prompt.\n4. Specify the details that matter\nIf you care about axis labels, titles, colors, or specific parameter values, say so. If you don’t specify, the AI will make assumptions that might not match what you need.\nLet’s look at some examples. Here’s a poor prompt:\n\n“Make a graph of the data”\n\nWhat’s wrong with this? There’s no context about what data we’re working with, what type of graph we want, or what we’re trying to show. The AI has to guess everything.\nHere’s a better prompt:\n\n“I’m working with a housing dataset that has columns for price, square_feet, bedrooms, and location. Create a histogram showing the distribution of house prices. Use 30 bins, add a title ‘Distribution of House Prices’, and label the x-axis as ‘Price ($)’ and y-axis as ‘Frequency’. I’m working in Python and using matplotlib for graphing.”\n\nSee the difference? This prompt tells the AI:\n\nWhat data we have\nWhat visualization we want (histogram)\nWhat variable to plot (price)\nSpecific parameters (30 bins)\nFormatting details (title, axis labels)\n\nLet’s see this in action. Here’s what you might get from that better prompt:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming you have a DataFrame called 'housing_df'\nplt.figure(figsize=(10, 6))\nplt.hist(housing_df['price'], bins=30, edgecolor='black')\nplt.title('Distribution of House Prices')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.show()\nNotice that even with a good prompt, you might need to iterate. Maybe you look at this histogram and realize you want different bin sizes, or you want to add grid lines. That’s fine! Just refine your prompt: “Great, now add grid lines to make it easier to read the values.”\nThe key is that you’re building on what works, not starting from scratch each time. This iterative approach is exactly how you should work with AI assistants throughout this course.\n\n\n1.3 When to Use AI vs. Coding By Hand\nHow do you decide when to use AI, and when to do things by hand? You should start by asking yourself the following question: “How well do I understand my data and the task I’m trying to accomplish?”\nIf you understand it deeply and know exactly what you want to accomplish, then AI can be a great tool. However, if you’re just starting to understand your data, and/or the task isn’t clear, you should be working by hand.\nBecause LLMs take time to generate code, and because they often don’t do quite what you had in mind, at the early stages you can spend more time modifying/re-prompting an LLM than you would if you were doing things by hand. You want a quick feedback loop between your brain and your screen. As soon as a question pops into your head (“I wonder what the relationship is between…”) you should be able to answer it quickly. Changing your focus to work with an LLM will likely take too long, potentially result in more errors, and take you away from the task at hand.\n\n\n\n\n\n\nNote\n\n\n\nAs you become more comfortable with your data and the task at hand, you can start to use AI to scale your work. You can use AI to generate code for you, and then modify it to fit your needs. You can also use AI to generate code for you, and then modify it to fit your needs. When you look at your work and say “I know I’m on the right track, now I just need to do more”, then you should be using AI to scale your work.\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nWrite prompts that effectively describe what you want the LLM to accomplish.\nCritique prompts as to whether or not they follow the suggestions outlined above.\nKnow when to use an LLM, and when to write code by hand."
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-2",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-2",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "2. Data Manipulation with Pandas",
    "text": "2. Data Manipulation with Pandas\nPandas is the workhorse library for data manipulation in Python. If you’re going to be a data scientist, you need to know Pandas inside and out. The good news is that once you learn the basics, everything else follows a similar pattern.\n\n2.1 Loading Data\nLet’s start with the most basic task: loading data into Python. Most of the time, your data will be stored in a CSV (comma-separated values) file. Pandas makes this incredibly easy.\n\nimport pandas as pd\n\n# Load the California housing dataset\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Take a look at the first few rows\nhousing_df.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nWhat’s happening here? We’re importing the pandas library (always abbreviated as pd), then using the read_csv() function to load our data. The result is a DataFrame, which you can think of as a table with rows and columns, similar to an Excel spreadsheet.\nThe .head() method shows us the first 5 rows. This is always a good first step when you load data—you want to see what you’re working with.\nNow we can see our columns and get a sense of what the data looks like. This is California housing data with information about different districts. Notice that each row has an index (0, 1, 2, 3, 4) on the left side. Pandas automatically creates this for us.\nLet’s explore our data a bit more:\n\n# How many rows and columns do we have?\nprint(f'df shape: {housing_df.shape}\\n')\n\n# What are the column names?\nprint(f'df columns: {housing_df.columns}\\n')\n\n# What data types are in each column?\nprint('df dtypes:')\nprint(housing_df.dtypes)\n\ndf shape: (20640, 10)\n\ndf columns: Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income',\n       'median_house_value', 'ocean_proximity'],\n      dtype='object')\n\ndf dtypes:\nlongitude             float64\nlatitude              float64\nhousing_median_age    float64\ntotal_rooms           float64\ntotal_bedrooms        float64\npopulation            float64\nhouseholds            float64\nmedian_income         float64\nmedian_house_value    float64\nocean_proximity        object\ndtype: object\n\n\nThe .shape attribute tells us the dimensions of our DataFrame. The .columns attribute gives us the column names. And .dtypes tells us what type of data is in each column (integers, floating point numbers, strings, etc.).\nWhy do we care about data types? Because you can’t calculate the average of text data, and you can’t use numbers as categories. Making sure your data types are correct is a crucial early step in any analysis. Even things that seem like numbers may not be. For example, suppose that we had a row in our housing data which looked like this:\nmedian_house_value  total_rooms  ...\nUnknown             1500.0       ...\nIf we try to calculate the average house value, we’ll get an error:\nhousing_df['median_house_value'].mean()\nbecause the median_house_value column isn’t numerical (it has a string value in it).\n\n\n\n\n\n\nNote\n\n\n\nYou should always start by looking at your data using df.head()! This will help you catch any data type issues early. Following up with basic data checks (column names, data types, etc.) is also best practices.\n\n\n\n\n2.2 Selecting and Filtering Data\nNow that we have data loaded, we need to know how to slice it up and look at specific parts. This is where Pandas really shines.\nSelecting columns:\n\n# Select a single column (returns a Series)\nhouse_values = housing_df['median_house_value']\n\n# Select multiple columns\nmultiple_cols = ['median_house_value', 'ocean_proximity']\n\n# Select multiple columns (returns a DataFrame)\nvalue_and_location = housing_df[multiple_cols]\n\nNotice that, when selecting multiple columns, you should enclose them in a list, like ['median_house_value', 'ocean_proximity']. Here we did this in two steps: 1) Write down the columns we want, 2) Extract them from the data. However, there’s no reason you can’t do them in a single step:\nvalue_and_location = housing_df[\n    ['median_house_value', \n    'ocean_proximity']]\nSometimes this can be confusing, because people think that double brackets [['median_house_value', 'ocean_proximity']] are some kind of special syntax. They’re not. They’re just a list of column names.\nFiltering rows:\nHere’s where things get really useful. Let’s say we only want to look at districts near the bay. We’ll break this out into multiple steps, and then show how to combine it in a single step.\n\n# Check if each row is near the bay by creating a \"mask\"\nnear_bay_mask = housing_df['ocean_proximity'] == 'NEAR BAY'\n# Output: [True, True, True, True, True, False, False, ...]\n\n# Use this \"mask\" to filter the DataFrame\nnear_bay_districts = housing_df[near_bay_mask]\n\nnear_bay_districts.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nWhat’s going on here? The expression housing_df['ocean_proximity'] == 'NEAR BAY' creates a True/False (boolean) value for each row—True if the district is near the bay, False otherwise. Then we use that boolean mask to filter the DataFrame.\nHere we split it out into two steps, where we first created the mask near_bay_mask = housing_df['ocean_proximity'] == 'NEAR BAY', and then used that mask to filter the DataFrame housing_df[near_bay_mask]. This is a common pattern when filtering data. However, you can also write this in just one step:\nnear_bay_districts = housing_df[housing_df['ocean_proximity'] == 'NEAR BAY']\nOnce again, people sometimes find this confusing because of the housing_df inside the outer housing_df. However, as we’ve seen, all that’s going on is two steps: 1) Create a mask to select what you want (e.g. districts near the bay), 2) Use that mask to filter the DataFrame.\nLet’s try some more complex filters:\n\n# Districts with more than 1000 total rooms\nlarge_districts = housing_df[housing_df['total_rooms'] &gt; 1000]\n\n# Districts that are both near the bay AND have expensive houses (&gt; $400,000)\nexpensive_bay_area = housing_df[(housing_df['ocean_proximity'] == 'NEAR BAY') &\n                                 (housing_df['median_house_value'] &gt; 400000)]\n\n# Districts that are either very cheap OR very expensive\nextreme_values = housing_df[(housing_df['median_house_value'] &lt; 150000) |\n                             (housing_df['median_house_value'] &gt; 400000)]\n\nexpensive_bay_area.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n89\n-122.27\n37.80\n52.0\n249.0\n78.0\n396.0\n85.0\n1.2434\n500001.0\nNEAR BAY\n\n\n128\n-122.21\n37.83\n40.0\n4991.0\n674.0\n1616.0\n654.0\n7.5544\n411500.0\nNEAR BAY\n\n\n140\n-122.18\n37.81\n30.0\n292.0\n38.0\n126.0\n52.0\n6.3624\n483300.0\nNEAR BAY\n\n\n155\n-122.23\n37.81\n52.0\n2315.0\n292.0\n861.0\n258.0\n8.8793\n410300.0\nNEAR BAY\n\n\n\n\n\n\n\nNotice a few things: - We use & for “and” and | for “or” - We need parentheses around each condition when combining them - The comparison operators (&gt;, &lt;, ==) work just like you’d expect\nHere’s a practical example. Let’s say you’re analyzing housing affordability and you want to find all inland districts with median incomes above 5 and median house values under $300,000:\n\n# Your target districts\naffordable_inland = housing_df[(housing_df['ocean_proximity'] == 'INLAND') &\n                               (housing_df['median_income'] &gt; 5) &\n                               (housing_df['median_house_value'] &lt; 300000)]\n\nprint(f\"Found {len(affordable_inland)} districts matching criteria\")\n\naffordable_inland.head()\n\nFound 534 districts matching criteria\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n968\n-121.88\n37.67\n25.0\n2244.0\n301.0\n937.0\n324.0\n6.4524\n296900.0\nINLAND\n\n\n969\n-121.89\n37.67\n20.0\n2948.0\n471.0\n1181.0\n474.0\n6.0604\n247900.0\nINLAND\n\n\n975\n-121.87\n37.66\n52.0\n775.0\n134.0\n315.0\n123.0\n5.0677\n233300.0\nINLAND\n\n\n979\n-121.87\n37.67\n10.0\n4337.0\n800.0\n1813.0\n743.0\n5.5000\n247200.0\nINLAND\n\n\n981\n-121.85\n37.68\n4.0\n4719.0\n741.0\n1895.0\n742.0\n6.8132\n282500.0\nINLAND\n\n\n\n\n\n\n\nThis kind of filtering is something you’ll do constantly in data science. You’ll often want to analyze specific subsets of your data to understand patterns.\n\n\n\n\n\n\nNote\n\n\n\nBreaking complex queries, such as the affordable inland districts example above, into smaller steps can make it easier to understand and debug. Compare the query above with the same query written on a single line:\naffordable_inland = housing_df[(housing_df['ocean_proximity'] == 'INLAND') & (housing_df['median_income'] &gt; 5) & (housing_df['median_house_value'] &lt; 300000)]\nThis is much more difficult to read and debug!\n\n\n\n\n2.3 Data Cleaning Basics\nReal-world data is messy. You’ll have missing values, duplicates, wrong data types—all sorts of problems. Let’s learn how to spot and fix them.\nChecking for missing values:\n\n# How many missing values in each column?\nhousing_df.isnull().sum()\n\nlongitude               0\nlatitude                0\nhousing_median_age      0\ntotal_rooms             0\ntotal_bedrooms        207\npopulation              0\nhouseholds              0\nmedian_income           0\nmedian_house_value      0\nocean_proximity         0\ndtype: int64\n\n\nIn this command, housing_df.isnull() is returning True/False for each value in the DataFrame. Then we use .sum() to count up the number of True values in each column.\nSo we have 207 missing values in the total_bedrooms column. What do we do about this?\nOption 1: Drop rows with missing values\n# Drop any row that has at least one missing value\nhousing_clean = housing_df.dropna()\n\n# Drop only rows where a specific column (total_bedrooms) is missing a value\n# This is useful if this column is extremely important, and your analysis wouldn't make sense without it\nhousing_clean = housing_df.dropna(subset=['total_bedrooms'])\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with this approach! If you have many columns and missing values are scattered throughout, you might end up dropping most of your data. This is especially true if you’re working with data with lots of columns and/or columns which aren’t especially important. For example, a store may have a rewards number column. However, not all customers are reward customers. If we dropped all rows with any missing value, then all non-rewards customers would disappear from our data!\n\n\nOption 2: Fill missing values\n# Fill with a specific value (e.g. zero)\nhousing_df['total_bedrooms'] = housing_df['total_bedrooms'].fillna(0)\n\n# Fill missing values with the median (for numeric columns)\nmedian_bedrooms = housing_df['total_bedrooms'].median()\nhousing_df['total_bedrooms'] = housing_df['total_bedrooms'].fillna(median_bedrooms)\nWhich approach should you use? It depends on your data and your analysis. If you have lots of data and relatively few missing values, dropping might be fine. If missing values are common, you’ll need to fill them thoughtfully.\n\nBe careful with filling missing values! Sometimes a missing value is a signal that something is wrong. For example, if a house is missing a price, it might be because it’s not for sale. Or it may indicate something, such as a lack of sale price meaning the home wasn’t sold. When you fill missing values with other values, you are making assumptions about the data that may not be true and run the risk of corrupting your data.\n\nChecking for duplicates:\nWe can check for duplicates using the .duplicated() method. As with other methods we’ve seen today, it returns True/False values according to whether the row is a duplicate or not. By using .sum() we add up (i.e. count) the number of duplicate rows.\n\n# Are there any duplicate rows?\nprint(f'Duplicates: {housing_df.duplicated().sum()}')\n\n# Remove duplicates\nhousing_clean = housing_df.drop_duplicates()\n\n# Remove duplicates based on specific columns\n# (e.g., if you only care about unique combinations of location and ocean proximity)\nhousing_unique = housing_df.drop_duplicates(subset=['longitude', 'latitude', 'ocean_proximity'])\n\nprint(f'Duplicates after removing: {housing_unique.duplicated().sum()}')\n\nDuplicates: 0\nDuplicates after removing: 0\n\n\n\nDuplicates aren’t necessarily bad! If I’m a customer at a store and I return multiple times in the same day, depending on the info the store saves above me (e.g. name, date of visit), this may look like a duplicate, when it was really just me shopping multiple times.\n\nFixing data types:\nSometimes Pandas doesn’t guess the right data type when loading your data. For example, a column of numbers might be loaded as strings:\n\n# Check current data type\nprint(housing_df['median_house_value'].dtype)  # Shows float64\n\n# Convert a column to categorical (useful for columns with few unique values)\nhousing_df['ocean_proximity'] = housing_df['ocean_proximity'].astype('category')\n\nfloat64\n\n\nSample data cleaning workflow:\nHere’s a complete example of a basic data cleaning workflow:\n\nimport pandas as pd\n\n# Load the data\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Check for issues\nprint(\"Shape:\", housing_df.shape)\nprint(\"\\nMissing values:\")\nprint(housing_df.isnull().sum())\nprint(\"\\nDuplicates:\", housing_df.duplicated().sum())\nprint(\"\\nData types:\")\nprint(housing_df.dtypes)\n\n# Clean the data\nhousing_clean = housing_df.copy()  # Make a copy so we don't modify the original\n\n# Remove duplicates (if any)\nhousing_clean = housing_clean.drop_duplicates()\n\n# Fill missing total_bedrooms with the median\nmedian_bedrooms = housing_clean['total_bedrooms'].median()\nhousing_clean['total_bedrooms'] = housing_clean['total_bedrooms'].fillna(median_bedrooms)\n\n# Convert ocean_proximity to categorical\nhousing_clean['ocean_proximity'] = housing_clean['ocean_proximity'].astype('category')\n\n# Verify the cleaning worked\nprint(\"\\nAfter cleaning:\")\nprint(\"Shape:\", housing_clean.shape)\nprint(\"Missing values:\", housing_clean.isnull().sum().sum())\n\nShape: (20640, 10)\n\nMissing values:\nlongitude               0\nlatitude                0\nhousing_median_age      0\ntotal_rooms             0\ntotal_bedrooms        207\npopulation              0\nhouseholds              0\nmedian_income           0\nmedian_house_value      0\nocean_proximity         0\ndtype: int64\n\nDuplicates: 0\n\nData types:\nlongitude             float64\nlatitude              float64\nhousing_median_age    float64\ntotal_rooms           float64\ntotal_bedrooms        float64\npopulation            float64\nhouseholds            float64\nmedian_income         float64\nmedian_house_value    float64\nocean_proximity        object\ndtype: object\n\nAfter cleaning:\nShape: (20640, 10)\nMissing values: 0\n\n\nThis workflow checks for issues, fixes them, and verifies the fixes worked. You’ll use patterns like this constantly in your EDA work.\nThe key takeaway: always inspect your data before you analyze it. You need to know what you’re working with, spot problems early, and fix them before they cause issues down the line. Don’t just assume your data is clean—check!\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nLoad data using df = pd.read_csv(...)\nUse .head() and .tail() to inspect your data.\nList the columns in the data\nList the data types and understand common types (e.g. int, object, float, bool)\nSelect one or more columns\nFilter rows using criteria (e.g. city == “Houston”, age &gt; 25, etc.)\nFilter rows use “and” (&) and “or” (|)\nCount the number of missing values using .isnull() and .sum()\nDrop rows with missing values, including dropping rows where only a certain column is missing values\nFill missing values with a fixed number (e.g. zero) or a calculated value (e.g. the mean value from the column)\nChange data types"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-3",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-3",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "3. Statistical Summaries and Data Profiling",
    "text": "3. Statistical Summaries and Data Profiling\nOnce your data is loaded and cleaned, the next step is understanding what it contains. You need to know the typical values, the spread of the data, and whether there are any weird patterns. This is where statistical summaries come in.\n\n3.1 Descriptive Statistics\nPandas makes it incredibly easy to get summary statistics for your data. Let’s start with the simplest approach:\n\n# Get summary statistics for all numeric columns\nhousing_df.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\nWhat is all this telling us? Let’s break it down row by row:\n\ncount: How many non-missing values exist\nmean: The average value\nstd: Standard deviation (how spread out the values are)\nmin: The smallest value\n25%: The 25th percentile (25% of values are below this)\n50%: The median (middle value when sorted)\n75%: The 75th percentile (75% of values are below this)\nmax: The largest value\n\nBut what do these numbers actually mean for our housing data? Let’s interpret:\nThe median house value is about $206,856 on average, with a standard deviation of $115,396. That’s a pretty big spread—prices vary a lot across California! The median (50th percentile) is $179,700, which is lower than the mean. What does that tell us? It suggests the distribution might be right-skewed, meaning there are some very expensive districts pulling the average up.\nFor total rooms, the typical district has around 2,636 rooms, with most districts falling between 1,448 and 3,148 rooms (the 25th to 75th percentile range). But look at that max value: 39,320 rooms! That’s a huge outlier that we might want to investigate.\nSometimes you want statistics for a single column:\n\n# Mean of a specific column\navg_value = housing_df['median_house_value'].mean()\nprint(f\"Average house value: ${avg_value:,.0f}\")\n\n# Median\nmedian_value = housing_df['median_house_value'].median()\nprint(f\"Median house value: ${median_value:,.0f}\")\n\n# Standard deviation\nvalue_std = housing_df['median_house_value'].std()\nprint(f\"House value standard deviation: ${value_std:,.0f}\")\n\n# Specific percentiles\npercentile_90 = housing_df['median_house_value'].quantile(0.90)\nprint(f\"90th percentile: ${percentile_90:,.0f}\")\n\nAverage house value: $206,856\nMedian house value: $179,700\nHouse value standard deviation: $115,396\n90th percentile: $376,600\n\n\nWhy would you care about the median vs. the mean? The median is more robust to outliers. If you have one very expensive district in an area of typical districts, the mean will be pulled way up, but the median will stay reasonable. When you’re trying to understand “typical” values, the median is often more useful.\nHere’s a practical example. Let’s say you want to understand house values in different proximity to the ocean:\n\n# Average value by ocean proximity\nvalue_by_proximity = housing_df.groupby('ocean_proximity')['median_house_value'].agg(['mean', 'median', 'count'])\nprint(value_by_proximity)\n\n                          mean    median  count\nocean_proximity                                \n&lt;1H OCEAN        240084.285464  214850.0   9136\nINLAND           124805.392001  108500.0   6551\nISLAND           380440.000000  414700.0      5\nNEAR BAY         259212.311790  233800.0   2290\nNEAR OCEAN       249433.977427  229450.0   2658\n\n\nNow we’re getting somewhere! Districts near the ocean (especially islands!) are significantly more expensive on average. This kind of breakdown is crucial for understanding your data—overall statistics can hide important patterns in subgroups.\n\n\n\n\n\n\nTip\n\n\n\nPandas can automatically do this (and more) for you with the .describe() method. Try running housing_df.describe() on your data and see what it gives you.\n\n\n\n\n3.2 Outliers\nOutliers are another important aspect of understanding your data. These are values that are far from the typical range. The typical way to identify outliers is to find data that is in the top and bottom 1% of the data.\n\n# Find the 1st and 99th percentiles\nbottom_1_percent = housing_df['median_house_value'].quantile(0.01)\ntop_1_percent = housing_df['median_house_value'].quantile(0.99)\n\nprint(f\"Bottom 1%: ${bottom_1_percent:,.0f}\")\nprint(f\"Top 1%: ${top_1_percent:,.0f}\")\n\n# Find potential outliers\noutliers = housing_df[(housing_df['median_house_value'] &lt; bottom_1_percent) |\n                       (housing_df['median_house_value'] &gt; top_1_percent)]\n\nprint(f\"\\nFound {len(outliers)} potential outliers\\n\")\n\nprint(outliers[['median_house_value', 'ocean_proximity', 'median_income']].head())\n\nBottom 1%: $50,000\nTop 1%: $500,001\n\nFound 199 potential outliers\n\n      median_house_value ocean_proximity  median_income\n1175             42500.0          INLAND         0.8252\n1176             45100.0          INLAND         1.0585\n1177             39400.0          INLAND         1.3289\n1181             41800.0          INLAND         2.2045\n1188             49000.0          INLAND         1.7727\n\n\nThere’s nothing particular about the top and bottom 1%. You could certainly look at the top and bottom 1%, or 2%, or 5%, or 10%, or any other percentage you want. The key is to look at the data and see if it makes sense. Primarily what you’re looking for is data that doesn’t make sense. If a district has very high house values, that could be totally reasonable, and of course there will always be some data in the top/bottom 1%. The problem is when those values don’t make sense. For example, if house values are shown as negative, that’s not real. You need to investigate before deciding what to do. Similarly, if values are impossibly high, that also doesn’t make sense.\n\n\n\n\n\n\nTip\n\n\n\nShould you remove outliers? Not automatically! They might be legitimate data (yes, some houses really are that expensive), or they might be errors (someone entered $5,000,000 instead of $500,000). You need to investigate before deciding what to do. Often this is a judgement call, more than a clear-cut decision.\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nUse .describe()\nCalculate the mean, median, standard deviation and percentiles from a column\nUse .quantile() to find potential outliers on the high and low end"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-4",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-4",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "4. Data Visualization Principles",
    "text": "4. Data Visualization Principles\nNumbers and summary statistics are useful, but humans are visual creatures. A good visualization can reveal patterns that would take hours to find in tables of numbers. But here’s the thing: not all visualizations are created equal. You need to match the right type of plot to the question you’re asking.\n\n4.1 Choosing the Right Visualization\nThe type of visualization you choose depends on what you’re trying to show. Here are the most common scenarios:\nWant to see the distribution of a single variable? Use a histogram or box plot.\nWant to see the relationship between two numeric variables? Use a scatter plot.\nWant to compare values across categories? Use a bar plot.\nWant to see how something changes over time? Use a line plot.\nLet’s break these down with examples.\n\n4.1.1 Histograms\nHistograms show you how data is distributed. They’re perfect for answering questions like “Are house values normally distributed?” or “How many districts fall into different value ranges?”\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of median house values\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value', bins=30)\nplt.title('Distribution of Median House Values')\nplt.xlabel('Median House Value ($)')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\nThis shows you the shape of your data. Is it symmetric? Skewed? Are there outliers? All of this becomes immediately visible.\n\n\n4.1.2 Box plots\nBox plots are another way to visualize distributions, especially useful for comparing across groups:\n\n# Compare house value distributions across ocean proximity categories\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value')\nplt.title('House Values by Ocean Proximity')\nplt.xlabel('Ocean Proximity')\nplt.ylabel('Median House Value ($)')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe box shows the 25th to 75th percentile (the middle 50% of data), the line in the middle is the median, and the “whiskers” extend to show the range. Points beyond the whiskers are potential outliers.\n\n\n\n\n\n\nTip\n\n\n\nBox plots are excellent for quickly comparing distributions across multiple groups. You can instantly see which group has higher medians, more variability, or more outliers.\n\n\n\n\n4.1.3 Scatter plots\nScatter plots reveal relationships between two numeric variables:\n\n# Relationship between median income and house value\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n\n\n\n\n\n\n\n\nIf you see points trending upward from left to right, that’s a positive relationship—higher incomes are associated with more expensive houses. If points are scattered randomly, there’s no clear relationship.\nYou can add a third variable using color:\n\n# Add ocean proximity as color\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', alpha=0.6)\nplt.title('House Value vs. Median Income by Ocean Proximity')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n\n\n\n\n\n\n\n\nNow you can see if the relationship between income and house values differs by ocean proximity. Maybe coastal districts are consistently more expensive for the same income level.\n\n\n4.1.4 Bar plots\nBar plots compare values across categories:\n\n# Average house value by ocean proximity\nplt.figure(figsize=(10, 6))\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', estimator='mean')\nplt.title('Average House Value by Ocean Proximity')\nplt.xlabel('Ocean Proximity')\nplt.ylabel('Average House Value ($)')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis makes comparisons obvious at a glance. You can immediately see which proximity category has the highest average house values.\n\n\n\n\n\n\nNote\n\n\n\nThe key to choosing visualizations: think about what question you’re asking. “How is this variable distributed?” → histogram. “Is there a relationship between these two things?” → scatter plot. “Which group is highest?” → bar plot. Match the visualization to the question.\n\n\n\n\n\n4.2 Creating Visualizations with Seaborn\nSeaborn is built on top of matplotlib and makes creating statistical visualizations much easier. It has sensible defaults, nice-looking styles, and works seamlessly with pandas DataFrames.\nLet’s walk through the basic visualizations you’ll use constantly:\n\n4.2.1 Histograms\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Basic histogram\nsns.histplot(data=housing_df, x='median_house_value')\nplt.show()\n\n# With less bins\nsns.histplot(data=housing_df, x='median_house_value', bins=10)\nplt.show()\n\n# With KDE (smooth density curve) overlay\nsns.histplot(data=housing_df, x='median_house_value', kde=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bins parameter controls how many bars you see. Too few bins and you lose detail. Too many and it gets noisy. Usually 20-50 bins is a good starting point.\n\n\n4.2.2 Scatter plots\n\n# Basic scatter plot\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n# With color by category\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plots are your go-to tool for finding relationships. If you’re doing EDA and wondering whether two variables are related, make a scatter plot. It takes 2 seconds and can reveal patterns that summary statistics miss.\n\n\n\n\n\n\nTip\n\n\n\nWhen exploring relationships, always make a scatter plot first. You might have the same correlation coefficient but completely different patterns. The classic example is Anscombe’s quartet—four datasets with identical statistics but totally different patterns when plotted.\n\n\n\n\n4.2.3 Bar plots\n\n# Count of observations by category\nsns.countplot(data=housing_df, x='ocean_proximity')\nplt.xticks(rotation=45)\nplt.show()\n\n# Average value by category\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', estimator='mean')\nplt.xticks(rotation=45)\nplt.show()\n\n# Grouped bar plot - create age groups first\nhousing_df['age_group'] = pd.cut(housing_df['housing_median_age'],\n                                  bins=[0, 20, 35, 100],\n                                  labels=['New', 'Mid', 'Old'])\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', hue='age_group')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBar plots are perfect for comparing across categories. The estimator parameter lets you choose what to show—mean, median, sum, etc.\n\n\n4.2.4 Box plots\n\n# Distribution by category\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value')\nplt.xticks(rotation=45)\nplt.show()\n\n# Horizontal (sometimes easier to read with long labels)\nsns.boxplot(data=housing_df, y='ocean_proximity', x='median_house_value')\nplt.show()\n\n# Multiple categories\nhousing_df['income_bracket'] = pd.cut(housing_df['median_income'],\n                                       bins=[0, 2.5, 4.5, 20],\n                                       labels=['Low', 'Medium', 'High'])\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value', hue='income_bracket')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBox plots pack a lot of information into a small space. You see the median, the spread (IQR), and outliers all at once.\n\n\n4.2.5 Pair plots (bonus)\nWhen you want to see relationships between multiple variables at once:\n\n# Scatter plots for all numeric variables\nsns.pairplot(housing_df[['median_house_value', 'median_income', 'housing_median_age', 'total_rooms']])\nplt.show()\n\n# Color by category\nsns.pairplot(housing_df[['median_house_value', 'median_income', 'housing_median_age',\n                         'total_rooms', 'ocean_proximity']],\n             hue='ocean_proximity')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis creates a grid of scatter plots showing every pair of variables. It’s incredibly useful for initial exploration—you can spot all the relationships at once.\n\n\n\n4.3 Best Practices for Effective Visualizations\nA visualization without labels is just decorative art. You need to make your plots readable and informative. Here are the key principles:\n1. Always add titles and axis labels\n\nprint('Bad: no labels')\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\nprint('Good: clear labels')\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income', fontsize=14, fontweight='bold')\nplt.xlabel('Median Income (in $10k)', fontsize=12)\nplt.ylabel('Median House Value ($)', fontsize=12)\nplt.show()\n\nBad: no labels\n\n\n\n\n\n\n\n\n\nGood: clear labels\n\n\n\n\n\n\n\n\n\nYour title should answer “What am I looking at?” Your axis labels should include units where relevant (dollars, income brackets, etc.).\n2. Make the plot big enough to read\n\n# Too small (default size is often cramped)\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n# Better: specify figure size\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figsize parameter takes (width, height) in inches. A good starting point is (10, 6) for most plots.\n3. Use color meaningfully\nColor should convey information, not just look pretty:\n\n# Color by category to show groups\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', palette='Set2')  # Use a colorblind-friendly palette\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn has many built-in palettes: ‘Set2’, ‘colorblind’, ‘husl’, etc. Choose one that’s easy to distinguish and colorblind-safe.\n\n\n\n\n\n\nWarning\n\n\n\nAvoid using red and green together as your only color distinction. About 8% of men have some form of color blindness that makes red-green distinctions difficult. Use colorblind-friendly palettes like ‘colorblind’ or ‘Set2’.\n\n\n4. Add grid lines for easier reading\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.grid(True, alpha=0.3)  # alpha makes it subtle\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n\n\n\n\n\n\n\n\nSubtle grid lines make it easier to read values from your plot.\n5. Format numbers appropriately\nLarge numbers benefit from formatting:\n\nimport matplotlib.ticker as mtick\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value')\n\n# Format y-axis as integers\nplt.gca().yaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))\n\n# Format x-axis as dollars\nplt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))\n\nplt.title('Distribution of Median House Values')\nplt.xlabel('Median House Value')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\nThis adds dollar signs and comma separators, making the plot much easier to read.\n6. Adjust plot limits when needed\nSometimes outliers make it hard to see the main pattern:\n\n# If you have extreme outliers\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.xlim(0, 10)  # Focus on median incomes under 10\nplt.ylim(0, 500000)  # Focus on house values under $500k\nplt.show()\n\n\n\n\n\n\n\n\nUse this carefully—you’re choosing to hide data. But sometimes it’s necessary to see the pattern in the majority of your data.\nComplete example with all best practices:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Set the style for all plots\nsns.set_style('whitegrid')\n\n# Create the plot\nplt.figure(figsize=(12, 7))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', palette='Set2', s=100, alpha=0.6)\n\n# Add labels and title\nplt.title('House Value vs. Median Income by Ocean Proximity',\n          fontsize=16, fontweight='bold', pad=20)\nplt.xlabel('Median Income (in $10k)', fontsize=13)\nplt.ylabel('Median House Value ($)', fontsize=13)\n\n# Format the y-axis as currency\nplt.gca().yaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))\n\n# Adjust legend\nplt.legend(title='Ocean Proximity', fontsize=11, title_fontsize=12)\n\n# Add subtle grid\nplt.grid(True, alpha=0.3)\n\n# Tight layout to prevent label cutoff\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis creates a professional-looking plot that’s easy to read and interpret.\n\n\n\n\n\n\nTip\n\n\n\nSave your well-formatted plotting code as a template. When you find styling that works well, reuse it. You don’t want to be tweaking font sizes every time you make a plot.\n\n\nThe key takeaway: visualizations are tools for understanding, not just decoration. Choose the right type of plot for your question, label everything clearly, and make it easy to read. When someone looks at your visualization, they should immediately understand what they’re seeing and why it matters.\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nPick an appropriate visualization for your data\nCompare and contrast the visualizations discussed (e.g. their use cases, what data they work with, etc.)\nCreate a basic histogram, scatter plot, bar chart and box plot using Seaborn\nAssign a title to your graph"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-5",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-5",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "5. Testing Your Data Analysis Code",
    "text": "5. Testing Your Data Analysis Code\n\n5.1 Why Testing Matters\nHere’s a scenario that happens all the time: You write some code to filter your data, run your analysis, get results, and present them to your team. Then someone asks “wait, why are there only 5 data points?”. Your entire analysis is now useless.\nTesting catches these errors before they become embarrassing mistakes. And in data science, testing doesn’t have to be complicated. Simple checks can save you from major headaches.\nThe goal is simple: make sure your code is doing what you think it’s doing. If you filter data, did you actually get the rows you expected? If you drop missing values, did the count of missing values actually drop? These are the kinds of things you should be checking.\n\n\n5.2 Using assert Statements\nThe simplest way to test your data analysis code is with assert statements. An assert is like a sanity check: you state what should be true, and if it’s not, Python stops and tells you something is wrong.\nHere’s the basic idea:\n\n# This will pass (no error)\nassert 5 &gt; 3\n\n# This will fail and raise an error\nassert 2 &gt; 3\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[32], line 5\n      2 assert 5 &gt; 3\n      4 # This will fail and raise an error\n----&gt; 5 assert 2 &gt; 3\n\nAssertionError: \n\n\n\nNotice that when an assertion fails, your code stops. That’s the point! You want to know immediately when something is wrong.\n\n\n\n\n\n\nTip\n\n\n\nYou can (and should!) add a message to the assert statement to help you understand what went wrong.\n\nassert 2 &gt; 3, \"Two is not greater than three!\"\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[33], line 1\n----&gt; 1 assert 2 &gt; 3, \"Two is not greater than three!\"\n\nAssertionError: Two is not greater than three!\n\n\n\n\n\nCommon mistake: Case sensitivity\nLet’s say you want to filter for districts near the bay. The code below looks good at first glance:\n\n# Intentional typo: \"near bay\" instead of \"NEAR BAY\"\nnear_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n\nHowever, later you decide to print out how many districts are near the bay.\n\nprint(f\"Found {len(near_bay)} districts near the bay\")\n\nFound 0 districts near the bay\n\n\nZero districts! But your code didn’t crash, so you might not notice. This is where asserts help:\n\n# Filter the data\nnear_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n\n# Assert that we got some results\nassert len(near_bay) &gt; 0, \"No districts found! Check your filter condition\"\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[36], line 5\n      2 near_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n      4 # Assert that we got some results\n----&gt; 5 assert len(near_bay) &gt; 0, \"No districts found! Check your filter condition\"\n\nAssertionError: No districts found! Check your filter condition\n\n\n\nNow the code fails loudly, and the error message tells you exactly what went wrong. The problem is that the values in that column are capitalized, and you forgot to capitalize them. By using an assert you immediately catch the problem and can investigate it, before it becomes a bigger issue.\nChecking data after operations\nUse asserts after every major operation to verify it did what you expected:\n\n# Start with the full dataset\nprint(f\"Starting with {len(housing_df)} rows\")\n\n# Drop missing values\nhousing_clean = housing_df.dropna()\n\n# Assert that we actually dropped some rows (since we know there were missing values)\nassert len(housing_clean) &lt; len(housing_df), \"Expected to drop some rows with missing values\"\n\nprint(f\"After dropping missing values: {len(housing_clean)} rows\")\n\nStarting with 20640 rows\nAfter dropping missing values: 20433 rows\n\n\nChecking ranges and values\nAfter computing statistics, verify they make sense:\n\n# Calculate median house value\nmedian_value = housing_df['median_house_value'].median()\nprint(f\"Median house value: ${median_value:,.0f}\")\n\n# Sanity checks\nassert median_value &gt; 0, \"House values should be positive\"\nassert median_value &lt; 10000000, \"Median seems unrealistically high - check your data\"\n\nMedian house value: $179,700\n\n\nChecking for duplicates\nAfter removing duplicates, verify they’re actually gone:\n\n# Remove duplicates\nhousing_no_dupes = housing_df.drop_duplicates()\n\n# Verify no duplicates remain\nassert housing_no_dupes.duplicated().sum() == 0, \"Duplicates still exist after dropping\"\n\nprint(f\"Successfully removed duplicates. {len(housing_df) - len(housing_no_dupes)} rows removed\")\n\nSuccessfully removed duplicates. 0 rows removed\n\n\nA practical example: Complete data cleaning with asserts\nHere’s how you might use asserts throughout a real data cleaning workflow:\n\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('../data/housing.csv')\n\n# Assert basic structure\nassert len(df) &gt; 0, \"DataFrame is empty\"\nassert 'median_house_value' in df.columns, \"Missing expected column\"\n\nprint(f\"Starting with {len(df)} rows\")\n\n# Check for missing values before cleaning\nmissing_before = df.isnull().sum().sum()\nprint(f\"Missing values before cleaning: {missing_before}\")\n\n# Fill missing values\ndf_clean = df.copy()\nmedian_bedrooms = df_clean['total_bedrooms'].median()\ndf_clean['total_bedrooms'] = df_clean['total_bedrooms'].fillna(median_bedrooms)\n\n# Assert that we reduced missing values\nmissing_after = df_clean.isnull().sum().sum()\nassert missing_after &lt; missing_before, \"Should have fewer missing values after cleaning\"\n\nprint(f\"Missing values after cleaning: {missing_after}\")\n\n# Filter for expensive houses\nexpensive = df_clean[df_clean['median_house_value'] &gt; 400000]\n\n# Assert we got some results\nassert len(expensive) &gt; 0, \"No expensive houses found - check threshold\"\nassert len(expensive) &lt; len(df_clean), \"All houses are expensive? Something is wrong\"\n\nprint(f\"Found {len(expensive)} expensive districts\")\n\nStarting with 20640 rows\nMissing values before cleaning: 207\nMissing values after cleaning: 0\nFound 1744 expensive districts\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse assert statements liberally during development. They’re like guardrails that keep your analysis on track. Once your code is working correctly, you can leave them in—they serve as documentation of what your code expects to be true.\n\n\nWhat about “real” testing?\nAssert statements are great for quick checks during data exploration. In later chapters, we’ll learn about proper unit tests, which are essential when you’re building production data pipelines or reusable analysis code. But for now, assert statements will catch 90% of your mistakes with 10% of the effort.\nThe key principle: Don’t trust your code blindly. Check that it’s doing what you think it’s doing. Your future self (and your team) will thank you.\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nUnderstand what assert statements do\nWrite assert statements to check conditions\nWrite appropriate responses that will be displayed if the assert fails\nUnderstand when assert statements are and are not appropriate to use"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-6",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-6",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "6. Scaling EDA with AI",
    "text": "6. Scaling EDA with AI\nYou now know how to do EDA by hand: load data, clean it, compute statistics, make visualizations, test your assumptions. This is crucial foundational knowledge. But here’s the thing: what if you have 50 columns instead of 10? What if you want to compare patterns across 20 different categories? What if you want to check 100 different hypotheses?\nThis is where AI coding assistants shine. Once you understand what you’re doing and why, you can use AI to scale your work massively. The key is that you already know what to look for, so you can verify the AI’s output and catch mistakes.\n\n6.1 Automating Exploratory Visualizations\nLet’s say you want to create histograms for every numeric column in your dataset. Doing this by hand would be tedious:\n# By hand: repetitive and error-prone\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value')\nplt.title('Distribution of Median House Value')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_income')\nplt.title('Distribution of Median Income')\nplt.show()\n\n# ... repeat for every column ...\nInstead, you can use AI to generate this systematically. Here’s an effective prompt:\n\n“I have a pandas DataFrame called housing_df with these numeric columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value.\nWrite Python code using Pandas and Seaborn that creates a histogram for each numeric column. Use seaborn’s histplot, make each figure 10x6 inches, add appropriate titles (formatted nicely with spaces instead of underscores), and save each plot as a PNG file named after the column.\nUse a loop to avoid repetitive code.”\n\nThe AI will generate something like this:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# List of numeric columns\nnumeric_cols = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n                'total_bedrooms', 'population', 'households', 'median_income',\n                'median_house_value']\n\nfor col in numeric_cols:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=housing_df, x=col, bins=30)\n\n    # Format title nicely\n    title = col.replace('_', ' ').title()\n    plt.title(f'Distribution of {title}')\n    plt.xlabel(title)\n    plt.ylabel('Count')\n\n    # Save the plot\n    plt.savefig(f'{col}_histogram.png', dpi=300, bbox_inches='tight')\n    plt.close()  # Close to avoid memory issues\n\nprint(f\"Created {len(numeric_cols)} histograms\")\n\n\n\n\n\n\nNote\n\n\n\nNotice how the prompt was specific about what you wanted. You told the AI the column names, the figure size, the formatting requirements, and the output format. Good prompts get good results.\n\n\nComparing distributions across groups\nWant to see how house values differ across all ocean proximity categories? That’s a perfect job for AI:\n\n“Using my housing_df DataFrame with columns ‘ocean_proximity’ and ‘median_house_value’, create box plots comparing house values across all ocean proximity categories. Make the plot readable with rotated x-axis labels. Also calculate summary statistics (mean, median, count) for each category and save them all in a dataframe.”\n\nCreating a comprehensive visualization suite\nFor a complete EDA report, you might prompt:\n\n“Create a comprehensive visualization suite for the California housing dataset. For each numeric variable, create: 1. A histogram showing the distribution 2. A box plot showing the distribution by ocean_proximity 3. A scatter plot against median_house_value (the target variable)\nOrganize these into subplots so each variable gets a row with 3 plots. Save the output as a single multi-page PDF.”\n\nThe AI will generate code with subplots, proper layout management, and file handling—saving you hours of work.\n\n\n\n\n\n\nNote\n\n\n\nNotice how, in all of these cases, you told the AI precisely what to do. In addition, you used the things you have learned this chapter to prompt it precisely. We do not want to give AI overly vague instructions like “investigate my data” or “find patterns”. It’s our job as the data scientist to guide the AI to help us, not to turn over the entire thought process to it.\n\n\n\n\n6.2 Testing Across Data Subsets\nImagine you want to analyze how the relationship between income and house values differs across different regions. By hand, you’d need to:\n\nFilter for each ocean proximity category\nCompute correlations for each\nCreate scatter plots for each\nCompare the results\n\nThat’s tedious. Instead, prompt the AI:\n\n“For the housing_df DataFrame, I want to analyze how the relationship between ‘median_income’ and ‘median_house_value’ varies by ‘ocean_proximity’.\nFor each ocean proximity category:\n\nCalculate the correlation coefficient\nCreate a scatter plot with a regression line\nPrint summary statistics\n\nStore the summary statistics and correlation coefficients in a dataframe. Save the scatter plots with a filename that includes the ocean proximity category.”\n\n\n\n\n\n\n\nNote\n\n\n\nOnce again, we give the AI precise instructions. Think of it as an assistant who is eager to help, but needs careful direction at every step.\n\n\nThe AI will generate code that loops through each category, performs the analysis, and presents it clearly. You then review the results with your understanding of the data, catching any issues.\nSystematic subset analysis\nYou can scale this to any grouping:\n# AI-generated code for systematic analysis\nfor proximity in housing_df['ocean_proximity'].unique():\n    subset = housing_df[housing_df['ocean_proximity'] == proximity]\n\n    print(f\"\\n=== Analysis for {proximity} ===\")\n    print(f\"Number of districts: {len(subset)}\")\n    print(f\"Median house value: ${subset['median_house_value'].median():,.0f}\")\n    print(f\"Median income: ${subset['median_income'].median():.2f}\")\n\n    # Correlation\n    corr = subset['median_income'].corr(subset['median_house_value'])\n    print(f\"Income-Value correlation: {corr:.3f}\")\n\n    # Visualization\n    plt.figure(figsize=(8, 5))\n    sns.regplot(data=subset, x='median_income', y='median_house_value',\n                scatter_kws={'alpha':0.5})\n    plt.title(f'Income vs Value: {proximity}')\n    plt.show()\nThe point isn’t that you couldn’t write this code—you absolutely can now! The point is that AI lets you analyze 10 subsets as easily as 1 subset. You’re scaling your analysis, not replacing your understanding.\n\n\n\n\n\n\nWarning\n\n\n\nAI is excellent at generating repetitive analysis code, but it cannot make judgment calls about your data. You need to understand what the analyses mean, whether the results make sense, and what actions to take. AI scales your work; it doesn’t replace your thinking.\n\n\nThe key pattern: Do it by hand first, scale with AI second. Learn to check one column carefully, then use AI to check all 50 columns. Learn to analyze one subset, then scale to all subsets. Your understanding guides the AI; the AI amplifies your productivity.\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nWrite effective prompts for doing extensive EDA"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-7",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-7",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "7. The Iterative Nature of EDA",
    "text": "7. The Iterative Nature of EDA\n\n7.1 EDA as a Process, Not a Checklist\nHere’s a mistake beginners make: they treat EDA like a todo list. Load data ✓, check missing values ✓, make histogram ✓, done!\nReal EDA doesn’t work like that. EDA is a conversation with your data. You ask a question, look at the answer, and that answer suggests new questions. You follow the thread wherever it leads.\nLet’s see this in action with our housing data:\nStarting question: “What affects house values?”\n\n# Initial exploration: which variables correlate with house value?\nnumeric_cols = ['housing_median_age', 'total_rooms', 'total_bedrooms',\n                'population', 'households', 'median_income']\n\nfor col in numeric_cols:\n    corr = housing_df[col].corr(housing_df['median_house_value'])\n    print(f\"{col:20s}: {corr:6.3f}\")\n\nhousing_median_age  :  0.106\ntotal_rooms         :  0.134\ntotal_bedrooms      :  0.050\npopulation          : -0.025\nhouseholds          :  0.066\nmedian_income       :  0.688\n\n\nInsight: Median income has the strongest correlation (0.688). That makes sense—wealthier areas have more expensive houses.\nNew question: “Does this relationship hold across all regions?”\n\n# Check if income-value relationship varies by ocean proximity\nfor proximity in housing_df['ocean_proximity'].unique():\n    subset = housing_df[housing_df['ocean_proximity'] == proximity]\n    corr = subset['median_income'].corr(subset['median_house_value'])\n    print(f\"{proximity:15s}: {corr:.3f} ({len(subset):5d} districts)\")\n\nNEAR BAY       : 0.633 ( 2290 districts)\n&lt;1H OCEAN      : 0.679 ( 9136 districts)\nINLAND         : 0.699 ( 6551 districts)\nNEAR OCEAN     : 0.704 ( 2658 districts)\nISLAND         : -0.540 (    5 districts)\n\n\nInsight: The correlation is weakest for islands (only 5 districts though—small sample!). Strongest for inland areas.\nNew question: “Why is the island correlation so different? Let’s look at those islands.”\n\n# Investigate the island districts\nislands = housing_df[housing_df['ocean_proximity'] == 'ISLAND']\nprint(islands[['median_income', 'median_house_value', 'housing_median_age']].to_string())\n\n      median_income  median_house_value  housing_median_age\n8314         2.1579            450000.0                27.0\n8315         2.8333            414700.0                52.0\n8316         3.3906            300000.0                52.0\n8317         2.7361            450000.0                52.0\n8318         2.6042            287500.0                29.0\n\n\nInsight: Only 5 island districts total, with a narrow range of incomes. The low correlation might just be due to small sample size and limited variation.\nSee what happened? One question led to another, which led to another. We started with “what affects house values?” and ended up investigating the peculiarities of island properties. This is normal. This is good.\nEDA is not linear. You don’t march through a predetermined set of steps. You explore, find something interesting, investigate it, find something else interesting, investigate that. Some paths lead nowhere. Some paths lead to important discoveries. You won’t know until you follow them.\n\n\n\n\n\n\nNote\n\n\n\nThe best data scientists are curious. When they see something unusual in the data, they don’t just note it and move on—they dig deeper. Why is this unusual? What does it mean? What else should I check?\n\n\n\n\n7.2 Documenting Your Findings\nHere’s a problem: you do all this exploration, find interesting patterns, make decisions about the data, and then… a week later you can’t remember what you found or why you made those decisions.\nDocumentation isn’t just for other people. It’s for future you.\nWhat to document:\n\nInteresting patterns you discovered\n\n“House values show clear geographic clustering—coastal areas are consistently more expensive”\n“Total_bedrooms has 207 missing values (1%), appears random, not systematically related to other variables”\n\nDecisions you made and why\n\n“Filled missing bedrooms with median rather than dropping rows—only 1% missing, no pattern suggesting systematic missingness”\n“Kept all outliers in house values—verified these are real (e.g., expensive areas like San Francisco), not data errors”\n\nQuestions that need follow-up\n\n“Island category has only 5 districts—may need to group with NEAR_OCEAN for modeling”\n“Longitude/latitude show clear clustering—should we create region categories?”\n\nHypotheses for modeling\n\n“Median_income will likely be the strongest predictor”\n“May need to include interactions between location and other features”\n\n\nHow to document:\nThe simplest approach is to use markdown cells in your Jupyter notebook or Quarto document:\n# Bad: No context\nhousing_df = housing_df.dropna()\n\n# Better: Explain your reasoning\n# Dropping 207 rows with missing total_bedrooms (1% of data)\n# Missing values appear random (no correlation with other variables)\n# Median imputation would be valid alternative, but with 20k+ rows,\n# dropping 1% has minimal impact\nhousing_df = housing_df.dropna()\nEven better, use markdown cells to write full explanations:\n## Data Cleaning Decisions\n\n### Missing Values\n- `total_bedrooms`: 207 missing (1%)\n  - Dropped these rows rather than imputing\n  - Rationale: Small percentage, appears random, large dataset can afford the loss\n  - Alternative considered: median imputation (would change results minimally)\n\n### Outliers\n- Kept all outliers in `median_house_value`\n  - Verified these are real (checked against known expensive areas)\n  - Removing would bias model toward typical houses\n  - Will monitor model performance on these points\nWhy this matters:\nThree months from now, someone (maybe you!) will ask “why did we drop those missing values instead of imputing them?” If you documented your reasoning, you can answer immediately. If you didn’t, you’ll have to re-do the analysis to remember.\n\n\n\n\n\n\nTip\n\n\n\nDocument as you go, not at the end. When you make a decision, write down why while it’s fresh in your mind. Your future self will thank you.\n\n\n\n\nLearning outcomes:\nBy hand you should be able to:\n\nDescribe how EDA is a process, as opposed to a checklist\nGive examples of how to go about EDA\nGive examples of things that should be done by hand, and things that should be done with the help of an AI\nWrite assert statements that check if AI generated code is properly performing EDA"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-summary",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-summary",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Summary",
    "text": "Summary\nYou’ve now learned the foundations of exploratory data analysis. Let’s recap the key points:\nThe Fundamentals:\n\nData manipulation with Pandas: loading, selecting, filtering, and cleaning data\nStatistical summaries: understanding distributions, detecting outliers, and computing meaningful statistics\nVisualization: choosing the right plots, creating them with Seaborn, and following best practices for readability\nTesting: using assert statements to catch errors early and verify your assumptions\n\nWorking with AI:\n\nWrite specific, detailed prompts that include context and requirements\nStart simple and iterate—don’t try to get everything perfect in one prompt\nUse AI to scale your work after you understand the fundamentals by hand\nAlways verify AI-generated code and results with your own understanding\n\nThe EDA Mindset:\n\nEDA is iterative, not linear—follow insights wherever they lead\nDocument your findings and decisions as you go\nLet EDA guide your modeling choices—every decision should be informed by what you learned about the data\nStay curious and ask “why?” when you see something interesting\n\nThe Core Philosophy:\nEDA is foundational to everything else in data science. You can’t build good models without understanding your data. AI is a powerful tool for scaling your work, but it’s not a replacement for understanding. Learn to do things by hand first, then use AI to do them at scale.\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-practice",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-practice",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nComplete these exercises using the California housing dataset to reinforce what you’ve learned:\n1. Prompt Engineering Practice\nWrite three prompts for an AI coding assistant to perform the following tasks with the housing dataset. Make your prompts specific and detailed:\n\nCreate a comprehensive correlation analysis between all numeric features and median_house_value, sorted by correlation strength, with a visualization\nAnalyze how housing age affects house values across different ocean proximity categories, including statistical tests and visualizations\nGenerate a data quality report that identifies potential issues (outliers, missing values, suspicious patterns) and suggests remediation strategies\n\n2. Visualization Design\nFor each scenario below, choose the appropriate visualization type and explain why:\n\nYou want to show how the distribution of house ages differs between coastal and inland areas\nYou want to investigate whether there’s a relationship between population density (population/households) and median income\nYou want to compare the median house values across all five ocean proximity categories\nYou want to show how house values have changed across different housing ages, looking for trends\n\n3. Data Quality Investigation\nUsing assert statements and pandas methods:\n\nWrite code to verify that after cleaning, no column has more than 5% missing values\nCreate assertions to check that all house values are positive and less than $10 million\nWrite a test to ensure that total_bedrooms is always less than or equal to total_rooms\nVerify that each ocean_proximity category has at least 100 samples\n\n4. Complete EDA Workflow\nPerform a complete exploratory data analysis on a subset of the housing data:\n\nFilter for districts with median_income between 3 and 6\nDocument three interesting patterns you discover in this subset\nCreate at least three visualizations that reveal insights\nWrite down two hypotheses about what would make a good predictive model for this subset\nUse assert statements to verify your filtering worked correctly\n\n5. Scaling with AI\nWithout actually running the code, write detailed prompts that would generate:\n\nCode to create scatter plots of median_house_value vs. each numeric feature, all on a single figure with subplots\nA function that calculates and reports summary statistics (mean, median, std, min, max) for any numeric column, grouped by any categorical column\nCode that identifies all pairs of numeric features with correlation &gt; 0.7 and creates scatter plots for each pair"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-additional-resources",
    "href": "Textbook/Chapter 1 - EDA/chapter-1-eda.html#ch1-additional-resources",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Additional Resources",
    "text": "Additional Resources\nPandas Documentation:\n\nPandas User Guide - Comprehensive guide to Pandas functionality\n10 Minutes to Pandas - Quick-start tutorial\nPandas Cookbook - Common recipes for data manipulation\n\nVisualization:\n\nSeaborn Gallery - Examples of every Seaborn plot type\nMatplotlib Tutorials - Deep dive into matplotlib\nFrom Data to Viz - Guide to choosing the right visualization\n\nData Quality and Cleaning:\n\nData Cleaning with Python - Comprehensive guide\nPandas Missing Data - Official guide to handling missing values\n\nWorking with AI:\n\nEffective Prompting Guide - Principles for writing good prompts\nGemini for Developers - Google’s AI documentation for developers\n\nStatistics and EDA:\n\nThink Stats (Free Book) - Statistical thinking for programmers\nStatistics for Hackers - Practical statistical approaches\n\nPractice Datasets:\n\nUCI Machine Learning Repository - Hundreds of datasets for practice\nKaggle Datasets - Real-world datasets with community analyses\nData.gov - US government public datasets"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/images/placeholder-info.html#required-images",
    "href": "Textbook/Chapter 1 - EDA/images/placeholder-info.html#required-images",
    "title": "Image Placeholders for Module 1 - EDA",
    "section": "Required Images",
    "text": "Required Images\n\n1. ai-workflow-placeholder.png\nDescription: Diagram showing the workflow of using AI coding assistants Suggested content: - Box 1: “Understand the problem/task” - Box 2: “Write specific prompt” - Box 3: “AI generates code” - Box 4: “Review and verify output” - Box 5: “Test and iterate” - Arrows connecting the boxes\nDimensions: ~800x400px\n\n\n\n2. visualization-examples-placeholder.png\nDescription: Grid of example visualizations showing good vs. poor practices Suggested content: - 2x2 grid showing: - Top left: Poorly labeled histogram - Top right: Well-labeled histogram with title, axis labels - Bottom left: Scatter plot with unclear colors - Bottom right: Scatter plot with clear legend and labels\nDimensions: ~1000x800px"
  },
  {
    "objectID": "Textbook/Chapter 1 - EDA/images/placeholder-info.html#notes",
    "href": "Textbook/Chapter 1 - EDA/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Module 1 - EDA",
    "section": "Notes",
    "text": "Notes\n\nImages should be placed in this /images/ folder\nUse .png format for diagrams and screenshots\nUse descriptive filenames\nEnsure images are readable when embedded in Quarto HTML output"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#confusion-matrix-diagram.png",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#confusion-matrix-diagram.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "1. confusion-matrix-diagram.png",
    "text": "1. confusion-matrix-diagram.png\nPurpose: A clean, annotated diagram explaining the structure of a confusion matrix\nDescription: - 2x2 grid showing Predicted (columns) vs Actual (rows) - Clearly labeled quadrants: - Top-left: True Negative (TN) - predicted negative, actually negative - Top-right: False Positive (FP) - predicted positive, actually negative (Type I error) - Bottom-left: False Negative (FN) - predicted negative, actually positive (Type II error) - Bottom-right: True Positive (TP) - predicted positive, actually positive - Color coding: greens for correct predictions (TN, TP), reds for errors (FP, FN) - Arrows showing how metrics are calculated: - Accuracy = (TP + TN) / Total - Precision = TP / (TP + FP) - Recall = TP / (TP + FN)\nStyle: Clean, professional diagram with clear labels. Use icons or simple graphics to make it visually appealing."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#decision-boundary-comparison.png",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#decision-boundary-comparison.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "2. decision-boundary-comparison.png",
    "text": "2. decision-boundary-comparison.png\nPurpose: Side-by-side comparison of decision boundaries for different classifiers\nDescription: - 2x3 grid of scatter plots (or 3x2) - Each subplot shows the same 2D classification dataset (e.g., two classes in different colors) - Six subplots showing decision boundaries for: 1. Logistic Regression (linear boundary) 2. Decision Tree depth=3 (rectangular/axis-aligned splits) 3. Decision Tree depth=10 (more complex rectangular regions) 4. Random Forest (smooth but irregular boundary) 5. SVM with linear kernel (straight line) 6. SVM with RBF kernel (curved boundary) 7. k-NN with k=1 (very jagged, local boundary) 8. k-NN with k=20 (smoother boundary) - Each subplot clearly labeled with the model type - Decision boundary shown as a bold line or colored region - Data points shown as scattered dots with class colors\nStyle: Consistent color scheme across all subplots. Make it easy to compare the shapes of different boundaries."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#roc-curve-interpretation.png",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#roc-curve-interpretation.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "3. roc-curve-interpretation.png",
    "text": "3. roc-curve-interpretation.png\nPurpose: Annotated ROC curve explaining how to interpret it\nDescription: - Classic ROC curve plot (True Positive Rate vs False Positive Rate) - Shows three example curves: 1. Perfect classifier (hugs the top-left corner, AUC = 1.0) 2. Good classifier (bow-shaped curve above diagonal, AUC ≈ 0.85) 3. Random classifier (diagonal line, AUC = 0.5) - Annotations pointing out: - “Best possible performance” at top-left corner (TPR=1, FPR=0) - “Random guessing” along the diagonal - Shaded area under curve with “AUC = Area Under Curve” - Arrow showing “Better models” direction (toward top-left) - Optionally show a few threshold values along the good classifier curve\nStyle: Clear, educational diagram. Use different line styles (solid, dashed, dotted) for the three curves."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#precision-recall-tradeoff.png",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#precision-recall-tradeoff.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "4. precision-recall-tradeoff.png",
    "text": "4. precision-recall-tradeoff.png\nPurpose: Visualize the precision-recall tradeoff with threshold adjustment\nDescription: - Two panels side-by-side or stacked: - Left/Top: Precision-Recall curve showing how precision and recall change as threshold varies - Right/Bottom: Line plot showing precision and recall as separate lines vs threshold value - Annotations showing: - “High threshold → High precision, Low recall” - “Low threshold → Low precision, High recall” - The optimal F1 score point - Visual example showing why the tradeoff exists (e.g., small threshold = predict positive more often = catch more positives but more false positives too)\nStyle: Clear, intuitive visualization. Use contrasting colors for precision vs recall lines."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#class-imbalance-problem.png",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#class-imbalance-problem.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "5. class-imbalance-problem.png",
    "text": "5. class-imbalance-problem.png\nPurpose: Illustrate why class imbalance is problematic\nDescription: - Side-by-side comparison of balanced vs imbalanced datasets - Left panel: Balanced dataset (50/50 split) - Pie chart or bar chart showing class distribution - Example confusion matrix from a reasonable classifier - Metrics: Accuracy = 85%, Precision = 83%, Recall = 87% - Right panel: Imbalanced dataset (95/5 split) - Pie chart or bar chart showing class distribution - Example confusion matrix from a naive classifier (predicts majority class) - Metrics: Accuracy = 95% (misleading!), Precision = 0%, Recall = 0% - Visual highlighting showing “95% accuracy but useless for minority class!”\nStyle: Use visual contrast to show the problem. Make it clear that high accuracy doesn’t mean good performance on imbalanced data."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#ensemble-method-illustration.png",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#ensemble-method-illustration.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "6. ensemble-method-illustration.png",
    "text": "6. ensemble-method-illustration.png\nPurpose: Conceptual diagram showing how Random Forests combine multiple decision trees\nDescription: - Show the Random Forest process: 1. Original dataset (shown as a grid or table icon) 2. Arrows pointing to multiple bootstrap samples (3-5 copies with slight variations) 3. Each sample trains a decision tree (show simplified tree diagrams) 4. Trees make individual predictions (show thumbs up/down or class labels) 5. Final prediction combines via voting (show majority vote mechanism) - Annotations explaining: - “Bootstrap sampling (with replacement)” - “Random feature subset at each split” - “Majority vote for final prediction” - Visual emphasis on diversity: show that trees are different from each other\nStyle: Flow diagram with clear arrows showing the process. Use icons and simple graphics to make it intuitive."
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#optional-additional-images",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#optional-additional-images",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "Optional Additional Images",
    "text": "Optional Additional Images\n\n7. svm-margin-concept.png\n\nDiagram showing support vectors and the maximum margin\nTwo parallel lines showing the margin\nSupport vectors highlighted\nDecision boundary in the middle\n\n\n\n8. knn-visualization.png\n\nScatter plot showing a new point to classify\nCircles of different radii showing k=1, k=3, k=5 neighbors\nVisual showing how the class is determined by majority vote\n\n\n\n9. sigmoid-function-detailed.png\n\nPlot of sigmoid function\nAnnotations showing asymptotic behavior (approaches 0 and 1)\nConnection to logistic regression (linear combination → sigmoid → probability)"
  },
  {
    "objectID": "Textbook/Chapter-3-Classification/images/placeholder-info.html#notes",
    "href": "Textbook/Chapter-3-Classification/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "Notes",
    "text": "Notes\n\nAll images should use a consistent color palette throughout the chapter\nUse colorblind-friendly colors (avoid red-green only distinctions)\nInclude clear labels and legends on all plots\nKeep diagrams simple and focused on one concept at a time\nUse professional fonts and consistent styling\nSave images as PNG with transparent backgrounds where appropriate\nRecommended size: 1200-2000 pixels wide for main diagrams"
  },
  {
    "objectID": "Textbook/table-of-contents.html",
    "href": "Textbook/table-of-contents.html",
    "title": "Textbook Table of Contents",
    "section": "",
    "text": "Chapter 1: Exploratory Data Analysis\nChapter 2: Regression\nChapter 3: Classification\nChapter 4: LLMs & Feature Engineering"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/images/placeholder-info.html#required-images",
    "href": "Textbook/Chapter-2-Regression/images/placeholder-info.html#required-images",
    "title": "Image Placeholders for Module 4 - Regression Models",
    "section": "Required Images",
    "text": "Required Images\n\n1. residual-patterns-placeholder.png\nDescription: Common patterns in residual plots and what they indicate Suggested content: - 2x3 grid showing six residual vs. fitted plots: - Top Left - Good: Random scatter around zero (no pattern) - Title: “Assumptions Met” - Top Middle - Non-linearity: Curved pattern (U-shaped or inverted-U) - Title: “Non-linear Relationship” - Top Right - Heteroscedasticity: Funnel shape (variance increases) - Title: “Non-constant Variance” - Bottom Left - Outliers: Most points near zero, few far away - Title: “Outliers Present” - Bottom Middle - Clusters: Distinct groups of points - Title: “Missing Categorical Variable” - Bottom Right - Systematic Pattern: Clear trend (upward or downward) - Title: “Missing Variable” - Each plot: X-axis = “Fitted Values”, Y-axis = “Residuals” - Horizontal line at y=0 on each plot - Red points for emphasis on problem areas\nDimensions: ~1200x800px\n\n\n\n2. multicollinearity-example-placeholder.png\nDescription: Visualization showing multicollinearity and its effects Suggested content: - Two side-by-side visualizations: - Left Panel - Correlation Heatmap: - 5x5 correlation matrix - Color-coded (dark blue = -1, white = 0, dark red = +1) - Highlight one pair with correlation ~0.95 - Labels: “Feature 1”, “Feature 2”, “Feature 3”, “Feature 4”, “Feature 5” - Right Panel - Coefficient Instability: - Bar plot showing coefficients for the same model fit on slightly different data - Two sets of bars (Data Sample 1 vs Data Sample 2) - Large differences between the two samples for highly correlated features - Stable coefficients for uncorrelated features - Title: “Unstable Coefficients with Multicollinearity”\nDimensions: ~1000x500px\n\n\n\n3. regression-workflow-placeholder.png\nDescription: Flowchart showing complete regression analysis workflow Suggested content: - Flowchart with boxes and arrows: 1. Start: “Fit Linear Regression” 2. Check: “Create Diagnostic Plots” (residuals, Q-Q, etc.) 3. Decision Diamond: “Assumptions Met?” - Yes arrow → “Validate Performance” → “Report Results” - No arrow → “Identify Problem” 4. Problem boxes: - “Non-linear?” → “Add Polynomial Features” - “Multicollinearity?” → “Use Ridge/Lasso” - “Outliers?” → “Investigate/Transform” 5. All problem solutions loop back to “Fit Model” (step 1) - Use different colors for different stages (blue=fit, orange=diagnose, green=fix) - Arrows showing the iterative nature\nDimensions: ~800x1000px\n\n\n\n4. regularization-comparison-placeholder.png (Optional)\nDescription: Visual comparison of Ridge, Lasso, and Linear Regression coefficients Suggested content: - Three bar plots side-by-side: - Left: Linear Regression coefficients (some very large positive/negative) - Middle: Ridge coefficients (all shrunk toward zero, none exactly zero) - Right: Lasso coefficients (some shrunk to exactly zero) - Same features on x-axis for all three - Y-axis: coefficient value - Different colors for each bar - Clear labels showing which features Lasso set to zero - Title above each: “Linear Regression”, “Ridge (α=1)”, “Lasso (α=1)”\nDimensions: ~1200x400px\n\n\n\n5. coefficient-path-placeholder.png (Optional)\nDescription: Regularization path showing how coefficients change with alpha Suggested content: - Line plot with: - X-axis: log(alpha) from small (0.001) to large (1000) - Y-axis: coefficient value - Multiple lines, one for each feature (different colors) - Lines shrinking toward zero as alpha increases (Ridge) or dropping to exactly zero at different alphas (Lasso) - Legend identifying each feature - Vertical dashed line showing optimal alpha selected by cross-validation - Title: “Lasso Regularization Path”\nDimensions: ~800x600px"
  },
  {
    "objectID": "Textbook/Chapter-2-Regression/images/placeholder-info.html#notes",
    "href": "Textbook/Chapter-2-Regression/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Module 4 - Regression Models",
    "section": "Notes",
    "text": "Notes\n\nImages should be placed in this /images/ folder\nUse .png format for diagrams and screenshots\nUse descriptive filenames\nEnsure images are readable when embedded in Quarto HTML output\nFor diagnostic plots, use realistic-looking synthetic data\nColor code consistently (e.g., red for problems, green for good)\nInclude clear labels and titles on all plots"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#chapter-resources",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#chapter-resources",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Chapter Resources",
    "text": "Chapter Resources\nRelated Assignments:\n\nChapter 4 Homework"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#introduction",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#introduction",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Introduction",
    "text": "Introduction\nYou’ve spent weeks learning how to build machine learning models—linear regression, logistic regression, random forests, SVMs. You know how to tune hyperparameters, evaluate performance, and diagnose problems. But here’s the thing: all those models need one critical ingredient before they can work their magic.\nFeatures.\nAnd not just any features—numeric or categorical features that capture the information hidden in your data. If you have structured data (age, income, house size), you’re set. But what about unstructured data? What about customer reviews, support tickets, job descriptions, social media posts, news articles? Most real-world data lives in text, and traditional machine learning models can’t directly consume text.\nThis is where Large Language Models (LLMs) come in—not as the end goal, but as powerful feature engineering tools. Think of LLMs as intelligent extractors that can read text, understand context, and pull out structured information. Need to know if a review is positive or negative? LLM. Want to extract job requirements from a posting? LLM. Need to categorize thousands of support tickets? LLM.\nThe beautiful part? You don’t need to train these models. You don’t need GPUs. You don’t even need to understand how they work internally. You just call an API, send some text with instructions, and get back structured data ready for your ML pipeline.\nBut LLMs aren’t free, and they aren’t perfect. Every API call costs money. Extraction quality varies. Some tasks work beautifully with simpler versions of LLMs, such as GPT-4o, Gemini Flash, Claude Haiku, while others need more powerful models like GPT-5, Gemini Pro, or Claude Sonnet. Sometimes a simple regex pattern works better than an expensive LLM call. The skill isn’t just using LLMs—it’s knowing when to use them, which one to use, and how to validate the results.\nThis chapter teaches you to use LLMs as practical data science tools. You’ll learn to write prompts that extract information reliably, parse and validate LLM responses, calculate costs, integrate extracted features into ML pipelines, and most importantly—judge when LLMs add value versus when simpler approaches suffice.\nLet’s jump in."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#the-feature-engineering-challenge-from-text-to-numbers",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#the-feature-engineering-challenge-from-text-to-numbers",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "1. The Feature Engineering Challenge: From Text to Numbers",
    "text": "1. The Feature Engineering Challenge: From Text to Numbers\n\n1.1 Why Traditional ML Needs Structured Features\nRemember our housing price predictor from earlier chapters? The input was clean: square footage (numeric), number of bedrooms (numeric), neighborhood (categorical). Easy. Train-test split, fit the model, done.\nBut look at this product review:\n\n“This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it’s a bit loud, but I can live with that for this price.”\n\nWhat features can we extract? What information is hidden here that might help predict if other customers will find this review helpful?\n\nimport pandas as pd\nimport numpy as np\n\n# Example: Product review data\nreview_text = \"This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud, but I can live with that for this price.\"\n\n# Traditional ML needs numbers or categories\n# We could count words, but that misses the meaning\nword_count = len(review_text.split())\nprint(f\"Word count: {word_count}\")\n\n# We could look for specific keywords\nhas_positive_words = any(word in review_text.lower() for word in ['amazing', 'great', 'love'])\nhas_negative_words = any(word in review_text.lower() for word in ['bad', 'terrible', 'hate'])\nprint(f\"Has positive words: {has_positive_words}\")\nprint(f\"Has negative words: {has_negative_words}\")\n\n# But we're missing so much: \n# - sentiment nuance\n# - specific features mentioned\n# - overall tone\n# - ...\n\nWord count: 28\nHas positive words: True\nHas negative words: False\n\n\nWord counts and keyword matching capture some information, but they miss the meaning. This review is mostly positive despite mentioning a downside. A keyword approach might rate it as mixed because it has both positive and negative words. But a human reading it understands: this person likes the product.\n\n\n1.2 What Information Is Hidden in Text?\nText contains structured information waiting to be extracted:\nSentiment/Opinion:\n\nOverall positive/negative/neutral\nStrength of sentiment (mildly positive vs. extremely positive)\nSentiment about specific aspects (loves taste, dislikes noise)\n\nCategories/Classification:\n\nProduct category (coffee maker, not coffee beans)\nIssue type in support tickets (billing vs. technical)\nJob seniority level (entry vs. senior)\n\nEntities and Attributes:\n\nBrands mentioned\nSpecific features discussed (speed, taste, noise)\nRequirements (in job postings: “5 years experience”, “Python required”)\n\nNumeric Values:\n\nImplicit ratings (“amazing” = 5 stars, “okay” = 3 stars)\nQuantities mentioned\nPrice ranges\n\n\n\n1.3 Traditional NLP Approaches\nBefore LLMs, we had a few options:\n1. Keyword/Regex Matching:\nThis approach attempts to search for specific keywords in the text. For example, look at the positive_words and negative_words in the code below.\n\nimport re\n\ndef simple_sentiment(text):\n    \"\"\"Basic sentiment using keyword matching\"\"\"\n    positive_words = ['amazing', 'great', 'excellent', 'love', 'perfect']\n    negative_words = ['bad', 'terrible', 'hate', 'awful', 'waste']\n\n    text_lower = text.lower()\n    pos_count = sum(1 for word in positive_words if word in text_lower)\n    neg_count = sum(1 for word in negative_words if word in text_lower)\n\n    if pos_count &gt; neg_count:\n        return \"positive\"\n    elif neg_count &gt; pos_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\nreview = \"This coffee maker is amazing! Brews quickly and the coffee tastes great.\"\nprint(f\"Simple sentiment: {simple_sentiment(review)}\")\n\nSimple sentiment: positive\n\n\nWhen to use this:\n\nWhen you have specific words you’re looking for, such as students mentioning a specific course\n\nWhen this fails:\n\nThis works for simple cases but breaks easily:\n\n“This product is not bad” → Incorrectly classified as negative\n“I expected amazing quality but got terrible service” → Confusing mix\nDoesn’t handle sarcasm, context, or nuance\n\n\n2. Bag-of-Words + ML: “Bag of words” is a simple way to represent text as a vector of word counts. For example, we might assign:\n\n“apple” = 0\n“banana” = 1\n“orange” = 2\n\nThese number represent the index of each word in our vector. So “apple” is at index 0, “banana” is at index 1, and “orange” is at index 2. Then we might count up how many times each word appears in the text. For example, the text “I have an apple and a banana” would be represented as [1, 1, 0] (“apple” apears once as indicated in index zero, and “banana” appears once, but “orange” never appears).\nThis gives us numeric values, which can then be used in any of the machine learning models you’ve learned so far this semester.\nProblems with this approach include:\n\nNeed a large labeled dataset, which is expensive to create\nNeed to retrain for new domains (i.e. the dataset you have may not cover the domain you’re interested in)\nSeparate model for each extraction task (i.e. you need a separate model for each feature you want to extract, such as sentiment, brand, price, etc.)\n\n3. Other approaches: Traditionally, people used many, many different approaches for feature extraction, such as TF-IDF, word embeddings, and more. These approaches were often ad-hoc and required a lot of expertise to implement.\nOnce LLMs arose, people quickly realized that we can simply ask the LLM to extract what we want. While not always perfect, this approach is highly flexible, tuneable, and easily implemented. Because of that, we’ll focus the majority of our efforts on this technique.\n\n\n1.4 The LLM Advantage: Zero-Shot Extraction\nLLMs offer something revolutionary: zero-shot learning, which means that you can extract information without training on your specific task, without labeled data, just by asking clearly.\nHere’s a preview (we’ll implement this properly soon):\n\nPrompt: “What is the sentiment of this review? Answer with just ‘positive’, ‘negative’, or ‘neutral’: This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it’s a bit loud.”\nLLM Response: “positive”\n\nNo training. No labeled data. Just clear instructions and the text. The LLM understands context, handles negation, grasps nuance. It knows “only downside” indicates a minor criticism in an otherwise positive review.\nThis is powerful. But it’s also expensive, sometimes inconsistent, and not always necessary. In addition, it’s clearly a black box, since we have no idea how the LLM decided this should be a positive review. Of course, we could continually tweak instructions to get outcomes closer to what we want. This is the art and science of working with LLMs."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#your-first-llm-extraction-sentiment-analysis",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#your-first-llm-extraction-sentiment-analysis",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "2. Your First LLM Extraction: Sentiment Analysis",
    "text": "2. Your First LLM Extraction: Sentiment Analysis\n\n2.1 Setting Up API Access\nTo use LLMs, you need API access. The main options:\n\nOpenAI (GPT-5): Most popular, good quality, moderate cost\nAnthropic (Claude): High quality, good for long texts, moderate cost\nGoogle (Gemini): Competitive quality, often cheaper\nOpen-source via Hugging Face: Free but requires more setup\n\nFor this chapter, we’ll use OpenAI’s API since it’s widely available. The concepts transfer to other providers.\n\n\n\n\n\n\nNote\n\n\n\nAbout Running These Examples: The code examples in this chapter that call the OpenAI API will require an API key to run. If you don’t have an API key set, the examples will be skipped during rendering. This is intentional - you can still learn from reading the code and understanding the patterns. When you’re ready to run these examples yourself:\n\nSign up for an OpenAI API account at platform.openai.com\nGenerate an API key\nSet it as an environment variable: export OPENAI_API_KEY=\"your-key-here\"\nBe mindful of costs - start with small tests before processing large datasets\n\n\n\n\n# First, install the OpenAI library if needed\n# pip install openai\n\nfrom openai import OpenAI\nimport os\n\n# Set up your API key\n# IMPORTANT: Never hard-code API keys! Use environment variables\n# Set this in your terminal: export OPENAI_API_KEY=\"your-key-here\"\n\n# Check if API key is set\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    print(\"⚠️  API key not found. Set OPENAI_API_KEY environment variable.\")\n    print(\"⚠️  API calls in this chapter will be skipped.\")\n    client = None\nelse:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    print(\"✓ API key is set\")\n\n⚠️  API key not found. Set OPENAI_API_KEY environment variable.\n⚠️  API calls in this chapter will be skipped.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNever commit API keys to GitHub! Use environment variables or config files that are in .gitignore. Exposed keys can lead to unexpected charges or account suspension.\n\n\n\n\n2.2 Making Your First Extraction Call\nLet’s extract sentiment from a product review:\n\ndef extract_sentiment_simple(review_text):\n    \"\"\"\n    Extract sentiment using GPT-3.5\n    Returns: 'positive', 'negative', or 'neutral'\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return \"neutral\"  # Default response\n\n    # Create the prompt\n    prompt = f\"\"\"What is the sentiment of this review?\nAnswer with just one word: 'positive', 'negative', or 'neutral'.\n\nReview: {review_text}\n\nSentiment:\"\"\"\n\n    # Call the API\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # Cheaper, faster model\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0,  # Deterministic output\n        max_tokens=10   # We only need one word\n    )\n\n    # Extract the response\n    sentiment = response.choices[0].message.content.strip().lower()\n    return sentiment\n\n# Test it (only if API key is available)\nif client is not None:\n    review1 = \"This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud.\"\n    review2 = \"Total waste of money. Broke after two uses. Terrible product.\"\n    review3 = \"It's okay. Does the job but nothing special.\"\n\n    print(f\"Review 1 sentiment: {extract_sentiment_simple(review1)}\")\n    print(f\"Review 2 sentiment: {extract_sentiment_simple(review2)}\")\n    print(f\"Review 3 sentiment: {extract_sentiment_simple(review3)}\")\nelse:\n    print(\"Skipping examples - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Review 1 sentiment: positive\")\n    print(\"Review 2 sentiment: negative\")\n    print(\"Review 3 sentiment: neutral\")\n\nSkipping examples - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nReview 1 sentiment: positive\nReview 2 sentiment: negative\nReview 3 sentiment: neutral\n\n\nWe sent a clear instruction, included the text, and got back exactly what we asked for. No training, no labeled data, no complex preprocessing. Notice how the LLM correctly identifies Review 1 as positive despite it mentioning a downside—it understands that “only downside” indicates a minor complaint in an otherwise positive review.\n\n\n2.3 Understanding the API Call\nLet’s break down the important parameters:\nmodel: Which LLM to use\n\ngpt-3.5-turbo: Cheaper ($0.0005 per 1K tokens), faster, good for most tasks\ngpt-4: More expensive ($0.03 per 1K tokens), smarter, better for complex extraction\nStart with 3.5, upgrade to 4 if quality isn’t sufficient\n\ntemperature: Controls randomness (0 to 2)\n\n0: Deterministic, same input → same output\n0.7-1.0: More creative/varied (for generation tasks)\nFor extraction: use 0 for consistency\n\nmax_tokens: Maximum length of response\n\nTokens ≈ words × 1.3 (rough estimate)\nFor simple extraction: 10-50 tokens\nFor detailed extraction: 100-500 tokens\nMore tokens = higher cost\n\n\n# Let's see token usage and cost\nif client is not None:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n        temperature=0\n    )\n\n    usage = response.usage\n    print(f\"Prompt tokens: {usage.prompt_tokens}\")\n    print(f\"Completion tokens: {usage.completion_tokens}\")\n    print(f\"Total tokens: {usage.total_tokens}\")\n\n    # Calculate cost (GPT-3.5-turbo pricing as of 2024)\n    cost_per_1k_tokens = 0.0005\n    cost = (usage.total_tokens / 1000) * cost_per_1k_tokens\n    print(f\"Cost for this call: ${cost:.6f}\")\nelse:\n    print(\"Skipping - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Prompt tokens: 14\")\n    print(\"Completion tokens: 1\")\n    print(\"Total tokens: 15\")\n    print(\"Cost for this call: $0.000008\")\n\nSkipping - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nPrompt tokens: 14\nCompletion tokens: 1\nTotal tokens: 15\nCost for this call: $0.000008\n\n\nFor our simple sentiment extraction, we’re using about 50-100 tokens per review. At $0.0005 per 1K tokens, that’s $0.00005 per review. To process 10,000 reviews: $0.50. Cheap!\n\n\n2.4 Handling Errors and Edge Cases\nLLMs don’t always follow instructions perfectly. Let’s make our extraction more robust:\n\ndef extract_sentiment_robust(review_text):\n    \"\"\"\n    Robust sentiment extraction with error handling\n    \"\"\"\n    try:\n        prompt = f\"\"\"What is the sentiment of this review?\nAnswer with ONLY one of these words: positive, negative, neutral\n\nReview: {review_text}\n\nSentiment:\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=10\n        )\n\n        sentiment = response.choices[0].message.content.strip().lower()\n\n        # Validate the response\n        valid_sentiments = ['positive', 'negative', 'neutral']\n        if sentiment in valid_sentiments:\n            return sentiment\n        else:\n            # Try to extract valid sentiment from response\n            for valid in valid_sentiments:\n                if valid in sentiment:\n                    return valid\n            # If we still can't find it, return None\n            return None\n\n    except Exception as e:\n        print(f\"Error during extraction: {e}\")\n        return None\n\n# Test with various inputs\nreviews = [\n    \"Love it!\",\n    \"Terrible product.\",\n    \"It's fine.\",\n    \"\"  # Empty review - will this break?\n]\n\nif client is not None:\n    for review in reviews:\n        result = extract_sentiment_robust(review)\n        print(f\"'{review}' → {result}\")\nelse:\n    print(\"Skipping examples - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"'Love it!' → positive\")\n    print(\"'Terrible product.' → negative\")\n    print(\"'It's fine.' → neutral\")\n    print(\"'' → None\")\n\nSkipping examples - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\n'Love it!' → positive\n'Terrible product.' → negative\n'It's fine.' → neutral\n'' → None\n\n\nKey improvements:\n\nTry-except block catches API errors\nValidation checks if response matches expected values\nFallback logic tries to extract valid sentiment if response is wordy\nReturns None if extraction fails (rather than crashing)\n\nNotice how the function handles the empty review gracefully by returning None instead of crashing."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#prompt-engineering-for-reliable-extraction",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#prompt-engineering-for-reliable-extraction",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "3. Prompt Engineering for Reliable Extraction",
    "text": "3. Prompt Engineering for Reliable Extraction\n\n3.1 The Anatomy of a Good Extraction Prompt\nA good prompt has four parts:\n\nClear task description: What do you want extracted?\nOutput format specification: Exactly how should the response look?\nThe input data: The text to analyze\nAny constraints or examples: Helps guide the LLM\n\nLet’s compare bad vs. good prompts:\n\n# BAD PROMPT - Vague, no format specified\nbad_prompt = \"\"\"\nTell me about this review:\n{review_text}\n\"\"\"\n\n# GOOD PROMPT - Clear, specific format\ngood_prompt = \"\"\"\nExtract the following information from this product review:\n- Sentiment: positive, negative, or neutral\n- Rating: estimated star rating from 1-5\n- Main complaint: brief description, or \"none\" if no complaints\n\nFormat your response as JSON:\n{{\"sentiment\": \"positive/negative/neutral\", \"rating\": 1-5, \"complaint\": \"text or none\"}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\nreview = \"Great coffee maker! Makes excellent coffee quickly. Wish it was quieter though. 4 stars.\"\n\n# The good prompt will give us structured, parseable output\n\nSee the difference? The good prompt: - Lists exactly what to extract - Specifies valid values for each field - Requests JSON format for easy parsing - Shows the structure we expect\n\n\n3.2 Requesting Structured Output (JSON)\nJSON is your best friend for extraction. It’s easy to parse and works with any number of fields:\n\nimport json\n\ndef extract_review_features(review_text):\n    \"\"\"\n    Extract multiple features from a review using JSON output\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return None\n\n    prompt = f\"\"\"Extract information from this product review.\n\nRespond with ONLY valid JSON in this exact format:\n{{\n    \"sentiment\": \"positive/negative/neutral\",\n    \"rating\": 1-5,\n    \"pros\": [\"list\", \"of\", \"positive\", \"aspects\"],\n    \"cons\": [\"list\", \"of\", \"negative\", \"aspects\"]\n}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=200\n    )\n\n    # Parse the JSON response\n    try:\n        result_text = response.choices[0].message.content.strip()\n        # Sometimes LLMs add markdown formatting, remove it\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n        result = json.loads(result_text)\n        return result\n    except json.JSONDecodeError as e:\n        print(f\"Failed to parse JSON: {e}\")\n        print(f\"Raw response: {result_text}\")\n        return None\n\n# Test it (only if API key is available)\nif client is not None:\n    review = \"\"\"This coffee maker is fantastic! The coffee tastes amazing and\n    it's very fast. Design is sleek. Only complaint is it's somewhat loud\n    during brewing, but that's minor. Highly recommend!\"\"\"\n\n    features = extract_review_features(review)\n    if features:\n        print(\"Extracted features:\")\n        print(json.dumps(features, indent=2))\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Extracted features:\")\n    print(\"\"\"{\n  \"sentiment\": \"positive\",\n  \"rating\": 4,\n  \"pros\": [\n    \"Coffee tastes amazing\",\n    \"Very fast\",\n    \"Sleek design\"\n  ],\n  \"cons\": [\n    \"Somewhat loud during brewing\"\n  ]\n}\"\"\")\n\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nExtracted features:\n{\n  \"sentiment\": \"positive\",\n  \"rating\": 4,\n  \"pros\": [\n    \"Coffee tastes amazing\",\n    \"Very fast\",\n    \"Sleek design\"\n  ],\n  \"cons\": [\n    \"Somewhat loud during brewing\"\n  ]\n}\n\n\nThis gives us structured data we can immediately put into a DataFrame! Notice how the LLM extracted multiple pieces of information from a single review: overall sentiment, an estimated rating, specific positive aspects, and even the one complaint mentioned.\n\n\n3.3 Few-Shot Learning: Teaching by Example\nSometimes clear instructions aren’t enough. LLMs learn better with examples:\n\ndef extract_with_examples(review_text):\n    \"\"\"\n    Use few-shot learning - provide examples of correct extraction\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return \"neutral\"\n\n    prompt = f\"\"\"Extract sentiment from product reviews.\n\nExamples:\n\nReview: \"Best purchase ever! Love this product.\"\nSentiment: positive\n\nReview: \"Broke after one day. Waste of money.\"\nSentiment: negative\n\nReview: \"It's okay. Does what it says.\"\nSentiment: neutral\n\nNow extract sentiment from this review:\n\nReview: {review_text}\nSentiment:\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=10\n    )\n\n    return response.choices[0].message.content.strip().lower()\n\n# Test with tricky example (only if API key is available)\nif client is not None:\n    tricky_review = \"I wanted to love this, but it's just not good enough.\"\n    sentiment = extract_with_examples(tricky_review)\n    print(f\"Tricky review sentiment: {sentiment}\")\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Tricky review sentiment: negative\")\n\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nTricky review sentiment: negative\n\n\nFew-shot prompting (providing 2-5 examples) helps with: - Ambiguous edge cases - Specific formatting requirements - Consistency across similar inputs - Domain-specific language\n\n\n\n\n\n\nTip\n\n\n\nFew-Shot Prompting Best Practices: - Use 2-5 examples (more doesn’t always help) - Examples should cover different scenarios (positive, negative, neutral) - Keep examples concise - Examples cost tokens—balance quality vs. cost\n\n\n\n\n3.4 Iterating on Prompts: A Real Example\nPrompts rarely work perfectly the first time. Let’s iterate:\n\n# V1: First attempt - too vague\nprompt_v1 = \"What category is this job posting?\"\n\n# V2: More specific\nprompt_v2 = \"\"\"What job category is this posting?\nChoose from: Engineering, Marketing, Sales, Customer Support, Other\"\"\"\n\n# V3: Even more specific with format\nprompt_v3 = \"\"\"Classify this job posting into ONE category.\nValid categories: Engineering, Marketing, Sales, Customer Support, Other\nRespond with ONLY the category name, nothing else.\n\nJob posting: {text}\n\nCategory:\"\"\"\n\n# V4: Add examples for edge cases\nprompt_v4 = \"\"\"Classify this job posting into ONE category.\n\nValid categories: Engineering, Marketing, Sales, Customer Support, Other\n\nExamples:\n\"Senior Python Developer needed\" → Engineering\n\"Social Media Manager wanted\" → Marketing\n\"Account Executive\" → Sales\n\nJob posting: {text}\n\nCategory:\"\"\"\n\n# This iterative process is normal and expected\n# Start simple, add specificity based on failures\n\nThe key is testing your prompts on real data and refining based on errors. We’ll see more on validation in the next section."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#parsing-validating-and-converting-to-dataframes",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#parsing-validating-and-converting-to-dataframes",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "4. Parsing, Validating, and Converting to DataFrames",
    "text": "4. Parsing, Validating, and Converting to DataFrames\n\n4.1 Parsing JSON Responses\nLet’s build a robust system for extracting and parsing:\n\ndef extract_job_features(job_text):\n    \"\"\"\n    Extract structured information from job postings\n    Returns a dictionary or None if extraction fails\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return None\n\n    prompt = f\"\"\"Extract these fields from the job posting.\n\nRespond with valid JSON:\n{{\n    \"title\": \"job title\",\n    \"category\": \"Engineering/Marketing/Sales/Support/Other\",\n    \"experience_level\": \"Entry/Mid/Senior/Lead\",\n    \"remote_policy\": \"Remote/Hybrid/Onsite\",\n    \"key_skills\": [\"skill1\", \"skill2\", \"skill3\"]\n}}\n\nJob posting: {job_text}\n\nJSON:\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=300\n        )\n\n        # Get response text\n        result_text = response.choices[0].message.content.strip()\n\n        # Clean up markdown formatting if present\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n\n        # Parse JSON\n        result = json.loads(result_text)\n\n        return result\n\n    except json.JSONDecodeError as e:\n        print(f\"JSON parsing error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"API error: {e}\")\n        return None\n\n# Test it (only if API key is available)\nif client is not None:\n    job_posting = \"\"\"\n    Senior Data Scientist - Remote\n    We're seeking an experienced data scientist with 5+ years experience.\n    Required: Python, SQL, machine learning, deep learning.\n    Fully remote position.\n    \"\"\"\n\n    features = extract_job_features(job_posting)\n    if features:\n        print(\"Extracted job features:\")\n        for key, value in features.items():\n            print(f\"  {key}: {value}\")\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Extracted job features:\")\n    print(\"  title: Senior Data Scientist\")\n    print(\"  category: Engineering\")\n    print(\"  experience_level: Senior\")\n    print(\"  remote_policy: Remote\")\n    print(\"  key_skills: ['Python', 'SQL', 'machine learning', 'deep learning']\")\n\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nExtracted job features:\n  title: Senior Data Scientist\n  category: Engineering\n  experience_level: Senior\n  remote_policy: Remote\n  key_skills: ['Python', 'SQL', 'machine learning', 'deep learning']\n\n\n\n\n4.2 Validating Extractions\nAlways validate LLM outputs—they don’t always follow instructions:\n\ndef validate_job_features(features):\n    \"\"\"\n    Validate that extracted features match expected format\n    Returns True if valid, False otherwise\n    \"\"\"\n    if features is None:\n        return False\n\n    # Define expected fields and valid values\n    expected_fields = ['title', 'category', 'experience_level', 'remote_policy', 'key_skills']\n    valid_categories = ['Engineering', 'Marketing', 'Sales', 'Support', 'Other']\n    valid_experience = ['Entry', 'Mid', 'Senior', 'Lead']\n    valid_remote = ['Remote', 'Hybrid', 'Onsite']\n\n    # Check all required fields present\n    if not all(field in features for field in expected_fields):\n        print(\"Missing required fields\")\n        return False\n\n    # Check category is valid\n    if features['category'] not in valid_categories:\n        print(f\"Invalid category: {features['category']}\")\n        return False\n\n    # Check experience level is valid\n    if features['experience_level'] not in valid_experience:\n        print(f\"Invalid experience level: {features['experience_level']}\")\n        return False\n\n    # Check remote policy is valid\n    if features['remote_policy'] not in valid_remote:\n        print(f\"Invalid remote policy: {features['remote_policy']}\")\n        return False\n\n    # Check key_skills is a list\n    if not isinstance(features['key_skills'], list):\n        print(\"key_skills should be a list\")\n        return False\n\n    return True\n\n# Test validation (only if we have features from previous cell)\nif client is not None:\n    # features was defined in previous cell when client is not None\n    is_valid = validate_job_features(features)\n    print(f\"\\nValidation result: {is_valid}\")\nelse:\n    print(\"Skipping validation - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Validation result: True\")\n\nSkipping validation - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nValidation result: True\n\n\nSee what happened? All the extracted fields are in the expected format: category is one of the valid options, experience level is valid, remote policy is valid, and key_skills is a list. The validation passed!\n\n\n\n\n\n\nWarning\n\n\n\nNever Trust LLM Output Blindly LLMs can: - Return invalid categories - Return wrong data types - Hallucinate information not in the text - Miss fields entirely\nAlways validate before using extracted features in ML models.\n\n\n\n\n4.3 Batch Processing Multiple Texts\nReal datasets have hundreds or thousands of texts. Let’s process them efficiently:\n\ndef extract_batch(texts, extract_func, show_progress=True):\n    \"\"\"\n    Extract features from multiple texts\n\n    Args:\n        texts: List of text strings\n        extract_func: Function that extracts features from one text\n        show_progress: Whether to print progress\n\n    Returns:\n        List of extracted features (same length as texts)\n    \"\"\"\n    results = []\n\n    for i, text in enumerate(texts):\n        if show_progress and (i % 10 == 0):\n            print(f\"Processing {i}/{len(texts)}...\")\n\n        result = extract_func(text)\n        results.append(result)\n\n    return results\n\n# Example: Batch process multiple reviews\nreviews = [\n    \"Amazing product! Best purchase ever.\",\n    \"Terrible quality. Broke immediately.\",\n    \"It's fine. Nothing special.\",\n    \"Great value for money!\",\n    \"Disappointed with this purchase.\"\n]\n\n# Note: This would actually call the API multiple times\n# We'll implement a more efficient version with error handling next\nsentiments = extract_batch(reviews, extract_sentiment_robust, show_progress=True)\n\nprint(\"\\nResults:\")\nfor review, sentiment in zip(reviews, sentiments):\n    print(f\"'{review[:30]}...' → {sentiment}\")\n\nProcessing 0/5...\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\n\nResults:\n'Amazing product! Best purchase...' → None\n'Terrible quality. Broke immedi...' → None\n'It's fine. Nothing special....' → None\n'Great value for money!...' → None\n'Disappointed with this purchas...' → None\n\n\n\n\n4.4 Converting to pandas DataFrame\nNow the payoff—converting extracted features to a clean DataFrame ready for ML:\n\nimport pandas as pd\n\n# Simulated extraction results (in practice, these come from API calls)\nextraction_results = [\n    {\n        \"title\": \"Senior Data Scientist\",\n        \"category\": \"Engineering\",\n        \"experience_level\": \"Senior\",\n        \"remote_policy\": \"Remote\",\n        \"key_skills\": [\"Python\", \"SQL\", \"ML\"]\n    },\n    {\n        \"title\": \"Marketing Manager\",\n        \"category\": \"Marketing\",\n        \"experience_level\": \"Mid\",\n        \"remote_policy\": \"Hybrid\",\n        \"key_skills\": [\"SEO\", \"Analytics\", \"Content\"]\n    },\n    {\n        \"title\": \"Sales Representative\",\n        \"category\": \"Sales\",\n        \"experience_level\": \"Entry\",\n        \"remote_policy\": \"Onsite\",\n        \"key_skills\": [\"Communication\", \"CRM\"]\n    }\n]\n\n# Convert to DataFrame\ndf = pd.DataFrame(extraction_results)\n\n# Handle the list field (key_skills)\ndf['num_skills'] = df['key_skills'].apply(len)\ndf['skills_str'] = df['key_skills'].apply(lambda x: ', '.join(x))\n\ndf.head()\n\n\n\n\n\n\n\n\ntitle\ncategory\nexperience_level\nremote_policy\nkey_skills\nnum_skills\nskills_str\n\n\n\n\n0\nSenior Data Scientist\nEngineering\nSenior\nRemote\n[Python, SQL, ML]\n3\nPython, SQL, ML\n\n\n1\nMarketing Manager\nMarketing\nMid\nHybrid\n[SEO, Analytics, Content]\n3\nSEO, Analytics, Content\n\n\n2\nSales Representative\nSales\nEntry\nOnsite\n[Communication, CRM]\n2\nCommunication, CRM\n\n\n\n\n\n\n\nNow we have a clean DataFrame! The categorical variables (category, experience_level, remote_policy) are ready for encoding. The skills are processed. This data is ready to be fed into machine learning models."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#integration-with-ml-pipelines",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#integration-with-ml-pipelines",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "5. Integration with ML Pipelines",
    "text": "5. Integration with ML Pipelines\n\n5.1 The Complete Pipeline: Text → Features → Model\nLet’s build a complete example: extract features from reviews, then predict review helpfulness:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Simulated dataset: product reviews with helpfulness labels\nreviews_data = {\n    'review_text': [\n        \"Amazing product! The quality is outstanding. Highly recommend.\",\n        \"Total waste of money. Broke after one use.\",\n        \"It's okay. Does the job but nothing special.\",\n        \"Best purchase I've made! Love everything about it.\",\n        \"Disappointed. Expected better quality for the price.\",\n        \"Works as described. No complaints.\",\n        \"Fantastic! Exceeded my expectations in every way.\",\n        \"Not worth it. Too expensive for what you get.\",\n        \"Pretty good. Would buy again.\",\n        \"Terrible product. Avoid at all costs.\"\n    ],\n    'helpful_votes': [45, 32, 8, 51, 28, 12, 48, 25, 15, 38]  # Number of helpful votes\n}\n\ndf_reviews = pd.DataFrame(reviews_data)\n\n# Create binary target: helpful (&gt;20 votes) or not helpful (≤20 votes)\ndf_reviews['is_helpful'] = (df_reviews['helpful_votes'] &gt; 20).astype(int)\n\nprint(\"Dataset:\")\nprint(df_reviews[['review_text', 'helpful_votes', 'is_helpful']].head())\n\nDataset:\n                                         review_text  helpful_votes  \\\n0  Amazing product! The quality is outstanding. H...             45   \n1         Total waste of money. Broke after one use.             32   \n2       It's okay. Does the job but nothing special.              8   \n3  Best purchase I've made! Love everything about...             51   \n4  Disappointed. Expected better quality for the ...             28   \n\n   is_helpful  \n0           1  \n1           1  \n2           0  \n3           1  \n4           1  \n\n\nNow let’s extract features using our LLM:\n\ndef extract_review_features_for_ml(review_text):\n    \"\"\"\n    Extract features specifically useful for predicting helpfulness\n    \"\"\"\n    if client is None:\n        # Return default values if no API key\n        return {\n            \"sentiment\": \"neutral\",\n            \"is_detailed\": False,\n            \"mentions_specific_features\": False,\n            \"mentions_price_value\": False,\n            \"has_comparison\": False\n        }\n\n    prompt = f\"\"\"Analyze this product review and extract features.\n\nRespond with valid JSON:\n{{\n    \"sentiment\": \"positive/negative/neutral\",\n    \"is_detailed\": true/false,\n    \"mentions_specific_features\": true/false,\n    \"mentions_price_value\": true/false,\n    \"has_comparison\": true/false\n}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=150\n        )\n\n        result_text = response.choices[0].message.content.strip()\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n        features = json.loads(result_text)\n        return features\n    except:\n        # Return default values if extraction fails\n        return {\n            \"sentiment\": \"neutral\",\n            \"is_detailed\": False,\n            \"mentions_specific_features\": False,\n            \"mentions_price_value\": False,\n            \"has_comparison\": False\n        }\n\n# Extract features for all reviews\n# NOTE: In practice, this would make API calls\n# For this example, we'll simulate the results\nllm_features = [\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"neutral\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": True, \"mentions_specific_features\": False,\n     \"mentions_price_value\": True, \"has_comparison\": False},\n    {\"sentiment\": \"neutral\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": True, \"mentions_specific_features\": False,\n     \"mentions_price_value\": True, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n]\n\n# Add LLM features to DataFrame\ndf_features = pd.DataFrame(llm_features)\ndf_reviews = pd.concat([df_reviews, df_features], axis=1)\n\ndf_reviews.head()\n\n\n\n\n\n\n\n\nreview_text\nhelpful_votes\nis_helpful\nsentiment\nis_detailed\nmentions_specific_features\nmentions_price_value\nhas_comparison\n\n\n\n\n0\nAmazing product! The quality is outstanding. H...\n45\n1\npositive\nTrue\nTrue\nFalse\nFalse\n\n\n1\nTotal waste of money. Broke after one use.\n32\n1\nnegative\nFalse\nFalse\nFalse\nFalse\n\n\n2\nIt's okay. Does the job but nothing special.\n8\n0\nneutral\nFalse\nFalse\nFalse\nFalse\n\n\n3\nBest purchase I've made! Love everything about...\n51\n1\npositive\nTrue\nTrue\nFalse\nFalse\n\n\n4\nDisappointed. Expected better quality for the ...\n28\n1\nnegative\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n\n5.2 Encoding and Training\nNow we have LLM-extracted features. Let’s train a model:\n\n# Encode sentiment\nle = LabelEncoder()\ndf_reviews['sentiment_encoded'] = le.fit_transform(df_reviews['sentiment'])\n\n# Select features for ML model\nfeature_columns = [\n    'sentiment_encoded',\n    'is_detailed',\n    'mentions_specific_features',\n    'mentions_price_value',\n    'has_comparison'\n]\n\nX = df_reviews[feature_columns]\ny = df_reviews['is_helpful']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train a classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"\\nModel Performance:\")\nprint(classification_report(y_test, y_pred))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': feature_columns,\n    'importance': clf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n\nModel Performance:\n              precision    recall  f1-score   support\n\n           0       1.00      0.50      0.67         2\n           1       0.50      1.00      0.67         1\n\n    accuracy                           0.67         3\n   macro avg       0.75      0.75      0.67         3\nweighted avg       0.83      0.67      0.67         3\n\n\nFeature Importance:\n                      feature  importance\n0           sentiment_encoded    0.589149\n1                 is_detailed    0.295655\n2  mentions_specific_features    0.090456\n3        mentions_price_value    0.024740\n4              has_comparison    0.000000\n\n\nSee what we did? We used LLM to extract features from text, encoded them properly, and trained a traditional ML model. The LLM extracted semantic information (sentiment, detail level, specific mentions) that would be hard to capture with simple word counts or regex patterns.\n\n\n5.3 Avoiding Data Leakage with LLM Features\nImportant consideration: when do you extract features?\n\n# WRONG WAY - Data leakage!\n# Don't do this: extracting features from full dataset before split\n# The LLM might learn patterns from test set during extraction\n\n# RIGHT WAY - Extract after split\n# But wait... LLMs don't \"learn\" from your prompts in real-time\n# So technically, this isn't data leakage in the traditional sense\n\n# However, best practice:\n# 1. Split your data first\n# 2. Develop/test prompts ONLY on training data\n# 3. Once prompt is finalized, apply to train and test separately\n# 4. Never iterate on prompts while looking at test set results\n\nThe principle: don’t use test set information to develop your extraction prompts, just like you wouldn’t use test set to select model hyperparameters.\n\n\n5.4 Comparing With and Without LLM Features\nLet’s see if LLM features actually help:\n\n# Baseline: Just simple features (no LLM)\ndf_reviews['review_length'] = df_reviews['review_text'].apply(len)\ndf_reviews['word_count'] = df_reviews['review_text'].apply(lambda x: len(x.split()))\ndf_reviews['exclamation_count'] = df_reviews['review_text'].apply(lambda x: x.count('!'))\n\n# Model 1: Without LLM features\nX_baseline = df_reviews[['review_length', 'word_count', 'exclamation_count']]\nX_train_base, X_test_base, y_train, y_test = train_test_split(\n    X_baseline, y, test_size=0.3, random_state=42\n)\n\nclf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_baseline.fit(X_train_base, y_train)\nbaseline_score = clf_baseline.score(X_test_base, y_test)\n\n# Model 2: With LLM features\nX_llm = df_reviews[['review_length', 'word_count', 'exclamation_count'] + feature_columns]\nX_train_llm, X_test_llm, y_train, y_test = train_test_split(\n    X_llm, y, test_size=0.3, random_state=42\n)\n\nclf_llm = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_llm.fit(X_train_llm, y_train)\nllm_score = clf_llm.score(X_test_llm, y_test)\n\nprint(f\"Baseline model (no LLM) accuracy: {baseline_score:.3f}\")\nprint(f\"With LLM features accuracy: {llm_score:.3f}\")\nprint(f\"Improvement: {llm_score - baseline_score:.3f}\")\n\nBaseline model (no LLM) accuracy: 0.000\nWith LLM features accuracy: 0.333\nImprovement: 0.333\n\n\nThis comparison tells you whether the LLM extraction was worth the cost. Sometimes it helps significantly. Sometimes simple features work just as well. Always compare!"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#cost-analysis-and-provider-comparison",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#cost-analysis-and-provider-comparison",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "6. Cost Analysis and Provider Comparison",
    "text": "6. Cost Analysis and Provider Comparison\n\n6.1 Understanding Token-Based Pricing\nLLM APIs charge per token. Understanding costs is crucial:\n\ndef estimate_tokens(text):\n    \"\"\"\n    Rough estimate: 1 token ≈ 0.75 words\n    Or approximately: 1 token ≈ 4 characters\n    \"\"\"\n    # Method 1: Based on words\n    word_estimate = len(text.split()) * 1.3\n\n    # Method 2: Based on characters\n    char_estimate = len(text) / 4\n\n    # Average the two methods\n    return int((word_estimate + char_estimate) / 2)\n\n# Example texts\ntexts = [\n    \"Short review.\",\n    \"This is a medium-length review with several sentences about the product.\",\n    \"\"\"This is a long, detailed review that goes into great depth about\n    various aspects of the product including quality, price, features,\n    customer service, shipping speed, packaging, and overall value for money.\n    I would definitely recommend this to anyone considering a purchase.\"\"\"\n]\n\nfor text in texts:\n    estimated = estimate_tokens(text)\n    print(f\"Text length: {len(text)} chars, ~{estimated} tokens\")\n\nText length: 13 chars, ~2 tokens\nText length: 72 chars, ~16 tokens\nText length: 285 chars, ~62 tokens\n\n\n\n\n6.2 Calculating Extraction Costs\nLet’s calculate real costs for a dataset:\n\ndef calculate_extraction_cost(\n    num_texts,\n    avg_text_length,\n    prompt_tokens,\n    response_tokens,\n    model='gpt-3.5-turbo'\n):\n    \"\"\"\n    Calculate total cost for extracting features from a dataset\n\n    Pricing (as of 2024):\n    - GPT-3.5-turbo: $0.0005 per 1K tokens (input and output)\n    - GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens\n    - Claude Sonnet: $0.003 per 1K input tokens, $0.015 per 1K output tokens\n    \"\"\"\n    # Estimate tokens per extraction\n    text_tokens = avg_text_length / 4  # Rough estimate\n    total_input_tokens = (prompt_tokens + text_tokens) * num_texts\n    total_output_tokens = response_tokens * num_texts\n\n    # Pricing\n    if model == 'gpt-3.5-turbo':\n        cost_per_1k_input = 0.0005\n        cost_per_1k_output = 0.0005\n    elif model == 'gpt-4':\n        cost_per_1k_input = 0.03\n        cost_per_1k_output = 0.06\n    elif model == 'claude-sonnet':\n        cost_per_1k_input = 0.003\n        cost_per_1k_output = 0.015\n    else:\n        raise ValueError(f\"Unknown model: {model}\")\n\n    input_cost = (total_input_tokens / 1000) * cost_per_1k_input\n    output_cost = (total_output_tokens / 1000) * cost_per_1k_output\n    total_cost = input_cost + output_cost\n\n    return {\n        'total_input_tokens': int(total_input_tokens),\n        'total_output_tokens': int(total_output_tokens),\n        'input_cost': input_cost,\n        'output_cost': output_cost,\n        'total_cost': total_cost\n    }\n\n# Example: Extract sentiment from 10,000 product reviews\nnum_reviews = 10000\navg_review_length = 200  # characters\nprompt_tokens = 50  # Our prompt\nresponse_tokens = 5  # Just \"positive\"/\"negative\"/\"neutral\"\n\nprint(\"Cost comparison for 10,000 reviews:\\n\")\n\nfor model in ['gpt-3.5-turbo', 'gpt-4', 'claude-sonnet']:\n    cost_info = calculate_extraction_cost(\n        num_reviews, avg_review_length, prompt_tokens, response_tokens, model\n    )\n    print(f\"{model}:\")\n    print(f\"  Total tokens: {cost_info['total_input_tokens'] + cost_info['total_output_tokens']:,}\")\n    print(f\"  Total cost: ${cost_info['total_cost']:.2f}\\n\")\n\nCost comparison for 10,000 reviews:\n\ngpt-3.5-turbo:\n  Total tokens: 1,050,000\n  Total cost: $0.53\n\ngpt-4:\n  Total tokens: 1,050,000\n  Total cost: $33.00\n\nclaude-sonnet:\n  Total tokens: 1,050,000\n  Total cost: $3.75\n\n\n\nThis shows the dramatic cost difference between models. For simple extraction, GPT-3.5 is often sufficient and much cheaper.\n\n\n6.3 When to Use Which Model\nDecision framework:\n\ndef recommend_model(task_complexity, dataset_size, budget):\n    \"\"\"\n    Recommend which LLM to use based on requirements\n\n    Args:\n        task_complexity: 'simple', 'moderate', 'complex'\n        dataset_size: number of texts to process\n        budget: maximum budget in dollars\n\n    Returns:\n        Recommended model and reasoning\n    \"\"\"\n    # Estimate costs (simplified)\n    gpt35_cost_per_item = 0.0001\n    gpt4_cost_per_item = 0.001\n\n    gpt35_total = gpt35_cost_per_item * dataset_size\n    gpt4_total = gpt4_cost_per_item * dataset_size\n\n    recommendations = []\n\n    if task_complexity == 'simple':\n        recommendations.append({\n            'model': 'gpt-3.5-turbo',\n            'reasoning': 'Simple extraction tasks work well with GPT-3.5',\n            'estimated_cost': gpt35_total\n        })\n\n    elif task_complexity == 'moderate':\n        if gpt35_total &lt;= budget:\n            recommendations.append({\n                'model': 'gpt-3.5-turbo (try first)',\n                'reasoning': 'Start with GPT-3.5, upgrade if quality insufficient',\n                'estimated_cost': gpt35_total\n            })\n        if gpt4_total &lt;= budget:\n            recommendations.append({\n                'model': 'gpt-4 (if GPT-3.5 fails)',\n                'reasoning': 'Better accuracy but 10x cost',\n                'estimated_cost': gpt4_total\n            })\n\n    else:  # complex\n        if gpt4_total &lt;= budget:\n            recommendations.append({\n                'model': 'gpt-4',\n                'reasoning': 'Complex tasks need GPT-4 reasoning',\n                'estimated_cost': gpt4_total\n            })\n        else:\n            recommendations.append({\n                'model': 'Consider alternatives',\n                'reasoning': 'Budget insufficient for GPT-4 at this scale. Consider: sampling, fine-tuning smaller model, or traditional NLP',\n                'estimated_cost': None\n            })\n\n    return recommendations\n\n# Examples\nscenarios = [\n    ('simple', 10000, 10),\n    ('moderate', 5000, 5),\n    ('complex', 1000, 50),\n]\n\nfor complexity, size, budget in scenarios:\n    print(f\"\\nScenario: {complexity} task, {size:,} items, ${budget} budget\")\n    recs = recommend_model(complexity, size, budget)\n    for rec in recs:\n        print(f\"  → {rec['model']}: {rec['reasoning']}\")\n        if rec['estimated_cost']:\n            print(f\"    Estimated cost: ${rec['estimated_cost']:.2f}\")\n\n\nScenario: simple task, 10,000 items, $10 budget\n  → gpt-3.5-turbo: Simple extraction tasks work well with GPT-3.5\n    Estimated cost: $1.00\n\nScenario: moderate task, 5,000 items, $5 budget\n  → gpt-3.5-turbo (try first): Start with GPT-3.5, upgrade if quality insufficient\n    Estimated cost: $0.50\n  → gpt-4 (if GPT-3.5 fails): Better accuracy but 10x cost\n    Estimated cost: $5.00\n\nScenario: complex task, 1,000 items, $50 budget\n  → gpt-4: Complex tasks need GPT-4 reasoning\n    Estimated cost: $1.00\n\n\n\n\n\n\n\n\nTip\n\n\n\nCost Optimization Strategies: 1. Start cheap: Try GPT-3.5 first, upgrade only if needed 2. Sample first: Test on 100 examples before processing 10,000 3. Shorten prompts: Every token counts—be concise 4. Cache results: Don’t reprocess the same text twice 5. Consider batching: Some providers offer batch APIs at lower cost\n\n\n\n\n6.4 ROI Analysis: LLM vs. Alternatives\nIs LLM extraction worth it? Compare alternatives:\n\n# Scenario: Classify 10,000 customer support tickets\n\nalternatives = {\n    'Manual labeling': {\n        'cost': 0.50 * 10000,  # $0.50 per ticket\n        'time_hours': 333,  # 2 mins per ticket\n        'accuracy': 0.95,\n        'scalability': 'Poor'\n    },\n    'Traditional ML (train from scratch)': {\n        'cost': 2000,  # Data labeling for training set\n        'time_hours': 40,  # Development time\n        'accuracy': 0.85,\n        'scalability': 'Excellent (after training)'\n    },\n    'LLM extraction (GPT-3.5)': {\n        'cost': 1.0 * 10000 * 0.0001,  # $1.00\n        'time_hours': 1,  # Just API calls\n        'accuracy': 0.90,\n        'scalability': 'Excellent'\n    },\n    'LLM extraction (GPT-4)': {\n        'cost': 10.0 * 10000 * 0.0001,  # $10.00\n        'time_hours': 1,\n        'accuracy': 0.93,\n        'scalability': 'Excellent'\n    }\n}\n\ncomparison = pd.DataFrame(alternatives).T\ncomparison.head()\n\n\n\n\n\n\n\n\ncost\ntime_hours\naccuracy\nscalability\n\n\n\n\nManual labeling\n5000.0\n333\n0.95\nPoor\n\n\nTraditional ML (train from scratch)\n2000\n40\n0.85\nExcellent (after training)\n\n\nLLM extraction (GPT-3.5)\n1.0\n1\n0.9\nExcellent\n\n\nLLM extraction (GPT-4)\n10.0\n1\n0.93\nExcellent\n\n\n\n\n\n\n\nFor this scenario, LLM extraction with GPT-3.5 is clearly the winner: cheap, fast, accurate enough, and scalable. But the best choice depends on your specific constraints."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#quality-control-and-validation",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#quality-control-and-validation",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "7. Quality Control and Validation",
    "text": "7. Quality Control and Validation\n\n7.1 Measuring Extraction Accuracy\nHow do you know if your LLM extraction is working well?\n\ndef calculate_extraction_accuracy(extracted, ground_truth):\n    \"\"\"\n    Compare LLM extractions to ground truth labels\n\n    Args:\n        extracted: List of LLM-extracted labels\n        ground_truth: List of correct labels\n\n    Returns:\n        Accuracy metrics\n    \"\"\"\n    correct = sum(e == g for e, g in zip(extracted, ground_truth))\n    total = len(ground_truth)\n    accuracy = correct / total\n\n    # Breakdown by category\n    from collections import defaultdict\n    category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n\n    for ext, truth in zip(extracted, ground_truth):\n        category_stats[truth]['total'] += 1\n        if ext == truth:\n            category_stats[truth]['correct'] += 1\n\n    return {\n        'overall_accuracy': accuracy,\n        'correct': correct,\n        'total': total,\n        'by_category': dict(category_stats)\n    }\n\n# Example: Sentiment extraction validation\nground_truth_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative']\nllm_extracted_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative']\n\naccuracy = calculate_extraction_accuracy(llm_extracted_sentiments, ground_truth_sentiments)\nprint(f\"Overall accuracy: {accuracy['overall_accuracy']:.2%}\")\nprint(f\"Correct: {accuracy['correct']}/{accuracy['total']}\")\n\nOverall accuracy: 100.00%\nCorrect: 5/5\n\n\n\n\n7.2 Spot-Checking and Manual Review\nYou can’t manually check everything, but strategic sampling helps:\n\ndef spot_check_extractions(texts, extractions, n_samples=20, random_state=42):\n    \"\"\"\n    Sample extractions for manual review\n\n    Shows random samples + edge cases for human verification\n    \"\"\"\n    np.random.seed(random_state)\n\n    # Random sample\n    indices = np.random.choice(len(texts), min(n_samples, len(texts)), replace=False)\n\n    print(\"Random sample for manual review:\\n\")\n    for i, idx in enumerate(indices, 1):\n        print(f\"{i}. Text: {texts[idx][:80]}...\")\n        print(f\"   Extraction: {extractions[idx]}\")\n        print(f\"   Correct? (Y/N): ___\")\n        print()\n\n    return indices\n\n# Example usage\nsample_reviews = [\n    \"This product is amazing! Love it.\",\n    \"Terrible. Waste of money.\",\n    \"It's okay, I guess.\",\n    \"Best purchase ever made!\",\n    \"Not great, not terrible.\"\n]\nsample_sentiments = ['positive', 'negative', 'neutral', 'positive', 'neutral']\n\nspot_check_extractions(sample_reviews, sample_sentiments, n_samples=3)\n\nRandom sample for manual review:\n\n1. Text: Terrible. Waste of money....\n   Extraction: negative\n   Correct? (Y/N): ___\n\n2. Text: Not great, not terrible....\n   Extraction: neutral\n   Correct? (Y/N): ___\n\n3. Text: It's okay, I guess....\n   Extraction: neutral\n   Correct? (Y/N): ___\n\n\n\narray([1, 4, 2])\n\n\n\n\n7.3 Identifying Systematic Errors\nLook for patterns in failures:\n\ndef analyze_errors(texts, extracted, ground_truth):\n    \"\"\"\n    Find patterns in extraction errors\n    \"\"\"\n    errors = []\n\n    for text, ext, truth in zip(texts, extracted, ground_truth):\n        if ext != truth:\n            errors.append({\n                'text': text,\n                'extracted': ext,\n                'ground_truth': truth,\n                'text_length': len(text),\n                'word_count': len(text.split())\n            })\n\n    if not errors:\n        print(\"No errors found!\")\n        return\n\n    error_df = pd.DataFrame(errors)\n\n    print(f\"Total errors: {len(errors)}/{len(texts)} ({len(errors)/len(texts):.1%})\\n\")\n\n    # Analyze error patterns\n    print(\"Errors by true category:\")\n    print(error_df['ground_truth'].value_counts())\n\n    print(\"\\nAverage length of texts with errors:\")\n    print(f\"  Characters: {error_df['text_length'].mean():.0f}\")\n    print(f\"  Words: {error_df['word_count'].mean():.0f}\")\n\n    print(\"\\nSample errors:\")\n    for _, row in error_df.head(3).iterrows():\n        print(f\"  Text: {row['text'][:60]}...\")\n        print(f\"  Extracted: {row['extracted']}, True: {row['ground_truth']}\\n\")\n\n    return error_df\n\n# Example with some errors\ntexts_with_errors = [\n    \"Love this product!\",\n    \"It's not bad.\",  # Tricky: double negative\n    \"Terrible quality.\",\n    \"I wouldn't say it's good.\",  # Tricky: negation\n    \"Amazing!\"\n]\nextracted_with_errors = ['positive', 'negative', 'negative', 'negative', 'positive']\nground_truth_with_errors = ['positive', 'positive', 'negative', 'negative', 'positive']\n\nerror_analysis = analyze_errors(texts_with_errors, extracted_with_errors, ground_truth_with_errors)\n\nTotal errors: 1/5 (20.0%)\n\nErrors by true category:\nground_truth\npositive    1\nName: count, dtype: int64\n\nAverage length of texts with errors:\n  Characters: 13\n  Words: 3\n\nSample errors:\n  Text: It's not bad....\n  Extracted: negative, True: positive\n\n\n\n\n\n7.4 When Extraction Quality Is “Good Enough”\nPerfect extraction isn’t always necessary:\n\ndef is_quality_sufficient(accuracy, task_requirements):\n    \"\"\"\n    Determine if extraction quality meets requirements\n\n    Args:\n        accuracy: Measured extraction accuracy (0-1)\n        task_requirements: Dict with requirements\n\n    Returns:\n        Boolean and explanation\n    \"\"\"\n    min_accuracy = task_requirements.get('min_accuracy', 0.85)\n    critical_task = task_requirements.get('critical', False)\n\n    if critical_task:\n        # High-stakes tasks need very high accuracy\n        threshold = max(min_accuracy, 0.95)\n        sufficient = accuracy &gt;= threshold\n        msg = f\"Critical task requires ≥{threshold:.0%} accuracy. \"\n    else:\n        # Normal tasks can tolerate some errors\n        threshold = min_accuracy\n        sufficient = accuracy &gt;= threshold\n        msg = f\"Task requires ≥{threshold:.0%} accuracy. \"\n\n    msg += f\"Current accuracy: {accuracy:.1%}. \"\n    msg += \"✓ Sufficient\" if sufficient else \"✗ Insufficient\"\n\n    return sufficient, msg\n\n# Examples\nscenarios_quality = [\n    (0.92, {'min_accuracy': 0.85, 'critical': False}),  # Good enough\n    (0.92, {'min_accuracy': 0.85, 'critical': True}),   # Not good enough (critical)\n    (0.78, {'min_accuracy': 0.85, 'critical': False}),  # Not good enough\n]\n\nfor acc, reqs in scenarios_quality:\n    sufficient, message = is_quality_sufficient(acc, reqs)\n    print(message)\n    print()\n\nTask requires ≥85% accuracy. Current accuracy: 92.0%. ✓ Sufficient\n\nCritical task requires ≥95% accuracy. Current accuracy: 92.0%. ✗ Insufficient\n\nTask requires ≥85% accuracy. Current accuracy: 78.0%. ✗ Insufficient\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuality Thresholds by Task Type: - Medical/Legal: 95%+ accuracy required (often manual review needed) - Financial: 90-95% accuracy - Marketing/Content: 80-85% accuracy often sufficient - Exploratory analysis: 70-80% may be acceptable\nRemember: LLM features are inputs to ML models. Small extraction errors might not significantly hurt final model performance."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#when-to-use-llms-vs.-traditional-nlp",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#when-to-use-llms-vs.-traditional-nlp",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "8. When to Use LLMs vs. Traditional NLP",
    "text": "8. When to Use LLMs vs. Traditional NLP\n\n8.1 Decision Framework\nNot every text problem needs an LLM. Here’s how to decide:\n\ndef should_use_llm(task_description):\n    \"\"\"\n    Decision tree: LLM vs. traditional NLP\n\n    Returns recommendation and reasoning\n    \"\"\"\n    # Simple pattern matching tasks\n    simple_patterns = [\n        'contains keyword',\n        'find email addresses',\n        'extract phone numbers',\n        'detect URLs',\n        'count words'\n    ]\n\n    # Traditional ML appropriate\n    traditional_ml_tasks = [\n        'you have large labeled dataset',\n        'need very fast inference',\n        'extremely cost-sensitive',\n        'offline/no internet'\n    ]\n\n    # LLM appropriate\n    llm_tasks = [\n        'understand context',\n        'extract categories not seen before',\n        'handle nuance',\n        'multiple languages',\n        'complex reasoning',\n        'no labeled data available'\n    ]\n\n    task_lower = task_description.lower()\n\n    # Check each category\n    if any(pattern in task_lower for pattern in simple_patterns):\n        return {\n            'recommendation': 'Use regex or simple string matching',\n            'reasoning': 'Simple pattern matching - no need for expensive LLM',\n            'example_tool': 're module in Python'\n        }\n\n    if any(indicator in task_lower for indicator in traditional_ml_tasks):\n        return {\n            'recommendation': 'Train traditional ML model',\n            'reasoning': 'Your constraints favor traditional ML over LLM APIs',\n            'example_tool': 'scikit-learn text classification'\n        }\n\n    if any(indicator in task_lower for indicator in llm_tasks):\n        return {\n            'recommendation': 'Use LLM extraction',\n            'reasoning': 'Task requires language understanding that LLMs excel at',\n            'example_tool': 'GPT-3.5 or GPT-4 via API'\n        }\n\n    # Default: try simple first\n    return {\n        'recommendation': 'Try simple methods first, then LLM if needed',\n        'reasoning': 'Always start with simplest solution',\n        'example_tool': 'Regex → Traditional ML → LLM (in that order)'\n    }\n\n# Test with different tasks\ntasks = [\n    \"Extract email addresses from text\",\n    \"Determine sentiment of product reviews with nuanced language\",\n    \"Classify support tickets into categories with 10,000 labeled examples\",\n    \"Extract job requirements that vary significantly across postings\",\n    \"Count how many times 'refund' appears in complaints\"\n]\n\nprint(\"Task recommendations:\\n\")\nfor task in tasks:\n    rec = should_use_llm(task)\n    print(f\"Task: {task}\")\n    print(f\"  → {rec['recommendation']}\")\n    print(f\"  Reasoning: {rec['reasoning']}\")\n    print(f\"  Tool: {rec['example_tool']}\\n\")\n\nTask recommendations:\n\nTask: Extract email addresses from text\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Determine sentiment of product reviews with nuanced language\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Classify support tickets into categories with 10,000 labeled examples\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Extract job requirements that vary significantly across postings\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Count how many times 'refund' appears in complaints\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\n\n\n\n\n8.2 Simple Regex vs. LLM: A Comparison\nLet’s see both approaches on the same task:\n\nimport re\n\n# Task: Extract product category from reviews\n\n# Approach 1: Regex/keyword matching\ndef extract_category_regex(review_text):\n    \"\"\"Simple keyword matching\"\"\"\n    text_lower = review_text.lower()\n\n    if any(word in text_lower for word in ['coffee', 'brew', 'espresso', 'caffeine']):\n        return 'Coffee Maker'\n    elif any(word in text_lower for word in ['vacuum', 'clean', 'suction', 'dirt']):\n        return 'Vacuum'\n    elif any(word in text_lower for word in ['headphone', 'audio', 'sound', 'music']):\n        return 'Headphones'\n    else:\n        return 'Other'\n\n# Approach 2: LLM extraction\ndef extract_category_llm(review_text):\n    \"\"\"LLM-based category extraction\"\"\"\n    prompt = f\"\"\"What product category is this review about?\nChoose from: Coffee Maker, Vacuum, Headphones, Other\n\nReview: {review_text}\n\nCategory:\"\"\"\n\n    # Simulated LLM response\n    # In practice, this would call the API\n    # For now, we'll simulate based on content\n    text_lower = review_text.lower()\n    if 'coffee' in text_lower or 'brew' in text_lower:\n        return 'Coffee Maker'\n    elif 'vacuum' in text_lower or 'clean' in text_lower:\n        return 'Vacuum'\n    elif 'headphone' in text_lower or 'sound' in text_lower:\n        return 'Headphones'\n    else:\n        return 'Other'\n\n# Test cases\ntest_reviews = [\n    \"This coffee maker brews excellent coffee.\",  # Simple - both work\n    \"Makes great espresso every morning.\",  # Simple - both work\n    \"The audio quality is amazing!\",  # LLM better (understands audio → headphones)\n    \"Keeps my floors spotless.\",  # LLM better (spotless → cleaning → vacuum)\n    \"Battery life could be better but the noise cancellation is top-notch.\"  # LLM much better\n]\n\nprint(\"Category extraction comparison:\\n\")\nfor review in test_reviews:\n    regex_cat = extract_category_regex(review)\n    llm_cat = extract_category_llm(review)\n    print(f\"Review: {review}\")\n    print(f\"  Regex: {regex_cat}\")\n    print(f\"  LLM: {llm_cat}\")\n    print()\n\nCategory extraction comparison:\n\nReview: This coffee maker brews excellent coffee.\n  Regex: Coffee Maker\n  LLM: Coffee Maker\n\nReview: Makes great espresso every morning.\n  Regex: Coffee Maker\n  LLM: Other\n\nReview: The audio quality is amazing!\n  Regex: Headphones\n  LLM: Other\n\nReview: Keeps my floors spotless.\n  Regex: Other\n  LLM: Other\n\nReview: Battery life could be better but the noise cancellation is top-notch.\n  Regex: Other\n  LLM: Other\n\n\n\nThe LLM shines when: - Keywords aren’t explicit (“noise cancellation” → headphones) - Context matters (“spotless” → cleaning → vacuum) - Synonyms and paraphrasing (“audio quality” = sound)\nRegex wins when: - Keywords are explicit and consistent - Speed is critical - Cost must be zero\n\n\n8.3 Cost-Benefit Comparison Table\n\ncomparison_data = {\n    'Approach': ['Regex/Keywords', 'Traditional ML', 'LLM (GPT-3.5)', 'LLM (GPT-4)'],\n    'Setup Cost': ['$0', '$500-5000', '$0', '$0'],\n    'Per-Item Cost': ['$0', '$0', '$0.0001', '$0.001'],\n    'Setup Time': ['1 hour', '1-4 weeks', '1-2 hours', '1-2 hours'],\n    'Accuracy (simple)': ['60-70%', '85-90%', '85-90%', '90-95%'],\n    'Accuracy (complex)': ['40-50%', '75-85%', '80-90%', '88-95%'],\n    'Scalability': ['Excellent', 'Excellent', 'Good', 'Good'],\n    'Flexibility': ['Poor', 'Poor', 'Excellent', 'Excellent']\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df\n\n\n\n\n\n\n\n\nApproach\nSetup Cost\nPer-Item Cost\nSetup Time\nAccuracy (simple)\nAccuracy (complex)\nScalability\nFlexibility\n\n\n\n\n0\nRegex/Keywords\n$0\n$0\n1 hour\n60-70%\n40-50%\nExcellent\nPoor\n\n\n1\nTraditional ML\n$500-5000\n$0\n1-4 weeks\n85-90%\n75-85%\nExcellent\nPoor\n\n\n2\nLLM (GPT-3.5)\n$0\n$0.0001\n1-2 hours\n85-90%\n80-90%\nGood\nExcellent\n\n\n3\nLLM (GPT-4)\n$0\n$0.001\n1-2 hours\n90-95%\n88-95%\nGood\nExcellent\n\n\n\n\n\n\n\n\n\n8.4 Hybrid Approaches\nOften, the best solution combines methods:\n\ndef hybrid_extraction(review_text):\n    \"\"\"\n    Hybrid approach: Use simple rules when possible, LLM for hard cases\n    \"\"\"\n    # Step 1: Try simple keyword matching\n    simple_result = extract_category_regex(review_text)\n\n    # Step 2: Check confidence\n    # If the review explicitly mentions category keywords, trust regex\n    text_lower = review_text.lower()\n    explicit_mentions = sum([\n        any(word in text_lower for word in ['coffee', 'brew', 'espresso']),\n        any(word in text_lower for word in ['vacuum', 'suction']),\n        any(word in text_lower for word in ['headphone', 'audio'])\n    ])\n\n    if explicit_mentions &gt; 0 and simple_result != 'Other':\n        # High confidence - use regex result\n        return {\n            'category': simple_result,\n            'method': 'regex',\n            'cost': 0\n        }\n    else:\n        # Low confidence - use LLM\n        llm_result = extract_category_llm(review_text)\n        return {\n            'category': llm_result,\n            'method': 'llm',\n            'cost': 0.0001\n        }\n\n# Test hybrid approach\nprint(\"Hybrid approach results:\\n\")\nfor review in test_reviews[:3]:\n    result = hybrid_extraction(review)\n    print(f\"Review: {review}\")\n    print(f\"  Category: {result['category']} (via {result['method']})\")\n    print(f\"  Cost: ${result['cost']:.6f}\\n\")\n\nHybrid approach results:\n\nReview: This coffee maker brews excellent coffee.\n  Category: Coffee Maker (via regex)\n  Cost: $0.000000\n\nReview: Makes great espresso every morning.\n  Category: Coffee Maker (via regex)\n  Cost: $0.000000\n\nReview: The audio quality is amazing!\n  Category: Headphones (via regex)\n  Cost: $0.000000\n\n\n\nThis hybrid approach saves money by using free regex when confidence is high, falling back to LLM only when needed.\n\n\n\n\n\n\nTip\n\n\n\nHybrid Strategy Benefits: - Use regex/keywords for 60-80% of cases (free, fast) - Use LLM for ambiguous 20-40% (accuracy boost where it matters) - Best of both worlds: low cost + high accuracy"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practical-considerations-and-best-practices",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practical-considerations-and-best-practices",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "9. Practical Considerations and Best Practices",
    "text": "9. Practical Considerations and Best Practices\n\n9.1 Rate Limiting and API Quotas\nAPIs have limits. Handle them gracefully:\n\nimport time\nfrom datetime import datetime\n\ndef extract_with_rate_limiting(texts, extract_func, requests_per_minute=60):\n    \"\"\"\n    Extract features with rate limiting to avoid API errors\n\n    Args:\n        texts: List of texts to process\n        extract_func: Function that extracts from one text\n        requests_per_minute: Max API calls per minute\n\n    Returns:\n        List of extracted features\n    \"\"\"\n    results = []\n    delay = 60 / requests_per_minute  # Seconds between requests\n\n    for i, text in enumerate(texts):\n        # Extract\n        result = extract_func(text)\n        results.append(result)\n\n        # Progress update\n        if (i + 1) % 10 == 0:\n            print(f\"Processed {i+1}/{len(texts)} texts...\")\n\n        # Rate limit (except for last item)\n        if i &lt; len(texts) - 1:\n            time.sleep(delay)\n\n    return results\n\n# Example usage (simulated)\nprint(\"Processing with rate limiting:\")\nprint(f\"Rate: 60 requests/minute (1 per second)\")\nprint(f\"For 100 texts, this will take ~100 seconds\\n\")\n\n# In practice:\n# results = extract_with_rate_limiting(my_texts, extract_sentiment_robust, requests_per_minute=60)\n\nProcessing with rate limiting:\nRate: 60 requests/minute (1 per second)\nFor 100 texts, this will take ~100 seconds\n\n\n\n\n\n9.2 Error Handling and Retries\nNetworks fail. APIs timeout. Handle it:\n\ndef extract_with_retry(text, extract_func, max_retries=3, backoff=2):\n    \"\"\"\n    Extract with exponential backoff retry logic\n\n    Args:\n        text: Text to extract from\n        extract_func: Extraction function\n        max_retries: Maximum retry attempts\n        backoff: Backoff multiplier (2 = double wait time each retry)\n    \"\"\"\n    wait_time = 1  # Start with 1 second wait\n\n    for attempt in range(max_retries):\n        try:\n            result = extract_func(text)\n            return result\n        except Exception as e:\n            if attempt == max_retries - 1:\n                # Last attempt failed\n                print(f\"Failed after {max_retries} attempts: {e}\")\n                return None\n            else:\n                # Retry with exponential backoff\n                print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\")\n                time.sleep(wait_time)\n                wait_time *= backoff\n\n    return None\n\n# Example usage\nprint(\"Example retry behavior (simulated):\")\nprint(\"Attempt 1 fails → wait 1s\")\nprint(\"Attempt 2 fails → wait 2s\")\nprint(\"Attempt 3 succeeds → return result\")\n\nExample retry behavior (simulated):\nAttempt 1 fails → wait 1s\nAttempt 2 fails → wait 2s\nAttempt 3 succeeds → return result\n\n\n\n\n9.3 Caching Results to Avoid Reprocessing\nNever process the same text twice:\n\nclass LLMCache:\n    \"\"\"Simple cache for LLM extraction results\"\"\"\n\n    def __init__(self):\n        self.cache = {}\n\n    def get(self, text):\n        \"\"\"Get cached result if available\"\"\"\n        # Use hash of text as key\n        key = hash(text)\n        return self.cache.get(key)\n\n    def set(self, text, result):\n        \"\"\"Store result in cache\"\"\"\n        key = hash(text)\n        self.cache[key] = result\n\n    def extract_with_cache(self, text, extract_func):\n        \"\"\"Extract with caching\"\"\"\n        # Check cache first\n        cached = self.get(text)\n        if cached is not None:\n            return cached\n\n        # Not in cache - extract and store\n        result = extract_func(text)\n        self.set(text, result)\n        return result\n\n# Usage example\ncache = LLMCache()\n\ntexts_with_duplicates = [\n    \"This is great!\",\n    \"This is terrible.\",\n    \"This is great!\",  # Duplicate - will use cache\n    \"This is okay.\",\n    \"This is great!\"   # Duplicate - will use cache\n]\n\nprint(\"Processing with cache:\")\nfor text in texts_with_duplicates:\n    # In practice, would call API\n    cached_result = cache.get(text)\n    if cached_result:\n        print(f\"'{text}' → CACHED\")\n    else:\n        # Simulate extraction\n        result = \"positive\"  # Simulated\n        cache.set(text, result)\n        print(f\"'{text}' → EXTRACTED (API call)\")\n\nProcessing with cache:\n'This is great!' → EXTRACTED (API call)\n'This is terrible.' → EXTRACTED (API call)\n'This is great!' → CACHED\n'This is okay.' → EXTRACTED (API call)\n'This is great!' → CACHED\n\n\n\n\n9.4 Logging and Monitoring\nTrack your extractions for debugging:\n\nimport logging\nfrom datetime import datetime\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\ndef extract_with_logging(text, extract_func):\n    \"\"\"Extract with detailed logging\"\"\"\n    start_time = datetime.now()\n\n    logging.info(f\"Starting extraction for text: {text[:50]}...\")\n\n    try:\n        result = extract_func(text)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        logging.info(f\"Extraction successful in {duration:.2f}s: {result}\")\n\n        return result\n\n    except Exception as e:\n        duration = (datetime.now() - start_time).total_seconds()\n        logging.error(f\"Extraction failed after {duration:.2f}s: {e}\")\n        return None\n\n# Example\nprint(\"Extraction with logging:\")\n# result = extract_with_logging(\"Test review\", extract_sentiment_robust)\n\nExtraction with logging:"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#real-world-example-complete-pipeline",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#real-world-example-complete-pipeline",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "10. Real-World Example: Complete Pipeline",
    "text": "10. Real-World Example: Complete Pipeline\nLet’s put it all together with a complete, realistic example:\n\n# Scenario: Extract features from customer support tickets\n# Use those features to predict ticket priority\n\n# Sample support tickets\ntickets_data = {\n    'ticket_id': [1, 2, 3, 4, 5],\n    'ticket_text': [\n        \"My account is locked and I can't access my billing information. This is urgent!\",\n        \"I have a question about your pricing plans. No rush.\",\n        \"The app keeps crashing every time I try to upload a file. Very frustrating!\",\n        \"Can you explain how the export feature works?\",\n        \"CRITICAL: Production server is down! Need immediate assistance!\"\n    ],\n    'actual_priority': ['high', 'low', 'medium', 'low', 'critical']  # Ground truth\n}\n\ndf_tickets = pd.DataFrame(tickets_data)\n\n# Step 1: Define extraction function\ndef extract_ticket_features(ticket_text):\n    \"\"\"\n    Extract features from support ticket\n    \"\"\"\n    # In real implementation, this would call OpenAI API\n    # For this example, we'll simulate the extraction\n\n    # Simulated LLM extraction logic\n    text_lower = ticket_text.lower()\n\n    # Determine urgency\n    if any(word in text_lower for word in ['critical', 'urgent', 'down', 'broken']):\n        urgency = 'high'\n    elif any(word in text_lower for word in ['frustrating', 'issue', 'problem']):\n        urgency = 'medium'\n    else:\n        urgency = 'low'\n\n    # Determine category\n    if any(word in text_lower for word in ['billing', 'account', 'payment']):\n        category = 'Billing'\n    elif any(word in text_lower for word in ['crash', 'bug', 'error', 'down']):\n        category = 'Technical'\n    else:\n        category = 'General'\n\n    # Sentiment\n    if any(word in text_lower for word in ['critical', 'frustrating', 'broken']):\n        sentiment = 'negative'\n    else:\n        sentiment = 'neutral'\n\n    return {\n        'urgency': urgency,\n        'category': category,\n        'sentiment': sentiment,\n        'has_exclamation': '!' in ticket_text,\n        'text_length': len(ticket_text)\n    }\n\n# Step 2: Extract features for all tickets\nprint(\"Extracting features from tickets...\\n\")\nextracted_features = []\n\nfor text in df_tickets['ticket_text']:\n    features = extract_ticket_features(text)\n    extracted_features.append(features)\n\ndf_features = pd.DataFrame(extracted_features)\n\n# Step 3: Combine with original data\ndf_complete = pd.concat([df_tickets, df_features], axis=1)\n\nprint(\"Extracted features:\")\nprint(df_complete[['ticket_id', 'urgency', 'category', 'sentiment']].head())\n\n# Step 4: Encode for ML\nle_urgency = LabelEncoder()\nle_category = LabelEncoder()\nle_sentiment = LabelEncoder()\n\ndf_complete['urgency_encoded'] = le_urgency.fit_transform(df_complete['urgency'])\ndf_complete['category_encoded'] = le_category.fit_transform(df_complete['category'])\ndf_complete['sentiment_encoded'] = le_sentiment.fit_transform(df_complete['sentiment'])\ndf_complete['has_exclamation_int'] = df_complete['has_exclamation'].astype(int)\n\n# Step 5: Train ML model\nfeature_cols = ['urgency_encoded', 'category_encoded', 'sentiment_encoded',\n                'has_exclamation_int', 'text_length']\n\nX = df_complete[feature_cols]\ny = df_complete['actual_priority']\n\n# Note: In practice, you'd have more data and do train-test split\n# This is just a demonstration\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=3, random_state=42)\nclf.fit(X, y)\n\n# Step 6: Make predictions\ndf_complete['predicted_priority'] = clf.predict(X)\n\n# Step 7: Evaluate\nprint(\"\\n\\nResults:\")\nprint(df_complete[['ticket_id', 'actual_priority', 'predicted_priority']])\n\naccuracy = (df_complete['actual_priority'] == df_complete['predicted_priority']).mean()\nprint(f\"\\nAccuracy: {accuracy:.1%}\")\n\nExtracting features from tickets...\n\nExtracted features:\n   ticket_id urgency   category sentiment\n0          1    high    Billing   neutral\n1          2     low    General   neutral\n2          3  medium  Technical  negative\n3          4     low    General   neutral\n4          5    high  Technical  negative\n\n\nResults:\n   ticket_id actual_priority predicted_priority\n0          1            high               high\n1          2             low                low\n2          3          medium             medium\n3          4             low                low\n4          5        critical           critical\n\nAccuracy: 100.0%\n\n\nThis complete pipeline shows the real workflow: 1. Define extraction prompt/function 2. Extract features from text 3. Validate and convert to DataFrame 4. Encode categorical variables 5. Train ML model on extracted features 6. Evaluate performance\nIn production, you’d add: - Error handling and retries - Rate limiting - Caching - Logging - Cost tracking - Quality monitoring\nBut the core pattern remains the same: text → LLM extraction → features → ML model → predictions."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#summary",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#summary",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Summary",
    "text": "Summary\nLLMs are powerful feature engineering tools that transform unstructured text into structured, ML-ready data. The key advantage is zero-shot learning—you can extract information without training data, just clear prompts. But they’re not magic bullets.\nYou’ve learned to write effective extraction prompts: clear task descriptions, specific output formats (JSON), few-shot examples when needed, and iterative refinement based on errors. You know how to parse and validate LLM responses, handle malformed JSON, check for invalid categories, and identify when extraction fails.\nCost matters. GPT-3.5 costs $0.0005 per 1K tokens; GPT-4 costs 60x more. For simple extraction on large datasets, that difference is huge. Start with GPT-3.5, upgrade only if quality demands it. Calculate costs before processing thousands of texts. Compare LLM cost to alternatives—sometimes manual labeling or traditional ML is cheaper.\nQuality control is critical. Spot-check extractions manually. Calculate accuracy against ground truth. Identify systematic errors and refine prompts. Decide when quality is “good enough” based on task requirements. Perfect extraction isn’t always necessary—remember that LLM features are inputs to ML models, and small errors might not hurt final performance.\nKnow when to use LLMs versus alternatives. Simple keyword matching works for pattern detection. Traditional ML works when you have labeled data and need very fast inference. LLMs excel at context understanding, handling nuanced language, extracting categories without training, and zero-shot learning. Often, hybrid approaches work best: use simple rules when confident, fall back to LLM for hard cases.\nIntegration with ML pipelines is straightforward: extract features, convert to DataFrame, encode categorical variables, train traditional ML models. The LLM doesn’t replace your ML workflow—it enhances it by creating better features from text.\nLLMs are tools, not solutions. They cost money, make mistakes, and sometimes overkill. The skill isn’t just using them—it’s knowing when they add value, which model to choose, how to validate output, and how to integrate them into your data science workflow. Use your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practice-exercises",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practice-exercises",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nPrompt Engineering: Write three different prompts to extract sentiment from movie reviews. Test them on 10 reviews. Which prompt gives the most consistent results? Why?\nCost Analysis: Calculate the cost to extract categories from 50,000 product descriptions using GPT-3.5 vs. GPT-4. Assume average description length of 150 characters, prompt of 60 tokens, response of 10 tokens.\nValidation: Extract job seniority levels (Entry/Mid/Senior) from 20 job postings. Manually label them yourself. Calculate your LLM’s accuracy. Identify which extractions failed and why.\nComplete Pipeline: Build a pipeline that: (a) extracts sentiment and rating from customer reviews, (b) converts to DataFrame, (c) trains a classifier to predict if review is helpful (&gt;10 votes), (d) evaluates performance.\nComparison: Take a simple classification task (e.g., categorizing emails as work/personal/spam). Implement three approaches: (1) keyword matching, (2) LLM extraction, (3) hybrid. Compare accuracy and cost.\nQuality Control: Extract product categories from 100 product titles. Create a validation function that flags suspicious extractions (e.g., category not in predefined list). How many would need manual review?"
  },
  {
    "objectID": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#additional-resources",
    "href": "Textbook/Chapter-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#additional-resources",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOpenAI API Documentation - Complete API reference and guides\nAnthropic Claude API - Alternative LLM provider with good context windows\nPrompt Engineering Guide - Comprehensive guide to writing effective prompts\nLangChain Documentation - Framework for building LLM applications (more advanced)\nOpenAI Cookbook - Practical examples and code snippets\nToken Counting Tool - See how text converts to tokens\nBest Practices for Prompt Engineering - Official OpenAI guide"
  },
  {
    "objectID": "Assignments/Chapter 2 - Regression/chapter-2-homework.html",
    "href": "Assignments/Chapter 2 - Regression/chapter-2-homework.html",
    "title": "Chapter 2 Homework: Regression Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational understanding of regression models, diagnostics, and regularization. Part B should be completed with AI assistance (like Gemini CLI) to practice comprehensive model selection and diagnostic analysis at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with three datasets:\nDataset 1: California Housing Dataset (StatLib)\n\nSource: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\nLoad using sklearn.datasets.fetch_california_housing\nTarget: MedHouseVal (median house value in $100,000s)\nFeatures: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n\nUse the following code to load the dataset into a DataFrame named housing_df:\n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\nhousing = fetch_california_housing(as_frame=True)\nhousing_df = housing.frame\nhousing_df.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\nDataset 2: Advertising Dataset (ISLR)\n\nDirect download: https://www.statlearning.com/s/Advertising.csv\nColumns:\n\nSales: Product sales in thousands (TARGET)\nTV: TV advertising budget in thousands\nRadio: Radio advertising budget in thousands\nNewspaper: Newspaper advertising budget in thousands\n\nNote: The file includes an Unnamed: 0 index column. Drop it after loading.\n\nUse the following code to load the dataset into a DataFrame named advertising_df:\n\nimport pandas as pd\n\nadvertising_url = \"https://www.statlearning.com/s/Advertising.csv\"\nadvertising_df = pd.read_csv(advertising_url)\nadvertising_df = advertising_df.drop(columns=[\"Unnamed: 0\"])\nadvertising_df.head()\n\n\n\n\n\n\n\n\nTV\nradio\nnewspaper\nsales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\nDataset 3: Bike Sharing Dataset (UCI)\n\nSource: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\nUse the day.csv file from the dataset\nTarget: cnt (total rentals)\nKey features: temp, atemp (highly correlated), hum, windspeed, season, workingday\nNote: instant and dteday are identifiers and should be dropped before modeling.\nDownload the dataset from: https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset\n\nUse the following code to load the dataset into a DataFrame named bikes_df:\n\nimport pandas as pd\n\n# Once you have the data downloaded, CHANGE THE CODE BELOW to point to where you have the file saved\n# bikes_df = pd.read_csv(\"day.csv\")\n# bikes_df = bikes_df.drop(columns=[\"instant\", \"dteday\"])\n# bikes_df.head()"
  },
  {
    "objectID": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#instructions",
    "href": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#instructions",
    "title": "Chapter 2 Homework: Regression Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational understanding of regression models, diagnostics, and regularization. Part B should be completed with AI assistance (like Gemini CLI) to practice comprehensive model selection and diagnostic analysis at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with three datasets:\nDataset 1: California Housing Dataset (StatLib)\n\nSource: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\nLoad using sklearn.datasets.fetch_california_housing\nTarget: MedHouseVal (median house value in $100,000s)\nFeatures: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n\nUse the following code to load the dataset into a DataFrame named housing_df:\n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\nhousing = fetch_california_housing(as_frame=True)\nhousing_df = housing.frame\nhousing_df.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\nDataset 2: Advertising Dataset (ISLR)\n\nDirect download: https://www.statlearning.com/s/Advertising.csv\nColumns:\n\nSales: Product sales in thousands (TARGET)\nTV: TV advertising budget in thousands\nRadio: Radio advertising budget in thousands\nNewspaper: Newspaper advertising budget in thousands\n\nNote: The file includes an Unnamed: 0 index column. Drop it after loading.\n\nUse the following code to load the dataset into a DataFrame named advertising_df:\n\nimport pandas as pd\n\nadvertising_url = \"https://www.statlearning.com/s/Advertising.csv\"\nadvertising_df = pd.read_csv(advertising_url)\nadvertising_df = advertising_df.drop(columns=[\"Unnamed: 0\"])\nadvertising_df.head()\n\n\n\n\n\n\n\n\nTV\nradio\nnewspaper\nsales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\nDataset 3: Bike Sharing Dataset (UCI)\n\nSource: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\nUse the day.csv file from the dataset\nTarget: cnt (total rentals)\nKey features: temp, atemp (highly correlated), hum, windspeed, season, workingday\nNote: instant and dteday are identifiers and should be dropped before modeling.\nDownload the dataset from: https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset\n\nUse the following code to load the dataset into a DataFrame named bikes_df:\n\nimport pandas as pd\n\n# Once you have the data downloaded, CHANGE THE CODE BELOW to point to where you have the file saved\n# bikes_df = pd.read_csv(\"day.csv\")\n# bikes_df = bikes_df.drop(columns=[\"instant\", \"dteday\"])\n# bikes_df.head()"
  },
  {
    "objectID": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#part-a-by-hand-no-ai-assistance",
    "href": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#part-a-by-hand-no-ai-assistance",
    "title": "Chapter 2 Homework: Regression Models",
    "section": "Part A: By Hand (No AI Assistance)",
    "text": "Part A: By Hand (No AI Assistance)\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational understanding of regression diagnostics and regularization.\n\nQuestion 1 (4 points)\nLoad the Advertising dataset and fit a Linear Regression model to predict Sales using all three advertising channels.\n\nExtract and display the coefficients for each feature\nInterpret each coefficient in plain language (e.g., “For every additional thousand dollars spent on TV advertising, Sales increase by…”)\nWhich advertising channel has the strongest impact on Sales?\nCalculate R² for this model. What does this value tell you about the model’s performance?\n\n\n\n\nQuestion 2 (5 points)\nContinuing with the Advertising dataset, manually calculate the following for your linear regression model:\n\nCalculate the residuals (actual - predicted) for the first 5 data points\nCompute the Mean Squared Error (MSE) manually from all residuals\nCompute the Mean Absolute Error (MAE) manually from all residuals\nVerify your calculations match sklearn’s mean_squared_error and mean_absolute_error\nWhich metric (MSE or MAE) is more sensitive to outliers? Why?\n\n\n\n\nQuestion 3 (5 points)\nCreate diagnostic plots for the linear regression model from Question 1:\n\nCreate a scatter plot of residuals vs. fitted values (predictions)\nAdd a horizontal line at y=0\nWhat pattern would you expect to see if the model assumptions are satisfied?\nDo you observe any concerning patterns in your plot? What might they indicate?\nCreate a histogram of the residuals. Do they appear approximately normally distributed?\n\n\n\n\nQuestion 4 (5 points)\nLoad the California Housing dataset. Calculate the correlation matrix for all numerical features.\n\nDisplay the correlation matrix\nWhich pair of features has the highest correlation (excluding correlations with the target)?\nWhat is multicollinearity and why is it problematic for linear regression?\nBased on the correlation matrix, do you see evidence of multicollinearity in this dataset?\n\n\n\n\nQuestion 5 (5 points)\nUsing the California Housing dataset, fit three models predicting MedHouseVal:\n\nModel 1: Linear Regression (no regularization)\nModel 2: Ridge Regression with alpha=1.0\nModel 3: Lasso Regression with alpha=1.0\n\nFor each model:\n\nFit the model using all features\nDisplay the coefficients\nCalculate R² on the test set (use 80/20 train/test split, random_state=42)\nHow do the coefficients differ between the three models? Which model has the smallest coefficient magnitudes?\n\n\n\n\nQuestion 6 (4 points)\nExplain in your own words (3-4 sentences each):\n\nWhat is the purpose of regularization in regression? When would you choose to use it?\nWhat is the difference between Ridge (L2) and Lasso (L1) regularization?\nWhy might Lasso set some coefficients to exactly zero while Ridge does not?\nIn what situation would you prefer Lasso over Ridge?\n\n\n\n\nQuestion 7 (5 points)\nUsing the Advertising dataset, create polynomial features of degree 2 for the TV feature only.\n\nUse PolynomialFeatures(degree=2, include_bias=False) to create polynomial features\nFit a Linear Regression model using the original features plus the polynomial features of TV\nCompare R² with the original linear model (without polynomial features)\nDid adding polynomial features improve performance? Why might this be the case?\nPlot Sales vs. TV budget, showing both the linear and polynomial model predictions\n\n\n\n\nQuestion 8 (4 points)\nYou fit a linear regression model and observe the following residual plot:\n\nFor low fitted values: residuals are mostly positive\nFor medium fitted values: residuals are mostly negative\nFor high fitted values: residuals are mostly positive\n\n\nWhat assumption of linear regression might be violated?\nWhat does this pattern suggest about the relationship between features and target?\nSuggest two approaches to address this issue.\n\n\n\n\nQuestion 9 (4 points)\nUsing the Bike Sharing dataset (day.csv), fit a Ridge regression model with alpha=10.0. Use the features temp, atemp, hum, windspeed, season, and workingday to predict cnt.\n\nSplit data into train/test (80/20, random_state=42)\nFit the model and calculate train R² and test R²\nNow fit Ridge models with alpha values: [0.1, 1.0, 10.0, 100.0]\nFor each alpha, record the test R². Which alpha gives the best test performance?\nWhat happens to the coefficients as alpha increases?\n\n\n\n\nQuestion 10 (4 points)\nWrite a function called plot_residual_diagnostics that:\n\nTakes a fitted model, X_test, and y_test as inputs\nCalculates residuals\nCreates two plots side by side: (1) residuals vs fitted values, (2) histogram of residuals\nReturns the residuals array\n\nTest your function on a linear regression model fitted to the Advertising dataset.\nBased on the plots, do the residuals appear randomly distributed? Are they centered around zero?"
  },
  {
    "objectID": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#part-b-with-ai-assistance",
    "href": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#part-b-with-ai-assistance",
    "title": "Chapter 2 Homework: Regression Models",
    "section": "Part B: With AI Assistance",
    "text": "Part B: With AI Assistance\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you perform comprehensive regression analysis. The goal is to practice systematic model selection and diagnostic analysis.\nImportant: For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n\nQuestion 11 (5 points)\nUse AI to perform a comprehensive grid search for Ridge regression on the California Housing dataset.\nTest alpha values: [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nYour code should:\n\nPerform grid search with cross-validation\nPlot test R² vs. alpha (use log scale for x-axis)\nIdentify the optimal alpha value\nShow the coefficients for the best model\n\nWhat happens to model performance as alpha increases? Is there a point where it gets worse?\nDeliverables:\n\nCode\nPerformance vs. alpha plot\nBest alpha and corresponding R²\nWritten interpretation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 12 (6 points)\nPrompt AI to create a comprehensive comparison of regularization techniques on the Bike Sharing dataset (which has multicollinearity between temp and atemp). Use the features temp, atemp, hum, windspeed, season, and workingday to predict cnt.\nCompare:\n\nLinear Regression (no regularization)\nRidge with optimal alpha\nLasso with optimal alpha\nElastic Net with optimal alpha\n\nYour code should:\n\nUse GridSearchCV to find optimal alpha for each regularized method\nCreate a bar plot comparing test R² across all methods\nCreate a heatmap showing coefficient values for all methods\nIdentify which features Lasso set to zero\n\nWrite a paragraph discussing how regularization helped with the multicollinearity problem.\nDeliverables:\n\nCode\nPerformance comparison plot\nCoefficient heatmap\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 13 (6 points)\nUse AI to test polynomial regression of different degrees on the Advertising dataset using the TV feature.\nTest polynomial degrees: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nYour code should:\n\nFit polynomial models for each degree\nCalculate train and test MSE for each\nPlot both MSE curves vs. polynomial degree\nCreate a visualization showing the fitted curves for degrees [1, 3, 5, 8] overlaid on the actual data\n\nAt what degree does overfitting begin to occur? How can you tell?\nDeliverables:\n\nCode\nMSE vs. degree plot\nFitted curves visualization\nWritten interpretation (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 14 (6 points)\nPrompt AI to generate a complete regression diagnostics report for a linear regression model on the California Housing dataset.\nThe report should include:\n\nResiduals vs. Fitted values plot\nResiduals vs. each feature plot (to detect non-linearity with specific features)\nHistogram of residuals\nScale-location plot (sqrt of standardized residuals vs. fitted)\nSummary statistics of residuals (mean, median, std, min, max)\n\nBased on these diagnostics, are the linear regression assumptions satisfied? Which assumptions (if any) are violated?\nDeliverables:\n\nCode for complete diagnostics\nAll diagnostic plots\nWritten assessment of assumptions (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 15 (5 points)\nUse AI to create a “coefficient path” visualization for Lasso regression.\nUsing the California Housing dataset:\n\nFit Lasso models with alpha values ranging from 0.001 to 1000 (log scale, 50 points)\nTrack the coefficient values for each feature at each alpha\nCreate a line plot showing how each coefficient changes with alpha\nMark the point where each coefficient becomes zero\n\nWhich features are most important (last to be zeroed out)? Which are least important?\nDeliverables:\n\nCode\nCoefficient path visualization\nWritten interpretation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 16 (6 points)\nPrompt AI to compare polynomial regression with and without regularization.\nUsing the Advertising dataset with TV feature:\n\nFit polynomial models of degree 10 with:\n\nNo regularization (Linear Regression)\nRidge regularization (optimal alpha)\nLasso regularization (optimal alpha)\n\nCompare test MSE for all three\nVisualize the fitted curves for all three\nShow the coefficient values for all three\n\nDoes regularization help control overfitting in high-degree polynomials? How?\nDeliverables:\n\nCode\nPerformance comparison\nFitted curves visualization\nCoefficient comparison\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 17 (5 points)\nUse AI to investigate the impact of feature scaling on Ridge and Lasso regression.\nUsing the California Housing dataset:\n\nFit Ridge and Lasso models (alpha=1.0) with:\n\nNo scaling (raw features)\nStandardScaler (z-score normalization)\nMinMaxScaler (0-1 scaling)\n\nCompare test R² for all combinations\nShow how coefficients differ with different scaling approaches\n\nWhy does scaling matter for regularized regression but not for standard linear regression?\nDeliverables:\n\nCode\nPerformance comparison table\nCoefficient comparison\nWritten explanation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 18 (6 points)\nPrompt AI to create an automated feature selection pipeline using Lasso.\nThe pipeline should:\n\nTake a dataset and alpha value as input\nFit Lasso regression\nIdentify features with non-zero coefficients\nRefit a Linear Regression model using only selected features\nCompare performance with using all features\nGenerate a report showing selected features and performance comparison\n\nTest this on the California Housing dataset with alpha=0.1.\nHow many features were selected? Did feature selection improve or hurt performance?\nDeliverables:\n\nPipeline code\nFeature selection results\nPerformance comparison\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 19 (6 points)\nUse AI to create a comprehensive model selection workflow that combines polynomial features with regularization.\nUsing the Advertising dataset:\n\nTest polynomial degrees [1, 2, 3, 4, 5]\nFor each degree, test Ridge, Lasso, and Linear Regression\nUse cross-validation to evaluate each combination\nCreate a heatmap showing performance (columns=method, rows=polynomial degree)\nIdentify the best combination\n\nYour code should:\n\nAutomatically test all combinations\nGenerate comprehensive comparison visualizations\nSelect the best model based on cross-validation score\nEvaluate the best model on a held-out test set\n\nDeliverables:\n\nCode for complete workflow\nPerformance heatmap\nBest model identification\nFinal test set results\nWritten summary (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 20 (7 points)\nFor this final question, ask AI to help you conduct a complete regression analysis from start to finish on the California Housing dataset.\nYour analysis should include:\n\nExploratory Data Analysis:\n\nSummary statistics\nCorrelation analysis\nMulticollinearity detection (VIF scores)\n\nModel Development:\n\nTrain/validation/test split (60/20/20)\nFit multiple models: Linear, Ridge, Lasso, Polynomial (with regularization)\nHyperparameter tuning using validation set\n\nModel Diagnostics:\n\nComplete diagnostic plots for best model\nResidual analysis\nAssumption checking\n\nModel Selection:\n\nCompare all models on validation set\nSelect best model\nEvaluate on test set (only once!)\n\nInterpretation:\n\nFeature importance\nCoefficient interpretation\nBusiness insights\n\n\nWrite a 1-page executive summary (300-400 words) that explains:\n\nData characteristics and challenges (multicollinearity, etc.)\nModels tested and why\nHow the best model was selected\nPerformance on test set and what it means\nKey features driving predictions\nRecommendations and limitations\n\nDeliverables:\n\nComplete analysis code (well-commented)\nAll visualizations (EDA, diagnostics, comparisons)\nExecutive summary\nYour AI prompt(s)\nReflection: How did understanding diagnostics and regularization improve your modeling approach compared to just trying different models blindly?"
  },
  {
    "objectID": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#submission-guidelines",
    "href": "Assignments/Chapter 2 - Regression/chapter-2-homework.html#submission-guidelines",
    "title": "Chapter 2 Homework: Regression Models",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nSubmit a ZIP file containing:\n\nCode files: All .py files or Jupyter notebooks\nVisualizations folder: All plots and charts generated\nWritten responses: A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\nData: Include the datasets if you made any modifications"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.2 - EDA First Look.html",
    "href": "In-class activities/Chapter 1/1.2 - EDA First Look.html",
    "title": "Chapter 1.2: EDA First Look",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: practice the first pass of EDA and ask good questions about the data.\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.2 - EDA First Look.html#what-is-pandas",
    "href": "In-class activities/Chapter 1/1.2 - EDA First Look.html#what-is-pandas",
    "title": "Chapter 1.2: EDA First Look",
    "section": "What is Pandas?",
    "text": "What is Pandas?\nData is typically stored in a CSV file, you can see an example here\nHow do we even work with this? For example, how could we find the average number of votes? Or all Republicans? We need to load it into Python to accomplish this.\nPandas is a Python package built for manipulating data.\nTo read a CSV file, you use the Pandas command .read_csv() and tell it where the file is located.\n\nBasics of file storage\nFiles are stored in folders. You can access folders by typing their name. Load example image in images folder.\nTo go backwards (i.e. to a “parent” folder), use .., for example ../images would go backwards one folder, then look for the images folder. Load example images in week02/images folder.\n\n\nLoading the file in Pandas\nWith that, we’re ready to load the CSV file using Pandas .read_csv(). The CSV file we just looked at it stored here: https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv. Let’s load it using the .read_csv() command.\n\n# Load the CSV file using Pandas .read_csv function\ndf = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv')\n\nNow, how do we actually look at the data? Try below.\n\n# Look at the data"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.2 - EDA First Look.html#subsetting-the-data",
    "href": "In-class activities/Chapter 1/1.2 - EDA First Look.html#subsetting-the-data",
    "title": "Chapter 1.2: EDA First Look",
    "section": "Subsetting the data",
    "text": "Subsetting the data\nTypically, we want to look at only subsets of the data. For example, who survived? Who died? How many children survived?\nIn Pandas, subsets are selected using square brackets []. For example:\n\n# Select the \"Survived\" column\ndf['Survived']\n\nWe only want rows where it says they Survived.\n\n# Find which rows have \"Survived\" equal to one (meaning they did survive)\ndf[\"Survived\"] == 1\n\nNote that this shows if a person survived or not, but it didn’t actually select those people.\nWhat we did above using df['Survived'] == 1 is called creating a mask. A mask means “what are that rows that satifsy X?”. In our case, it was “what are the rows that are people who survived?”.\nTo actually select those rows, we use those square brackets. If I want to select rows, that’s what square brackets [] are for.\n\nsurvived_mask = df[\"Survived\"] == 1\n\nsurvived_df = df[survived_mask]\n\nsurvived_df.head()\n\nIn summary: - Square brackets mean to select things according to the criteria inside them - The criteria can be a column name, which selects the whole column - The criteria can be a list of True/False values, telling it whether or not to select a row\n\nPractice:\n\nSelect all people who died (“Survived” equals zero)\nSelect all females\nSelect all people under the age of 18\nSelect all people under the age of 18, and then select the people under 18 who survived\n\n\n# 1. Select all people who survived\n\n\n\n# 2. Select all females\n\n\n# 3. Select all people under the age of 18\n\n\n# 4. Select all people under the age of 18, then select the people under 18 who survived\n\n\n# 4b. You can do #4 either in two separate steps, or in one single step, using \"&\" to combine two conditions,\n# such as &lt;condition 1&gt; & &lt;condition 2&gt;. Try doing it in oine step using &.\n\n\n# 4c. You can use \"&\" to express \"and\". Similarly, you can use \"|\" (a vertical pipe, just about the enter key)\n# on your keyboard) to express \"or\". Use this to select all people who are under 18 OR who survived."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.2 - EDA First Look.html#summarizing-data",
    "href": "In-class activities/Chapter 1/1.2 - EDA First Look.html#summarizing-data",
    "title": "Chapter 1.2: EDA First Look",
    "section": "Summarizing data",
    "text": "Summarizing data\nDataFrames can be summarized by applying a function to them using df.my_function(). Here are some examples:\n\n.mean()\n.max()\n.shape (Similar to the shape of a matrix)\n\n\n# Compute the mean age\n\n\n# Compute the max ticket price\n\n\n# Use .shape to find out how many people on board were under the age of 18"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html",
    "href": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html",
    "title": "Chapter 1.4 and 1.5: Visualization and Testing",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Create appropriate visualizations and write assertions to verify your analysis code."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#loading-the-data",
    "href": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#loading-the-data",
    "title": "Chapter 1.4 and 1.5: Visualization and Testing",
    "section": "Loading the Data",
    "text": "Loading the Data\nWe’ll use the Tips dataset, which comes built into Seaborn. It contains information about restaurant bills and tips, including the total bill, tip amount, day of the week, and whether the customer was a smoker.\n\n# Load the tips dataset from Seaborn\ntips = sns.load_dataset('tips')\n\n\n# Look at your data\n\n\n# How many rows and columns?\n\n\n# Any missing values?\n\n\n# Any weird outliers?"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#choosing-the-right-visualization",
    "href": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#choosing-the-right-visualization",
    "title": "Chapter 1.4 and 1.5: Visualization and Testing",
    "section": "Choosing the Right Visualization",
    "text": "Choosing the Right Visualization\nDifferent questions need different visualizations:\n\nHistogram: “What’s the distribution of X?” (one numeric variable)\nScatter plot: “What’s the relationship between X and Y?” (two numeric variables)\nBar plot: “How does Y compare across categories?” (one category, one numeric)\n\nLet’s see each in action.\n\nHistograms\nA histogram shows how values are distributed. For example: What’s the distribution of total bills?\n\n# Histogram of total_bill column using Seaborn (sns)\n...\n\nIt’s good practice to give our graphs axis labels and a title. Whenever you access a column (e.g. x='total_bill'), it will automatically add that title to the x-axis. However, you may want to include your own title with a nicer name. Also, since no y value is specific, there’s no y-axis label. Let’s add both of these, as well as a title.\n\n# Histogram of total_bill\nsns.???\n# Give the graph a title, as well as x and y labels\n...\n\nplt.show() # This is optional, but it makes sure the graph displays\n\n\n\nScatter Plots\nA scatter plot shows the relationship between two numeric variables. For example: How does tip relate to total bill?\n\n# Scatter plot of total_bill vs tip\n...\n\n# Add a title, x and y axis labels\n...\n\nSee how tips generally increase with larger bills? That’s a positive relationship.\n\n\nBar Plots\nA bar plot compares values across categories. For example: What’s the average tip on each day of the week?\n\n# Bar plot of average tip by day\n...\n\n# Add nice title, x and y axis labels\n...\n\nThe black lines on top of the bars show the confidence interval - they tell you how uncertain we are about the true average."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#practice-visualizations",
    "href": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#practice-visualizations",
    "title": "Chapter 1.4 and 1.5: Visualization and Testing",
    "section": "Practice: Visualizations",
    "text": "Practice: Visualizations\n\nCreate a histogram of tip amounts\nCreate a scatter plot of total_bill vs tip, colored by smoker\nCreate a bar plot showing average total_bill by day\nAdd a title and axis labels to your bar plot\n\n\n# 1. Create a histogram of tip amounts\n# Step 1: Use sns.histplot() with data=tips and x='tip'\n# Step 2: Add plt.show() at the end\n\n\n# 2. Create a scatter plot of total_bill vs tip, colored by smoker\n# Step 1: Use sns.scatterplot() with data=tips, x='total_bill', y='tip'\n# Step 2: Add hue='smoker' to color by smoking status\n# Step 3: Add plt.show()\n\n\n# 3. Create a bar plot showing average total_bill by day\n# Step 1: Use sns.barplot() with data=tips, x='day', y='total_bill'\n# Step 2: Add plt.show()\n\n\n# 4. Add a title and axis labels to your bar plot\n# Step 1: Copy your bar plot code from above\n# Step 2: Add plt.title('Your Title Here')\n# Step 3: Add plt.xlabel('X Label') and plt.ylabel('Y Label')\n# Step 4: Add plt.show()"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#introduction-to-assert",
    "href": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#introduction-to-assert",
    "title": "Chapter 1.4 and 1.5: Visualization and Testing",
    "section": "Introduction to assert",
    "text": "Introduction to assert\nWhen you’re analyzing data, things can go wrong in subtle ways. Maybe you filtered wrong, or a calculation gave unexpected results. assert statements help catch these errors early.\nAn assert statement says: “This should be true. If it’s not, something is wrong.”\n\n# This passes silently (the condition is true)\nassert len(tips) &gt; 0\n\n\n# This would fail (the condition is false)\nassert len(tips) &gt; 1000\n\nYou may have noticed, that when the assert fails, it just says “AssertionError”. If you wrote this code a week ago, you may not remember why you thought len(tips) should be greater than 1000. That means lots of time investigating why you thought that was important, and why it’s not passing.\nassert statement allow us to add a comment that will display if it fails. This can be useful for exaclty these kinds of circumstances. To add a message, simply do\nassert(...), \"My message\"\n\nassert len(tips) &gt; 1000, \"Start with 10k rows, after cleaning should be around 2k rows\"\n\nThe syntax is:\nassert condition, \"Error message if condition is False\"\nSome useful patterns: - assert len(df) &gt; 0 - DataFrame isn’t empty - assert (df['col'] &gt; 0).all() - All values in column are positive - assert df['col'].notna().all() - No missing values - assert len(filtered_df) &lt; len(original_df) - Filter actually removed rows\n\nWhy use asserts?\nThey’re like sanity checks. After filtering data, you can assert that the filter worked correctly. Consider the code below, it seems completely reasonable.\n\n# Remove Sunday\nnon_sunday_tips = tips[tips['day'] != 'Sun']\n\nprint(f\"Days besides Sunday has {len(non_sunday_tips)} transactions out of {len(tips)} total\")\n\nEverything runs. However, if you actually look at the data non_sunday_tips, you’ll realize Sunday is still there! The problem is that in this code we abbreviated Sunday to “Sun”, which is incorrect (that’s not how it’s stored in the data). So it removed rows with \"Sun\", but the problem is there’s no rows like this, so it actually didn’t remove anything!\n\n# Remove Sunday\nnon_sunday_tips = tips[tips['day'] != 'Sun']\n\n# Sanity check: we should have fewer rows than the original\nassert len(non_sunday_tips) &lt; len(tips), \"Something's wrong - No rows were removed!\"\n\nprint(f\"Days besides Sunday has {len(non_sunday_tips)} transactions out of {len(tips)} total\")"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#practice-writing-assertions",
    "href": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#practice-writing-assertions",
    "title": "Chapter 1.4 and 1.5: Visualization and Testing",
    "section": "Practice: Writing Assertions",
    "text": "Practice: Writing Assertions\n\nAfter filtering to smokers only, assert that all values in the smoker column are “Yes”\nAssert that all tip values in the original dataset are positive\nAfter filtering to Saturday only, assert the row count is less than the original\nAssert that the mean tip is between $1 and $10 (a reasonable sanity check)\n\n\n# 1. Filter to smokers and assert all are \"Yes\"\n# Step 1: Create smokers_only = tips[tips['smoker'] == 'Yes']\n# Step 2: Assert that (smokers_only['smoker'] == 'Yes').all() is True\n\nsmokers_only = tips[tips['smoker'] == 'Yes']\n\n\n# 2. Assert that all tip values are positive\n# Step 1: Check if (tips['tip'] &gt; 0).all() is True\n# Step 2: Write the assert with a helpful error message\n\n\n# 3. Filter to Saturday and assert row count is smaller\n# Step 1: Create saturday_only = tips[tips['day'] == 'Sat']\n# Step 2: Assert that len(saturday_only) &lt; len(tips)\n\nsaturday_only = tips[tips['day'] == 'Sat']\n\n\n# 4. Assert that mean tip is between $1 and $10\n# Step 1: Compute mean_tip = tips['tip'].mean()\n# Step 2: Assert that 1 &lt; mean_tip &lt; 10\n\nmean_tip = tips['tip'].mean()"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#wrap-up",
    "href": "In-class activities/Chapter 1/1.4-1.5 - Visualization and Testing.html#wrap-up",
    "title": "Chapter 1.4 and 1.5: Visualization and Testing",
    "section": "Wrap-up",
    "text": "Wrap-up\nToday you learned: - How to choose the right visualization (histogram, scatter, bar) - How to create visualizations with Seaborn - How to use assert statements to catch bugs in your analysis\nGet in the habit of adding assertions after any filtering or data transformation. Your future self will thank you!"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Practice computing and interpreting summary statistics to understand data distributions."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#loading-the-data",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#loading-the-data",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "Loading the Data",
    "text": "Loading the Data\nWe’ll use the California Housing dataset, which contains information about housing in various districts in California. Each row represents a district, with features like median income, house age, and median house value.\n\n# Load the California Housing dataset\n# url: https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\ndf = ...\n\n# Inspect your data to see if it looks ok\n\n\n# Check how many rows and columns are in your data"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#using-.describe",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#using-.describe",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "Using .describe()",
    "text": "Using .describe()\nThe .describe() method gives you a quick summary of all numeric columns in your DataFrame. It tells you: - count: how many non-missing values - mean: the average - std: standard deviation (how spread out the data is) - min/max: smallest and largest values - 25%, 50%, 75%: the quartiles (50% is the median)\n\n# Get summary statistics for the entire DataFrame\n...\n\nSee how that gives you a quick overview of every numeric column? You can also run .describe() on a single column:\n\n# Summary of just the median_house_value column\n..."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#individual-statistics",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#individual-statistics",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "Individual Statistics",
    "text": "Individual Statistics\nSometimes you want just one statistic. Pandas gives you methods for each. Take a guess at what each one is:\n\n# Mean (average) house value\n...\n\n\n# Median (middle value) house value\n...\n\n\n# Standard deviation\n...\n\n\nMean vs. Median\nNotice that the mean and median are different! The mean is about $207,000 and the median is about $180,000. When the mean is higher than the median, it usually means there are some very high values pulling the average up. The median is more “robust” because it isn’t affected by extreme values."
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#practice-basic-statistics",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#practice-basic-statistics",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "Practice: Basic Statistics",
    "text": "Practice: Basic Statistics\n\nCompute the mean of median_income\nCompute the median of housing_median_age\nCompare mean vs median for total_rooms - which is larger and why?\nFind the standard deviation of population\n\n\n# 1. Compute the mean of median_income\n# Step 1: Select the median_income column\n# Step 2: Apply the .mean() method\n\n\n# 2. Compute the median of housing_median_age\n# Step 1: Select the housing_median_age column\n# Step 2: Apply the .median() method\n\n\n# 3. Compare mean vs median for total_rooms\n# Compute both and print them out\n# Which is larger? Why do you think that is?\n\n\n# 4. Find the standard deviation of population"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#identifying-outliers",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#identifying-outliers",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "Identifying Outliers",
    "text": "Identifying Outliers\nOutliers are extreme values that might be errors or just unusual cases. One simple way to find them is using percentiles. The .quantile() method lets you find any percentile:\n\n# 1st percentile (only 1% of values are below this)\ndf['median_house_value'].quantile(0.01)\n\n\n# 99th percentile (only 1% of values are above this)\ndf['median_house_value'].quantile(0.99)\n\nValues below the 1st percentile or above the 99th percentile might be outliers worth investigating. You can combine this with filtering to actually see those rows:\n\n# Find districts with extremely high house values (above 99th percentile)\n\n# Start by finding the 99th percentile of the median_house_value column\nhigh_value_threshold = ???\n\n# Create a mask for houses with a median_house_value above this threshold\nmask = ???\n\n# Use this mask to filter to only these rows\nexpensive_districts = df[???]\n\n# Inspect the resulst\nexpensive_districts.head()"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#grouping-with-.groupby",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#grouping-with-.groupby",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "Grouping with .groupby()",
    "text": "Grouping with .groupby()\nHere’s where things get interesting. What if we want to compare statistics across different groups? For example, what’s the average house value for districts near the ocean vs. inland?\nThe ocean_proximity column tells us how close each district is to the ocean. Let’s see what values it has:\n\n# What are the unique values in ocean_proximity?\ndf['ocean_proximity'].unique()\n\nNow let’s find the average house value for each of these categories:\n\n# Average house value by ocean_proximity\ndf.groupby('ocean_proximity')['median_house_value'].mean()\n\nThe syntax is: df.groupby('grouping_column')['column_to_summarize'].statistic()\nYou can use any statistic: .mean(), .median(), .max(), .min(), .count(), etc.\n\n# Maximum house value by ocean_proximity\ndf.groupby('ocean_proximity')['median_house_value'].max()"
  },
  {
    "objectID": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#practice-grouped-statistics",
    "href": "In-class activities/Chapter 1/1.3 - Statistical Summaries.html#practice-grouped-statistics",
    "title": "Chapter 1.3: Statistical Summaries",
    "section": "Practice: Grouped Statistics",
    "text": "Practice: Grouped Statistics\n\nFind average median_income by ocean_proximity\nFind the maximum population for each ocean_proximity category\nFind the median housing_median_age for each ocean_proximity\nWhich ocean_proximity category has the highest average house value? (You already computed this above - just answer the question!)\n\n\n# 1. Average median_income by ocean_proximity\n# Step 1: Group by 'ocean_proximity'\n# Step 2: Select the 'median_income' column\n# Step 3: Apply .mean()\n\n\n# 2. Maximum population for each ocean_proximity category\n# Same pattern as above, but use .max() instead of .mean()\n\n\n# 3. Median housing_median_age for each ocean_proximity\n\n\n# 4. Which ocean_proximity category has the highest average house value?"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.6 - Multicollinearity.html",
    "href": "In-class activities/Chapter 2/2.6 - Multicollinearity.html",
    "title": "Chapter 2.6: Multicollinearity",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Detect multicollinearity using correlation matrices and VIF."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.6 - Multicollinearity.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.6 - Multicollinearity.html#quick-recap",
    "title": "Chapter 2.6: Multicollinearity",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nMulticollinearity = features that are highly correlated with each other\nProblem: coefficients become unstable and hard to interpret\nDetection:\n\nCorrelation matrix: look for |r| &gt; 0.7\nVIF &gt; 5 indicates problematic multicollinearity\nVIF &gt; 10 is severe\n\n\n\n# Load California Housing data\nhousing = fetch_california_housing(as_frame=True)\ndf = housing.frame\ndf.head()\n\n\n# Select all features for X except for the target variable MedHouseVal\n..."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.6 - Multicollinearity.html#practice",
    "href": "In-class activities/Chapter 2/2.6 - Multicollinearity.html#practice",
    "title": "Chapter 2.6: Multicollinearity",
    "section": "Practice",
    "text": "Practice\n\n1. Create correlation matrix with df[features].corr()\n\n# Calculate and show the correlation matrix between all features in X\n...\n\n\n\n2. Create heatmap with sns.heatmap(corr_matrix, annot=True)\n\n# Turn this into a heatmap. After making your initial heatmap, consider ways you could improve it visually, then implement those changes.\n...\n\n\n\n3. List all pairs with correlation &gt; 0.7 (or &lt; -0.7)\n\n# Find all pairs of variables with correlation above 0.7 (nothing special about this value, just a high correlation)\n...\n\nYour observation: Which features are highly correlated? Does this make sense intuitively?\n(Write your answer here)\n\n\n4. Calculate VIF for each feature\nVIF measures how much the variance of a coefficient is inflated due to correlation with other features.\n\n# Calculate VIF for all pairs of features. Turn the output into a DataFrame.\n\nWhat do you learn from this VIF?\n(Write your answer here)\n\n\n5. Remove one problematic feature, recalculate VIF - did it improve?\n\n# Step 1: Choose a feature with high VIF to remove, then remove it from X\n...\n\n# Step 2: Recalculate VIF, what changes?\n...\n\nYour analysis: Did removing the feature improve the VIF values? If there are still features with high VIF, what would you do next?\n(Write your answer here)\n\n\n6. See the effect on coefficients\nFit a linear regression model using all variables in X, and then all variables in X minus the one you removed. How did the model coefficients change?\n\n# Fit models with and without the problematic feature\n...\n\n# Compare coefficients from this model with coefficients from full model (all features in X)\n..."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html",
    "href": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html",
    "title": "Chapter 2.3: Evaluation Metrics",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Calculate and interpret MSE, MAE, RMSE, and R² to understand model performance."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html#quick-recap",
    "title": "Chapter 2.3: Evaluation Metrics",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nMSE: Average of squared errors - penalizes large errors heavily\nRMSE: Square root of MSE - same units as target variable\nMAE: Average of absolute errors - robust to outliers\nR²: Proportion of variance explained (0 to 1, higher is better)\n\n\n# Load the Diamonds dataset\ndiamonds = sns.load_dataset('diamonds')\ndiamonds.head()\n\n\n# X = carats column, y = price column\nX = ...\ny = ...\n\n# Train-test split the data (80/20)\n\n\n# Fit the model\n\n\n# Get predictions"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html#practice",
    "href": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html#practice",
    "title": "Chapter 2.3: Evaluation Metrics",
    "section": "Practice",
    "text": "Practice\n\n1. Fit regression: carat → price, calculate predictions\n(Already done above - just verify the predictions look reasonable)\n\n# Look at first 5 actual vs predicted values\n\n\n\n2. By hand - Calculate MSE: ((y_true - y_pred)**2).mean()\n\n# Step 1: Calculate the errors (residuals)\nerrors = y_test - y_pred\n\n# Step 2: Square the errors\n\n\n# Step 3: Take the mean\n\n\n# Print it out\n\n\n\n3. By hand - Calculate MAE: (abs(y_true - y_pred)).mean()\n\n# Step 1: Take absolute value of errors\n\n\n# Step 2: Take the mean\n\n\n# Print it out\n\n\n\n4. Use sklearn functions to verify your calculations\n\n# Calculate MSE, MAE, and R² using sklearn\n\nQuestion: Which metric is most interpretable for this problem? Why?\n(Write your answer here - hint: RMSE is in the same units as price)\n\n\n5. Add 5 extreme outliers to y_test, recalculate MSE vs MAE - which changed more?\n\n# Create a copy of y_test with outliers\ny_test_outliers = y_test.copy()\n\n# Add 5 extreme outliers (errors of $50,000)\noutlier_indices = y_test_outliers.index[:5]\ny_pred_outliers = y_pred.copy()\ny_pred_outliers[:5] = y_pred_outliers[:5] + 50000  # Predictions are way off\n\n# Calculate new metrics with outliers\n\n\n# Print results (write summary in markdown cell below)\n\nYour interpretation: Which metric changed more? Why does this happen?\n(Write your answer here)\n\n\n6. Calculate Adjusted R²\nAdjusted R² penalizes adding features that don’t help:\n\\[\\text{Adjusted } R^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\\]\nWhere: - n = number of samples - p = number of features\n\n# Calculate Adjusted R²\n\n\n# Compare R² to adjusted R²\n\nQuestion: How are they different? What does this tell us?\n(Write your answer here)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html#summary",
    "href": "In-class activities/Chapter 2/2.3 - Evaluation Metrics.html#summary",
    "title": "Chapter 2.3: Evaluation Metrics",
    "section": "Summary",
    "text": "Summary\n\n\n\nMetric\nValue\nInterpretation\n\n\n\n\nMSE\n\nSensitive to large errors\n\n\nRMSE\n\nAverage error in dollars\n\n\nMAE\n\nRobust to outliers\n\n\nR²\n\nVariance explained"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.2 - Linear Regression.html",
    "href": "In-class activities/Chapter 2/2.2 - Linear Regression.html",
    "title": "Chapter 2.2: Linear Regression",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Fit linear regression models and interpret coefficients in plain English."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.2 - Linear Regression.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.2 - Linear Regression.html#quick-recap",
    "title": "Chapter 2.2: Linear Regression",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nLinear regression minimizes MSE (mean squared error) to find the “best” line\nThe coefficient tells you: for every 1-unit increase in X, Y changes by this amount\nThe intercept is the predicted Y when all features are 0\nWith multiple features, interpret each coefficient as “holding other features constant”\n\n\n# Load the California Housing data\nhousing = fetch_california_housing(as_frame=True)\ndf = housing.frame\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\n\n# Quick look at the feature descriptions\nprint(housing.DESCR[:1500])\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 20640\n\n:Number of Attributes: 8 numeric, predictive attributes and the target\n\n:Attribute Information:\n    - MedInc        median income in block group\n    - HouseAge      median house age in block group\n    - AveRooms      average number of rooms per household\n    - AveBedrms     average number of bedrooms per household\n    - Population    block group population\n    - AveOccup      average number of household members\n    - Latitude      block group latitude\n    - Longitude     block group longitude\n\n:Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.2 - Linear Regression.html#practice",
    "href": "In-class activities/Chapter 2/2.2 - Linear Regression.html#practice",
    "title": "Chapter 2.2: Linear Regression",
    "section": "Practice",
    "text": "Practice\n\n1. Fit simple regression: MedInc → MedHouseVal\nPredict median house value using only median income.\n\n# Step 1: Prepare X (just MedInc) and y (MedHouseVal)\n# Note: X needs to be 2D, so use df[['MedInc']] with double brackets\n\n\n# Step 2: Split into train/test (80/20)\n\n\n# Step 3: Create and fit LinearRegression\n\n\n\n2. Plot the data with the regression line overlaid\n\n# Step 1 (by hand): Create scatter plot of MedInc vs MedHouseVal (use test data)\n\n\n# Step 2 (by hand): Plot model predictions on the same graph\n\n\n# Step 3 (by hand): Add labels and title\n\n\n\n3. What does the coefficient mean in plain English?\nExtract the coefficient and intercept, then write an interpretation.\n\n# Extract and print model coefficient and intercept\n\nYour interpretation:\n(Write your full interpretation here)\n\n\n4. Fit multiple regression with 4 features of your choice\n\n# Step 1: Choose 4 features (look at df.columns for options)\nfeatures = ['MedInc', 'HouseAge', 'AveRooms', 'Population']\n\n# Step 2: Prepare X and y\n\n\n# Step 3: Split into train/test\n\n\n# Step 4: Fit the model\n\n\n\n5. Which feature has the largest coefficient? Smallest?\n\n# Create a DataFrame showing feature names and their coefficients\n\n\n# Sort so that biggest coefficients are first\n\nNote: Be careful comparing coefficients directly! They’re on different scales. A coefficient of 0.5 for MedInc (measured in $10,000s) means something different than 0.5 for Population (measured in people).\n\n\n6. Write interpretation: “For every unit increase in X, Y changes by…”\nPick TWO of your features and write full interpretations.\nFeature 1 interpretation:\n(Write your interpretation here)\nFeature 2 interpretation:\n(Write your interpretation here)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.2 - Linear Regression.html#discussion-question",
    "href": "In-class activities/Chapter 2/2.2 - Linear Regression.html#discussion-question",
    "title": "Chapter 2.2: Linear Regression",
    "section": "Discussion Question",
    "text": "Discussion Question\nWhy do we say “holding other features constant” when interpreting multiple regression coefficients? What could go wrong if we didn’t add that caveat?\n(Discuss with a neighbor)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.4 - Residual Analysis.html",
    "href": "In-class activities/Chapter 2/2.4 - Residual Analysis.html",
    "title": "Chapter 2.4: Residual Analysis",
    "section": "",
    "text": "Download Jupyter notebook (.ipynb)\nGoal: Create residual plots and diagnose assumption violations."
  },
  {
    "objectID": "In-class activities/Chapter 2/2.4 - Residual Analysis.html#quick-recap",
    "href": "In-class activities/Chapter 2/2.4 - Residual Analysis.html#quick-recap",
    "title": "Chapter 2.4: Residual Analysis",
    "section": "Quick Recap",
    "text": "Quick Recap\n\nResiduals = Actual - Predicted (how wrong was the model?)\nGood residual plot: random scatter around 0, no patterns\nFunnel shape → heteroscedasticity (variance not constant)\nCurved pattern → non-linearity (need polynomial terms or transformation)\nClusters → missing categorical variable\n\n\n# Load and prepare data\nhousing = fetch_california_housing(as_frame=True)\ndf = housing.frame\n\n# Use features 'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population' to predict 'MedHouseVal'\n\n\n# 80/20 train/test split\n\n\n# Fit the model\n\n\n# Get predictions on test set\n\n\n# Priunt out test R²"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.4 - Residual Analysis.html#practice",
    "href": "In-class activities/Chapter 2/2.4 - Residual Analysis.html#practice",
    "title": "Chapter 2.4: Residual Analysis",
    "section": "Practice",
    "text": "Practice\n\n1. Fit regression, calculate residuals: y_test - y_pred\n\n# Calculate residuals\nresiduals = y_test - y_pred\n\n# Calculate and interpret mean of residuals\n\nWhat do we learn from looking at the mean residual?\n(Write your answer here)\n\n\n2. Create scatter plot: fitted values (x) vs residuals (y)\n\n# Step 1: Scatter plot with fitted values on x-axis, residuals on y-axis\n\n\n# Step 2: Add labels and title\n\nWhat do we learn from this graph?\n(Write your answer here)\n\n\n3. Add a horizontal line at y=0 for reference\n\n# Add horizontal line at y=0\n\n\n\n4. Create histogram of residuals - does it look normal?\n\n# Step 1: Create histogram of residuals\n\nQuestion: Does the histogram look approximately normal (bell-shaped)? Or is it skewed? What does this tell us?\n(Write your answer here)\n\n\n5. Describe any patterns you see in the residual plot\nLook back at your residuals vs fitted values plot. Answer these questions:\na) Is there a funnel shape (wider spread on one side)?\n(Write your observation here)\nb) Is there a curved pattern (U-shape or arch)?\n(Write your observation here)\nc) Are there any obvious clusters or groups?\n(Write your observation here)\nd) Is there a horizontal band of points at any particular value?\n(Write your observation here)\n\n\n6. Based on patterns, which assumption might be violated?\nMatch the patterns to assumptions: - Funnel shape → Homoscedasticity violated (variance not constant) - Curved pattern → Linearity violated (relationship is non-linear) - Non-normal histogram → Normality violated - Clusters → Possible independence issues or missing variables\nYour diagnosis: Based on your plots, which assumption(s) appear to be violated?\n(Write your diagnosis here)\nWhat would you recommend doing to fix this?\n(Write your recommendation here)"
  },
  {
    "objectID": "In-class activities/Chapter 2/2.4 - Residual Analysis.html#discussion-question",
    "href": "In-class activities/Chapter 2/2.4 - Residual Analysis.html#discussion-question",
    "title": "Chapter 2.4: Residual Analysis",
    "section": "Discussion Question",
    "text": "Discussion Question\nYou fit a model and the R² is 0.85, which seems great. But the residual plot shows a clear curved pattern. Should you trust this model? Why or why not?\n(Discuss with a neighbor)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "",
    "text": "Textbook\nCourse Website (this page)"
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "",
    "text": "Textbook\nCourse Website (this page)"
  },
  {
    "objectID": "index.html#chapter-1-exploratory-data-analysis-ai-assisted-coding",
    "href": "index.html#chapter-1-exploratory-data-analysis-ai-assisted-coding",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Chapter 1: Exploratory Data Analysis & AI-Assisted Coding",
    "text": "Chapter 1: Exploratory Data Analysis & AI-Assisted Coding\n\nLearning Objectives\n\nUse AI coding assistants effectively with good prompts\nManipulate data with Pandas (loading, filtering, cleaning)\nCalculate and interpret statistical summaries\nCreate effective visualizations with Seaborn\nTest data analysis code with assert statements\n\n\n\nSchedule\n\n\n\nDate\nTopic\nTextbook\nIn-class Activity\n\n\n\n\nMon, Jan 12\nCourse intro, What is EDA?, AI coding assistants\n\nLink\n\n\nWed, Jan 14\nWriting effective prompts, when to use AI vs by hand\nSection 1\nLink\n\n\nFri, Jan 16\nQuiz 1a\n\n\n\n\nWed, Jan 21\nPandas basics: loading data, selecting/filtering, data cleaning\nSection 2\nLink\n\n\nFri, Jan 23\nStatistical summaries, outlier detection\nSection 3\nLink\n\n\nMon, Jan 26\nQuiz 1b\n\n\n\n\nWed, Jan 28\nData visualization and testing\nSections 4 and 5\nLink\n\n\nFri, Jan 30\nIn-class activity: EDA in practice\nSections 6 and 7\nLink\n\n\nMon, Feb 2\nQuiz 1c\n\n\n\n\n\nHomework 1 due date: Mon, Feb 2\n\n\nResources\n\nChapter 1: EDA and AI-Assisted Coding\nChapter 1 Homework"
  },
  {
    "objectID": "index.html#mini-project-1-eda",
    "href": "index.html#mini-project-1-eda",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Mini Project 1: EDA",
    "text": "Mini Project 1: EDA\nMini project 1: EDA\n\n\n\nDate\nActivity\n\n\n\n\nWed, Feb 4\nMini project work day\n\n\nFri, Feb 6\nMini project work day\n\n\nMon, Feb 9\nMini project presentations"
  },
  {
    "objectID": "index.html#chapter-2-regression-models",
    "href": "index.html#chapter-2-regression-models",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Chapter 2: Regression Models",
    "text": "Chapter 2: Regression Models\n\nLearning Objectives\n\nUnderstand proper train/validation/test methodology\nUse cross-validation for reliable model evaluation\nUnderstand and check linear regression assumptions\nInterpret regression coefficients correctly\nUse residual analysis to diagnose model problems\nApply polynomial features for non-linear relationships\nRecognize and handle multicollinearity\nUse Ridge, Lasso, and Elastic Net regularization\n\n\n\nSchedule\n\n\n\nDate\nTopic\nTextbook\nIn-class Activity\n\n\n\n\nWed, Feb 11\nTrain/validation/test methodology, cross-validation intro\nSection 1\nLink\n\n\nFri, Feb 13\nLinear regression fundamentals & assumptions\nSection 2\nLink\n\n\nMon, Feb 16\nQuiz 2a\n\n\n\n\nWed, Feb 18\nRegression evaluation metrics (MSE, MAE, R², Adjusted R²)\nSection 3\nLink\n\n\nFri, Feb 20\nResidual analysis and diagnostic plots\nSection 4\nLink\n\n\nMon, Feb 23\nQuiz 2b\n\n\n\n\nWed, Feb 25\nPolynomial regression and feature expansion\nSection 5\nLink\n\n\nFri, Feb 27\nMulticollinearity: detection and handling\nSection 6\nLink\n\n\nMon, Mar 2\nQuiz 2c\n\n\n\n\nWed, Mar 4\nRegularization: Ridge and Lasso regression\nSection 7\nLink\n\n\nFri, Mar 6\nElastic Net and choosing between approaches\nSections 8-10\nLink\n\n\nMon, Mar 9\nQuiz 2d\n\n\n\n\n\nHomework 2 due date: Mon, Mar 9\n\n\nResources\n\nChapter 2: Regression Models\nChapter 2 Homework"
  },
  {
    "objectID": "index.html#mini-project-2-regression-models",
    "href": "index.html#mini-project-2-regression-models",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Mini Project 2: Regression models",
    "text": "Mini Project 2: Regression models\n\n\n\nDate\nActivity\n\n\n\n\nWed, Mar 11\nMini project work day\n\n\nFri, Mar 13\nMini project work day\n\n\nMon, Mar 23\nMini project presentations"
  },
  {
    "objectID": "index.html#chapter-3-classification-models",
    "href": "index.html#chapter-3-classification-models",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Chapter 3: Classification Models",
    "text": "Chapter 3: Classification Models\n\nLearning Objectives\n\nUnderstand supervised vs unsupervised learning paradigms\nUnderstand logistic regression and interpret odds ratios\nRead and interpret confusion matrices\nCalculate precision, recall, F1 score and choose appropriately\nFit and tune decision trees, random forests, SVMs, k-NN\nUse ROC curves and AUC for model comparison\nHandle class imbalance with weights and resampling\n\n\n\nSchedule\n\n\n\n\n\n\n\n\nDate\nTopic\nTextbook\n\n\n\n\nWed, Mar 25\nSupervised vs unsupervised learning, classification intro\nSections 1 and 2\n\n\nFri, Mar 27\nLogistic regression and confusion matrices\nSections 3\n\n\nMon, Mar 30\nQuiz 3a\n\n\n\nWed, Apr 1\nClassification metrics: precision, recall, F1, ROC/AUC\nSection 4\n\n\nFri, Apr 3\nDecision trees and random forests\nSections 5 and 6\n\n\nMon, Apr 6\nQuiz 3b\n\n\n\nWed, Apr 8\nSupport vector machines and k-Nearest neighbors\nSections 7 and 8\n\n\nFri, Apr 10\nClass imbalance handling (SMOTE, class weights)\nSection 10\n\n\nMon, Apr 13\nQuiz 3c\n\n\n\n\nHomework 3 due date: Mon, Apr 13\n\n\nResources\n\nChapter 3: Classification Models\nChapter 3 Homework"
  },
  {
    "objectID": "index.html#chapter-4-llms-for-feature-engineering-and-data-extraction",
    "href": "index.html#chapter-4-llms-for-feature-engineering-and-data-extraction",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "text": "Chapter 4: LLMs for Feature Engineering and Data Extraction\n\nLearning Objectives\n\nUnderstand how LLMs can transform unstructured text into ML-ready features\nWrite effective prompts for information extraction tasks\nIntegrate LLM APIs (OpenAI, Anthropic, Google) into data pipelines\nParse and validate JSON responses from LLMs\nCalculate and compare API costs across different providers\nValidate LLM-extracted features for quality and consistency\nIntegrate LLM-generated features into traditional ML models\nMake informed decisions about when to use LLMs vs. traditional approaches\n\n\n\nSchedule\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\nWed, Apr 15\nIntroduction to LLMs for feature extraction; Zero-shot extraction; Writing effective prompts, Begin final project\n\n\nFri, Apr 17\nAPI integration (OpenAI, Anthropic, Google); Parameters (temperature, max_tokens); Cost calculation\n\n\nMon, Apr 20\nQuiz 4a: Prompt engineering and API basics\n\n\nWed, Apr 22\nJSON parsing and validation; Handling malformed responses; Building DataFrames\n\n\nFri, Apr 24\nFew-shot prompting; Quality assessment; Integration with ML pipelines\n\n\nMon, Apr 27\nQuiz 4b: Integration and quality control\n\n\n\nHomework 4 due date: Mon, Apr 27\n\n\nResources\n\nTextbook: Chapter 4: LLMs for Feature Engineering\nHomework: Chapter 4 Homework\n\n–"
  },
  {
    "objectID": "index.html#final-exam-week-presentations-wrap-up",
    "href": "index.html#final-exam-week-presentations-wrap-up",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Final exam week: Presentations & Wrap-up",
    "text": "Final exam week: Presentations & Wrap-up\n\n\n\nDate\nTopic\n\n\n\n\nWed, Apr 29\nFinal project work day\n\n\nFri, May 1\nFinal project work day\n\n\nWed, May 6\nFinal project presentations"
  },
  {
    "objectID": "index.html#assessment-overview",
    "href": "index.html#assessment-overview",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Assessment Overview",
    "text": "Assessment Overview\n\n\n\nAssessment\nWeight\n\n\n\n\nQuizzes\n25%\n\n\nHomework\n20%\n\n\nMini projects\n12.5% each = 25%\n\n\nAttendance\n10%\n\n\nFinal Project\n20%"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html",
    "href": "Templates/textbook-chapter-template.html",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "All textbook chapters should follow this structure:\n\nFront Matter (YAML header with title, format settings)\nModule Resources (links to homework, quiz)\nIntroduction (engaging overview of the chapter)\nMain Content Sections (8-10 major sections covering core topics)\nSummary (recap of key points)\nPractice Exercises (4-6 exercises)\nAdditional Resources (links to external resources)\n\n\n\n\nFollow the professor’s casual, probing style:\n\nWrite conversationally, as if talking to students\nUse probing questions rather than direct answers\nExample: “But wait—why does that work? Let’s think about it…”\nAvoid overly formal academic language\nInclude phrases like “Let’s jump in”, “Here’s the thing”, “Don’t worry about X right now”\nEnd sections with thought-provoking questions or connections\n\nCode demonstrations:\n\nInclude code every 2-3 paragraphs maximum\nDon’t just show code—explain what it’s doing and why\nBuild up complexity gradually\nUse realistic datasets and examples\nComment code to explain reasoning, not just what the code does\n\nConceptual explanations:\n\nStart with intuition, then formalize\nUse analogies and real-world examples\nConnect to what students already know\nExplain the “why” before the “how”\nMake abstract concepts concrete through visualization\n\n\n\n\nMajor sections (numbered with ##):\n\n8-10 major sections per chapter\nEach covers one core topic from the module\nShould flow logically, building on previous sections\n\nSubsections (numbered with ###):\n\n2-4 subsections per major section\nBreak down the major topic into digestible pieces\nInclude at least one code example per subsection\n\n\n\n\nRequirements:\n\nEvery subsection should have at least one code example\nMaximum 2-3 paragraphs before showing code\nCode blocks should be executable (use proper imports)\nInclude output or visualization when relevant\nBuild complexity gradually across the chapter\n\nFormat:\n# Example structure\nimport pandas as pd\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv('example.csv')\n\n# Do something meaningful\nresult = df.groupby('category')['value'].mean()\nprint(result)\nExplanation pattern:\n\nBrief intro to what you’re going to do\nShow the code\nExplain what happened and why it matters\nConnect to the bigger picture\n\n\n\n\nRequirements:\n\nInclude 2-3 images/diagrams per chapter\nCreate an images/ subfolder in each module’s textbook folder\nUse descriptive filenames (e.g., bias-variance-tradeoff.png)\nReference images with relative paths: ![Description](images/filename.png)\nCreate a placeholder-info.md file describing needed images\n\nTypes of images:\n\nWorkflow diagrams\nConceptual illustrations\nExample visualizations (good vs. bad)\nScreenshots of tools/interfaces\nMathematical concepts visualized\n\n\n\n\nAt the top of each chapter, include:\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\nNote: Use proper URL encoding for spaces in paths (%20)\n\n\n\nPurpose:\n\nHook the reader with why this topic matters\nPreview what they’ll learn\nConnect to previous modules (if applicable)\nSet expectations and tone\n\nLength: 3-5 paragraphs\nStyle:\n\nStart with a compelling question or scenario\nUse conversational tone\nAvoid listing learning objectives formally\nBuild excitement about the topic\n\n\n\n\nPurpose:\n\nRecap the main concepts covered\nEmphasize key takeaways\nConnect back to the big picture\nPrepare students for what’s next\n\nLength: 3-4 paragraphs\nStyle:\n\nDon’t just list topics covered\nSynthesize the main ideas\nReinforce why these concepts matter\nEnd with: “Use your brain. That’s what it’s there for.”\n\n\n\n\nRequirements:\n\n4-6 exercises per chapter\nRange from simple to complex\nMix conceptual and coding exercises\nShould be doable without AI assistance\nAlign with “Topics Done By-Hand” from module overview\n\nFormat:\n## Practice Exercises\n\n1. **[Exercise name]:** [Description of what to do]\n\n2. **[Exercise name]:** [Description of what to do]\n\n[etc.]\n\n\n\nInclude:\n\nOfficial documentation links (Pandas, Scikit-learn, etc.)\nRelevant tutorials or guides\nTool-specific resources (Gemini CLI, etc.)\nVisualization galleries\nArticles or videos for deeper learning\n\nFormat:\n## Additional Resources\n\n- [Resource name](URL) - Brief description\n- [Resource name](URL) - Brief description\n[etc.]\n\n\n\nWhen creating a template or draft chapter:\n\nUse XXX for content placeholders\nInclude brief descriptions of what should go there\nLeave code blocks with comments showing what example to include\nMark image locations with descriptive placeholder filenames\n\n\n\n\n\n\n---\ntitle: \"Chapter [X]: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\n\n---\n\n## Introduction\n\nXXX - Engaging introduction (3-5 paragraphs)\n- Why this topic matters\n- What they'll learn\n- Connection to data science workflow\n\n---\n\n## 1. [First Major Topic]\n\n### 1.1 [First Subtopic]\n\nXXX - Conceptual explanation (2-3 paragraphs)\n\n```python\n# XXX - Code example demonstrating the concept\n# code here\nXXX - Explanation of what the code does and why it matters\n\n\nXXX - Continue pattern…\n\n\n\nXXX - Continue pattern…\n\n\n\nPlaceholder for diagram\n\n\n\n\n\n\n\n\n\nXXX - Content…\n# XXX - Code example\n\n\n\nXXX - Content…\n\n[Continue for 8-10 major sections…]\n\n\n\n\n\nXXX - Synthesis of main concepts (3-4 paragraphs) - Key takeaways - Why these concepts matter - Connection to future work - End with: “Use your brain. That’s what it’s there for.”\n\n\n\n\n\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n\n\n\n\n\n\nResource name - Description\nResource name - Description\nResource name - Description\n\n\n---\n\n## Directory Structure\n\nFor each module's textbook chapter:\n\nTextbook/ Module-X-Topic/ chapter-x-topic.qmd # Main chapter file images/ # Images folder placeholder-info.md # Description of needed images diagram1.png # Actual images (to be created) diagram2.png ```\n\n\n\n\n\n\n\nEmphasize AI prompting throughout\nInclude examples of good vs. bad prompts\nShow when to use AI vs. hand-coding\nFocus on Pandas, Seaborn, testing\n\n\n\n\n\nFocus on model instantiation, fitting, predicting\nShow different model types in action\nEmphasize evaluation metrics\nCompare models systematically\n\n\n\n\n\nHeavy use of visualization to explain concepts\nLoss functions shown with code and plots\nBias-variance visualized extensively\nTheoretical concepts made concrete\n\n\n\n\n\nExtensive diagnostic plots\nResidual analysis examples\nCoefficient interpretation in context\nRegularization effects visualized\n\n\n\n\n\nConfusion matrices and ROC curves\nMetric interpretation in business context\nClass imbalance examples\nDecision boundaries visualized\n\n\n\n\n\nPyTorch code structure emphasized\nTraining curves and interpretation\nArchitecture diagrams\nConnection to earlier concepts\n\n\n\n\n\nHugging Face examples\nModel card interpretation\nTransfer learning workflow\nFine-tuning examples\n\n\n\n\n\n\nBefore finalizing a chapter, verify:\n\nModule Resources section links to correct homework/quiz\nIntroduction is engaging and sets proper expectations\n8-10 major sections covering all core topics\nCode examples every 2-3 paragraphs\nAll code is executable and properly formatted\n2-3 images included with placeholder info\nWriting style matches professor’s casual tone\nConcepts explained with intuition before formalization\nSummary synthesizes key ideas\n4-6 practice exercises included\nAdditional Resources section populated\nEnds with “Use your brain. That’s what it’s there for.”"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#template-guidelines",
    "href": "Templates/textbook-chapter-template.html#template-guidelines",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "All textbook chapters should follow this structure:\n\nFront Matter (YAML header with title, format settings)\nModule Resources (links to homework, quiz)\nIntroduction (engaging overview of the chapter)\nMain Content Sections (8-10 major sections covering core topics)\nSummary (recap of key points)\nPractice Exercises (4-6 exercises)\nAdditional Resources (links to external resources)\n\n\n\n\nFollow the professor’s casual, probing style:\n\nWrite conversationally, as if talking to students\nUse probing questions rather than direct answers\nExample: “But wait—why does that work? Let’s think about it…”\nAvoid overly formal academic language\nInclude phrases like “Let’s jump in”, “Here’s the thing”, “Don’t worry about X right now”\nEnd sections with thought-provoking questions or connections\n\nCode demonstrations:\n\nInclude code every 2-3 paragraphs maximum\nDon’t just show code—explain what it’s doing and why\nBuild up complexity gradually\nUse realistic datasets and examples\nComment code to explain reasoning, not just what the code does\n\nConceptual explanations:\n\nStart with intuition, then formalize\nUse analogies and real-world examples\nConnect to what students already know\nExplain the “why” before the “how”\nMake abstract concepts concrete through visualization\n\n\n\n\nMajor sections (numbered with ##):\n\n8-10 major sections per chapter\nEach covers one core topic from the module\nShould flow logically, building on previous sections\n\nSubsections (numbered with ###):\n\n2-4 subsections per major section\nBreak down the major topic into digestible pieces\nInclude at least one code example per subsection\n\n\n\n\nRequirements:\n\nEvery subsection should have at least one code example\nMaximum 2-3 paragraphs before showing code\nCode blocks should be executable (use proper imports)\nInclude output or visualization when relevant\nBuild complexity gradually across the chapter\n\nFormat:\n# Example structure\nimport pandas as pd\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv('example.csv')\n\n# Do something meaningful\nresult = df.groupby('category')['value'].mean()\nprint(result)\nExplanation pattern:\n\nBrief intro to what you’re going to do\nShow the code\nExplain what happened and why it matters\nConnect to the bigger picture\n\n\n\n\nRequirements:\n\nInclude 2-3 images/diagrams per chapter\nCreate an images/ subfolder in each module’s textbook folder\nUse descriptive filenames (e.g., bias-variance-tradeoff.png)\nReference images with relative paths: ![Description](images/filename.png)\nCreate a placeholder-info.md file describing needed images\n\nTypes of images:\n\nWorkflow diagrams\nConceptual illustrations\nExample visualizations (good vs. bad)\nScreenshots of tools/interfaces\nMathematical concepts visualized\n\n\n\n\nAt the top of each chapter, include:\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\nNote: Use proper URL encoding for spaces in paths (%20)\n\n\n\nPurpose:\n\nHook the reader with why this topic matters\nPreview what they’ll learn\nConnect to previous modules (if applicable)\nSet expectations and tone\n\nLength: 3-5 paragraphs\nStyle:\n\nStart with a compelling question or scenario\nUse conversational tone\nAvoid listing learning objectives formally\nBuild excitement about the topic\n\n\n\n\nPurpose:\n\nRecap the main concepts covered\nEmphasize key takeaways\nConnect back to the big picture\nPrepare students for what’s next\n\nLength: 3-4 paragraphs\nStyle:\n\nDon’t just list topics covered\nSynthesize the main ideas\nReinforce why these concepts matter\nEnd with: “Use your brain. That’s what it’s there for.”\n\n\n\n\nRequirements:\n\n4-6 exercises per chapter\nRange from simple to complex\nMix conceptual and coding exercises\nShould be doable without AI assistance\nAlign with “Topics Done By-Hand” from module overview\n\nFormat:\n## Practice Exercises\n\n1. **[Exercise name]:** [Description of what to do]\n\n2. **[Exercise name]:** [Description of what to do]\n\n[etc.]\n\n\n\nInclude:\n\nOfficial documentation links (Pandas, Scikit-learn, etc.)\nRelevant tutorials or guides\nTool-specific resources (Gemini CLI, etc.)\nVisualization galleries\nArticles or videos for deeper learning\n\nFormat:\n## Additional Resources\n\n- [Resource name](URL) - Brief description\n- [Resource name](URL) - Brief description\n[etc.]\n\n\n\nWhen creating a template or draft chapter:\n\nUse XXX for content placeholders\nInclude brief descriptions of what should go there\nLeave code blocks with comments showing what example to include\nMark image locations with descriptive placeholder filenames"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#chapter-template-structure",
    "href": "Templates/textbook-chapter-template.html#chapter-template-structure",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "---\ntitle: \"Chapter [X]: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\n\n---\n\n## Introduction\n\nXXX - Engaging introduction (3-5 paragraphs)\n- Why this topic matters\n- What they'll learn\n- Connection to data science workflow\n\n---\n\n## 1. [First Major Topic]\n\n### 1.1 [First Subtopic]\n\nXXX - Conceptual explanation (2-3 paragraphs)\n\n```python\n# XXX - Code example demonstrating the concept\n# code here\nXXX - Explanation of what the code does and why it matters\n\n\nXXX - Continue pattern…\n\n\n\nXXX - Continue pattern…\n\n\n\nPlaceholder for diagram"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#second-major-topic",
    "href": "Templates/textbook-chapter-template.html#second-major-topic",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "XXX - Content…\n# XXX - Code example\n\n\n\nXXX - Content…\n\n[Continue for 8-10 major sections…]"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#summary",
    "href": "Templates/textbook-chapter-template.html#summary",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "XXX - Synthesis of main concepts (3-4 paragraphs) - Key takeaways - Why these concepts matter - Connection to future work - End with: “Use your brain. That’s what it’s there for.”"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#practice-exercises",
    "href": "Templates/textbook-chapter-template.html#practice-exercises",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#additional-resources",
    "href": "Templates/textbook-chapter-template.html#additional-resources",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "Resource name - Description\nResource name - Description\nResource name - Description\n\n\n---\n\n## Directory Structure\n\nFor each module's textbook chapter:\n\nTextbook/ Module-X-Topic/ chapter-x-topic.qmd # Main chapter file images/ # Images folder placeholder-info.md # Description of needed images diagram1.png # Actual images (to be created) diagram2.png ```"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#module-specific-adaptations",
    "href": "Templates/textbook-chapter-template.html#module-specific-adaptations",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "Emphasize AI prompting throughout\nInclude examples of good vs. bad prompts\nShow when to use AI vs. hand-coding\nFocus on Pandas, Seaborn, testing\n\n\n\n\n\nFocus on model instantiation, fitting, predicting\nShow different model types in action\nEmphasize evaluation metrics\nCompare models systematically\n\n\n\n\n\nHeavy use of visualization to explain concepts\nLoss functions shown with code and plots\nBias-variance visualized extensively\nTheoretical concepts made concrete\n\n\n\n\n\nExtensive diagnostic plots\nResidual analysis examples\nCoefficient interpretation in context\nRegularization effects visualized\n\n\n\n\n\nConfusion matrices and ROC curves\nMetric interpretation in business context\nClass imbalance examples\nDecision boundaries visualized\n\n\n\n\n\nPyTorch code structure emphasized\nTraining curves and interpretation\nArchitecture diagrams\nConnection to earlier concepts\n\n\n\n\n\nHugging Face examples\nModel card interpretation\nTransfer learning workflow\nFine-tuning examples"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#quality-checklist",
    "href": "Templates/textbook-chapter-template.html#quality-checklist",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "Before finalizing a chapter, verify:\n\nModule Resources section links to correct homework/quiz\nIntroduction is engaging and sets proper expectations\n8-10 major sections covering all core topics\nCode examples every 2-3 paragraphs\nAll code is executable and properly formatted\n2-3 images included with placeholder info\nWriting style matches professor’s casual tone\nConcepts explained with intuition before formalization\nSummary synthesizes key ideas\n4-6 practice exercises included\nAdditional Resources section populated\nEnds with “Use your brain. That’s what it’s there for.”"
  },
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Assignments/: Quarto .qmd files for homework and quizzes, organized by module.\nTextbook/: Module chapters and supporting assets (including images/ subfolders).\nTemplates/: Required templates for assignments, quizzes, and textbook chapters.\nClaude-planning/: Planning notes for module development (one Markdown file per topic).\n_site/: Rendered website output (generated).\n_freeze/: Quarto execution cache (generated).\n\n\n\n\n\nquarto preview: Serve the site locally with live reload for .qmd changes.\nquarto render: Build the full site into _site/.\nquarto render Textbook/Module 1 - EDA/chapter-1-eda.qmd: Render a single page when iterating.\n\n\n\n\n\nContent is Quarto Markdown (.qmd) with Python 3.12 code blocks.\nUse executable blocks with {python} and keep code outputs visible unless noted.\nPrefer descriptive file names that mirror module titles (e.g., chapter-1-eda.qmd).\nFollow template structure in Templates/ for assignments, quizzes, and chapters.\n\n\n\n\n\nNo automated test suite is present.\nValidate changes by rendering the relevant page(s) with quarto render and reviewing the HTML in _site/.\n\n\n\n\n\nCommit messages are short, sentence-case summaries (e.g., “Update course schedule for new 3-module structure”).\nPRs should describe what changed, which pages were rendered, and link any related issues.\nIf updating content, include a screenshot or link to the rendered _site/ output for the modified page.\n\n\n\n\n\nFollow Claude.md for course-writing rules, templates, and pedagogy constraints.\nWhen creating assignments or chapters, consult content-overview.md and update Claude-planning/ first."
  },
  {
    "objectID": "AGENTS.html#project-structure-module-organization",
    "href": "AGENTS.html#project-structure-module-organization",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Assignments/: Quarto .qmd files for homework and quizzes, organized by module.\nTextbook/: Module chapters and supporting assets (including images/ subfolders).\nTemplates/: Required templates for assignments, quizzes, and textbook chapters.\nClaude-planning/: Planning notes for module development (one Markdown file per topic).\n_site/: Rendered website output (generated).\n_freeze/: Quarto execution cache (generated)."
  },
  {
    "objectID": "AGENTS.html#build-test-and-development-commands",
    "href": "AGENTS.html#build-test-and-development-commands",
    "title": "Repository Guidelines",
    "section": "",
    "text": "quarto preview: Serve the site locally with live reload for .qmd changes.\nquarto render: Build the full site into _site/.\nquarto render Textbook/Module 1 - EDA/chapter-1-eda.qmd: Render a single page when iterating."
  },
  {
    "objectID": "AGENTS.html#coding-style-naming-conventions",
    "href": "AGENTS.html#coding-style-naming-conventions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Content is Quarto Markdown (.qmd) with Python 3.12 code blocks.\nUse executable blocks with {python} and keep code outputs visible unless noted.\nPrefer descriptive file names that mirror module titles (e.g., chapter-1-eda.qmd).\nFollow template structure in Templates/ for assignments, quizzes, and chapters."
  },
  {
    "objectID": "AGENTS.html#testing-guidelines",
    "href": "AGENTS.html#testing-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "No automated test suite is present.\nValidate changes by rendering the relevant page(s) with quarto render and reviewing the HTML in _site/."
  },
  {
    "objectID": "AGENTS.html#commit-pull-request-guidelines",
    "href": "AGENTS.html#commit-pull-request-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Commit messages are short, sentence-case summaries (e.g., “Update course schedule for new 3-module structure”).\nPRs should describe what changed, which pages were rendered, and link any related issues.\nIf updating content, include a screenshot or link to the rendered _site/ output for the modified page."
  },
  {
    "objectID": "AGENTS.html#agent-specific-instructions",
    "href": "AGENTS.html#agent-specific-instructions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Follow Claude.md for course-writing rules, templates, and pedagogy constraints.\nWhen creating assignments or chapters, consult content-overview.md and update Claude-planning/ first."
  }
]