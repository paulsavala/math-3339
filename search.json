[
  {
    "objectID": "Claude.html",
    "href": "Claude.html",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "This course emphasizes conceptual understanding and practical application over implementation details. Students will develop the analytical thinking needed to select appropriate models, recognize when assumptions are violated, and critically evaluate model performance. While AI coding assistants are powerful tools, this course ensures students build the foundational knowledge to use them effectively and debug their output.\n\n\n\nWhen developing this course, Claude should follow the following guidelines:\n\nUtilize the existing folder Claude-planning by creating Markdown files to keep track of progress. Claude should keep one file per topic, and each file should have a clear title and a clear description of the topic. Each file should outline specifically what is being taught, which parts are done “by hand” (either programming or mathematically) and which parts are done with AI coding assistants. Claude should describe the in-class activities and homework which will enforce learning these topics.\nWhen designing a lesson and/or topic, Claude should refer back to the existing Claude-planning folder to ensure consistency and avoid redundancy. In addition, when possible, Claude should try to build on previous lessons and topics to create a cohesive and progressive learning experience. This should be transparent to the learner, with lessons refering back to previous lessons and topics whenever possible.\n\n\n\n\nWhen creating homework assignments or quizzes, Claude must follow the templates defined in the Templates/ folder:\n\n\nTemplate Location: Templates/homework-template.md\nKey Requirements:\n\nTwo-Part Structure:\n\nPart A: By Hand (40-45 points) - 10 questions without AI assistance to build foundational skills\nPart B: With AI Assistance (55-60 points) - 10 questions using AI coding assistants to scale analysis\nTotal: 100 points across 20 questions\n\nPart A Focus (By Hand):\n\nBasic implementation of techniques/models\nEvaluation and interpretation of results\nWriting helper functions to demonstrate understanding\nExplaining concepts in own words\nCreating simple visualizations\nQuestion range: 3-5 points each\n\nPart B Focus (AI-Assisted):\n\nHyperparameter tuning at scale\nComparing 5+ models/approaches simultaneously\nBuilding automation pipelines\nCross-validation and robustness testing\nFinal comprehensive analysis with executive summary\nQuestion range: 5-7 points each\nMUST require students to submit their AI prompts used\n\nRequired Sections:\n\nInstructions (including dataset descriptions)\nPart A questions (1-10)\nPart B questions (11-20)\nSubmission Guidelines\nGrading Rubric\nTips for Success (separate subsections for Part A, Part B, and General)\n\nRequired Deliverables for Part B:\n\nCode (AI-generated but student-verified)\nOutputs (visualizations, tables, results)\nWritten interpretation (student’s own analysis)\nThe prompt(s) used with AI assistant\n\nTips Section Must End With:\n\n“Use your brain. That’s what it’s there for.”\n\n\n\n\n\nTemplate Location: Templates/quiz-template.md\nKey Requirements:\n\nTwo-Section Structure:\n\nSection A: Conceptual Questions (~50% of points) - Testing understanding without a computer\nSection B: Code Writing (~50% of points) - Writing code by hand\nTotal: Typically 25-50 points\nTime: 30-45 minutes\n\nSection A Focus (Conceptual):\n\nScenario identification (classification vs regression, problem types)\nMetric interpretation and trade-off reasoning\nModel selection justification\nConceptual explanations\nProblem diagnosis\nQuestion range: 2-4 points each\n6-8 questions total\n\nSection B Focus (Code Writing):\n\nData manipulation (load, filter, split)\nModel instantiation with parameters\nFit/predict workflow\nMetric calculation\nSimple function writing\nQuestion range: 2-4 points each\n5-7 questions total\nMust include assumption section listing all imports\n\nRequired Elements:\n\nInstructions (time, format, note about partial credit)\nSection A questions\nAssumption section before Section B (listing imports and available variables)\nSection B questions\nGrading Rubric with statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\nCode Writing Philosophy:\n\nSyntax perfection NOT required\nFocus on logic and correct approach\nGrading: 70-80% for correct logic, 20-30% for syntax\nQuestions must be realistic to write by hand\n\n\n\n\n\nTemplate Location: Templates/textbook-chapter-template.md\nKey Requirements:\n\nStructure:\n\nFront matter (YAML with title, format settings)\nModule Resources section linking to homework and quiz\nIntroduction (3-5 engaging paragraphs)\n8-10 major sections covering core topics\nSummary (synthesizing key concepts)\nPractice Exercises (4-6 exercises)\nAdditional Resources\n\nCode Examples:\n\nInclude code every 2-3 paragraphs maximum\nUse executable Python code with proper imports\nBuild complexity gradually throughout the chapter\nExplain what code does and why it matters\nUse realistic datasets and examples\n\nImages and Visualizations:\n\nInclude 2-3 images/diagrams per chapter\nCreate images/ subfolder in module folder\nCreate placeholder-info.md describing needed images\nUse relative paths: ![Description](images/filename.png)\n\nWriting Style:\n\nConversational and casual tone\nUse probing questions rather than direct statements\nMaximum 2-3 paragraphs before showing code\nStart with intuition, then formalize\nConnect to what students already know\nEnd summary with: “Use your brain. That’s what it’s there for.”\n\nQuarto-Specific Requirements:\n\nUse {python} for executable code blocks (not standard markdown code blocks)\nInclude jupyter: python3 in YAML front matter\nSet code-fold: false to show all code by default\nUse callout boxes for tips, notes, and warnings:\n\n::: {.callout-tip} for helpful tips\n::: {.callout-note} for important information\n::: {.callout-warning} for cautions\n\n\n\n\n\n\nWhen asked to create homework, quiz, or textbook chapter for a module:\n\nRead the appropriate template from Templates/homework-template.md, Templates/quiz-template.md, or Templates/textbook-chapter-template.md\nReview the module-specific adaptations section in the template\nFollow the structure exactly - do not deviate from point distributions, section organization, or required elements\nRefer to content-overview.md to understand:\n\nCore topics for the module\nWhat should be done by-hand vs with AI\nLearning objectives\nAssessment focus\n\nCreate or update the Claude-planning file for the module first, then generate content\nEnsure content aligns with the “Topics Done By-Hand” and “Topics Done With AI” from content-overview.md\n\n\n\n\n\n\nLessons will be written as Quarto Markdown documents, with the programming language used being Python 3.12.\nStudents will use the Gemini CLI as the primary AI coding assistant. The primary model will be Flash 2.5, which means code generation skills are limited. Because of this, the text should encourage students to work on small components at a time, rather than prompting the model to build entire applications all at once.\nStudents will host their work on Github.\nPython packages used include:\n\nPandas\nNumPy\nSeaborn\nScikit-learn\nPyTorch\nStreamlit\nHuggingface\n\n\n\n\n\n\n\nThe professor writes in a casual, conversational manner that emphasizes understanding over formality. Key characteristics:\nTone and Voice:\n\nWrite as if talking directly to students\nUse probing questions to guide thinking rather than directly stating answers\nAvoid overly formal academic language\nBe direct and honest about challenges and complexities\nShow enthusiasm for the subject matter\n\nCommon Phrases and Patterns:\n\n“Let’s jump in” / “Let’s start with…”\n“Here’s the thing…” / “Here’s what’s happening…”\n“But wait—why does that work?”\n“Don’t worry about X right now, just…”\n“See what happened?” / “Notice what’s going on here?”\n“Use your brain. That’s what it’s there for.” (ending phrase)\n\nExample of the Professor’s Writing Style:\n\"Linear regression is perhaps the most important idea in machine learning. Once you understand the ins-and-outs of linear regression you can understand most other machine learning models as well. This is because the ideas developed for machine learning were first perfected on linear regression and then applied to other models. Let's jump in.\n\nLinear regression (LR) is what you have probably referred to as \"line of best fit\". It is a line meant to fit the data \"as well as possible\". I put that last phrase in quotes, because what exactly do we mean by a \"best fit\"? We will formulate this mathematically in the next section.\n\nWith that out of the way, we now turn to our next question: why do we care about linear regression? Linear regression is extremely important because it allows us to make predictions. Up until this point we have only explored and described the past by looking at datasets which (necessarily) had data about the past. However, the point of data science is largely to make predictions about the future using data from the past. This works because a line doesn't care what data we plug in. We can plug in data from the past in order to verify and explain past performance. But we can also plug in future numbers (dates, pricing changes, expected changes to our products, etc.) and see what the model returns.\n\nLet's start with a simple example. Don't worry about the code right now, just look at the graphs. We will work again with our data/boston.csv dataset, describing the median home value in various neighborhoods in Boston.\n\nIn blue is the actual measurements of poverty and home value for all neighborhoods. In red is the predicted value using the line of best fit. We can see that the regression line seems to fit the data fairly well, at least in all except the far left and right ends.\n\nOne interesting thing to note is that the regression line generally seems too high. For example, if we draw the same graph, but only keep poverty values between 5% and 20% we get the following:\n\nWhy is that? The reason is that the regression line is heavily affected by outliers. So the neighborhoods with low crime and high home value are throwing off the line and \"dragging it up.\" In general, you want the following four things to be true before using LR for making predictions:\n\n1. The data should be approximately linear\n\n2. The observations should be independent (so the crime rate and median home value in one neighborhood should be independent of other neighborhoods)\n\n3. The variance between the measurements should be approximately the same throughout the graph (the graph is more or less spread out the same amount in different areas)\n\n4. The points should be approximately normally distributed around the regression line.\n\nNone of these four are perfectly satisfied. However, that doesn't mean you can't use LR. It just means that you need to be careful when making predictions. Don't just make a regression line and say \"see, this predicts the future!\" Use your brain, that's what it's there for.\"\n\n\n\nConceptual Explanations:\n\nStart with intuition and concrete examples before formalizing\nUse analogies and real-world scenarios to ground abstract concepts\nAsk guiding questions that lead students to understanding\nBuild complexity gradually—simple cases first, then complications\nConnect new concepts to what students already know\nExplain the “why” before diving into the “how”\n\nCode Demonstrations:\n\nInclude code examples every 2-3 paragraphs maximum\nNever show code without explaining what it does and why it matters\nBuild code examples incrementally (don’t dump large blocks)\nUse realistic datasets that students can relate to\nComment code to explain reasoning, not just mechanics\nShow output or visualizations when relevant\nFollow this pattern:\n\nBrief intro to what you’re demonstrating\nShow the code\nExplain what happened and why\nConnect to the bigger picture\n\n\nCode Formatting for Quarto:\n\nUse {python} for executable code blocks: ```{python}\nUse regular markdown code blocks only for non-executable examples\nAdd #| error: true to code blocks that intentionally raise errors\nUse #| echo: true (default) to show code\nUse #| output: true to display output\n\nExamples and Datasets:\n\nUse consistent, realistic datasets throughout a chapter\nCalifornia housing data (from Module 1) is the primary dataset\nInclude real-world context for all examples\nMake examples build on each other when possible\nShow both successes and failures (what not to do)\n\nVisual Elements:\n\nInclude 2-3 images/diagrams per chapter minimum\nCreate images/ subfolder with placeholder-info.md\nUse callout boxes for important points:\n\n::: {.callout-tip} for helpful tips and best practices\n::: {.callout-note} for important information students must remember\n::: {.callout-warning} for common mistakes and cautions\n\nUse concrete visualizations to explain abstract concepts\n\nSection Flow:\n\nEach major section should have a clear narrative arc\nStart sections with context: why does this matter?\nEnd sections with synthesis: what did we learn?\nUse transitions to connect sections logically\nReference earlier content when building on it\nPreview upcoming content when it’s relevant\n\nPractical Examples:\n\nAlways ground theory in practice\nShow real scenarios where the concept matters\nInclude “what could go wrong” examples\nDemonstrate debugging and problem-solving approaches\nConnect to real data science workflows\n\nEnding Every Chapter:\n\nSummary must synthesize, not just list topics\nEmphasize key takeaways and connections\nAlways end with: “Use your brain. That’s what it’s there for.”\n\nWriting Theoretical Concepts:\nWhen explaining theoretical or conceptual material (like ML theory, loss functions, optimization, etc.):\n\nStart with the practical problem before introducing theory\n\nExample: “Models fail. They overfit. They underfit. Understanding what’s going on ‘under the hood’ helps you recognize these problems…”\nLead with why this matters in practice, then explain the theory\n\nUse concrete, visual examples first\n\nShow actual code and visualizations before equations\nLet students see the concept in action before formalizing it\nExample: Demonstrate gradient descent with a simple quadratic function and visualization before explaining the algorithm\n\nExplain concepts in plain English\n\n“Cross-entropy heavily penalizes confident wrong predictions”\n“The gradient is the slope of the loss with respect to parameters”\nAvoid jargon or explain it immediately when used\n\nUse analogies and metaphors\n\n“Think of it as navigating a landscape where height represents loss, and you’re trying to find the lowest valley”\n“Bias: fitting a straight line to clearly curved data”\n“Variance: fitting a wiggly line that goes through every single training point”\n\nBreak down mathematical concepts step-by-step\n\nIntroduce notation gradually\nExplain what each symbol means in context\nShow the formula, then explain it in words\nExample: Show cross-entropy formula, then explain “this heavily penalizes confident wrong predictions”\n\nUse callout boxes strategically\n\n::: {.callout-note} for connecting concepts (“Cross-entropy is just the log-likelihood for binary classification”)\n::: {.callout-warning} for common misconceptions or pitfalls\n::: {.callout-tip} for practical advice about when/how to use concepts\n\nConnect theory back to practice repeatedly\n\n“In practice, you rarely need to implement loss functions yourself—scikit-learn handles it”\n“Understanding what they’re doing helps you choose the right one and diagnose problems”\nAlways answer “why does this matter for my work?”\n\nUse contrasts and comparisons\n\n“MSE: Penalizes large errors heavily (good when outliers are costly)”\n“MAE: Treats all errors equally (good when outliers shouldn’t dominate)”\nSide-by-side visualizations showing different approaches\n\nEmphasize intuition over rigor\n\nGet the concept across first, worry about edge cases later\n“The idea is beautifully simple: calculate the gradient, take a step downhill, repeat”\nDon’t get bogged down in mathematical proofs\n\nUse progressive disclosure\n\nStart with 1D examples before moving to higher dimensions\n“In one dimension, the gradient is simply the derivative. In higher dimensions…”\nBuild from simple → complex gradually\n\nFrame concepts as tools, not just theory\n\n“Loss functions define what ‘good’ means”\n“The bias-variance tradeoff is your guide for choosing model complexity”\nPresent concepts as practical tools students will use\n\n\n\n\n\n\n\nWhen writing code examples in textbook chapters and assignments, follow these package-specific best practices to ensure code is clear, correct, and pedagogically sound.\n\n\nDataFrame Display:\n\nNever use print() for DataFrames - Instead, put the DataFrame (or df.head()) as the last line of the cell to use Jupyter’s rich display\nAlways use .head() to show DataFrames - This prevents overwhelming output and teaches good practices\n\n# BAD: Using print()\nprint(housing_df)\n\n# GOOD: Let Jupyter display it\nhousing_df.head()\nDataFrame Creation:\n\nNever subset data when creating DataFrames - Create the full DataFrame first, then display a subset\nThis prevents confusing students about what’s actually in the DataFrame\n\n# BAD: Subsetting during creation\ncomparison = pd.DataFrame({\n    'Actual': y_test.head(10),\n    'Predicted': predictions[:10]\n})\ncomparison\n\n# GOOD: Full DataFrame, then show subset\ncomparison = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': predictions\n})\ncomparison.head()\nColumn Selection:\n\nAlways demonstrate both single-column (Series) and multi-column (DataFrame) selection\nEmphasize that double brackets [[]] for multiple columns are just a list inside brackets\n\n# Single column (returns Series)\nhouse_values = housing_df['median_house_value']\n\n# Multiple columns (returns DataFrame)\nsubset = housing_df[['median_house_value', 'ocean_proximity']]\nChaining Operations:\n\nBreak complex chains across multiple lines for readability\nAdd comments explaining each step\n\n# GOOD: Clear, multi-line chaining\nresult = (housing_df\n          .dropna()  # Remove missing values\n          .groupby('ocean_proximity')  # Group by location\n          .agg({'median_house_value': 'mean'}))  # Calculate means\n\n\n\nTrain-Test Split:\n\nAlways set random_state for reproducibility\nAlways split data BEFORE any other operations\nUse meaningful variable names: X_train, X_test, y_train, y_test\n\n# GOOD: Always set random_state\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42\n)\nModel Workflow:\n\nAlways follow the same pattern: instantiate → fit → predict → evaluate\nShow the complete workflow in early examples\nUse consistent variable names: model, predictions, y_pred\n\n# The standard pattern\nmodel = LinearRegression()  # 1. Instantiate\nmodel.fit(X_train, y_train)  # 2. Fit\ny_pred = model.predict(X_test)  # 3. Predict\nscore = model.score(X_test, y_test)  # 4. Evaluate\nFeature Scaling:\n\nAlways fit scaler on training data only, then transform both train and test\nNever fit on the full dataset or test set\n\n# GOOD: Fit on train, transform both\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Only transform, don't fit\n\n# BAD: Fitting on test data\n# X_test_scaled = scaler.fit_transform(X_test)  # DON'T DO THIS\nModel Comparison:\n\nWhen comparing models, use consistent train/val/test splits\nStore results in a DataFrame for easy comparison\n\n# GOOD: Systematic comparison\nresults = []\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    results.append({'Model': name, 'Score': score})\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\nFigure Display:\n\nAlways use plt.show() to display figures in Quarto documents\nSet figure size explicitly with plt.figure(figsize=(width, height))\n\n# GOOD: Explicit figure size and show\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\nLabels and Titles:\n\nAlways include title, xlabel, and ylabel\nUse descriptive labels with units\nMake figures self-explanatory\n\nColor Usage:\n\nUse colorblind-friendly palettes: 'Set2', 'colorblind', 'husl'\nNever rely solely on red-green distinctions\n\n\n\n\nRandom State:\n\nAlways set np.random.seed() for reproducibility in examples\n\n# GOOD: Set seed for reproducibility\nnp.random.seed(42)\nrandom_values = np.random.randn(100)\nArray Creation:\n\nUse np.array() for small examples, but prefer Pandas for tabular data\nShow array shape and dtype when relevant\n\n\n\n\nVariable Naming:\n\nUse descriptive variable names: housing_df, X_train, y_pred\nFollow conventions: X (features), y (target), df (DataFrame)\nAvoid single letters except in mathematical contexts or brief examples\n\nComments:\n\nComment the “why”, not the “what”\nUse comments to explain reasoning and decisions\nAdd comments for code that students will modify or extend\n\n# GOOD: Explains reasoning\n# Use median instead of mean to avoid influence of outliers\nmedian_bedrooms = housing_df['total_bedrooms'].median()\n\n# BAD: States the obvious\n# Calculate median\nmedian_bedrooms = housing_df['total_bedrooms'].median()\nError Handling:\n\nUse #| error: true in Quarto code blocks that intentionally raise errors\nShow errors when teaching debugging or demonstrating common mistakes\n\n#| error: true\n# This will raise an error to demonstrate the problem\nassert 2 &gt; 3, \"This assertion will fail\"\n\n\n\nProgressive Complexity:\n\nStart with simplest working example\nAdd complexity gradually\nShow complete workflow first, then optimize\n\nReusable Patterns:\n\nEstablish patterns early and reuse them\nHighlight when you’re using a pattern seen before\nMake patterns explicit: “This is the same workflow we used for…”\n\nAvoid:\n\nMagic numbers without explanation\nUnexplained imports (always show imports)\nCopy-pasted code without variation\nExamples that don’t run or produce errors unintentionally\nDeprecated functions or outdated syntax"
  },
  {
    "objectID": "Claude.html#course-philosophy",
    "href": "Claude.html#course-philosophy",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "This course emphasizes conceptual understanding and practical application over implementation details. Students will develop the analytical thinking needed to select appropriate models, recognize when assumptions are violated, and critically evaluate model performance. While AI coding assistants are powerful tools, this course ensures students build the foundational knowledge to use them effectively and debug their output."
  },
  {
    "objectID": "Claude.html#agentic-planning",
    "href": "Claude.html#agentic-planning",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "When developing this course, Claude should follow the following guidelines:\n\nUtilize the existing folder Claude-planning by creating Markdown files to keep track of progress. Claude should keep one file per topic, and each file should have a clear title and a clear description of the topic. Each file should outline specifically what is being taught, which parts are done “by hand” (either programming or mathematically) and which parts are done with AI coding assistants. Claude should describe the in-class activities and homework which will enforce learning these topics.\nWhen designing a lesson and/or topic, Claude should refer back to the existing Claude-planning folder to ensure consistency and avoid redundancy. In addition, when possible, Claude should try to build on previous lessons and topics to create a cohesive and progressive learning experience. This should be transparent to the learner, with lessons refering back to previous lessons and topics whenever possible."
  },
  {
    "objectID": "Claude.html#assignment-templates",
    "href": "Claude.html#assignment-templates",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "When creating homework assignments or quizzes, Claude must follow the templates defined in the Templates/ folder:\n\n\nTemplate Location: Templates/homework-template.md\nKey Requirements:\n\nTwo-Part Structure:\n\nPart A: By Hand (40-45 points) - 10 questions without AI assistance to build foundational skills\nPart B: With AI Assistance (55-60 points) - 10 questions using AI coding assistants to scale analysis\nTotal: 100 points across 20 questions\n\nPart A Focus (By Hand):\n\nBasic implementation of techniques/models\nEvaluation and interpretation of results\nWriting helper functions to demonstrate understanding\nExplaining concepts in own words\nCreating simple visualizations\nQuestion range: 3-5 points each\n\nPart B Focus (AI-Assisted):\n\nHyperparameter tuning at scale\nComparing 5+ models/approaches simultaneously\nBuilding automation pipelines\nCross-validation and robustness testing\nFinal comprehensive analysis with executive summary\nQuestion range: 5-7 points each\nMUST require students to submit their AI prompts used\n\nRequired Sections:\n\nInstructions (including dataset descriptions)\nPart A questions (1-10)\nPart B questions (11-20)\nSubmission Guidelines\nGrading Rubric\nTips for Success (separate subsections for Part A, Part B, and General)\n\nRequired Deliverables for Part B:\n\nCode (AI-generated but student-verified)\nOutputs (visualizations, tables, results)\nWritten interpretation (student’s own analysis)\nThe prompt(s) used with AI assistant\n\nTips Section Must End With:\n\n“Use your brain. That’s what it’s there for.”\n\n\n\n\n\nTemplate Location: Templates/quiz-template.md\nKey Requirements:\n\nTwo-Section Structure:\n\nSection A: Conceptual Questions (~50% of points) - Testing understanding without a computer\nSection B: Code Writing (~50% of points) - Writing code by hand\nTotal: Typically 25-50 points\nTime: 30-45 minutes\n\nSection A Focus (Conceptual):\n\nScenario identification (classification vs regression, problem types)\nMetric interpretation and trade-off reasoning\nModel selection justification\nConceptual explanations\nProblem diagnosis\nQuestion range: 2-4 points each\n6-8 questions total\n\nSection B Focus (Code Writing):\n\nData manipulation (load, filter, split)\nModel instantiation with parameters\nFit/predict workflow\nMetric calculation\nSimple function writing\nQuestion range: 2-4 points each\n5-7 questions total\nMust include assumption section listing all imports\n\nRequired Elements:\n\nInstructions (time, format, note about partial credit)\nSection A questions\nAssumption section before Section B (listing imports and available variables)\nSection B questions\nGrading Rubric with statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\nCode Writing Philosophy:\n\nSyntax perfection NOT required\nFocus on logic and correct approach\nGrading: 70-80% for correct logic, 20-30% for syntax\nQuestions must be realistic to write by hand\n\n\n\n\n\nTemplate Location: Templates/textbook-chapter-template.md\nKey Requirements:\n\nStructure:\n\nFront matter (YAML with title, format settings)\nModule Resources section linking to homework and quiz\nIntroduction (3-5 engaging paragraphs)\n8-10 major sections covering core topics\nSummary (synthesizing key concepts)\nPractice Exercises (4-6 exercises)\nAdditional Resources\n\nCode Examples:\n\nInclude code every 2-3 paragraphs maximum\nUse executable Python code with proper imports\nBuild complexity gradually throughout the chapter\nExplain what code does and why it matters\nUse realistic datasets and examples\n\nImages and Visualizations:\n\nInclude 2-3 images/diagrams per chapter\nCreate images/ subfolder in module folder\nCreate placeholder-info.md describing needed images\nUse relative paths: ![Description](images/filename.png)\n\nWriting Style:\n\nConversational and casual tone\nUse probing questions rather than direct statements\nMaximum 2-3 paragraphs before showing code\nStart with intuition, then formalize\nConnect to what students already know\nEnd summary with: “Use your brain. That’s what it’s there for.”\n\nQuarto-Specific Requirements:\n\nUse {python} for executable code blocks (not standard markdown code blocks)\nInclude jupyter: python3 in YAML front matter\nSet code-fold: false to show all code by default\nUse callout boxes for tips, notes, and warnings:\n\n::: {.callout-tip} for helpful tips\n::: {.callout-note} for important information\n::: {.callout-warning} for cautions\n\n\n\n\n\n\nWhen asked to create homework, quiz, or textbook chapter for a module:\n\nRead the appropriate template from Templates/homework-template.md, Templates/quiz-template.md, or Templates/textbook-chapter-template.md\nReview the module-specific adaptations section in the template\nFollow the structure exactly - do not deviate from point distributions, section organization, or required elements\nRefer to content-overview.md to understand:\n\nCore topics for the module\nWhat should be done by-hand vs with AI\nLearning objectives\nAssessment focus\n\nCreate or update the Claude-planning file for the module first, then generate content\nEnsure content aligns with the “Topics Done By-Hand” and “Topics Done With AI” from content-overview.md"
  },
  {
    "objectID": "Claude.html#tech-stack",
    "href": "Claude.html#tech-stack",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "Lessons will be written as Quarto Markdown documents, with the programming language used being Python 3.12.\nStudents will use the Gemini CLI as the primary AI coding assistant. The primary model will be Flash 2.5, which means code generation skills are limited. Because of this, the text should encourage students to work on small components at a time, rather than prompting the model to build entire applications all at once.\nStudents will host their work on Github.\nPython packages used include:\n\nPandas\nNumPy\nSeaborn\nScikit-learn\nPyTorch\nStreamlit\nHuggingface"
  },
  {
    "objectID": "Claude.html#style-guidelines",
    "href": "Claude.html#style-guidelines",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "The professor writes in a casual, conversational manner that emphasizes understanding over formality. Key characteristics:\nTone and Voice:\n\nWrite as if talking directly to students\nUse probing questions to guide thinking rather than directly stating answers\nAvoid overly formal academic language\nBe direct and honest about challenges and complexities\nShow enthusiasm for the subject matter\n\nCommon Phrases and Patterns:\n\n“Let’s jump in” / “Let’s start with…”\n“Here’s the thing…” / “Here’s what’s happening…”\n“But wait—why does that work?”\n“Don’t worry about X right now, just…”\n“See what happened?” / “Notice what’s going on here?”\n“Use your brain. That’s what it’s there for.” (ending phrase)\n\nExample of the Professor’s Writing Style:\n\"Linear regression is perhaps the most important idea in machine learning. Once you understand the ins-and-outs of linear regression you can understand most other machine learning models as well. This is because the ideas developed for machine learning were first perfected on linear regression and then applied to other models. Let's jump in.\n\nLinear regression (LR) is what you have probably referred to as \"line of best fit\". It is a line meant to fit the data \"as well as possible\". I put that last phrase in quotes, because what exactly do we mean by a \"best fit\"? We will formulate this mathematically in the next section.\n\nWith that out of the way, we now turn to our next question: why do we care about linear regression? Linear regression is extremely important because it allows us to make predictions. Up until this point we have only explored and described the past by looking at datasets which (necessarily) had data about the past. However, the point of data science is largely to make predictions about the future using data from the past. This works because a line doesn't care what data we plug in. We can plug in data from the past in order to verify and explain past performance. But we can also plug in future numbers (dates, pricing changes, expected changes to our products, etc.) and see what the model returns.\n\nLet's start with a simple example. Don't worry about the code right now, just look at the graphs. We will work again with our data/boston.csv dataset, describing the median home value in various neighborhoods in Boston.\n\nIn blue is the actual measurements of poverty and home value for all neighborhoods. In red is the predicted value using the line of best fit. We can see that the regression line seems to fit the data fairly well, at least in all except the far left and right ends.\n\nOne interesting thing to note is that the regression line generally seems too high. For example, if we draw the same graph, but only keep poverty values between 5% and 20% we get the following:\n\nWhy is that? The reason is that the regression line is heavily affected by outliers. So the neighborhoods with low crime and high home value are throwing off the line and \"dragging it up.\" In general, you want the following four things to be true before using LR for making predictions:\n\n1. The data should be approximately linear\n\n2. The observations should be independent (so the crime rate and median home value in one neighborhood should be independent of other neighborhoods)\n\n3. The variance between the measurements should be approximately the same throughout the graph (the graph is more or less spread out the same amount in different areas)\n\n4. The points should be approximately normally distributed around the regression line.\n\nNone of these four are perfectly satisfied. However, that doesn't mean you can't use LR. It just means that you need to be careful when making predictions. Don't just make a regression line and say \"see, this predicts the future!\" Use your brain, that's what it's there for.\"\n\n\n\nConceptual Explanations:\n\nStart with intuition and concrete examples before formalizing\nUse analogies and real-world scenarios to ground abstract concepts\nAsk guiding questions that lead students to understanding\nBuild complexity gradually—simple cases first, then complications\nConnect new concepts to what students already know\nExplain the “why” before diving into the “how”\n\nCode Demonstrations:\n\nInclude code examples every 2-3 paragraphs maximum\nNever show code without explaining what it does and why it matters\nBuild code examples incrementally (don’t dump large blocks)\nUse realistic datasets that students can relate to\nComment code to explain reasoning, not just mechanics\nShow output or visualizations when relevant\nFollow this pattern:\n\nBrief intro to what you’re demonstrating\nShow the code\nExplain what happened and why\nConnect to the bigger picture\n\n\nCode Formatting for Quarto:\n\nUse {python} for executable code blocks: ```{python}\nUse regular markdown code blocks only for non-executable examples\nAdd #| error: true to code blocks that intentionally raise errors\nUse #| echo: true (default) to show code\nUse #| output: true to display output\n\nExamples and Datasets:\n\nUse consistent, realistic datasets throughout a chapter\nCalifornia housing data (from Module 1) is the primary dataset\nInclude real-world context for all examples\nMake examples build on each other when possible\nShow both successes and failures (what not to do)\n\nVisual Elements:\n\nInclude 2-3 images/diagrams per chapter minimum\nCreate images/ subfolder with placeholder-info.md\nUse callout boxes for important points:\n\n::: {.callout-tip} for helpful tips and best practices\n::: {.callout-note} for important information students must remember\n::: {.callout-warning} for common mistakes and cautions\n\nUse concrete visualizations to explain abstract concepts\n\nSection Flow:\n\nEach major section should have a clear narrative arc\nStart sections with context: why does this matter?\nEnd sections with synthesis: what did we learn?\nUse transitions to connect sections logically\nReference earlier content when building on it\nPreview upcoming content when it’s relevant\n\nPractical Examples:\n\nAlways ground theory in practice\nShow real scenarios where the concept matters\nInclude “what could go wrong” examples\nDemonstrate debugging and problem-solving approaches\nConnect to real data science workflows\n\nEnding Every Chapter:\n\nSummary must synthesize, not just list topics\nEmphasize key takeaways and connections\nAlways end with: “Use your brain. That’s what it’s there for.”\n\nWriting Theoretical Concepts:\nWhen explaining theoretical or conceptual material (like ML theory, loss functions, optimization, etc.):\n\nStart with the practical problem before introducing theory\n\nExample: “Models fail. They overfit. They underfit. Understanding what’s going on ‘under the hood’ helps you recognize these problems…”\nLead with why this matters in practice, then explain the theory\n\nUse concrete, visual examples first\n\nShow actual code and visualizations before equations\nLet students see the concept in action before formalizing it\nExample: Demonstrate gradient descent with a simple quadratic function and visualization before explaining the algorithm\n\nExplain concepts in plain English\n\n“Cross-entropy heavily penalizes confident wrong predictions”\n“The gradient is the slope of the loss with respect to parameters”\nAvoid jargon or explain it immediately when used\n\nUse analogies and metaphors\n\n“Think of it as navigating a landscape where height represents loss, and you’re trying to find the lowest valley”\n“Bias: fitting a straight line to clearly curved data”\n“Variance: fitting a wiggly line that goes through every single training point”\n\nBreak down mathematical concepts step-by-step\n\nIntroduce notation gradually\nExplain what each symbol means in context\nShow the formula, then explain it in words\nExample: Show cross-entropy formula, then explain “this heavily penalizes confident wrong predictions”\n\nUse callout boxes strategically\n\n::: {.callout-note} for connecting concepts (“Cross-entropy is just the log-likelihood for binary classification”)\n::: {.callout-warning} for common misconceptions or pitfalls\n::: {.callout-tip} for practical advice about when/how to use concepts\n\nConnect theory back to practice repeatedly\n\n“In practice, you rarely need to implement loss functions yourself—scikit-learn handles it”\n“Understanding what they’re doing helps you choose the right one and diagnose problems”\nAlways answer “why does this matter for my work?”\n\nUse contrasts and comparisons\n\n“MSE: Penalizes large errors heavily (good when outliers are costly)”\n“MAE: Treats all errors equally (good when outliers shouldn’t dominate)”\nSide-by-side visualizations showing different approaches\n\nEmphasize intuition over rigor\n\nGet the concept across first, worry about edge cases later\n“The idea is beautifully simple: calculate the gradient, take a step downhill, repeat”\nDon’t get bogged down in mathematical proofs\n\nUse progressive disclosure\n\nStart with 1D examples before moving to higher dimensions\n“In one dimension, the gradient is simply the derivative. In higher dimensions…”\nBuild from simple → complex gradually\n\nFrame concepts as tools, not just theory\n\n“Loss functions define what ‘good’ means”\n“The bias-variance tradeoff is your guide for choosing model complexity”\nPresent concepts as practical tools students will use"
  },
  {
    "objectID": "Claude.html#technical-guidelines",
    "href": "Claude.html#technical-guidelines",
    "title": "MATH 3339 - Introduction to Data Science - AI-assisted redesign",
    "section": "",
    "text": "When writing code examples in textbook chapters and assignments, follow these package-specific best practices to ensure code is clear, correct, and pedagogically sound.\n\n\nDataFrame Display:\n\nNever use print() for DataFrames - Instead, put the DataFrame (or df.head()) as the last line of the cell to use Jupyter’s rich display\nAlways use .head() to show DataFrames - This prevents overwhelming output and teaches good practices\n\n# BAD: Using print()\nprint(housing_df)\n\n# GOOD: Let Jupyter display it\nhousing_df.head()\nDataFrame Creation:\n\nNever subset data when creating DataFrames - Create the full DataFrame first, then display a subset\nThis prevents confusing students about what’s actually in the DataFrame\n\n# BAD: Subsetting during creation\ncomparison = pd.DataFrame({\n    'Actual': y_test.head(10),\n    'Predicted': predictions[:10]\n})\ncomparison\n\n# GOOD: Full DataFrame, then show subset\ncomparison = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': predictions\n})\ncomparison.head()\nColumn Selection:\n\nAlways demonstrate both single-column (Series) and multi-column (DataFrame) selection\nEmphasize that double brackets [[]] for multiple columns are just a list inside brackets\n\n# Single column (returns Series)\nhouse_values = housing_df['median_house_value']\n\n# Multiple columns (returns DataFrame)\nsubset = housing_df[['median_house_value', 'ocean_proximity']]\nChaining Operations:\n\nBreak complex chains across multiple lines for readability\nAdd comments explaining each step\n\n# GOOD: Clear, multi-line chaining\nresult = (housing_df\n          .dropna()  # Remove missing values\n          .groupby('ocean_proximity')  # Group by location\n          .agg({'median_house_value': 'mean'}))  # Calculate means\n\n\n\nTrain-Test Split:\n\nAlways set random_state for reproducibility\nAlways split data BEFORE any other operations\nUse meaningful variable names: X_train, X_test, y_train, y_test\n\n# GOOD: Always set random_state\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42\n)\nModel Workflow:\n\nAlways follow the same pattern: instantiate → fit → predict → evaluate\nShow the complete workflow in early examples\nUse consistent variable names: model, predictions, y_pred\n\n# The standard pattern\nmodel = LinearRegression()  # 1. Instantiate\nmodel.fit(X_train, y_train)  # 2. Fit\ny_pred = model.predict(X_test)  # 3. Predict\nscore = model.score(X_test, y_test)  # 4. Evaluate\nFeature Scaling:\n\nAlways fit scaler on training data only, then transform both train and test\nNever fit on the full dataset or test set\n\n# GOOD: Fit on train, transform both\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Only transform, don't fit\n\n# BAD: Fitting on test data\n# X_test_scaled = scaler.fit_transform(X_test)  # DON'T DO THIS\nModel Comparison:\n\nWhen comparing models, use consistent train/val/test splits\nStore results in a DataFrame for easy comparison\n\n# GOOD: Systematic comparison\nresults = []\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    results.append({'Model': name, 'Score': score})\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\nFigure Display:\n\nAlways use plt.show() to display figures in Quarto documents\nSet figure size explicitly with plt.figure(figsize=(width, height))\n\n# GOOD: Explicit figure size and show\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\nLabels and Titles:\n\nAlways include title, xlabel, and ylabel\nUse descriptive labels with units\nMake figures self-explanatory\n\nColor Usage:\n\nUse colorblind-friendly palettes: 'Set2', 'colorblind', 'husl'\nNever rely solely on red-green distinctions\n\n\n\n\nRandom State:\n\nAlways set np.random.seed() for reproducibility in examples\n\n# GOOD: Set seed for reproducibility\nnp.random.seed(42)\nrandom_values = np.random.randn(100)\nArray Creation:\n\nUse np.array() for small examples, but prefer Pandas for tabular data\nShow array shape and dtype when relevant\n\n\n\n\nVariable Naming:\n\nUse descriptive variable names: housing_df, X_train, y_pred\nFollow conventions: X (features), y (target), df (DataFrame)\nAvoid single letters except in mathematical contexts or brief examples\n\nComments:\n\nComment the “why”, not the “what”\nUse comments to explain reasoning and decisions\nAdd comments for code that students will modify or extend\n\n# GOOD: Explains reasoning\n# Use median instead of mean to avoid influence of outliers\nmedian_bedrooms = housing_df['total_bedrooms'].median()\n\n# BAD: States the obvious\n# Calculate median\nmedian_bedrooms = housing_df['total_bedrooms'].median()\nError Handling:\n\nUse #| error: true in Quarto code blocks that intentionally raise errors\nShow errors when teaching debugging or demonstrating common mistakes\n\n#| error: true\n# This will raise an error to demonstrate the problem\nassert 2 &gt; 3, \"This assertion will fail\"\n\n\n\nProgressive Complexity:\n\nStart with simplest working example\nAdd complexity gradually\nShow complete workflow first, then optimize\n\nReusable Patterns:\n\nEstablish patterns early and reuse them\nHighlight when you’re using a pattern seen before\nMake patterns explicit: “This is the same workflow we used for…”\n\nAvoid:\n\nMagic numbers without explanation\nUnexplained imports (always show imports)\nCopy-pasted code without variation\nExamples that don’t run or produce errors unintentionally\nDeprecated functions or outdated syntax"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html",
    "href": "Templates/textbook-chapter-template.html",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "All textbook chapters should follow this structure:\n\nFront Matter (YAML header with title, format settings)\nModule Resources (links to homework, quiz)\nIntroduction (engaging overview of the chapter)\nMain Content Sections (8-10 major sections covering core topics)\nSummary (recap of key points)\nPractice Exercises (4-6 exercises)\nAdditional Resources (links to external resources)\n\n\n\n\nFollow the professor’s casual, probing style:\n\nWrite conversationally, as if talking to students\nUse probing questions rather than direct answers\nExample: “But wait—why does that work? Let’s think about it…”\nAvoid overly formal academic language\nInclude phrases like “Let’s jump in”, “Here’s the thing”, “Don’t worry about X right now”\nEnd sections with thought-provoking questions or connections\n\nCode demonstrations:\n\nInclude code every 2-3 paragraphs maximum\nDon’t just show code—explain what it’s doing and why\nBuild up complexity gradually\nUse realistic datasets and examples\nComment code to explain reasoning, not just what the code does\n\nConceptual explanations:\n\nStart with intuition, then formalize\nUse analogies and real-world examples\nConnect to what students already know\nExplain the “why” before the “how”\nMake abstract concepts concrete through visualization\n\n\n\n\nMajor sections (numbered with ##):\n\n8-10 major sections per chapter\nEach covers one core topic from the module\nShould flow logically, building on previous sections\n\nSubsections (numbered with ###):\n\n2-4 subsections per major section\nBreak down the major topic into digestible pieces\nInclude at least one code example per subsection\n\n\n\n\nRequirements:\n\nEvery subsection should have at least one code example\nMaximum 2-3 paragraphs before showing code\nCode blocks should be executable (use proper imports)\nInclude output or visualization when relevant\nBuild complexity gradually across the chapter\n\nFormat:\n# Example structure\nimport pandas as pd\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv('example.csv')\n\n# Do something meaningful\nresult = df.groupby('category')['value'].mean()\nprint(result)\nExplanation pattern:\n\nBrief intro to what you’re going to do\nShow the code\nExplain what happened and why it matters\nConnect to the bigger picture\n\n\n\n\nRequirements:\n\nInclude 2-3 images/diagrams per chapter\nCreate an images/ subfolder in each module’s textbook folder\nUse descriptive filenames (e.g., bias-variance-tradeoff.png)\nReference images with relative paths: ![Description](images/filename.png)\nCreate a placeholder-info.md file describing needed images\n\nTypes of images:\n\nWorkflow diagrams\nConceptual illustrations\nExample visualizations (good vs. bad)\nScreenshots of tools/interfaces\nMathematical concepts visualized\n\n\n\n\nAt the top of each chapter, include:\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\nNote: Use proper URL encoding for spaces in paths (%20)\n\n\n\nPurpose:\n\nHook the reader with why this topic matters\nPreview what they’ll learn\nConnect to previous modules (if applicable)\nSet expectations and tone\n\nLength: 3-5 paragraphs\nStyle:\n\nStart with a compelling question or scenario\nUse conversational tone\nAvoid listing learning objectives formally\nBuild excitement about the topic\n\n\n\n\nPurpose:\n\nRecap the main concepts covered\nEmphasize key takeaways\nConnect back to the big picture\nPrepare students for what’s next\n\nLength: 3-4 paragraphs\nStyle:\n\nDon’t just list topics covered\nSynthesize the main ideas\nReinforce why these concepts matter\nEnd with: “Use your brain. That’s what it’s there for.”\n\n\n\n\nRequirements:\n\n4-6 exercises per chapter\nRange from simple to complex\nMix conceptual and coding exercises\nShould be doable without AI assistance\nAlign with “Topics Done By-Hand” from module overview\n\nFormat:\n## Practice Exercises\n\n1. **[Exercise name]:** [Description of what to do]\n\n2. **[Exercise name]:** [Description of what to do]\n\n[etc.]\n\n\n\nInclude:\n\nOfficial documentation links (Pandas, Scikit-learn, etc.)\nRelevant tutorials or guides\nTool-specific resources (Gemini CLI, etc.)\nVisualization galleries\nArticles or videos for deeper learning\n\nFormat:\n## Additional Resources\n\n- [Resource name](URL) - Brief description\n- [Resource name](URL) - Brief description\n[etc.]\n\n\n\nWhen creating a template or draft chapter:\n\nUse XXX for content placeholders\nInclude brief descriptions of what should go there\nLeave code blocks with comments showing what example to include\nMark image locations with descriptive placeholder filenames\n\n\n\n\n\n\n---\ntitle: \"Chapter [X]: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\n\n---\n\n## Introduction\n\nXXX - Engaging introduction (3-5 paragraphs)\n- Why this topic matters\n- What they'll learn\n- Connection to data science workflow\n\n---\n\n## 1. [First Major Topic]\n\n### 1.1 [First Subtopic]\n\nXXX - Conceptual explanation (2-3 paragraphs)\n\n```python\n# XXX - Code example demonstrating the concept\n# code here\nXXX - Explanation of what the code does and why it matters\n\n\nXXX - Continue pattern…\n\n\n\nXXX - Continue pattern…\n\n\n\nPlaceholder for diagram\n\n\n\n\n\n\n\n\n\nXXX - Content…\n# XXX - Code example\n\n\n\nXXX - Content…\n\n[Continue for 8-10 major sections…]\n\n\n\n\n\nXXX - Synthesis of main concepts (3-4 paragraphs) - Key takeaways - Why these concepts matter - Connection to future work - End with: “Use your brain. That’s what it’s there for.”\n\n\n\n\n\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n\n\n\n\n\n\nResource name - Description\nResource name - Description\nResource name - Description\n\n\n---\n\n## Directory Structure\n\nFor each module's textbook chapter:\n\nTextbook/ Module-X-Topic/ chapter-x-topic.qmd # Main chapter file images/ # Images folder placeholder-info.md # Description of needed images diagram1.png # Actual images (to be created) diagram2.png ```\n\n\n\n\n\n\n\nEmphasize AI prompting throughout\nInclude examples of good vs. bad prompts\nShow when to use AI vs. hand-coding\nFocus on Pandas, Seaborn, testing\n\n\n\n\n\nFocus on model instantiation, fitting, predicting\nShow different model types in action\nEmphasize evaluation metrics\nCompare models systematically\n\n\n\n\n\nHeavy use of visualization to explain concepts\nLoss functions shown with code and plots\nBias-variance visualized extensively\nTheoretical concepts made concrete\n\n\n\n\n\nExtensive diagnostic plots\nResidual analysis examples\nCoefficient interpretation in context\nRegularization effects visualized\n\n\n\n\n\nConfusion matrices and ROC curves\nMetric interpretation in business context\nClass imbalance examples\nDecision boundaries visualized\n\n\n\n\n\nPyTorch code structure emphasized\nTraining curves and interpretation\nArchitecture diagrams\nConnection to earlier concepts\n\n\n\n\n\nHugging Face examples\nModel card interpretation\nTransfer learning workflow\nFine-tuning examples\n\n\n\n\n\n\nBefore finalizing a chapter, verify:\n\nModule Resources section links to correct homework/quiz\nIntroduction is engaging and sets proper expectations\n8-10 major sections covering all core topics\nCode examples every 2-3 paragraphs\nAll code is executable and properly formatted\n2-3 images included with placeholder info\nWriting style matches professor’s casual tone\nConcepts explained with intuition before formalization\nSummary synthesizes key ideas\n4-6 practice exercises included\nAdditional Resources section populated\nEnds with “Use your brain. That’s what it’s there for.”"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#template-guidelines",
    "href": "Templates/textbook-chapter-template.html#template-guidelines",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "All textbook chapters should follow this structure:\n\nFront Matter (YAML header with title, format settings)\nModule Resources (links to homework, quiz)\nIntroduction (engaging overview of the chapter)\nMain Content Sections (8-10 major sections covering core topics)\nSummary (recap of key points)\nPractice Exercises (4-6 exercises)\nAdditional Resources (links to external resources)\n\n\n\n\nFollow the professor’s casual, probing style:\n\nWrite conversationally, as if talking to students\nUse probing questions rather than direct answers\nExample: “But wait—why does that work? Let’s think about it…”\nAvoid overly formal academic language\nInclude phrases like “Let’s jump in”, “Here’s the thing”, “Don’t worry about X right now”\nEnd sections with thought-provoking questions or connections\n\nCode demonstrations:\n\nInclude code every 2-3 paragraphs maximum\nDon’t just show code—explain what it’s doing and why\nBuild up complexity gradually\nUse realistic datasets and examples\nComment code to explain reasoning, not just what the code does\n\nConceptual explanations:\n\nStart with intuition, then formalize\nUse analogies and real-world examples\nConnect to what students already know\nExplain the “why” before the “how”\nMake abstract concepts concrete through visualization\n\n\n\n\nMajor sections (numbered with ##):\n\n8-10 major sections per chapter\nEach covers one core topic from the module\nShould flow logically, building on previous sections\n\nSubsections (numbered with ###):\n\n2-4 subsections per major section\nBreak down the major topic into digestible pieces\nInclude at least one code example per subsection\n\n\n\n\nRequirements:\n\nEvery subsection should have at least one code example\nMaximum 2-3 paragraphs before showing code\nCode blocks should be executable (use proper imports)\nInclude output or visualization when relevant\nBuild complexity gradually across the chapter\n\nFormat:\n# Example structure\nimport pandas as pd\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv('example.csv')\n\n# Do something meaningful\nresult = df.groupby('category')['value'].mean()\nprint(result)\nExplanation pattern:\n\nBrief intro to what you’re going to do\nShow the code\nExplain what happened and why it matters\nConnect to the bigger picture\n\n\n\n\nRequirements:\n\nInclude 2-3 images/diagrams per chapter\nCreate an images/ subfolder in each module’s textbook folder\nUse descriptive filenames (e.g., bias-variance-tradeoff.png)\nReference images with relative paths: ![Description](images/filename.png)\nCreate a placeholder-info.md file describing needed images\n\nTypes of images:\n\nWorkflow diagrams\nConceptual illustrations\nExample visualizations (good vs. bad)\nScreenshots of tools/interfaces\nMathematical concepts visualized\n\n\n\n\nAt the top of each chapter, include:\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\nNote: Use proper URL encoding for spaces in paths (%20)\n\n\n\nPurpose:\n\nHook the reader with why this topic matters\nPreview what they’ll learn\nConnect to previous modules (if applicable)\nSet expectations and tone\n\nLength: 3-5 paragraphs\nStyle:\n\nStart with a compelling question or scenario\nUse conversational tone\nAvoid listing learning objectives formally\nBuild excitement about the topic\n\n\n\n\nPurpose:\n\nRecap the main concepts covered\nEmphasize key takeaways\nConnect back to the big picture\nPrepare students for what’s next\n\nLength: 3-4 paragraphs\nStyle:\n\nDon’t just list topics covered\nSynthesize the main ideas\nReinforce why these concepts matter\nEnd with: “Use your brain. That’s what it’s there for.”\n\n\n\n\nRequirements:\n\n4-6 exercises per chapter\nRange from simple to complex\nMix conceptual and coding exercises\nShould be doable without AI assistance\nAlign with “Topics Done By-Hand” from module overview\n\nFormat:\n## Practice Exercises\n\n1. **[Exercise name]:** [Description of what to do]\n\n2. **[Exercise name]:** [Description of what to do]\n\n[etc.]\n\n\n\nInclude:\n\nOfficial documentation links (Pandas, Scikit-learn, etc.)\nRelevant tutorials or guides\nTool-specific resources (Gemini CLI, etc.)\nVisualization galleries\nArticles or videos for deeper learning\n\nFormat:\n## Additional Resources\n\n- [Resource name](URL) - Brief description\n- [Resource name](URL) - Brief description\n[etc.]\n\n\n\nWhen creating a template or draft chapter:\n\nUse XXX for content placeholders\nInclude brief descriptions of what should go there\nLeave code blocks with comments showing what example to include\nMark image locations with descriptive placeholder filenames"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#chapter-template-structure",
    "href": "Templates/textbook-chapter-template.html#chapter-template-structure",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "---\ntitle: \"Chapter [X]: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Module Resources\n\n**Related Assignments:**\n\n- [Module X Homework](../../Assignments/Module%20X%20-%20Topic/module-x-homework.qmd)\n- [Module X Quiz](../../Assignments/Module%20X%20-%20Topic/module-x-quiz.qmd)\n\n---\n\n## Introduction\n\nXXX - Engaging introduction (3-5 paragraphs)\n- Why this topic matters\n- What they'll learn\n- Connection to data science workflow\n\n---\n\n## 1. [First Major Topic]\n\n### 1.1 [First Subtopic]\n\nXXX - Conceptual explanation (2-3 paragraphs)\n\n```python\n# XXX - Code example demonstrating the concept\n# code here\nXXX - Explanation of what the code does and why it matters\n\n\nXXX - Continue pattern…\n\n\n\nXXX - Continue pattern…\n\n\n\nPlaceholder for diagram"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#second-major-topic",
    "href": "Templates/textbook-chapter-template.html#second-major-topic",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "XXX - Content…\n# XXX - Code example\n\n\n\nXXX - Content…\n\n[Continue for 8-10 major sections…]"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#summary",
    "href": "Templates/textbook-chapter-template.html#summary",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "XXX - Synthesis of main concepts (3-4 paragraphs) - Key takeaways - Why these concepts matter - Connection to future work - End with: “Use your brain. That’s what it’s there for.”"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#practice-exercises",
    "href": "Templates/textbook-chapter-template.html#practice-exercises",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description\n[Exercise name]: XXX - Description"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#additional-resources",
    "href": "Templates/textbook-chapter-template.html#additional-resources",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "Resource name - Description\nResource name - Description\nResource name - Description\n\n\n---\n\n## Directory Structure\n\nFor each module's textbook chapter:\n\nTextbook/ Module-X-Topic/ chapter-x-topic.qmd # Main chapter file images/ # Images folder placeholder-info.md # Description of needed images diagram1.png # Actual images (to be created) diagram2.png ```"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#module-specific-adaptations",
    "href": "Templates/textbook-chapter-template.html#module-specific-adaptations",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "Emphasize AI prompting throughout\nInclude examples of good vs. bad prompts\nShow when to use AI vs. hand-coding\nFocus on Pandas, Seaborn, testing\n\n\n\n\n\nFocus on model instantiation, fitting, predicting\nShow different model types in action\nEmphasize evaluation metrics\nCompare models systematically\n\n\n\n\n\nHeavy use of visualization to explain concepts\nLoss functions shown with code and plots\nBias-variance visualized extensively\nTheoretical concepts made concrete\n\n\n\n\n\nExtensive diagnostic plots\nResidual analysis examples\nCoefficient interpretation in context\nRegularization effects visualized\n\n\n\n\n\nConfusion matrices and ROC curves\nMetric interpretation in business context\nClass imbalance examples\nDecision boundaries visualized\n\n\n\n\n\nPyTorch code structure emphasized\nTraining curves and interpretation\nArchitecture diagrams\nConnection to earlier concepts\n\n\n\n\n\nHugging Face examples\nModel card interpretation\nTransfer learning workflow\nFine-tuning examples"
  },
  {
    "objectID": "Templates/textbook-chapter-template.html#quality-checklist",
    "href": "Templates/textbook-chapter-template.html#quality-checklist",
    "title": "Textbook Chapter Template",
    "section": "",
    "text": "Before finalizing a chapter, verify:\n\nModule Resources section links to correct homework/quiz\nIntroduction is engaging and sets proper expectations\n8-10 major sections covering all core topics\nCode examples every 2-3 paragraphs\nAll code is executable and properly formatted\n2-3 images included with placeholder info\nWriting style matches professor’s casual tone\nConcepts explained with intuition before formalization\nSummary synthesizes key ideas\n4-6 practice exercises included\nAdditional Resources section populated\nEnds with “Use your brain. That’s what it’s there for.”"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html",
    "href": "Claude-planning/module-4-llms-feature-engineering.html",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Duration: 2-3 weeks\nCore Philosophy: This module bridges the gap between unstructured data and traditional machine learning by showing students how to use Large Language Models (LLMs) as powerful feature engineering tools. Rather than diving into training LLMs or fine-tuning (covered in Module 8), this module focuses on the practical task of extracting structured information from text data to create features for ML models. Students learn to think of LLMs as intelligent data processors that can understand context, extract entities, classify sentiments, and convert messy text into clean, ML-ready features.\n\n\n\nThis module comes after students have learned: - Traditional ML models (Modules 2-5) - How to evaluate and select models - Feature engineering basics from EDA (Module 1)\nAnd before they learn: - Neural networks and deep learning (Module 7) - Fine-tuning pretrained models (Module 8)\nThis positioning is intentional: students understand the value of good features from working with traditional ML, but haven’t yet learned the complexity of training deep learning models. Using LLMs via API demonstrates the power of modern AI while keeping the focus on practical data science workflows.\n\n\n\n\nLLMs as Feature Extractors\n\nWhat LLMs can extract: sentiment, categories, entities, numeric ratings\nWhen to use LLMs vs. traditional NLP (regex, keyword matching, etc.)\nThe “zero-shot” capability: extraction without training\n\nPrompt Engineering for Data Extraction\n\nWriting clear, specific prompts for extraction\nUsing structured output formats (JSON)\nFew-shot prompting: providing examples to guide extraction\nIterating on prompts to improve consistency\n\nCalling LLM APIs\n\nUsing OpenAI API, Anthropic API, or Gemini API\nUnderstanding API parameters (temperature, max_tokens, etc.)\nHandling rate limits and errors\nCost considerations: tokens and pricing\n\nStructured Output Parsing\n\nRequesting JSON responses from LLMs\nParsing JSON into Python dictionaries\nHandling malformed or unexpected responses\nConverting extracted data to pandas DataFrames\n\nValidation and Quality Control\n\nChecking extraction consistency across similar inputs\nIdentifying when LLM fails to extract information\nManual spot-checking vs. automated validation\nHandling missing or low-confidence extractions\n\nIntegration with ML Pipelines\n\nAdding LLM-extracted features to existing datasets\nEncoding categorical features from LLM output\nCreating numeric features from text descriptions\nTraining ML models on LLM-enhanced datasets\n\nCost-Benefit Analysis\n\nCalculating API costs for batch processing\nComparing different LLM providers (GPT-4 vs. GPT-3.5 vs. Claude vs. Gemini)\nWhen cheaper/faster models suffice vs. when quality matters\nCaching and reusing extractions to reduce costs\n\nPractical Considerations\n\nBatch processing large datasets\nManaging API quotas and rate limits\nLatency: real-time vs. offline processing\nLocal models vs. API models (Hugging Face alternatives)\n\n\n\n\n\n\n\n\nBy hand: Writing simple prompts, calling APIs, parsing responses\nWith AI: None yet—building foundational skills\nIn-class: Live demos of LLM extraction, experimenting with different prompts for the same task, discussing what makes a good prompt\n\n\n\n\n\nBy hand: Validating extraction quality, converting to DataFrames, training ML models on extracted features\nWith AI: Building batch processing pipelines, implementing retry logic\nIn-class: Debugging failed extractions, comparing LLM-extracted features with manual labels, discussing when extraction quality is “good enough”\n\n\n\n\n\nBy hand: Cost-benefit analysis, deciding between LLM approaches, interpreting ML model performance with LLM features\nWith AI: Complete end-to-end pipelines, extensive experiments with different LLMs and prompts\nIn-class: Project presentations showing real-world use cases\n\n\n\n\n\nStudents must be able to do these without AI assistance:\n\nPrompt Engineering:\n\nWrite a clear prompt that extracts specific information (sentiment, category, rating)\nInclude examples in prompts (few-shot prompting)\nRequest structured JSON output\nIterate on prompts to improve results\n\nAPI Interaction:\n\nSet up API keys and authentication\nMake a single API call to extract information from text\nUnderstand API parameters (temperature, max_tokens, model selection)\nRead API documentation to understand capabilities\n\nResponse Parsing:\n\nParse JSON responses from LLMs\nHandle cases where JSON is malformed or unexpected\nExtract specific fields from nested JSON\nConvert lists of extractions to pandas DataFrames\n\nQuality Assessment:\n\nManually review 10-20 LLM extractions for accuracy\nIdentify patterns in extraction failures\nCalculate agreement between LLM extractions and ground truth\nDecide when extraction quality is acceptable\n\nCost Calculation:\n\nCount tokens in input and output\nCalculate cost for processing a dataset given token counts and pricing\nCompare costs across different LLM providers\nEstimate budget for batch processing\n\nIntegration:\n\nAdd LLM-extracted features to an existing DataFrame\nEncode categorical LLM outputs for ML models\nSplit data properly (don’t use test set for prompt development!)\nTrain a classifier using LLM-generated features\n\nWhen to Use LLMs:\n\nIdentify when LLM extraction is overkill (simple regex would work)\nIdentify when LLM extraction adds value (complex language understanding needed)\nCompare LLM approach to traditional NLP alternatives\n\n\n\n\n\nAI assistants should help with:\n\nBatch Processing Pipelines:\n\nProcessing hundreds or thousands of text samples\nImplementing rate limiting to avoid API errors\nAdding progress bars and logging\nSaving intermediate results to avoid re-processing\n\nError Handling:\n\nRetry logic for failed API calls\nHandling timeouts and network errors\nGracefully handling malformed JSON responses\nLogging errors for later review\n\nPrompt Template Management:\n\nCreating reusable prompt templates\nParameterizing prompts for different extraction tasks\nA/B testing different prompt variations\nOrganizing prompts for different use cases\n\nQuality Reporting:\n\nGenerating comprehensive reports comparing extraction methods\nCalculating inter-rater reliability between LLMs\nVisualizing extraction distributions\nCreating confusion matrices for classification extractions\n\nComplete End-to-End Pipelines:\n\nText → LLM extraction → DataFrame → ML model → evaluation\nAutomating the entire workflow\nComparing multiple LLM providers programmatically\nHyperparameter tuning on ML models using LLM features\n\nAdvanced Techniques:\n\nImplementing local LLM alternatives (Hugging Face models)\nBuilding ensemble extractors (multiple LLMs voting)\nCreating confidence scores for extractions\nImplementing human-in-the-loop validation workflows\n\n\n\n\n\nStudents will work with datasets requiring text-to-feature extraction:\n\nProduct Reviews (primary dataset)\n\nText: Customer review text\nExtractions: sentiment (positive/negative/neutral), rating prediction (1-5), product category, key complaints/praises\nML task: Predict helpfulness ratings using review features\n\nJob Postings\n\nText: Job description\nExtractions: experience level (entry/mid/senior), required skills, remote/hybrid/onsite, salary range estimation\nML task: Predict job category or salary range\n\nNews Articles\n\nText: Article content\nExtractions: topic/category, sentiment about entities, key entities mentioned, article bias indicator\nML task: Classify article topic or predict engagement\n\nCustomer Support Tickets\n\nText: Support request\nExtractions: urgency level, issue category, sentiment, technical vs. non-technical\nML task: Predict resolution time or priority\n\n\n\n\n\n\n\n\nWrite three different prompts to extract sentiment from reviews; compare results (5 pts)\nMake API calls to extract categories from 10 job postings; parse JSON responses (4 pts)\nCalculate cost to process 10,000 reviews with GPT-4 vs. GPT-3.5 (3 pts)\nConvert LLM-extracted categories into a DataFrame with proper encoding (4 pts)\nValidate LLM sentiment extractions against 20 labeled examples; calculate accuracy (5 pts)\nIdentify 5 examples where LLM extraction failed; explain why (4 pts)\nAdd LLM-generated features to existing dataset and train a classifier (5 pts)\nCompare LLM extraction to a simple keyword-based approach for urgency detection (4 pts)\nWrite a prompt that uses few-shot learning with 3 examples (3 pts)\nInterpret confusion matrix comparing LLM-predicted categories to ground truth (4 pts)\n\n\n\n\n\nBuild a batch processing pipeline to extract features from 500+ reviews (7 pts)\nImplement retry logic and error handling for API calls (6 pts)\nCompare 3 different LLMs (GPT-4, GPT-3.5, Claude, or Gemini) on extraction quality and cost (8 pts)\nCreate prompt templates for multiple extraction tasks with parameterization (5 pts)\nBuild complete pipeline: text → extraction → encoding → train/test split → model → evaluation (8 pts)\nGenerate comprehensive report comparing extraction methods with visualizations (6 pts)\nImplement validation checks that flag low-confidence or suspicious extractions (6 pts)\nTest 10+ prompt variations and find the optimal prompt for your task (5 pts)\nCreate a cost-benefit analysis dashboard showing tradeoffs between LLM choices (4 pts)\nFinal analysis: Compare ML model performance with vs. without LLM features (5 pts + submit prompts)\n\n\n\n\n\n\n\n\nGiven a text extraction task, decide if LLM is appropriate or if simpler methods suffice (3 pts)\nIdentify problems with a given prompt and suggest improvements (4 pts)\nCalculate API cost given token counts and pricing (3 pts)\nExplain when to use GPT-4 vs. GPT-3.5 for extraction tasks (3 pts)\nGiven example LLM outputs, identify which need validation/correction (4 pts)\nExplain the concept of “few-shot learning” in prompt engineering (3 pts)\nCompare using LLM APIs vs. fine-tuning a smaller model for extraction (3 pts)\nIdentify data leakage risks when using LLMs for feature engineering (2 pts)\n\n\n\n\nAssumptions: import openai, import pandas as pd, import json, API key configured\n\nWrite a prompt to extract sentiment (positive/negative/neutral) from review text (4 pts)\nMake an API call to an LLM with given text and prompt (4 pts)\nParse JSON response and extract specific field (3 pts)\nConvert list of extracted categories into a pandas DataFrame (4 pts)\nWrite code to calculate token count for a string (3 pts)\nAdd extracted features to existing DataFrame as new columns (4 pts)\nWrite validation code that checks if extraction matches expected format (3 pts)\n\n\n\n\n\n\n\nIntroduction (3-5 paragraphs) - Hook: Most real-world data is unstructured text. LLMs can transform it into ML-ready features. - Preview: How to use LLMs as intelligent feature extractors - Connection to course: Bridges traditional ML with modern AI capabilities\nMajor Sections:\n\nThe Feature Engineering Problem: From Text to Numbers\n\nWhy traditional ML needs numeric/categorical features\nThe challenge of unstructured text\nWhat information is “hidden” in text\nExample: Product reviews containing implicit features\n\nLLMs as Zero-Shot Feature Extractors\n\nWhat “zero-shot” means: no training required\nWhat LLMs can extract: sentiment, categories, entities, ratings\nWhen to use LLMs vs. traditional NLP (regex, keyword matching)\nCode: Simple sentiment extraction example\n\nPrompt Engineering for Extraction\n\nWriting clear, specific prompts\nRequesting structured output (JSON)\nFew-shot learning: providing examples\nIterating on prompts to improve consistency\nCode: Comparing different prompts for same task\n\nCalling LLM APIs: OpenAI, Anthropic, and Gemini\n\nSetting up API access\nMaking your first extraction call\nUnderstanding API parameters (temperature, max_tokens)\nCode: Complete example with API call and response\n\nParsing and Validating Responses\n\nParsing JSON responses\nHandling malformed output\nConverting to pandas DataFrames\nSpotting extraction errors\nCode: Robust parsing with error handling\n\nIntegration with ML Pipelines\n\nAdding extracted features to datasets\nEncoding categorical extractions\nTrain/test split considerations (avoid data leakage!)\nTraining models on LLM-enhanced data\nCode: Complete pipeline from text to trained model\n\nCost and Quality Tradeoffs\n\nCalculating API costs\nComparing LLM providers (GPT-4 vs. GPT-3.5 vs. Claude vs. Gemini)\nWhen to use expensive high-quality models vs. cheap fast models\nCaching and reusing extractions\nCode: Cost calculation and comparison\n\nBatch Processing and Error Handling\n\nProcessing hundreds or thousands of texts\nRate limiting and API quotas\nRetry logic for failures\nProgress tracking and logging\nCode: Batch processing pipeline with error handling\n\nQuality Control and Validation\n\nManual spot-checking\nCalculating agreement with ground truth\nIdentifying systematic errors\nWhen extraction quality is “good enough”\nCode: Validation metrics and reporting\n\nWhen to Use LLMs (and When Not To)\n\nProblems where LLMs excel at extraction\nProblems where simpler methods work fine\nCost-benefit decision framework\nAlternatives: traditional NLP, fine-tuned models, human labeling\nDecision flowchart\n\n\nSummary (3-4 paragraphs) - Synthesize: LLMs as powerful but expensive feature engineering tools - Key tradeoffs: quality vs. cost vs. latency - Connection to next module: Neural networks provide foundation for understanding how LLMs work - Ending: “Use your brain. That’s what it’s there for.”\nPractice Exercises (4-6 exercises) 1. Extract product categories from 20 product descriptions using an LLM 2. Compare extraction quality between GPT-4 and GPT-3.5 on your own dataset 3. Calculate the cost to process your entire dataset with different LLM providers 4. Build a complete pipeline: reviews → sentiment extraction → classifier → evaluation 5. Write prompts for three different extraction tasks and evaluate their consistency 6. Identify when LLM extraction adds value vs. when regex/keywords would suffice\nAdditional Resources - OpenAI API documentation - Anthropic Claude API documentation - Google Gemini API documentation - Prompt engineering guides - Best practices for structured output - Cost estimation tools\n\n\n\n\n\nllm-extraction-workflow.png - Flowchart showing: Text → LLM API → JSON → DataFrame → ML Model\nprompt-comparison.png - Side-by-side comparison of outputs from different prompts\ncost-comparison-chart.png - Bar chart comparing costs across LLM providers\nquality-vs-cost.png - Scatter plot: extraction quality (x-axis) vs. cost per extraction (y-axis)\nextraction-validation.png - Example showing correct vs. incorrect extractions with explanations\nwhen-to-use-llms.png - Decision tree for choosing LLM vs. traditional NLP\n\n\n\n\nFrom Previous Modules: - Module 1: Feature engineering concepts, data cleaning, DataFrame manipulation - Module 2-5: Traditional ML models that will use the extracted features - All modules: Importance of train/test splits, avoiding data leakage\nTo Future Modules: - Module 7: Neural networks provide foundation for understanding how LLMs work internally - Module 8: Fine-tuning models (when API extraction isn’t sufficient)\n\n\n\n\nFocus on practical extraction, not LLM theory: Students learn to use LLMs as tools, not how they work internally. That comes in Modules 7-8.\nAPI-first approach: Using APIs teaches modern workflow and avoids needing GPU infrastructure. Local alternatives mentioned but not emphasized.\nCost awareness from day one: Real-world use requires understanding costs. Students calculate costs repeatedly throughout module.\nComparison emphasis: Students constantly compare LLM extraction to alternatives (traditional NLP, different LLMs, different prompts) to develop judgment.\nQuality over quantity: Better to extract 100 features reliably than 1000 features poorly. Emphasis on validation and spot-checking.\nIntegration with existing knowledge: LLM features are inputs to the ML models students already know. This reinforces that LLMs are tools in a larger pipeline, not magic solutions.\nRealistic expectations: LLMs make mistakes. Extraction isn’t perfect. Students learn to validate and handle errors, not assume LLM output is ground truth.\n\n\n\n\nBy-hand work focuses on: - Writing and iterating on prompts - Understanding API mechanics and costs - Validating extraction quality - Integrating extractions into ML workflows - Deciding when to use LLMs\nAI-assisted work focuses on: - Building production-quality pipelines - Batch processing at scale - Comprehensive experimentation across LLMs and prompts - Robust error handling and retry logic - Automated quality reporting\nThis division ensures students understand the strategy of using LLMs for extraction while acknowledging that implementing production pipelines benefits greatly from AI assistance.\n\n\n\nThis module occupies a unique position in data science education:\n\nBridges traditional ML and modern AI: Students see how cutting-edge LLM capabilities enhance traditional ML workflows\nPractical and immediately useful: Every data scientist encounters unstructured text. This skill applies immediately.\nCost-aware AI: Unlike many AI courses that ignore costs, this module makes cost-benefit analysis central\nTool-use mindset: Students learn to use powerful models via API without needing to understand their internals first\nPrompt engineering skill: Writing effective prompts is increasingly important across all AI tools\n\nThis prepares students for modern data science where AI tools augment human judgment rather than replacing it."
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#module-overview",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#module-overview",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Duration: 2-3 weeks\nCore Philosophy: This module bridges the gap between unstructured data and traditional machine learning by showing students how to use Large Language Models (LLMs) as powerful feature engineering tools. Rather than diving into training LLMs or fine-tuning (covered in Module 8), this module focuses on the practical task of extracting structured information from text data to create features for ML models. Students learn to think of LLMs as intelligent data processors that can understand context, extract entities, classify sentiments, and convert messy text into clean, ML-ready features."
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#strategic-position-in-course",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#strategic-position-in-course",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "This module comes after students have learned: - Traditional ML models (Modules 2-5) - How to evaluate and select models - Feature engineering basics from EDA (Module 1)\nAnd before they learn: - Neural networks and deep learning (Module 7) - Fine-tuning pretrained models (Module 8)\nThis positioning is intentional: students understand the value of good features from working with traditional ML, but haven’t yet learned the complexity of training deep learning models. Using LLMs via API demonstrates the power of modern AI while keeping the focus on practical data science workflows."
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#core-topics-covered",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#core-topics-covered",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "LLMs as Feature Extractors\n\nWhat LLMs can extract: sentiment, categories, entities, numeric ratings\nWhen to use LLMs vs. traditional NLP (regex, keyword matching, etc.)\nThe “zero-shot” capability: extraction without training\n\nPrompt Engineering for Data Extraction\n\nWriting clear, specific prompts for extraction\nUsing structured output formats (JSON)\nFew-shot prompting: providing examples to guide extraction\nIterating on prompts to improve consistency\n\nCalling LLM APIs\n\nUsing OpenAI API, Anthropic API, or Gemini API\nUnderstanding API parameters (temperature, max_tokens, etc.)\nHandling rate limits and errors\nCost considerations: tokens and pricing\n\nStructured Output Parsing\n\nRequesting JSON responses from LLMs\nParsing JSON into Python dictionaries\nHandling malformed or unexpected responses\nConverting extracted data to pandas DataFrames\n\nValidation and Quality Control\n\nChecking extraction consistency across similar inputs\nIdentifying when LLM fails to extract information\nManual spot-checking vs. automated validation\nHandling missing or low-confidence extractions\n\nIntegration with ML Pipelines\n\nAdding LLM-extracted features to existing datasets\nEncoding categorical features from LLM output\nCreating numeric features from text descriptions\nTraining ML models on LLM-enhanced datasets\n\nCost-Benefit Analysis\n\nCalculating API costs for batch processing\nComparing different LLM providers (GPT-4 vs. GPT-3.5 vs. Claude vs. Gemini)\nWhen cheaper/faster models suffice vs. when quality matters\nCaching and reusing extractions to reduce costs\n\nPractical Considerations\n\nBatch processing large datasets\nManaging API quotas and rate limits\nLatency: real-time vs. offline processing\nLocal models vs. API models (Hugging Face alternatives)"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#learning-progression",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#learning-progression",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "By hand: Writing simple prompts, calling APIs, parsing responses\nWith AI: None yet—building foundational skills\nIn-class: Live demos of LLM extraction, experimenting with different prompts for the same task, discussing what makes a good prompt\n\n\n\n\n\nBy hand: Validating extraction quality, converting to DataFrames, training ML models on extracted features\nWith AI: Building batch processing pipelines, implementing retry logic\nIn-class: Debugging failed extractions, comparing LLM-extracted features with manual labels, discussing when extraction quality is “good enough”\n\n\n\n\n\nBy hand: Cost-benefit analysis, deciding between LLM approaches, interpreting ML model performance with LLM features\nWith AI: Complete end-to-end pipelines, extensive experiments with different LLMs and prompts\nIn-class: Project presentations showing real-world use cases"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#topics-done-by-hand",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#topics-done-by-hand",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Students must be able to do these without AI assistance:\n\nPrompt Engineering:\n\nWrite a clear prompt that extracts specific information (sentiment, category, rating)\nInclude examples in prompts (few-shot prompting)\nRequest structured JSON output\nIterate on prompts to improve results\n\nAPI Interaction:\n\nSet up API keys and authentication\nMake a single API call to extract information from text\nUnderstand API parameters (temperature, max_tokens, model selection)\nRead API documentation to understand capabilities\n\nResponse Parsing:\n\nParse JSON responses from LLMs\nHandle cases where JSON is malformed or unexpected\nExtract specific fields from nested JSON\nConvert lists of extractions to pandas DataFrames\n\nQuality Assessment:\n\nManually review 10-20 LLM extractions for accuracy\nIdentify patterns in extraction failures\nCalculate agreement between LLM extractions and ground truth\nDecide when extraction quality is acceptable\n\nCost Calculation:\n\nCount tokens in input and output\nCalculate cost for processing a dataset given token counts and pricing\nCompare costs across different LLM providers\nEstimate budget for batch processing\n\nIntegration:\n\nAdd LLM-extracted features to an existing DataFrame\nEncode categorical LLM outputs for ML models\nSplit data properly (don’t use test set for prompt development!)\nTrain a classifier using LLM-generated features\n\nWhen to Use LLMs:\n\nIdentify when LLM extraction is overkill (simple regex would work)\nIdentify when LLM extraction adds value (complex language understanding needed)\nCompare LLM approach to traditional NLP alternatives"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#topics-done-with-ai",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#topics-done-with-ai",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "AI assistants should help with:\n\nBatch Processing Pipelines:\n\nProcessing hundreds or thousands of text samples\nImplementing rate limiting to avoid API errors\nAdding progress bars and logging\nSaving intermediate results to avoid re-processing\n\nError Handling:\n\nRetry logic for failed API calls\nHandling timeouts and network errors\nGracefully handling malformed JSON responses\nLogging errors for later review\n\nPrompt Template Management:\n\nCreating reusable prompt templates\nParameterizing prompts for different extraction tasks\nA/B testing different prompt variations\nOrganizing prompts for different use cases\n\nQuality Reporting:\n\nGenerating comprehensive reports comparing extraction methods\nCalculating inter-rater reliability between LLMs\nVisualizing extraction distributions\nCreating confusion matrices for classification extractions\n\nComplete End-to-End Pipelines:\n\nText → LLM extraction → DataFrame → ML model → evaluation\nAutomating the entire workflow\nComparing multiple LLM providers programmatically\nHyperparameter tuning on ML models using LLM features\n\nAdvanced Techniques:\n\nImplementing local LLM alternatives (Hugging Face models)\nBuilding ensemble extractors (multiple LLMs voting)\nCreating confidence scores for extractions\nImplementing human-in-the-loop validation workflows"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#practical-datasets-for-module",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#practical-datasets-for-module",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Students will work with datasets requiring text-to-feature extraction:\n\nProduct Reviews (primary dataset)\n\nText: Customer review text\nExtractions: sentiment (positive/negative/neutral), rating prediction (1-5), product category, key complaints/praises\nML task: Predict helpfulness ratings using review features\n\nJob Postings\n\nText: Job description\nExtractions: experience level (entry/mid/senior), required skills, remote/hybrid/onsite, salary range estimation\nML task: Predict job category or salary range\n\nNews Articles\n\nText: Article content\nExtractions: topic/category, sentiment about entities, key entities mentioned, article bias indicator\nML task: Classify article topic or predict engagement\n\nCustomer Support Tickets\n\nText: Support request\nExtractions: urgency level, issue category, sentiment, technical vs. non-technical\nML task: Predict resolution time or priority"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#homework-structure-following-template",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#homework-structure-following-template",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Write three different prompts to extract sentiment from reviews; compare results (5 pts)\nMake API calls to extract categories from 10 job postings; parse JSON responses (4 pts)\nCalculate cost to process 10,000 reviews with GPT-4 vs. GPT-3.5 (3 pts)\nConvert LLM-extracted categories into a DataFrame with proper encoding (4 pts)\nValidate LLM sentiment extractions against 20 labeled examples; calculate accuracy (5 pts)\nIdentify 5 examples where LLM extraction failed; explain why (4 pts)\nAdd LLM-generated features to existing dataset and train a classifier (5 pts)\nCompare LLM extraction to a simple keyword-based approach for urgency detection (4 pts)\nWrite a prompt that uses few-shot learning with 3 examples (3 pts)\nInterpret confusion matrix comparing LLM-predicted categories to ground truth (4 pts)\n\n\n\n\n\nBuild a batch processing pipeline to extract features from 500+ reviews (7 pts)\nImplement retry logic and error handling for API calls (6 pts)\nCompare 3 different LLMs (GPT-4, GPT-3.5, Claude, or Gemini) on extraction quality and cost (8 pts)\nCreate prompt templates for multiple extraction tasks with parameterization (5 pts)\nBuild complete pipeline: text → extraction → encoding → train/test split → model → evaluation (8 pts)\nGenerate comprehensive report comparing extraction methods with visualizations (6 pts)\nImplement validation checks that flag low-confidence or suspicious extractions (6 pts)\nTest 10+ prompt variations and find the optimal prompt for your task (5 pts)\nCreate a cost-benefit analysis dashboard showing tradeoffs between LLM choices (4 pts)\nFinal analysis: Compare ML model performance with vs. without LLM features (5 pts + submit prompts)"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#quiz-structure-following-template",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#quiz-structure-following-template",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Given a text extraction task, decide if LLM is appropriate or if simpler methods suffice (3 pts)\nIdentify problems with a given prompt and suggest improvements (4 pts)\nCalculate API cost given token counts and pricing (3 pts)\nExplain when to use GPT-4 vs. GPT-3.5 for extraction tasks (3 pts)\nGiven example LLM outputs, identify which need validation/correction (4 pts)\nExplain the concept of “few-shot learning” in prompt engineering (3 pts)\nCompare using LLM APIs vs. fine-tuning a smaller model for extraction (3 pts)\nIdentify data leakage risks when using LLMs for feature engineering (2 pts)\n\n\n\n\nAssumptions: import openai, import pandas as pd, import json, API key configured\n\nWrite a prompt to extract sentiment (positive/negative/neutral) from review text (4 pts)\nMake an API call to an LLM with given text and prompt (4 pts)\nParse JSON response and extract specific field (3 pts)\nConvert list of extracted categories into a pandas DataFrame (4 pts)\nWrite code to calculate token count for a string (3 pts)\nAdd extracted features to existing DataFrame as new columns (4 pts)\nWrite validation code that checks if extraction matches expected format (3 pts)"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#textbook-chapter-structure",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#textbook-chapter-structure",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Introduction (3-5 paragraphs) - Hook: Most real-world data is unstructured text. LLMs can transform it into ML-ready features. - Preview: How to use LLMs as intelligent feature extractors - Connection to course: Bridges traditional ML with modern AI capabilities\nMajor Sections:\n\nThe Feature Engineering Problem: From Text to Numbers\n\nWhy traditional ML needs numeric/categorical features\nThe challenge of unstructured text\nWhat information is “hidden” in text\nExample: Product reviews containing implicit features\n\nLLMs as Zero-Shot Feature Extractors\n\nWhat “zero-shot” means: no training required\nWhat LLMs can extract: sentiment, categories, entities, ratings\nWhen to use LLMs vs. traditional NLP (regex, keyword matching)\nCode: Simple sentiment extraction example\n\nPrompt Engineering for Extraction\n\nWriting clear, specific prompts\nRequesting structured output (JSON)\nFew-shot learning: providing examples\nIterating on prompts to improve consistency\nCode: Comparing different prompts for same task\n\nCalling LLM APIs: OpenAI, Anthropic, and Gemini\n\nSetting up API access\nMaking your first extraction call\nUnderstanding API parameters (temperature, max_tokens)\nCode: Complete example with API call and response\n\nParsing and Validating Responses\n\nParsing JSON responses\nHandling malformed output\nConverting to pandas DataFrames\nSpotting extraction errors\nCode: Robust parsing with error handling\n\nIntegration with ML Pipelines\n\nAdding extracted features to datasets\nEncoding categorical extractions\nTrain/test split considerations (avoid data leakage!)\nTraining models on LLM-enhanced data\nCode: Complete pipeline from text to trained model\n\nCost and Quality Tradeoffs\n\nCalculating API costs\nComparing LLM providers (GPT-4 vs. GPT-3.5 vs. Claude vs. Gemini)\nWhen to use expensive high-quality models vs. cheap fast models\nCaching and reusing extractions\nCode: Cost calculation and comparison\n\nBatch Processing and Error Handling\n\nProcessing hundreds or thousands of texts\nRate limiting and API quotas\nRetry logic for failures\nProgress tracking and logging\nCode: Batch processing pipeline with error handling\n\nQuality Control and Validation\n\nManual spot-checking\nCalculating agreement with ground truth\nIdentifying systematic errors\nWhen extraction quality is “good enough”\nCode: Validation metrics and reporting\n\nWhen to Use LLMs (and When Not To)\n\nProblems where LLMs excel at extraction\nProblems where simpler methods work fine\nCost-benefit decision framework\nAlternatives: traditional NLP, fine-tuned models, human labeling\nDecision flowchart\n\n\nSummary (3-4 paragraphs) - Synthesize: LLMs as powerful but expensive feature engineering tools - Key tradeoffs: quality vs. cost vs. latency - Connection to next module: Neural networks provide foundation for understanding how LLMs work - Ending: “Use your brain. That’s what it’s there for.”\nPractice Exercises (4-6 exercises) 1. Extract product categories from 20 product descriptions using an LLM 2. Compare extraction quality between GPT-4 and GPT-3.5 on your own dataset 3. Calculate the cost to process your entire dataset with different LLM providers 4. Build a complete pipeline: reviews → sentiment extraction → classifier → evaluation 5. Write prompts for three different extraction tasks and evaluate their consistency 6. Identify when LLM extraction adds value vs. when regex/keywords would suffice\nAdditional Resources - OpenAI API documentation - Anthropic Claude API documentation - Google Gemini API documentation - Prompt engineering guides - Best practices for structured output - Cost estimation tools"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#images-needed",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#images-needed",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "llm-extraction-workflow.png - Flowchart showing: Text → LLM API → JSON → DataFrame → ML Model\nprompt-comparison.png - Side-by-side comparison of outputs from different prompts\ncost-comparison-chart.png - Bar chart comparing costs across LLM providers\nquality-vs-cost.png - Scatter plot: extraction quality (x-axis) vs. cost per extraction (y-axis)\nextraction-validation.png - Example showing correct vs. incorrect extractions with explanations\nwhen-to-use-llms.png - Decision tree for choosing LLM vs. traditional NLP"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#connections-to-other-modules",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#connections-to-other-modules",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "From Previous Modules: - Module 1: Feature engineering concepts, data cleaning, DataFrame manipulation - Module 2-5: Traditional ML models that will use the extracted features - All modules: Importance of train/test splits, avoiding data leakage\nTo Future Modules: - Module 7: Neural networks provide foundation for understanding how LLMs work internally - Module 8: Fine-tuning models (when API extraction isn’t sufficient)"
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#key-pedagogical-decisions",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#key-pedagogical-decisions",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "Focus on practical extraction, not LLM theory: Students learn to use LLMs as tools, not how they work internally. That comes in Modules 7-8.\nAPI-first approach: Using APIs teaches modern workflow and avoids needing GPU infrastructure. Local alternatives mentioned but not emphasized.\nCost awareness from day one: Real-world use requires understanding costs. Students calculate costs repeatedly throughout module.\nComparison emphasis: Students constantly compare LLM extraction to alternatives (traditional NLP, different LLMs, different prompts) to develop judgment.\nQuality over quantity: Better to extract 100 features reliably than 1000 features poorly. Emphasis on validation and spot-checking.\nIntegration with existing knowledge: LLM features are inputs to the ML models students already know. This reinforces that LLMs are tools in a larger pipeline, not magic solutions.\nRealistic expectations: LLMs make mistakes. Extraction isn’t perfect. Students learn to validate and handle errors, not assume LLM output is ground truth."
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#assessment-philosophy",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#assessment-philosophy",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "By-hand work focuses on: - Writing and iterating on prompts - Understanding API mechanics and costs - Validating extraction quality - Integrating extractions into ML workflows - Deciding when to use LLMs\nAI-assisted work focuses on: - Building production-quality pipelines - Batch processing at scale - Comprehensive experimentation across LLMs and prompts - Robust error handling and retry logic - Automated quality reporting\nThis division ensures students understand the strategy of using LLMs for extraction while acknowledging that implementing production pipelines benefits greatly from AI assistance."
  },
  {
    "objectID": "Claude-planning/module-4-llms-feature-engineering.html#unique-value-of-this-module",
    "href": "Claude-planning/module-4-llms-feature-engineering.html#unique-value-of-this-module",
    "title": "Module 4: LLMs for Feature Engineering and Data Extraction - Planning Document",
    "section": "",
    "text": "This module occupies a unique position in data science education:\n\nBridges traditional ML and modern AI: Students see how cutting-edge LLM capabilities enhance traditional ML workflows\nPractical and immediately useful: Every data scientist encounters unstructured text. This skill applies immediately.\nCost-aware AI: Unlike many AI courses that ignore costs, this module makes cost-benefit analysis central\nTool-use mindset: Students learn to use powerful models via API without needing to understand their internals first\nPrompt engineering skill: Writing effective prompts is increasingly important across all AI tools\n\nThis prepares students for modern data science where AI tools augment human judgment rather than replacing it."
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html",
    "href": "Claude-planning/module-7-neural-networks.html",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Duration: 3-4 weeks\nCore Philosophy: This module introduces students to neural networks and deep learning using PyTorch. Unlike traditional ML models covered in earlier modules, neural networks require understanding of different abstractions: layers, activation functions, backpropagation, and the training loop. The focus is on conceptual understanding—students should know what neural networks are doing and when to use them, while using AI assistants to handle the intricate implementation details.\n\n\n\n\nPerceptrons and Multi-Layer Perceptrons (MLPs)\n\nThe basic building block: the perceptron\nHow layers stack to create MLPs\nThe role of weights and biases\n\nActivation Functions\n\nWhy we need non-linearity\nCommon activation functions (ReLU, Sigmoid, Tanh, Softmax)\nWhen to use which activation\n\nBackpropagation (Conceptual)\n\nThe chain rule intuition\nHow gradients flow backward through layers\nWhy this enables learning\n\nLoss Functions for Neural Networks\n\nMSE for regression (connecting to Module 4)\nCross-Entropy for classification (connecting to Module 5)\nUnderstanding what the loss landscape looks like\n\nRegularization in Deep Learning\n\nDropout: randomly turning off neurons\nBatch normalization: stabilizing training\nL2 regularization revisited for neural networks\n\nArchitecture Design Principles\n\nHow many layers? How many neurons?\nWhen to go deeper vs. wider\nRecognizing when you need more capacity\n\nHyperparameter Tuning Strategies\n\nLearning rate: the most important hyperparameter\nBatch size and its effects\nNumber of epochs and early stopping\nSystematic experimentation\n\nPyTorch Fundamentals\n\nTensors and automatic differentiation\nBuilding models with nn.Module\nThe training loop pattern\nDataLoaders and data handling\n\n\n\n\n\n\n\n\nBy hand: Understanding perceptrons conceptually, reading PyTorch model definitions, identifying activation functions\nWith AI: None yet—building foundational understanding\nIn-class: Visual explanations of how perceptrons work, drawing neural network diagrams, interpreting simple PyTorch code\n\n\n\n\n\nBy hand: Creating simple PyTorch models (1-2 layers), loading data into DataLoaders, plotting training curves\nWith AI: Writing complete training loops with proper validation\nIn-class: Implementing a basic training loop together, discussing what each component does, interpreting training/validation loss curves\n\n\n\n\n\nBy hand: Identifying overfitting from plots, understanding dropout conceptually, adding regularization to existing models\nWith AI: Hyperparameter tuning experiments (learning rate, batch size, architecture), implementing dropout and batch normalization\nIn-class: Debugging overfitting problems, comparing models with/without regularization\n\n\n\n\n\nBy hand: Sketching network architectures for different problems, justifying design choices\nWith AI: Building complex architectures, running extensive experiments\nIn-class: Project work—students design and justify architectures for their specific problems\n\n\n\n\n\nStudents must be able to do these without AI assistance:\n\nConceptual Understanding:\n\nExplain what a perceptron does and how it relates to linear models\nDescribe why activation functions create non-linearity\nUnderstand conceptually how backpropagation works (chain rule intuition)\nIdentify when a model is overfitting or underfitting from training curves\n\nReading PyTorch Code:\n\nRead and understand a PyTorch model definition\nIdentify the number of layers, neurons, and activation functions\nUnderstand what each part of the training loop does\n\nData Preparation:\n\nLoad datasets and create train/validation/test splits\nCreate PyTorch DataLoaders with appropriate batch sizes\nNormalize/standardize features for neural networks\n\nBasic Model Creation:\n\nCreate simple sequential models (2-3 layers)\nChoose appropriate activation functions\nSpecify input/output dimensions correctly\n\nTraining and Evaluation:\n\nUnderstand the training loop: forward pass → loss → backward pass → optimization\nPlot training and validation loss curves\nInterpret these curves to diagnose problems\n\nDiagnosis and Debugging:\n\nIdentify overfitting (training loss &lt;&lt; validation loss)\nIdentify underfitting (both losses high)\nRecognize when learning rate is too high or too low\nUnderstand when to stop training (early stopping)\n\n\n\n\n\nAI assistants should help with:\n\nComplete Training Loop Implementation:\n\nWriting the full training loop with proper error handling\nImplementing validation loops\nAdding early stopping logic\nSaving and loading model checkpoints\n\nData Pipeline Creation:\n\nCustom Dataset classes for complex data\nData augmentation pipelines\nEfficient data loading and preprocessing\n\nArchitecture Experimentation:\n\nBuilding complex network architectures\nImplementing skip connections, residual blocks\nCreating custom layers and modules\n\nHyperparameter Tuning:\n\nGrid search over learning rates, batch sizes, architectures\nRunning multiple experiments in parallel\nOrganizing and visualizing results\n\nAdvanced Regularization:\n\nImplementing dropout at multiple layers\nAdding batch normalization correctly\nImplementing learning rate scheduling\n\nProduction Code:\n\nError handling and edge cases\nLogging and monitoring training progress\nCreating modular, reusable code\n\n\n\n\n\n\n\n\nRead a PyTorch model definition and identify its components (3 pts)\nCreate train/val/test splits and DataLoaders (4 pts)\nBuild a simple 2-layer MLP for a classification problem (5 pts)\nPlot training and validation loss curves and interpret them (4 pts)\nIdentify whether a model is overfitting or underfitting from curves (3 pts)\nExplain why ReLU activation is commonly used (3 pts)\nCalculate the number of parameters in a given neural network (4 pts)\nCreate a learning rate vs. loss plot to find optimal learning rate (5 pts)\nImplement early stopping logic manually (5 pts)\nWrite a function to evaluate model accuracy on test set (4 pts)\n\n\n\n\n\nBuild and train 5+ neural network architectures with different configurations (7 pts)\nImplement a complete training pipeline with validation and checkpointing (7 pts)\nHyperparameter tuning: test 20+ combinations of learning rate and batch size (8 pts)\nImplement dropout and batch normalization, compare with baseline (6 pts)\nCreate comprehensive visualization of training dynamics across experiments (6 pts)\nBuild a custom dataset class for a specific data format (5 pts)\nImplement learning rate scheduling and compare different schedules (6 pts)\nCreate an automated model selection pipeline based on validation performance (5 pts)\nGenerate plots comparing all models on multiple metrics (5 pts)\nFinal analysis: Executive summary of findings with recommendations (5 pts + submit prompts)\n\n\n\n\n\n\n\n\nIdentify whether a problem needs neural networks or if traditional ML would suffice (3 pts)\nGiven training/validation curves, diagnose the problem (overfitting/underfitting/good fit) (4 pts)\nExplain why we need activation functions (3 pts)\nCompare ReLU vs. Sigmoid for hidden layers (3 pts)\nIdentify what dropout does and when to use it (3 pts)\nGiven a model architecture, calculate number of parameters (4 pts)\nExplain the bias-variance tradeoff in the context of neural network depth (3 pts)\nIdentify appropriate loss functions for different problem types (2 pts)\n\n\n\n\nAssumptions: import torch, import torch.nn as nn, from torch.utils.data import DataLoader, dataset objects available\n\nDefine a simple 2-layer neural network class (5 pts)\nCreate a DataLoader with specified batch size (3 pts)\nWrite the forward pass through a model (3 pts)\nInstantiate an optimizer and calculate loss (4 pts)\nWrite the backward pass and optimization step (4 pts)\nAdd dropout to an existing model (3 pts)\nCreate train/val/test split using PyTorch utilities (3 pts)\n\n\n\n\n\n\n\nIntroduction (3-5 paragraphs) - Hook: Traditional ML models are like drawing with straight lines and simple curves. Neural networks are like having an entire art studio at your disposal. - Preview: What neural networks are, why they’re powerful, when to use them - Connection to previous modules: Builds on loss functions, gradient descent, and regularization from Module 3\nMajor Sections:\n\nWhat Are Neural Networks? The Building Blocks\n\nThe perceptron: A more complex linear model\nStacking layers: Why depth matters\nVisualizing a simple network\nCode: Building a 2-layer network in PyTorch\n\nActivation Functions: Adding Non-Linearity\n\nWhy linear layers aren’t enough\nCommon activation functions (ReLU, Sigmoid, Tanh, Softmax)\nChoosing the right activation\nCode: Comparing networks with different activations\n\nThe Training Process: Forward and Backward\n\nForward pass: Making predictions\nLoss calculation: Measuring error\nBackward pass: Computing gradients (conceptual)\nOptimization: Updating weights\nCode: A minimal training loop\n\nPyTorch Fundamentals: Building Your First Network\n\nTensors: The fundamental data structure\nnn.Module: The model building class\nDataLoaders: Feeding data efficiently\nCode: Complete example from data to trained model\n\nLoss Functions for Neural Networks\n\nMSE for regression tasks (connecting to Module 4)\nCross-entropy for classification (connecting to Module 5)\nUnderstanding the loss landscape\nCode: Visualizing loss during training\n\nDiagnosing Training: Reading the Curves\n\nTraining vs. validation loss\nOverfitting: When your model memorizes\nUnderfitting: When your model is too simple\nThe goldilocks zone: Just right\nCode: Plotting and interpreting training curves\n\nRegularization in Deep Learning\n\nDropout: Randomly forgetting to prevent memorization\nBatch normalization: Stabilizing training\nL2 regularization: Penalizing large weights\nEarly stopping: Knowing when to quit\nCode: Adding regularization to models\n\nArchitecture Design: How Many Layers? How Many Neurons?\n\nGoing deeper vs. going wider\nThe capacity question\nRules of thumb for architecture design\nCode: Experimenting with different architectures\n\nHyperparameter Tuning: The Art of Training Neural Networks\n\nLearning rate: The most important hyperparameter\nBatch size and its effects\nNumber of epochs and early stopping\nSystematic experimentation\nCode: Finding the optimal learning rate\n\nWhen to Use Neural Networks (and When Not To)\n\nProblems where neural networks excel\nProblems where traditional ML is better\nComputational considerations\nInterpretability tradeoffs\nDecision framework\n\n\nSummary (3-4 paragraphs) - Synthesize: Neural networks as universal function approximators - Key tradeoffs: Power vs. interpretability, data requirements vs. performance - Connection to next module: Setting up for pretrained models and transfer learning - Ending: “Use your brain. That’s what it’s there for.”\nPractice Exercises (4-6 exercises) 1. Build a 3-layer MLP for MNIST digit classification 2. Diagnose overfitting in a model and fix it with regularization 3. Experiment with different activation functions and compare results 4. Find the optimal learning rate for a given problem 5. Design an architecture for a specific problem and justify your choices\nAdditional Resources - PyTorch documentation and tutorials - Neural networks and deep learning (Michael Nielsen’s book) - CS231n lecture notes on neural networks - Distill.pub articles on neural network visualization\n\n\n\n\n\nperceptron-diagram.png - Visual showing inputs, weights, sum, activation, output\nmlp-architecture.png - Multi-layer network diagram showing how layers connect\nactivation-functions.png - Side-by-side plots of ReLU, Sigmoid, Tanh\ntraining-curves-good.png - Example of healthy training/validation curves\ntraining-curves-overfitting.png - Example showing overfitting\ntraining-curves-underfitting.png - Example showing underfitting\ndropout-visualization.png - Diagram showing how dropout randomly disables neurons\nloss-landscape.png - 3D surface plot showing loss landscape and gradient descent path\n\n\n\n\nFrom Previous Modules: - Module 3: Loss functions, gradient descent, bias-variance tradeoff, train/val/test splits - Module 4: MSE loss for regression, feature scaling - Module 5: Cross-entropy loss for classification, multi-class problems\nTo Future Modules: - Module 8: Neural networks are the foundation for pretrained models, transfer learning, and fine-tuning\n\n\n\n\nFocus on PyTorch over TensorFlow/Keras: PyTorch’s explicit control flow makes the training process more transparent and easier to understand conceptually. Students see exactly what’s happening.\nConceptual backpropagation only: Students should understand that gradients flow backward and why this enables learning, but not the detailed mathematics. The focus is on using neural networks effectively, not implementing them from scratch.\nHeavy emphasis on training curves: The ability to diagnose problems from training/validation loss curves is the most practical skill. This gets extensive coverage.\nArchitecture as experimentation: Rather than memorizing rules, students learn to experiment systematically and interpret results. This prepares them for real-world work where there are no cookbook solutions.\nWhen NOT to use neural networks: Just as important as knowing when to use them. Students should understand that sometimes logistic regression is the better choice.\nConnecting to previous concepts: Constantly reference loss functions from Module 3, regularization from Module 4, classification metrics from Module 5. Show how neural networks are an extension of what they already know.\n\n\n\n\nBy-hand work focuses on: - Conceptual understanding (what is each component doing?) - Reading and interpreting code and results - Basic model building with simple architectures - Diagnosis and debugging skills\nAI-assisted work focuses on: - Complete implementation pipelines - Extensive experimentation and hyperparameter tuning - Complex architectures and custom components - Production-quality code with proper error handling\nThis division ensures students understand what neural networks are doing while acknowledging that implementing production deep learning pipelines is where AI assistance truly shines."
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#module-overview",
    "href": "Claude-planning/module-7-neural-networks.html#module-overview",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Duration: 3-4 weeks\nCore Philosophy: This module introduces students to neural networks and deep learning using PyTorch. Unlike traditional ML models covered in earlier modules, neural networks require understanding of different abstractions: layers, activation functions, backpropagation, and the training loop. The focus is on conceptual understanding—students should know what neural networks are doing and when to use them, while using AI assistants to handle the intricate implementation details."
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#core-topics-covered",
    "href": "Claude-planning/module-7-neural-networks.html#core-topics-covered",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Perceptrons and Multi-Layer Perceptrons (MLPs)\n\nThe basic building block: the perceptron\nHow layers stack to create MLPs\nThe role of weights and biases\n\nActivation Functions\n\nWhy we need non-linearity\nCommon activation functions (ReLU, Sigmoid, Tanh, Softmax)\nWhen to use which activation\n\nBackpropagation (Conceptual)\n\nThe chain rule intuition\nHow gradients flow backward through layers\nWhy this enables learning\n\nLoss Functions for Neural Networks\n\nMSE for regression (connecting to Module 4)\nCross-Entropy for classification (connecting to Module 5)\nUnderstanding what the loss landscape looks like\n\nRegularization in Deep Learning\n\nDropout: randomly turning off neurons\nBatch normalization: stabilizing training\nL2 regularization revisited for neural networks\n\nArchitecture Design Principles\n\nHow many layers? How many neurons?\nWhen to go deeper vs. wider\nRecognizing when you need more capacity\n\nHyperparameter Tuning Strategies\n\nLearning rate: the most important hyperparameter\nBatch size and its effects\nNumber of epochs and early stopping\nSystematic experimentation\n\nPyTorch Fundamentals\n\nTensors and automatic differentiation\nBuilding models with nn.Module\nThe training loop pattern\nDataLoaders and data handling"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#learning-progression",
    "href": "Claude-planning/module-7-neural-networks.html#learning-progression",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "By hand: Understanding perceptrons conceptually, reading PyTorch model definitions, identifying activation functions\nWith AI: None yet—building foundational understanding\nIn-class: Visual explanations of how perceptrons work, drawing neural network diagrams, interpreting simple PyTorch code\n\n\n\n\n\nBy hand: Creating simple PyTorch models (1-2 layers), loading data into DataLoaders, plotting training curves\nWith AI: Writing complete training loops with proper validation\nIn-class: Implementing a basic training loop together, discussing what each component does, interpreting training/validation loss curves\n\n\n\n\n\nBy hand: Identifying overfitting from plots, understanding dropout conceptually, adding regularization to existing models\nWith AI: Hyperparameter tuning experiments (learning rate, batch size, architecture), implementing dropout and batch normalization\nIn-class: Debugging overfitting problems, comparing models with/without regularization\n\n\n\n\n\nBy hand: Sketching network architectures for different problems, justifying design choices\nWith AI: Building complex architectures, running extensive experiments\nIn-class: Project work—students design and justify architectures for their specific problems"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#topics-done-by-hand",
    "href": "Claude-planning/module-7-neural-networks.html#topics-done-by-hand",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Students must be able to do these without AI assistance:\n\nConceptual Understanding:\n\nExplain what a perceptron does and how it relates to linear models\nDescribe why activation functions create non-linearity\nUnderstand conceptually how backpropagation works (chain rule intuition)\nIdentify when a model is overfitting or underfitting from training curves\n\nReading PyTorch Code:\n\nRead and understand a PyTorch model definition\nIdentify the number of layers, neurons, and activation functions\nUnderstand what each part of the training loop does\n\nData Preparation:\n\nLoad datasets and create train/validation/test splits\nCreate PyTorch DataLoaders with appropriate batch sizes\nNormalize/standardize features for neural networks\n\nBasic Model Creation:\n\nCreate simple sequential models (2-3 layers)\nChoose appropriate activation functions\nSpecify input/output dimensions correctly\n\nTraining and Evaluation:\n\nUnderstand the training loop: forward pass → loss → backward pass → optimization\nPlot training and validation loss curves\nInterpret these curves to diagnose problems\n\nDiagnosis and Debugging:\n\nIdentify overfitting (training loss &lt;&lt; validation loss)\nIdentify underfitting (both losses high)\nRecognize when learning rate is too high or too low\nUnderstand when to stop training (early stopping)"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#topics-done-with-ai",
    "href": "Claude-planning/module-7-neural-networks.html#topics-done-with-ai",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "AI assistants should help with:\n\nComplete Training Loop Implementation:\n\nWriting the full training loop with proper error handling\nImplementing validation loops\nAdding early stopping logic\nSaving and loading model checkpoints\n\nData Pipeline Creation:\n\nCustom Dataset classes for complex data\nData augmentation pipelines\nEfficient data loading and preprocessing\n\nArchitecture Experimentation:\n\nBuilding complex network architectures\nImplementing skip connections, residual blocks\nCreating custom layers and modules\n\nHyperparameter Tuning:\n\nGrid search over learning rates, batch sizes, architectures\nRunning multiple experiments in parallel\nOrganizing and visualizing results\n\nAdvanced Regularization:\n\nImplementing dropout at multiple layers\nAdding batch normalization correctly\nImplementing learning rate scheduling\n\nProduction Code:\n\nError handling and edge cases\nLogging and monitoring training progress\nCreating modular, reusable code"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#homework-structure-following-template",
    "href": "Claude-planning/module-7-neural-networks.html#homework-structure-following-template",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Read a PyTorch model definition and identify its components (3 pts)\nCreate train/val/test splits and DataLoaders (4 pts)\nBuild a simple 2-layer MLP for a classification problem (5 pts)\nPlot training and validation loss curves and interpret them (4 pts)\nIdentify whether a model is overfitting or underfitting from curves (3 pts)\nExplain why ReLU activation is commonly used (3 pts)\nCalculate the number of parameters in a given neural network (4 pts)\nCreate a learning rate vs. loss plot to find optimal learning rate (5 pts)\nImplement early stopping logic manually (5 pts)\nWrite a function to evaluate model accuracy on test set (4 pts)\n\n\n\n\n\nBuild and train 5+ neural network architectures with different configurations (7 pts)\nImplement a complete training pipeline with validation and checkpointing (7 pts)\nHyperparameter tuning: test 20+ combinations of learning rate and batch size (8 pts)\nImplement dropout and batch normalization, compare with baseline (6 pts)\nCreate comprehensive visualization of training dynamics across experiments (6 pts)\nBuild a custom dataset class for a specific data format (5 pts)\nImplement learning rate scheduling and compare different schedules (6 pts)\nCreate an automated model selection pipeline based on validation performance (5 pts)\nGenerate plots comparing all models on multiple metrics (5 pts)\nFinal analysis: Executive summary of findings with recommendations (5 pts + submit prompts)"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#quiz-structure-following-template",
    "href": "Claude-planning/module-7-neural-networks.html#quiz-structure-following-template",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Identify whether a problem needs neural networks or if traditional ML would suffice (3 pts)\nGiven training/validation curves, diagnose the problem (overfitting/underfitting/good fit) (4 pts)\nExplain why we need activation functions (3 pts)\nCompare ReLU vs. Sigmoid for hidden layers (3 pts)\nIdentify what dropout does and when to use it (3 pts)\nGiven a model architecture, calculate number of parameters (4 pts)\nExplain the bias-variance tradeoff in the context of neural network depth (3 pts)\nIdentify appropriate loss functions for different problem types (2 pts)\n\n\n\n\nAssumptions: import torch, import torch.nn as nn, from torch.utils.data import DataLoader, dataset objects available\n\nDefine a simple 2-layer neural network class (5 pts)\nCreate a DataLoader with specified batch size (3 pts)\nWrite the forward pass through a model (3 pts)\nInstantiate an optimizer and calculate loss (4 pts)\nWrite the backward pass and optimization step (4 pts)\nAdd dropout to an existing model (3 pts)\nCreate train/val/test split using PyTorch utilities (3 pts)"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#textbook-chapter-structure",
    "href": "Claude-planning/module-7-neural-networks.html#textbook-chapter-structure",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Introduction (3-5 paragraphs) - Hook: Traditional ML models are like drawing with straight lines and simple curves. Neural networks are like having an entire art studio at your disposal. - Preview: What neural networks are, why they’re powerful, when to use them - Connection to previous modules: Builds on loss functions, gradient descent, and regularization from Module 3\nMajor Sections:\n\nWhat Are Neural Networks? The Building Blocks\n\nThe perceptron: A more complex linear model\nStacking layers: Why depth matters\nVisualizing a simple network\nCode: Building a 2-layer network in PyTorch\n\nActivation Functions: Adding Non-Linearity\n\nWhy linear layers aren’t enough\nCommon activation functions (ReLU, Sigmoid, Tanh, Softmax)\nChoosing the right activation\nCode: Comparing networks with different activations\n\nThe Training Process: Forward and Backward\n\nForward pass: Making predictions\nLoss calculation: Measuring error\nBackward pass: Computing gradients (conceptual)\nOptimization: Updating weights\nCode: A minimal training loop\n\nPyTorch Fundamentals: Building Your First Network\n\nTensors: The fundamental data structure\nnn.Module: The model building class\nDataLoaders: Feeding data efficiently\nCode: Complete example from data to trained model\n\nLoss Functions for Neural Networks\n\nMSE for regression tasks (connecting to Module 4)\nCross-entropy for classification (connecting to Module 5)\nUnderstanding the loss landscape\nCode: Visualizing loss during training\n\nDiagnosing Training: Reading the Curves\n\nTraining vs. validation loss\nOverfitting: When your model memorizes\nUnderfitting: When your model is too simple\nThe goldilocks zone: Just right\nCode: Plotting and interpreting training curves\n\nRegularization in Deep Learning\n\nDropout: Randomly forgetting to prevent memorization\nBatch normalization: Stabilizing training\nL2 regularization: Penalizing large weights\nEarly stopping: Knowing when to quit\nCode: Adding regularization to models\n\nArchitecture Design: How Many Layers? How Many Neurons?\n\nGoing deeper vs. going wider\nThe capacity question\nRules of thumb for architecture design\nCode: Experimenting with different architectures\n\nHyperparameter Tuning: The Art of Training Neural Networks\n\nLearning rate: The most important hyperparameter\nBatch size and its effects\nNumber of epochs and early stopping\nSystematic experimentation\nCode: Finding the optimal learning rate\n\nWhen to Use Neural Networks (and When Not To)\n\nProblems where neural networks excel\nProblems where traditional ML is better\nComputational considerations\nInterpretability tradeoffs\nDecision framework\n\n\nSummary (3-4 paragraphs) - Synthesize: Neural networks as universal function approximators - Key tradeoffs: Power vs. interpretability, data requirements vs. performance - Connection to next module: Setting up for pretrained models and transfer learning - Ending: “Use your brain. That’s what it’s there for.”\nPractice Exercises (4-6 exercises) 1. Build a 3-layer MLP for MNIST digit classification 2. Diagnose overfitting in a model and fix it with regularization 3. Experiment with different activation functions and compare results 4. Find the optimal learning rate for a given problem 5. Design an architecture for a specific problem and justify your choices\nAdditional Resources - PyTorch documentation and tutorials - Neural networks and deep learning (Michael Nielsen’s book) - CS231n lecture notes on neural networks - Distill.pub articles on neural network visualization"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#images-needed",
    "href": "Claude-planning/module-7-neural-networks.html#images-needed",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "perceptron-diagram.png - Visual showing inputs, weights, sum, activation, output\nmlp-architecture.png - Multi-layer network diagram showing how layers connect\nactivation-functions.png - Side-by-side plots of ReLU, Sigmoid, Tanh\ntraining-curves-good.png - Example of healthy training/validation curves\ntraining-curves-overfitting.png - Example showing overfitting\ntraining-curves-underfitting.png - Example showing underfitting\ndropout-visualization.png - Diagram showing how dropout randomly disables neurons\nloss-landscape.png - 3D surface plot showing loss landscape and gradient descent path"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#connections-to-other-modules",
    "href": "Claude-planning/module-7-neural-networks.html#connections-to-other-modules",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "From Previous Modules: - Module 3: Loss functions, gradient descent, bias-variance tradeoff, train/val/test splits - Module 4: MSE loss for regression, feature scaling - Module 5: Cross-entropy loss for classification, multi-class problems\nTo Future Modules: - Module 8: Neural networks are the foundation for pretrained models, transfer learning, and fine-tuning"
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#key-pedagogical-decisions",
    "href": "Claude-planning/module-7-neural-networks.html#key-pedagogical-decisions",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "Focus on PyTorch over TensorFlow/Keras: PyTorch’s explicit control flow makes the training process more transparent and easier to understand conceptually. Students see exactly what’s happening.\nConceptual backpropagation only: Students should understand that gradients flow backward and why this enables learning, but not the detailed mathematics. The focus is on using neural networks effectively, not implementing them from scratch.\nHeavy emphasis on training curves: The ability to diagnose problems from training/validation loss curves is the most practical skill. This gets extensive coverage.\nArchitecture as experimentation: Rather than memorizing rules, students learn to experiment systematically and interpret results. This prepares them for real-world work where there are no cookbook solutions.\nWhen NOT to use neural networks: Just as important as knowing when to use them. Students should understand that sometimes logistic regression is the better choice.\nConnecting to previous concepts: Constantly reference loss functions from Module 3, regularization from Module 4, classification metrics from Module 5. Show how neural networks are an extension of what they already know."
  },
  {
    "objectID": "Claude-planning/module-7-neural-networks.html#assessment-philosophy",
    "href": "Claude-planning/module-7-neural-networks.html#assessment-philosophy",
    "title": "Module 7: Neural Networks and Deep Learning - Planning Document",
    "section": "",
    "text": "By-hand work focuses on: - Conceptual understanding (what is each component doing?) - Reading and interpreting code and results - Basic model building with simple architectures - Diagnosis and debugging skills\nAI-assisted work focuses on: - Complete implementation pipelines - Extensive experimentation and hyperparameter tuning - Complex architectures and custom components - Production-quality code with proper error handling\nThis division ensures students understand what neural networks are doing while acknowledging that implementing production deep learning pipelines is where AI assistance truly shines."
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "",
    "text": "Course Website\nTextbook\nProjects"
  },
  {
    "objectID": "course-schedule.html#links",
    "href": "course-schedule.html#links",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "",
    "text": "Course Website\nTextbook\nProjects"
  },
  {
    "objectID": "course-schedule.html#course-information",
    "href": "course-schedule.html#course-information",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Course Information",
    "text": "Course Information\n\nMeeting Pattern: 3 days per week, 50 minutes per class\nIn-class quizzes: Last class of each week is a quiz class. Quizzes are by-hand, no computer.\nDuration: 15 weeks (45 class sessions)\nContent Sessions: ~2 per week (new material)"
  },
  {
    "objectID": "course-schedule.html#module-1-exploratory-data-analysis-ai-assisted-coding-weeks-1-3",
    "href": "course-schedule.html#module-1-exploratory-data-analysis-ai-assisted-coding-weeks-1-3",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Module 1: Exploratory Data Analysis & AI-Assisted Coding (Weeks 1-3)",
    "text": "Module 1: Exploratory Data Analysis & AI-Assisted Coding (Weeks 1-3)\n\nLearning Objectives\n\nUse AI coding assistants effectively with good prompts\nManipulate data with Pandas (loading, filtering, cleaning)\nCalculate and interpret statistical summaries\nCreate effective visualizations with Seaborn\nTest data analysis code with assert statements\n\n\n\nSchedule\n\n\n\nWeek\nDay\nTopic\n\n\n\n\n1\n1\nCourse intro, What is EDA?, AI coding assistants\n\n\n1\n2\nWriting effective prompts, when to use AI vs by hand\n\n\n1\n3\nQuiz 1a\n\n\n2\n1\nPandas basics: loading data, selecting/filtering\n\n\n2\n2\nData cleaning: missing values, duplicates, data types\n\n\n2\n3\nQuiz 1b\n\n\n3\n1\nStatistical summaries, outlier detection\n\n\n3\n2\nIn-class activity: EDA practice\n\n\n3\n3\nQuiz 1c\n\n\n\n\n\nResources\n\nChapter 1: EDA and AI-Assisted Coding\nModule 1 Homework"
  },
  {
    "objectID": "course-schedule.html#module-2-regression-models-weeks-4-7",
    "href": "course-schedule.html#module-2-regression-models-weeks-4-7",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Module 2: Regression Models (Weeks 4-7)",
    "text": "Module 2: Regression Models (Weeks 4-7)\n\nLearning Objectives\n\nUnderstand proper train/validation/test methodology\nUse cross-validation for reliable model evaluation\nUnderstand and check linear regression assumptions\nInterpret regression coefficients correctly\nUse residual analysis to diagnose model problems\nApply polynomial features for non-linear relationships\nRecognize and handle multicollinearity\nUse Ridge, Lasso, and Elastic Net regularization\n\n\n\nSchedule\n\n\n\nWeek\nDay\nTopic\n\n\n\n\n4\n1\nTrain/validation/test methodology, cross-validation intro\n\n\n4\n2\nLinear regression fundamentals & assumptions\n\n\n4\n3\nQuiz 2a\n\n\n5\n1\nRegression evaluation metrics (MSE, MAE, R², Adjusted R²)\n\n\n5\n2\nResidual analysis and diagnostic plots\n\n\n5\n3\nQuiz 2b\n\n\n6\n1\nPolynomial regression and feature expansion\n\n\n6\n2\nMulticollinearity: detection and handling\n\n\n6\n3\nQuiz 2c\n\n\n7\n1\nRegularization: Ridge and Lasso regression\n\n\n7\n2\nElastic Net and choosing between approaches\n\n\n7\n3\nQuiz 2d\n\n\n\n\n\nResources\n\nChapter 2: Regression Models\nModule 2 Homework"
  },
  {
    "objectID": "course-schedule.html#module-3-classification-models-weeks-8-10",
    "href": "course-schedule.html#module-3-classification-models-weeks-8-10",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Module 3: Classification Models (Weeks 8-10)",
    "text": "Module 3: Classification Models (Weeks 8-10)\n\nLearning Objectives\n\nUnderstand supervised vs unsupervised learning paradigms\nUnderstand logistic regression and interpret odds ratios\nRead and interpret confusion matrices\nCalculate precision, recall, F1 score and choose appropriately\nFit and tune decision trees, random forests, SVMs, k-NN\nUse ROC curves and AUC for model comparison\nHandle class imbalance with weights and resampling\n\n\n\nSchedule\n\n\n\nWeek\nDay\nTopic\n\n\n\n\n8\n1\nSupervised vs unsupervised learning, classification intro\n\n\n8\n2\nLogistic regression and confusion matrices\n\n\n8\n3\nQuiz 3a\n\n\n9\n1\nClassification metrics: precision, recall, F1, ROC/AUC\n\n\n9\n2\nDecision trees and random forests\n\n\n9\n3\nQuiz 3b\n\n\n10\n1\nSupport vector machines and k-Nearest neighbors\n\n\n10\n2\nClass imbalance handling (SMOTE, class weights)\n\n\n10\n3\nQuiz 3c\n\n\n\n\n\nResources\n\nChapter 3: Classification Models\nModule 3 Homework\nModule 3 Quiz"
  },
  {
    "objectID": "course-schedule.html#module-4-llms-for-feature-engineering-and-data-extraction-weeks-11-12",
    "href": "course-schedule.html#module-4-llms-for-feature-engineering-and-data-extraction-weeks-11-12",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Module 4: LLMs for Feature Engineering and Data Extraction (Weeks 11-12)",
    "text": "Module 4: LLMs for Feature Engineering and Data Extraction (Weeks 11-12)\n\nLearning Objectives\n\nUnderstand how LLMs can transform unstructured text into ML-ready features\nWrite effective prompts for information extraction tasks\nIntegrate LLM APIs (OpenAI, Anthropic, Google) into data pipelines\nParse and validate JSON responses from LLMs\nCalculate and compare API costs across different providers\nValidate LLM-extracted features for quality and consistency\nIntegrate LLM-generated features into traditional ML models\nMake informed decisions about when to use LLMs vs. traditional approaches\n\n\n\nSchedule\n\n\n\n\n\n\n\n\nWeek\nDay\nTopic\n\n\n\n\n11\n1\nIntroduction to LLMs for feature extraction; Zero-shot extraction; Writing effective prompts\n\n\n11\n2\nAPI integration (OpenAI, Anthropic, Google); Parameters (temperature, max_tokens); Cost calculation\n\n\n11\n3\nQuiz 4a: Prompt engineering and API basics\n\n\n12\n1\nJSON parsing and validation; Handling malformed responses; Building DataFrames\n\n\n12\n2\nFew-shot prompting; Quality assessment; Integration with ML pipelines\n\n\n12\n3\nQuiz 4b: Integration and quality control\n\n\n\n\n\nResources\n\nTextbook: Chapter 4: LLMs for Feature Engineering\nHomework: Module 4 Homework\nQuiz: Module 4 Quiz"
  },
  {
    "objectID": "course-schedule.html#weeks-13-15-projects-additional-topics",
    "href": "course-schedule.html#weeks-13-15-projects-additional-topics",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Weeks 13-15: Projects & Additional Topics",
    "text": "Weeks 13-15: Projects & Additional Topics\n\nWeek 13: Class Imbalance & Advanced Topics\n\n\n\nDay\nTopic\n\n\n\n\n1\nClass imbalance: detection and handling\n\n\n2\nSMOTE, class weights, threshold tuning\n\n\n3\nHackathon Day: Imbalanced data challenge\n\n\n\n\n\nWeek 14: Final Project Work\n\n\n\nDay\nTopic\n\n\n\n\n1\nProject work session\n\n\n2\nProject work session\n\n\n3\nProject work session / Office hours\n\n\n\n\n\nWeek 15: Presentations & Wrap-up\n\n\n\nDay\nTopic\n\n\n\n\n1\nFinal project presentations\n\n\n2\nFinal project presentations\n\n\n3\nCourse wrap-up and review"
  },
  {
    "objectID": "course-schedule.html#assessment-overview",
    "href": "course-schedule.html#assessment-overview",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Assessment Overview",
    "text": "Assessment Overview\n\n\n\nAssessment\nWeight\nNotes\n\n\n\n\nModule Quizzes (11 total)\n25%\nConceptual + code writing\n\n\nModule Homework (4 total)\n35%\nPart A by-hand, Part B AI-assisted\n\n\nHackathons/Activities\n15%\nIn-class participation\n\n\nFinal Project\n25%\nEnd-to-end data science project"
  },
  {
    "objectID": "course-schedule.html#important-dates",
    "href": "course-schedule.html#important-dates",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Important Dates",
    "text": "Important Dates\n\nWeek 7: Midterm check-in (End of Module 2)\nWeek 12: Last day to submit late homework\nWeek 14-15: Final project presentations\nFinals Week: No final exam (project-based course)"
  },
  {
    "objectID": "course-schedule.html#tips-for-success",
    "href": "course-schedule.html#tips-for-success",
    "title": "MATH 3339: Introduction to Data Science",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nDo the by-hand work first. Part A of homework builds the foundation you need.\nUse AI as a partner, not a replacement. Understand what the code does before using it.\nStart homework early. Part B requires iteration with AI assistants.\nCome to hackathon days prepared. Review the relevant chapter beforehand.\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2a.html",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2a.html",
    "title": "Quiz 2a: Train/Test Methodology and Linear Regression",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#instructions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#instructions",
    "title": "Quiz 2a: Train/Test Methodology and Linear Regression",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#section-a-conceptual-questions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#section-a-conceptual-questions",
    "title": "Quiz 2a: Train/Test Methodology and Linear Regression",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou examine a residuals vs. fitted values plot for a linear regression model and observe the following:\n\nResiduals are randomly scattered around zero\nNo clear pattern or trend\nVariance appears constant across all fitted values\n\na) What does this suggest about the model’s assumptions?\nb) Would you be confident using this model for predictions? Why or why not?\n\n\n\nQuestion 2 (2 points)\nWhat is R² (R-squared)? In 2-3 sentences, explain what this metric tells you about a regression model.\n\n\n\nQuestion 3 (2 points)\nWhy is it important to split data into training and test sets before fitting a regression model? What problem does this help prevent?"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#section-b-code-writing",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#section-b-code-writing",
    "title": "Quiz 2a: Train/Test Methodology and Linear Regression",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 4-8, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nAlso assume you have a DataFrame called df with features in X and target in y.\n\n\nQuestion 4 (2 points)\nWrite code to split data into training and test sets with an 80/20 split and random_state=42. Store the results in variables named X_train, X_test, y_train, y_test.\n\n\n\nQuestion 5 (3 points)\nWrite code to fit a Linear Regression model and calculate residuals:\n\nFit the model on X_train and y_train\nMake predictions on X_test\nCalculate residuals (actual - predicted)\nStore residuals in a variable called residuals\n\n\n\n\nQuestion 6 (3 points)\nWrite code to create a residuals vs. fitted values plot:\n\nUsing the residuals from Question 5\nPlot residuals (y-axis) vs. predictions (x-axis)\nAdd a horizontal line at y=0\nAdd axis labels: “Fitted Values” and “Residuals”\n\n\n\n\nQuestion 7 (4 points)\nWrite a function called calculate_mse_mae that:\n\nTakes y_true and y_pred as inputs\nCalculates both MSE (Mean Squared Error) and MAE (Mean Absolute Error)\nReturns them as a tuple: (mse, mae)\n\nYou can use numpy functions like np.mean(), np.abs(), and squaring.\n\n\n\nQuestion 8 (3 points)\nWrite code to:\n\nFit a linear regression model on X_train and y_train\nCalculate the R² score on the test set\nPrint the R² value"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#grading-rubric",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2a.html#grading-rubric",
    "title": "Quiz 2a: Train/Test Methodology and Linear Regression",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (7 points)\n\nUnderstanding residual plots and diagnostics: 3 points\nUnderstanding R² metric: 2 points\nUnderstanding train/test split purpose: 2 points\n\nSection B: Code Writing (15 points)\n\nTrain/test splitting: 2 points\nModel fitting and residual calculation: 6 points\nResidual plot creation: 3 points\nFunction writing (MSE/MAE): 4 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 22 points"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2c.html",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2c.html",
    "title": "Quiz 2c: Polynomial Regression and Multicollinearity",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#instructions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#instructions",
    "title": "Quiz 2c: Polynomial Regression and Multicollinearity",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#section-a-conceptual-questions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#section-a-conceptual-questions",
    "title": "Quiz 2c: Polynomial Regression and Multicollinearity",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou have a dataset with two highly correlated features: years_experience and months_employed (correlation = 0.95).\na) What is this problem called?\nb) How does it affect linear regression coefficient estimates?\nc) Name two approaches to address this problem.\n\n\n\nQuestion 2 (3 points)\nYou fit polynomial regression models of different degrees:\n\nDegree 1: Train MSE = 25, Test MSE = 26\nDegree 3: Train MSE = 10, Test MSE = 11\nDegree 8: Train MSE = 2, Test MSE = 35\n\na) Which model is overfitting?\nb) Which model would you choose for deployment?\nc) What could you do to improve the degree 8 model?\n\n\n\nQuestion 3 (3 points)\nYou’re building a regression model to predict house prices using features like square_feet, bedrooms, bathrooms, and total_rooms.\na) Why might bedrooms, bathrooms, and total_rooms be problematic to include together?\nb) How could you detect whether multicollinearity is actually a problem in your model?\n\n\n\nQuestion 4 (2 points)\nWhen fitting polynomial regression, why is it important to create polynomial features from the training data only, then apply the same transformation to the test data?"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#section-b-code-writing",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#section-b-code-writing",
    "title": "Quiz 2c: Polynomial Regression and Multicollinearity",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 5-9, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nAlso assume you have X_train, X_test, y_train, y_test already defined.\n\n\nQuestion 5 (4 points)\nWrite code to create polynomial features of degree 3 and fit a linear regression model:\n\nCreate polynomial features from X_train\nFit a Linear Regression model on the polynomial features\nTransform X_test to polynomial features\nMake predictions on the transformed test data\n\n\n\n\nQuestion 6 (3 points)\nWrite code to check for multicollinearity:\n\nCreate a correlation matrix for X_train (assume it’s a DataFrame)\nPrint the correlation values\nIdentify pairs of features with correlation &gt; 0.8\n\nYou can print the correlation matrix directly or use any reasonable approach.\n\n\n\nQuestion 7 (4 points)\nWrite code to compare polynomial models of different degrees:\n\nCreate a list of degrees: [1, 2, 3, 4, 5]\nFor each degree, create polynomial features, fit a model, and calculate test MSE\nStore results in a list or print them\nIdentify which degree has the lowest test MSE\n\n\n\n\nQuestion 8 (4 points)\nWrite a function called fit_polynomial_model that:\n\nTakes X_train, y_train, X_test, degree as parameters\nCreates polynomial features of the specified degree\nFits a linear regression model\nReturns the fitted model and the transformed test features\n\n\n\n\nQuestion 9 (3 points)\nYou have a DataFrame X_train with columns: feature_a, feature_b, feature_c.\nWrite code to:\n\nCalculate the correlation between feature_a and feature_b\nIf the correlation is greater than 0.9, drop feature_b from the DataFrame\nPrint a message indicating whether the feature was dropped"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#grading-rubric",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2c.html#grading-rubric",
    "title": "Quiz 2c: Polynomial Regression and Multicollinearity",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (11 points)\n\nUnderstanding multicollinearity: 6 points\nUnderstanding polynomial regression and overfitting: 5 points\n\nSection B: Code Writing (18 points)\n\nPolynomial feature creation and fitting: 8 points\nMulticollinearity detection: 3 points\nModel comparison: 4 points\nFunction writing: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 29 points"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz.html",
    "href": "Assignments/Module 2 - Regression/module-2-quiz.html",
    "title": "Module 2 Quiz: Regression Models",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz.html#instructions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz.html#instructions",
    "title": "Module 2 Quiz: Regression Models",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz.html#section-a-conceptual-questions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz.html#section-a-conceptual-questions",
    "title": "Module 2 Quiz: Regression Models",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou examine a residuals vs. fitted values plot for a linear regression model and observe the following:\n\nResiduals are randomly scattered around zero\nNo clear pattern or trend\nVariance appears constant across all fitted values\n\na) What does this suggest about the model’s assumptions?\nb) Would you be confident using this model for predictions? Why or why not?\n\n\n\nQuestion 2 (4 points)\nConsider these two linear regression models predicting house prices:\nModel A: Uses features: sqft, bedrooms, bathrooms - Coefficients: sqft=150, bedrooms=10000, bathrooms=5000 - R² = 0.75\nModel B: Same features after Ridge regularization (alpha=10) - Coefficients: sqft=120, bedrooms=8000, bathrooms=4000 - R² = 0.73\na) Why are the coefficients smaller in Model B?\nb) Which model is likely to generalize better to new data? Why?\nc) What is the tradeoff we’re making by using regularization?\n\n\n\nQuestion 3 (3 points)\nYou have a dataset with two highly correlated features: years_experience and months_employed (correlation = 0.95).\na) What is this problem called?\nb) How does it affect linear regression coefficient estimates?\nc) Name two approaches to address this problem.\n\n\n\nQuestion 4 (4 points)\nCompare Ridge and Lasso regularization:\na) What is the key difference in how they shrink coefficients?\nb) Which one can perform automatic feature selection? Why?\nc) If you have 100 features and suspect only 10 are truly important, which regularization method would you prefer?\n\n\n\nQuestion 5 (3 points)\nYou fit polynomial regression models of different degrees:\n\nDegree 1: Train MSE = 25, Test MSE = 26\nDegree 3: Train MSE = 10, Test MSE = 11\nDegree 8: Train MSE = 2, Test MSE = 35\n\na) Which model is overfitting?\nb) Which model would you choose for deployment?\nc) What could you do to improve the degree 8 model?\n\n\n\nQuestion 6 (3 points)\nExplain in 2-3 sentences:\na) What does a histogram of residuals tell you about your model, and what should you look for?\nb) Why is homoscedasticity (constant variance of residuals) important for linear regression?\n\n\n\nQuestion 7 (3 points)\nYou fit a Lasso model with different alpha values:\n\nAlpha = 0.1: 8 features have non-zero coefficients\nAlpha = 1.0: 5 features have non-zero coefficients\nAlpha = 10.0: 2 features have non-zero coefficients\n\na) As alpha increases, what happens to the number of selected features?\nb) What happens to the magnitude of the remaining non-zero coefficients as alpha increases?\nc) How would you choose the optimal alpha value?\n\n\n\nQuestion 8 (2 points)\nWhat is the difference between R² and adjusted R²? When would adjusted R² be more useful?"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz.html#section-b-code-writing",
    "href": "Assignments/Module 2 - Regression/module-2-quiz.html#section-b-code-writing",
    "title": "Module 2 Quiz: Regression Models",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 9-15, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nAlso assume you have a DataFrame called df with features in X and target in y.\n\n\nQuestion 9 (3 points)\nWrite code to fit a Linear Regression model and calculate residuals:\n\nFit the model on X_train and y_train\nMake predictions on X_test\nCalculate residuals (actual - predicted)\nStore residuals in a variable called residuals\n\n\n\n\nQuestion 10 (3 points)\nWrite code to create a residuals vs. fitted values plot:\n\nUsing the residuals from Question 9\nPlot residuals (y-axis) vs. predictions (x-axis)\nAdd a horizontal line at y=0\nAdd axis labels: “Fitted Values” and “Residuals”\n\n\n\n\nQuestion 11 (4 points)\nWrite code to fit a Ridge regression model with alpha=10.0:\n\nCreate the Ridge model with alpha=10.0\nFit it on X_train and y_train\nExtract the coefficients\nCalculate the R² score on X_test\n\n\n\n\nQuestion 12 (4 points)\nWrite code to create polynomial features of degree 3 and fit a linear regression model:\n\nCreate polynomial features from X_train\nFit a Linear Regression model on the polynomial features\nTransform X_test to polynomial features\nMake predictions on the transformed test data\n\n\n\n\nQuestion 13 (3 points)\nWrite code to fit Lasso regression with alpha=1.0 and identify which features were selected:\n\nFit Lasso with alpha=1.0\nGet the coefficients\nPrint which features have non-zero coefficients\n\nAssume features are named in a list called feature_names.\n\n\n\nQuestion 14 (4 points)\nWrite code to compare Ridge models with different alpha values:\n\nCreate a list of alpha values: [0.1, 1.0, 10.0, 100.0]\nFor each alpha, fit a Ridge model\nCalculate and store the test R² for each\nPrint the alpha and corresponding R²\n\n\n\n\nQuestion 15 (4 points)\nWrite a function called calculate_mse_mae that:\n\nTakes y_true and y_pred as inputs\nCalculates both MSE and MAE\nReturns them as a tuple: (mse, mae)\n\nYou can use numpy functions like np.mean(), np.abs(), and squaring."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz.html#grading-rubric",
    "href": "Assignments/Module 2 - Regression/module-2-quiz.html#grading-rubric",
    "title": "Module 2 Quiz: Regression Models",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (25 points)\n\nUnderstanding residual plots and diagnostics: 6 points\nUnderstanding regularization (Ridge, Lasso): 7 points\nRecognizing multicollinearity and overfitting: 6 points\nUnderstanding evaluation metrics and assumptions: 6 points\n\nSection B: Code Writing (25 points)\n\nModel fitting and prediction: 7 points\nResidual calculation and visualization: 6 points\nRegularization implementation: 7 points\nPolynomial features and feature selection: 5 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 50 points"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html",
    "title": "Quiz 4a: LLM Prompting and API Basics",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#instructions",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#instructions",
    "title": "Quiz 4a: LLM Prompting and API Basics",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#section-a-conceptual-questions",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#section-a-conceptual-questions",
    "title": "Quiz 4a: LLM Prompting and API Basics",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou have three text extraction tasks:\nTask A: Extract email addresses from customer support tickets (format: name@company.com)\nTask B: Extract sentiment (positive/negative/neutral) from product reviews with complex, nuanced language\nTask C: Extract dates from legal documents (format: MM/DD/YYYY or Month DD, YYYY)\na) For which task(s) would using an LLM API be most appropriate? For which would regex or traditional methods be better?\nb) Explain your reasoning for each choice.\n\n\n\nQuestion 2 (3 points)\nCompare these two prompts for extracting product categories from reviews:\nPrompt A: “What category is this product?”\nPrompt B: “Read this product review and extract the product category. Return your answer as JSON with a single field ‘category’. Valid categories are: Electronics, Clothing, Home & Garden, Books, Toys. Review: {review_text}”\na) Which prompt will produce more reliable, consistent extractions?\nb) Explain two specific improvements Prompt B makes over Prompt A.\n\n\n\nQuestion 3 (4 points)\nYou need to extract sentiment from 1,000 customer reviews.\nGiven:\n\nGPT-4: $0.03 per 1,000 input tokens, $0.06 per 1,000 output tokens\nGPT-3.5-turbo: $0.001 per 1,000 input tokens, $0.002 per 1,000 output tokens\nAverage review: 150 tokens\nAverage response: 20 tokens\n\na) Calculate the total cost to process all 1,000 reviews using GPT-4.\nb) Calculate the total cost using GPT-3.5-turbo.\nc) If GPT-4 achieves 95% accuracy and GPT-3.5-turbo achieves 88% accuracy, which would you choose and why?\n\n\n\nQuestion 4 (3 points)\nYou’re using an LLM API to extract information from text. Explain the effect of each parameter:\na) What happens when you set temperature=0 vs temperature=1.0? When would you use each setting for extraction tasks?\nb) What does max_tokens control? Why might you set it to a low value like 50 for extraction tasks?"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#section-b-code-writing",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#section-b-code-writing",
    "title": "Quiz 4a: LLM Prompting and API Basics",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 5-8, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nimport json\nfrom openai import OpenAI\nAlso assume you have an OpenAI client initialized as client = OpenAI().\n\n\nQuestion 5 (3 points)\nWrite a prompt (as a Python string) to extract sentiment from a product review. The prompt should:\n\nAsk the LLM to analyze the review text\nRequest the response in JSON format with a field called “sentiment”\nSpecify that sentiment should be one of: “positive”, “negative”, or “neutral”\n\nStore the prompt in a variable called prompt.\n\n\n\nQuestion 6 (3 points)\nWrite a prompt (as a Python string) to extract the urgency level from a customer support ticket. The prompt should:\n\nAsk the LLM to analyze urgency\nRequest JSON format with a field called “urgency”\nSpecify urgency levels as: “low”, “medium”, “high”, or “critical”\n\nStore the prompt in a variable called urgency_prompt.\n\n\n\nQuestion 7 (4 points)\nWrite code to make an API call to extract sentiment from a review. Your code should:\n\nCreate a message with the system role: “You extract sentiment from reviews”\nUse the user role with: “Extract sentiment from this review: {review_text}. Return JSON.”\nCall client.chat.completions.create() with model=“gpt-3.5-turbo”\nSet temperature=0 and max_tokens=50\n\nStore the response in a variable called response.\n(Note: You can use simplified syntax; exact parameter names don’t need to be perfect)\n\n\n\nQuestion 8 (4 points)\nWrite a function called calculate_extraction_cost that:\n\nTakes three parameters: num_texts, tokens_per_text, and price_per_1k_tokens\nCalculates the total tokens as num_texts * tokens_per_text\nCalculates the cost as (total_tokens / 1000) * price_per_1k_tokens\nReturns the total cost\n\n\n\n\nQuestion 9 (3 points)\nYou need to calculate the total cost for processing 500 customer reviews where: - Each review is approximately 200 tokens - API pricing is $0.002 per 1,000 tokens\nWrite code to: 1. Use the calculate_extraction_cost function (assume it’s already defined) 2. Calculate and store the cost in a variable called total_cost 3. Print the result with a label"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#grading-rubric",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4a.html#grading-rubric",
    "title": "Quiz 4a: LLM Prompting and API Basics",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (13 points)\n\nUnderstanding when to use LLMs vs alternatives: 3 points\nPrompt quality and best practices: 3 points\nCost calculation and comparison: 4 points\nAPI parameters (temperature, max_tokens): 3 points\n\nSection B: Code Writing (17 points)\n\nWriting effective prompts: 6 points (Q5, Q6)\nMaking API calls: 4 points\nCost calculation function: 4 points\nUsing cost function: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 30 points"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz.html",
    "href": "Assignments/Module 1 - EDA/module-1-quiz.html",
    "title": "Module 1 Quiz: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz.html#instructions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz.html#instructions",
    "title": "Module 1 Quiz: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz.html#section-a-conceptual-questions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz.html#section-a-conceptual-questions",
    "title": "Module 1 Quiz: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (2 points)\nYou’re exploring a dataset of customer purchases with columns for customer_id, purchase_amount, date, and product_category. You create a histogram of purchase_amount and notice it’s heavily right-skewed with a few very large outliers.\na) Why might a right-skewed distribution with outliers make it harder to understand typical customer behavior?\nb) Name two approaches you could take during EDA to better understand this distribution.\n\n\n\nQuestion 2 (2 points)\nYou’re working with a dataset that has a column called income with many missing values (NaN). Your teammate suggests: “Let’s just drop all rows with missing income values.”\na) What’s one potential problem with this approach?\nb) What would you do during EDA to decide whether dropping these rows is a good idea?\n\n\n\nQuestion 3 (2 points)\nYou want to visualize the relationship between two numerical variables: age and salary.\na) What type of visualization would you use and why?\nb) If you wanted to add a third categorical variable (like department) to this visualization, how could you incorporate it?\n\n\n\nQuestion 4 (2 points)\nWhen prompting an AI coding assistant like Gemini, which of the following prompts is likely to produce better results? Briefly explain why.\nPrompt A: “Make a graph of the data, and make the graph easy to read, intuitive, and visually appealing.”\nPrompt B: “Create a scatter plot showing the relationship between age (x-axis) and salary (y-axis). Use different colors for each department. Add axis labels and a title.”\n\n\n\nQuestion 5 (2 points)\nYou’re creating an automated EDA function that takes a DataFrame and generates visualizations and summary statistics.\na) Describe what this function should do and what kinds of outputs it should produce.\nb) Write three unit tests (using assert statements) you would use to verify the function works correctly. For each test, explain what it checks."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz.html#section-b-code-writing",
    "href": "Assignments/Module 1 - EDA/module-1-quiz.html#section-b-code-writing",
    "title": "Module 1 Quiz: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 6-11, assume you have already imported pandas as pd and seaborn as sns.\n\nQuestion 6 (2 points)\nWrite code to load a CSV file called customers.csv into a DataFrame called df.\n\n\n\nQuestion 7 (2 points)\nWrite code to select only the rows where the age column is greater than 25 and the city column equals “Houston”.\n\n\n\nQuestion 8 (2 points)\nWrite code to display the first 10 rows of a DataFrame called df and show summary statistics for all numerical columns.\n\n\n\nQuestion 9 (3 points)\nWrite code to create a histogram of a column called price with 20 bins. Add a title “Distribution of Product Prices” and label the x-axis as “Price ($)”.\n\n\n\nQuestion 10 (3 points)\nWrite code to create a scatter plot showing the relationship between square_feet (x-axis) and price (y-axis) for a DataFrame called housing_df. Add appropriate labels and a title.\n\n\n\nQuestion 11 (4 points)\nWrite a simple unit test function called test_price_is_positive that checks whether all values in a price column are greater than zero. The function should:\n\nTake a DataFrame as input\nReturn True if all prices are positive\nReturn False otherwise\n\nYou can write this as a simple Python function (you don’t need to use a testing framework)."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz.html#grading-rubric",
    "href": "Assignments/Module 1 - EDA/module-1-quiz.html#grading-rubric",
    "title": "Module 1 Quiz: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nConceptual Questions (10 points total)\n\nFull credit for demonstrating understanding of EDA principles\nPartial credit for correct reasoning even if incomplete\n\nCode Writing (16 points total)\n\nCorrect logic and approach: 80% of points\nSyntax and details: 20% of points\nMinor syntax errors will not be heavily penalized\n\nTotal: 26 points"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1a.html",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1a.html",
    "title": "Quiz 1a: AI-Assisted Coding and Prompting",
    "section": "",
    "text": "Time: 10-20 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1a.html#instructions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1a.html#instructions",
    "title": "Quiz 1a: AI-Assisted Coding and Prompting",
    "section": "",
    "text": "Time: 10-20 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1a.html#section-a-conceptual-questions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1a.html#section-a-conceptual-questions",
    "title": "Quiz 1a: AI-Assisted Coding and Prompting",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (2 points)\nWhen prompting an AI coding assistant like Gemini, which of the following prompts is likely to produce better results? Explain your answer.\nPrompt A: “Make a graph of the data, and make the graph easy to read, intuitive, and visually appealing.”\nPrompt B: “Create a scatter plot showing the relationship between age (x-axis) and salary (y-axis). Use different colors for each department. Add axis labels and a title.”\nAnswer:\n\n\n\n\n\n\nQuestion 2 (2 points)\nYou’re creating an automated EDA function that takes a DataFrame and generates visualizations and summary statistics.\na) Describe what this function should do and what kinds of outputs it should produce.\nb) Write three unit tests (using assert statements) you would use to verify the function works correctly. For each test, explain what it checks.\nAnswer:"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1a.html#section-b-code-writing",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1a.html#section-b-code-writing",
    "title": "Quiz 1a: AI-Assisted Coding and Prompting",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 3-4, assume you have already imported pandas as pd and seaborn as sns.\n\n\nQuestion 3 (3 points)\nYou want to write a simple function that checks if a DataFrame has any missing values. Write a function called has_missing_values that:\n\nTakes a DataFrame as input\nReturns True if there are any missing values (NaN) in the DataFrame\nReturns False otherwise\n\nYou can use DataFrame methods to help with this task.\nAnswer:\n\n\n\n\n\n\nQuestion 4 (3 points)\nYou have a DataFrame called df with a column called age. Write code to create a simple visualization showing the distribution of ages. Include a title for the plot.\nAnswer:"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1b.html",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1b.html",
    "title": "Quiz 1b: Pandas Fundamentals",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#instructions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#instructions",
    "title": "Quiz 1b: Pandas Fundamentals",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#section-a-conceptual-questions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#section-a-conceptual-questions",
    "title": "Quiz 1b: Pandas Fundamentals",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (2 points)\nYou’re working with a dataset that has a column called income with many missing values (NaN). Your teammate suggests: “Let’s just drop all rows with missing income values.”\na) What’s one potential problem with this approach?\nb) What would you do during EDA to decide whether dropping these rows is a good idea?"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#section-b-code-writing",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#section-b-code-writing",
    "title": "Quiz 1b: Pandas Fundamentals",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 2-5, assume you have already imported pandas as pd and seaborn as sns.\n\n\nQuestion 2 (2 points)\nWrite code to load a CSV file called customers.csv into a DataFrame called df.\n\n\n\nQuestion 3 (2 points)\nWrite code to select only the rows where the age column is greater than 25 and the city column equals “Houston”.\n\n\n\nQuestion 4 (2 points)\nWrite code to display the first 10 rows of a DataFrame called df and show summary statistics for all numerical columns.\n\n\n\nQuestion 5 (3 points)\nYou have a DataFrame called df with columns customer_id, name, and email. Write code to:\n\nSelect only the customer_id and email columns\nFilter to rows where the email contains “gmail.com”\nDisplay the first 5 rows of the result"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#grading-rubric",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1b.html#grading-rubric",
    "title": "Quiz 1b: Pandas Fundamentals",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (2 points)\n\nUnderstanding missing data handling: 2 points\n\nSection B: Code Writing (9 points)\n\nLoading data: 2 points\nFiltering operations: 4 points\nDisplaying and summarizing data: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 11 points"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz.html",
    "href": "Assignments/Module 3 - Classification/module-3-quiz.html",
    "title": "Module 3 Quiz: Classification Models",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz.html#instructions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz.html#instructions",
    "title": "Module 3 Quiz: Classification Models",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz.html#section-a-conceptual-questions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz.html#section-a-conceptual-questions",
    "title": "Module 3 Quiz: Classification Models",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou’re building a model to detect fraudulent credit card transactions. In your dataset, only 0.5% of transactions are fraud.\na) If you build a model that predicts “not fraud” for every transaction, what would its accuracy be?\nb) Why is this model useless despite high accuracy?\nc) What metric would be more appropriate to evaluate fraud detection models?\n\n\n\nQuestion 2 (4 points)\nA medical screening model produces the following confusion matrix:\n\n\n\n\nPredicted: No Disease\nPredicted: Disease\n\n\n\n\nActual: No Disease\n850\n50\n\n\nActual: Disease\n20\n80\n\n\n\na) Calculate the accuracy of this model.\nb) Calculate the precision for detecting disease.\nc) Calculate the recall for detecting disease.\nd) In medical screening, is precision or recall more important? Explain why.\n\n\n\nQuestion 3 (3 points)\nCompare logistic regression and decision trees:\na) What shape of decision boundary does logistic regression create? What about decision trees?\nb) Which model is more interpretable to a non-technical stakeholder? Why?\nc) Which model requires feature scaling? Why?\n\n\n\nQuestion 4 (4 points)\nYou’re training a decision tree classifier and observe:\n\nTraining accuracy: 98%\nTest accuracy: 72%\n\na) What problem does this indicate?\nb) Name two hyperparameters you could adjust to fix this problem.\nc) How do Random Forests address this problem?\n\n\n\nQuestion 5 (3 points)\nExplain the purpose of each in 1-2 sentences:\na) The ROC curve\nb) AUC (Area Under the Curve)\nc) The diagonal line on an ROC plot\n\n\n\nQuestion 6 (4 points)\nFor k-Nearest Neighbors:\na) What happens to the model’s decision boundary as k increases from 1 to 100?\nb) Why is feature scaling critical for k-NN but not for decision trees?\nc) Name two distance metrics you could use and when each is appropriate.\n\n\n\nQuestion 7 (3 points)\nYou have a dataset with these features for predicting customer churn: - monthly_charges (continuous, $20-$150) - tenure_months (continuous, 1-72) - contract_type (categorical: “Month-to-month”, “One year”, “Two year”)\na) Why can’t you directly apply Euclidean distance to this dataset?\nb) What should you do before using k-NN with this data?\n\n\n\nQuestion 8 (3 points)\nA spam classifier has: - Precision = 0.95 - Recall = 0.70\na) What does the precision value mean in plain English?\nb) What does the recall value mean in plain English?\nc) If you adjust the classification threshold from 0.5 to 0.3, what likely happens to precision and recall?"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz.html#section-b-code-writing",
    "href": "Assignments/Module 3 - Classification/module-3-quiz.html#section-b-code-writing",
    "title": "Module 3 Quiz: Classification Models",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 9-15, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (confusion_matrix, accuracy_score,\n                             precision_score, recall_score, f1_score,\n                             classification_report, roc_curve, roc_auc_score)\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nAlso assume you have X_train, X_test, y_train, y_test already defined.\n\n\nQuestion 9 (3 points)\nWrite code to:\n\nFit a Logistic Regression model\nMake predictions on X_test\nGet probability predictions for the positive class\n\n\n\n\nQuestion 10 (4 points)\nWrite code to:\n\nCreate a confusion matrix from y_test and predictions\nCalculate accuracy, precision, recall, and F1 score\nPrint all four metrics\n\n\n\n\nQuestion 11 (3 points)\nWrite code to fit a Decision Tree classifier with:\n\nMaximum depth of 5\nrandom_state=42\n\nThen calculate and print the training and test accuracy.\n\n\n\nQuestion 12 (4 points)\nWrite code to fit a Random Forest classifier with:\n\n100 trees\nMaximum depth of 5\nrandom_state=42\n\nThen extract and display the feature importances (assume features are in a list called feature_names).\n\n\n\nQuestion 13 (3 points)\nWrite code to:\n\nScale features using StandardScaler (fit on train, transform both)\nFit a k-NN classifier with k=5 on the scaled data\nCalculate the test accuracy\n\n\n\n\nQuestion 14 (4 points)\nWrite code to plot an ROC curve:\n\nGet probability predictions from a fitted logistic regression model (called log_model)\nCalculate false positive rate and true positive rate using roc_curve\nPlot the ROC curve with the diagonal reference line\nAdd a legend showing the AUC score\n\n\n\n\nQuestion 15 (4 points)\nWrite code to handle class imbalance using class weights:\n\nFit a Logistic Regression model WITH class_weight=‘balanced’\nFit another WITHOUT class weights\nPrint the classification report for both models"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz.html#grading-rubric",
    "href": "Assignments/Module 3 - Classification/module-3-quiz.html#grading-rubric",
    "title": "Module 3 Quiz: Classification Models",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (27 points)\n\nUnderstanding class imbalance and metrics: 7 points\nConfusion matrix interpretation: 4 points\nModel comparison and selection: 7 points\nUnderstanding k-NN and distance metrics: 6 points\nPrecision-recall tradeoff: 3 points\n\nSection B: Code Writing (25 points)\n\nModel fitting and prediction: 7 points\nMetrics calculation: 7 points\nFeature scaling and k-NN: 3 points\nROC curve plotting: 4 points\nClass imbalance handling: 4 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 52 points"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3b.html",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3b.html",
    "title": "Quiz 3b: Classification Metrics and Tree Models",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#instructions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#instructions",
    "title": "Quiz 3b: Classification Metrics and Tree Models",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#section-a-conceptual-questions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#section-a-conceptual-questions",
    "title": "Quiz 3b: Classification Metrics and Tree Models",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (4 points)\nYou’re training a decision tree classifier and observe:\n\nTraining accuracy: 98%\nTest accuracy: 72%\n\na) What problem does this indicate?\nb) Name two hyperparameters you could adjust to fix this problem.\nc) How do Random Forests address this problem?\n\n\n\nQuestion 2 (3 points)\nExplain the purpose of each in 1-2 sentences:\na) The ROC curve\nb) AUC (Area Under the Curve)\nc) The diagonal line on an ROC plot\n\n\n\nQuestion 3 (2 points)\nA spam classifier has precision = 0.95 and recall = 0.70.\na) If you adjust the classification threshold from 0.5 to 0.3, what likely happens to precision and recall?\nb) Why does this tradeoff occur?\n\n\n\nQuestion 4 (3 points)\nFor decision trees:\na) What are two advantages of using decision trees for classification?\nb) What is one major disadvantage of decision trees compared to Random Forests?\nc) What does “feature importance” tell you in a tree-based model?"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#section-b-code-writing",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#section-b-code-writing",
    "title": "Quiz 3b: Classification Metrics and Tree Models",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 5-10, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\nAlso assume you have X_train, X_test, y_train, y_test already defined.\n\n\nQuestion 5 (3 points)\nWrite code to fit a Decision Tree classifier with:\n\nMaximum depth of 5\nrandom_state=42\n\nThen calculate and print the training and test accuracy.\n\n\n\nQuestion 6 (4 points)\nWrite code to fit a Random Forest classifier with:\n\n100 trees\nMaximum depth of 5\nrandom_state=42\n\nThen extract and display the feature importances (assume features are in a list called feature_names).\n\n\n\nQuestion 7 (3 points)\nWrite code to create a simple bar plot showing feature importances from a fitted Random Forest model called rf_model.\nUse the feature names from a list called feature_names.\n\n\n\nQuestion 8 (4 points)\nWrite code to plot an ROC curve:\n\nGet probability predictions from a fitted logistic regression model (called log_model)\nCalculate false positive rate and true positive rate using roc_curve\nPlot the ROC curve with the diagonal reference line\nAdd a legend showing the AUC score\n\n\n\n\nQuestion 9 (3 points)\nYou have predictions from two different models stored in pred_model1 and pred_model2, both applied to the same test set y_test.\nWrite code to compare which model is better by calculating and printing the accuracy for both models."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#grading-rubric",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3b.html#grading-rubric",
    "title": "Quiz 3b: Classification Metrics and Tree Models",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (12 points)\n\nUnderstanding overfitting and Random Forests: 4 points\nROC curve and AUC understanding: 3 points\nThreshold adjustment effects: 2 points\nDecision tree advantages and feature importance: 3 points\n\nSection B: Code Writing (17 points)\n\nFitting decision tree with hyperparameters: 3 points\nFitting Random Forest and extracting feature importance: 4 points\nVisualizing feature importance: 3 points\nCreating ROC curves: 4 points\nModel comparison: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 29 points"
  },
  {
    "objectID": "content-overview.html",
    "href": "content-overview.html",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "This course emphasizes conceptual understanding and practical application over implementation details. Students will develop the analytical thinking needed to select appropriate models, recognize when assumptions are violated, and critically evaluate model performance. While AI coding assistants are powerful tools, this course ensures students build the foundational knowledge to use them effectively and debug their output.\n\n\n\n\nCourse Structure: The user is aiming to balance foundational concepts with AI-assisted coding in their machine learning course. They want students to understand key concepts deeply, while also gaining experience with AI tools\nHybrid Project-Based Format: The course is split into two parts. In the first half of the course, students learn and practice topics, similar to a traditional course. While there are no major projects in this part, there are mini-projects to let students practice the skills they are learning. In the second half of the course, students work on projects over a four-week period and present their findings at the end (presentation, submit code on Github, write-up on LinkedIn).\nChallenges with AI: A key challenge is ensuring htat students are genuinely understanding the material and not just relying in AI tools.\nIn-Class Structure: Alternate class sessions between hands-on foundational learning (e.g. mathematics of concepts, how models work and differ from one another, etc) and AI-assisted coding. For example, one session could focus on understanding and implementing concepts like regularization by hand, and the next session could explore using AI tools to extend that knowledge.\n\n\n\n\nDuration: 2 weeks\n\n\n\nPrompting strategies for AI coding assistants\nMaking and understanding unit tests with assert statements\nData manipulation and cleaning with Pandas\nExtracting and analyzing subsets of the data\nStatistical summaries and data profiling\nData visualization principles and best practices\nIdentifying patterns, outliers, and data quality issues\nFeature engineering and transformation techniques (Not covered in Chapter 1 - may be covered in later modules)\n\n\n\n\n\nUnderstand how data exploration informs future modeling decisions\nRecognize when data preprocessing is needed\nDevelop techniques for exploring and visualizing data distributions and relationships\nMaster the iterative nature of EDA\nWrite effective prompts for AI coding assistants\nTest data analysis code with assert statements\nDocument findings and decisions during EDA\n\n\n\n\n\nHands-on EDA projects requiring interpretation of findings\nIdentifying appropriate visualizations for different data types\nDocumenting insights that inform subsequent modeling choices\nWriting effective AI prompts for data analysis tasks\n\n\n\n\n\nWriting AI prompts for coding assistants\nWriting basic unit tests with assert statements\nLoading, selecting and filtering data in Pandas\nCreating basic data summaries and statistics\nMaking graphs in Seaborn (histograms, scatter plots, bar plots, box plots, heatmaps)\nIdentifying when to use AI vs. code by hand\n\n\n\n\n\nScaling data exploration (e.g. making many graphs, evaluating metrics across many subsets, etc)\nAutomating visualization generation for multiple variables\nTesting data analysis code across multiple scenarios\n\n\n\n\n\n\nDuration: 2-3 weeks\n\n\n\nTraining, validation, and test set methodology\nCross-validation for model selection and hyperparameter tuning\nLinear regression: assumptions, interpretation, diagnostics\nPolynomial regression and feature expansion\nRegularization techniques (Ridge, Lasso, Elastic Net)\nWhen and why regularization helps\nRegression evaluation metrics (MSE, MAE, R², adjusted R²)\nResidual analysis and assumption checking\n\n\n\n\n\nUnderstand when linear models are appropriate\nRecognize the impact of multicollinearity and how regularization addresses it\nInterpret coefficients and assess model assumptions\nChoose evaluation metrics based on problem context\n\n\n\n\n\nModel selection justification assignments\nDiagnostic interpretation exercises\nComparing regularized vs. non-regularized approaches on real datasets\n\n\n\n\n\nCreating train/validation/test splits properly\nImplementing k-fold cross-validation in scikit-learn\nManually calculating residuals and interpreting residual plots\nComputing MSE, MAE, R² by hand on small datasets to understand what they measure\nFitting linear regression with and without regularization in scikit-learn\nInterpreting diagnostic plots (residuals vs fitted, Q-Q plots)\nInterpreting regression coefficients and statistical significance\nUnderstanding when linear model assumptions are violated (looking at plots)\n\n\n\n\n\nTesting many hyperparameter values using cross-validation\nGrid search over many regularization parameters (alpha values)\nTesting polynomial features of various degrees at scale\nGenerating comprehensive model diagnostic reports\nComparing regularized vs non-regularized models across multiple datasets\n\n\n\n\n\n\nDuration: 3-4 weeks\n\n\n\nMachine learning paradigms: supervised vs unsupervised learning\nLogistic regression: concepts, assumptions, interpretation\nDecision trees: splitting criteria, pruning, interpretability\nRandom forests and ensemble methods\nSupport vector machines: margin concepts and kernel trick\nk-Nearest neighbors: distance metrics and curse of dimensionality\nClassification evaluation metrics (accuracy, precision, recall, F1, ROC-AUC)\nHandling class imbalance\n\n\n\n\n\nUnderstand trade-offs between different classification approaches\nRecognize when tree-based vs. linear vs. instance-based methods are appropriate\nNavigate the precision-recall tradeoff based on problem requirements\nIdentify and address class imbalance issues\n\n\n\n\n\nModel comparison projects with justification of choices\nEvaluation metric selection based on business context\nHandling real-world classification challenges\n\n\n\n\n\nUnderstanding supervised vs unsupervised learning paradigms\nImplementing each classifier in scikit-learn (LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, SVC, KNeighborsClassifier)\nCreating and interpreting confusion matrices\nPlotting ROC curves and understanding AUC\nUnderstanding decision boundaries through visualization\nInterpreting feature importance from tree-based models\nRecognizing class imbalance in datasets (looking at value_counts())\n\n\n\n\n\nHyperparameter tuning across many classifiers simultaneously\nTesting different strategies for handling class imbalance (SMOTE, class weights, etc.)\nGenerating comprehensive classification reports across multiple metrics, including ROC and AUC, along with other evaluation metrics\nComparing model performance across many train/test splits\n\n\n\n\n\n\nDuration: 2-3 weeks\n\n\n\nUsing LLMs (via APIs or local models) to extract structured data from unstructured text\nPrompt engineering for data extraction tasks\nConverting text to categorical and numerical features for ML models\nSentiment analysis, entity extraction, and classification with LLMs\nValidating and cleaning LLM-generated features\nCost and latency considerations when using LLM APIs\nWhen to use LLMs vs. traditional NLP techniques\n\n\n\n\n\nUnderstand how LLMs can transform unstructured data into ML-ready features\nWrite effective prompts for information extraction tasks\nRecognize the tradeoffs between LLM quality, cost, and speed\nValidate LLM outputs and handle extraction errors\nIntegrate LLM-generated features into traditional ML pipelines\nUnderstand when LLMs add value vs. when simpler approaches suffice\n\n\n\n\n\nDesigning prompts that extract specific information reliably\nEvaluating quality of LLM-generated features\nBuilding complete pipelines: unstructured text → LLM extraction → ML model\nCost-benefit analysis of using LLMs vs. alternatives\n\n\n\n\n\nWriting prompts for extraction tasks (sentiment, categories, numeric values)\nCalling LLM APIs (OpenAI, Anthropic, or local models via Hugging Face)\nParsing and validating LLM JSON responses\nConverting LLM outputs to pandas DataFrames\nIdentifying when LLM extraction fails or is inconsistent\nCalculating API costs and latency for different approaches\nIntegrating LLM-generated features with existing datasets\nTraining ML models on LLM-extracted features\n\n\n\n\n\nBuilding batch processing pipelines for large-scale extraction\nImplementing retry logic and error handling for API calls\nCreating prompt templates for multiple extraction scenarios\nGenerating comprehensive quality reports comparing LLM extraction methods\nBuilding complete pipelines that extract features, train models, and evaluate performance\nTesting multiple LLMs and comparing extraction quality/cost\n\n\n\n\n\n\nDuration: 3-4 weeks\n\n\n\nPerceptrons and multi-layer perceptrons (MLPs)\nActivation functions: purpose and selection criteria\nBackpropagation algorithm (conceptual understanding)\nLoss functions for neural networks\nRegularization in deep learning (dropout, batch normalization)\nArchitecture design principles\nHyperparameter tuning strategies\n\n\n\n\n\nUnderstand conceptually how neural networks approximate complex functions\nRecognize when deep learning is advantageous vs. traditional ML\nGrasp the role of different activation functions and loss functions\nUnderstand architecture design principles (when to add layers, neurons, etc.)\nLearn to use PyTorch for building and training networks\n\n\n\n\n\nUnderstanding when neural networks are appropriate vs. overkill\nInterpreting training curves and diagnosing overfitting/underfitting\nMaking architecture decisions with justification\n\n\n\n\n\nUnderstanding the conceptual structure of a neural network (layers, neurons, activation functions)\nReading and interpreting PyTorch model definitions\nLoading data and creating PyTorch DataLoaders\nUnderstanding the training loop conceptually (forward pass, loss computation, backward pass, optimization)\nPlotting and interpreting training/validation loss curves\nIdentifying overfitting and underfitting from plots\n\n\n\n\n\nWriting complete PyTorch training loops\nImplementing data augmentation and preprocessing pipelines\nHyperparameter tuning (learning rate, batch size, architecture variations)\nBuilding complex network architectures\nImplementing regularization techniques (dropout, batch normalization)\nRunning extensive experiments with different architectures\n\n\n\n\n\n\nDuration: 3-4 weeks\n\n\n\nIntroduction to pretrained models and transfer learning\nUsing Hugging Face model hub\nText preprocessing and tokenization\nFine-tuning pretrained language models for classification\nImage data representation and preprocessing\nFine-tuning pretrained vision models\nWhen to use pretrained models vs. training from scratch\nEvaluating fine-tuned models\n\n\n\n\n\nUnderstand the value of pretrained models and when to use them\nNavigate the Hugging Face ecosystem to find appropriate models\nRecognize the difference between using a model out-of-the-box vs. fine-tuning\nGrasp the basics of text and image preprocessing for modern models\nLearn to evaluate model performance on domain-specific tasks\n\n\n\n\n\nSelecting appropriate pretrained models for specific tasks\nFine-tuning projects with justification of model choice\nComparing pretrained model performance across different architectures\nUnderstanding the tradeoffs between model size, speed, and accuracy\n\n\n\n\n\nSearching Hugging Face for appropriate models\nLoading and using pretrained models for inference (text classification, sentiment analysis, image classification)\nUnderstanding model cards and documentation\nPreparing datasets in the format required by pretrained models\nRunning inference on small batches of data\nEvaluating model outputs and interpreting results\n\n\n\n\n\nWriting complete fine-tuning pipelines\nImplementing custom data loaders for text/image data\nSetting up training arguments and hyperparameters for fine-tuning\nRunning extensive experiments with different pretrained models\nCreating comprehensive evaluation reports\nBuilding end-to-end applications that use pretrained models\n\n\n\n\n\n\n\n\nEach module builds conceptual understanding before introducing implementation details. Students master the “why” before the “how,” enabling them to effectively use AI coding assistants for implementation while maintaining critical thinking about model choice and evaluation.\n\n\n\nAll modules include case studies and projects using messy, real-world datasets that require students to make modeling decisions under uncertainty—mirroring actual data science work.\n\n\n\nEarly modules emphasize hands-on coding to build foundational skills. Later modules increasingly allow AI coding assistance, but require detailed justification of modeling choices and interpretation of results.\n\n\n\nThe course culminates in a comprehensive project where students select and justify their approach across multiple modeling paradigms, demonstrating mastery of both technical skills and decision-making frameworks."
  },
  {
    "objectID": "content-overview.html#course-philosophy",
    "href": "content-overview.html#course-philosophy",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "This course emphasizes conceptual understanding and practical application over implementation details. Students will develop the analytical thinking needed to select appropriate models, recognize when assumptions are violated, and critically evaluate model performance. While AI coding assistants are powerful tools, this course ensures students build the foundational knowledge to use them effectively and debug their output."
  },
  {
    "objectID": "content-overview.html#key-points",
    "href": "content-overview.html#key-points",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Course Structure: The user is aiming to balance foundational concepts with AI-assisted coding in their machine learning course. They want students to understand key concepts deeply, while also gaining experience with AI tools\nHybrid Project-Based Format: The course is split into two parts. In the first half of the course, students learn and practice topics, similar to a traditional course. While there are no major projects in this part, there are mini-projects to let students practice the skills they are learning. In the second half of the course, students work on projects over a four-week period and present their findings at the end (presentation, submit code on Github, write-up on LinkedIn).\nChallenges with AI: A key challenge is ensuring htat students are genuinely understanding the material and not just relying in AI tools.\nIn-Class Structure: Alternate class sessions between hands-on foundational learning (e.g. mathematics of concepts, how models work and differ from one another, etc) and AI-assisted coding. For example, one session could focus on understanding and implementing concepts like regularization by hand, and the next session could explore using AI tools to extend that knowledge."
  },
  {
    "objectID": "content-overview.html#module-1-ai-assisted-coding-and-exploratory-data-analysis-with-python",
    "href": "content-overview.html#module-1-ai-assisted-coding-and-exploratory-data-analysis-with-python",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Duration: 2 weeks\n\n\n\nPrompting strategies for AI coding assistants\nMaking and understanding unit tests with assert statements\nData manipulation and cleaning with Pandas\nExtracting and analyzing subsets of the data\nStatistical summaries and data profiling\nData visualization principles and best practices\nIdentifying patterns, outliers, and data quality issues\nFeature engineering and transformation techniques (Not covered in Chapter 1 - may be covered in later modules)\n\n\n\n\n\nUnderstand how data exploration informs future modeling decisions\nRecognize when data preprocessing is needed\nDevelop techniques for exploring and visualizing data distributions and relationships\nMaster the iterative nature of EDA\nWrite effective prompts for AI coding assistants\nTest data analysis code with assert statements\nDocument findings and decisions during EDA\n\n\n\n\n\nHands-on EDA projects requiring interpretation of findings\nIdentifying appropriate visualizations for different data types\nDocumenting insights that inform subsequent modeling choices\nWriting effective AI prompts for data analysis tasks\n\n\n\n\n\nWriting AI prompts for coding assistants\nWriting basic unit tests with assert statements\nLoading, selecting and filtering data in Pandas\nCreating basic data summaries and statistics\nMaking graphs in Seaborn (histograms, scatter plots, bar plots, box plots, heatmaps)\nIdentifying when to use AI vs. code by hand\n\n\n\n\n\nScaling data exploration (e.g. making many graphs, evaluating metrics across many subsets, etc)\nAutomating visualization generation for multiple variables\nTesting data analysis code across multiple scenarios"
  },
  {
    "objectID": "content-overview.html#module-2-regression-models",
    "href": "content-overview.html#module-2-regression-models",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Duration: 2-3 weeks\n\n\n\nTraining, validation, and test set methodology\nCross-validation for model selection and hyperparameter tuning\nLinear regression: assumptions, interpretation, diagnostics\nPolynomial regression and feature expansion\nRegularization techniques (Ridge, Lasso, Elastic Net)\nWhen and why regularization helps\nRegression evaluation metrics (MSE, MAE, R², adjusted R²)\nResidual analysis and assumption checking\n\n\n\n\n\nUnderstand when linear models are appropriate\nRecognize the impact of multicollinearity and how regularization addresses it\nInterpret coefficients and assess model assumptions\nChoose evaluation metrics based on problem context\n\n\n\n\n\nModel selection justification assignments\nDiagnostic interpretation exercises\nComparing regularized vs. non-regularized approaches on real datasets\n\n\n\n\n\nCreating train/validation/test splits properly\nImplementing k-fold cross-validation in scikit-learn\nManually calculating residuals and interpreting residual plots\nComputing MSE, MAE, R² by hand on small datasets to understand what they measure\nFitting linear regression with and without regularization in scikit-learn\nInterpreting diagnostic plots (residuals vs fitted, Q-Q plots)\nInterpreting regression coefficients and statistical significance\nUnderstanding when linear model assumptions are violated (looking at plots)\n\n\n\n\n\nTesting many hyperparameter values using cross-validation\nGrid search over many regularization parameters (alpha values)\nTesting polynomial features of various degrees at scale\nGenerating comprehensive model diagnostic reports\nComparing regularized vs non-regularized models across multiple datasets"
  },
  {
    "objectID": "content-overview.html#module-3-classification-models",
    "href": "content-overview.html#module-3-classification-models",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Duration: 3-4 weeks\n\n\n\nMachine learning paradigms: supervised vs unsupervised learning\nLogistic regression: concepts, assumptions, interpretation\nDecision trees: splitting criteria, pruning, interpretability\nRandom forests and ensemble methods\nSupport vector machines: margin concepts and kernel trick\nk-Nearest neighbors: distance metrics and curse of dimensionality\nClassification evaluation metrics (accuracy, precision, recall, F1, ROC-AUC)\nHandling class imbalance\n\n\n\n\n\nUnderstand trade-offs between different classification approaches\nRecognize when tree-based vs. linear vs. instance-based methods are appropriate\nNavigate the precision-recall tradeoff based on problem requirements\nIdentify and address class imbalance issues\n\n\n\n\n\nModel comparison projects with justification of choices\nEvaluation metric selection based on business context\nHandling real-world classification challenges\n\n\n\n\n\nUnderstanding supervised vs unsupervised learning paradigms\nImplementing each classifier in scikit-learn (LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, SVC, KNeighborsClassifier)\nCreating and interpreting confusion matrices\nPlotting ROC curves and understanding AUC\nUnderstanding decision boundaries through visualization\nInterpreting feature importance from tree-based models\nRecognizing class imbalance in datasets (looking at value_counts())\n\n\n\n\n\nHyperparameter tuning across many classifiers simultaneously\nTesting different strategies for handling class imbalance (SMOTE, class weights, etc.)\nGenerating comprehensive classification reports across multiple metrics, including ROC and AUC, along with other evaluation metrics\nComparing model performance across many train/test splits"
  },
  {
    "objectID": "content-overview.html#module-6-llms-for-feature-engineering-and-data-extraction",
    "href": "content-overview.html#module-6-llms-for-feature-engineering-and-data-extraction",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Duration: 2-3 weeks\n\n\n\nUsing LLMs (via APIs or local models) to extract structured data from unstructured text\nPrompt engineering for data extraction tasks\nConverting text to categorical and numerical features for ML models\nSentiment analysis, entity extraction, and classification with LLMs\nValidating and cleaning LLM-generated features\nCost and latency considerations when using LLM APIs\nWhen to use LLMs vs. traditional NLP techniques\n\n\n\n\n\nUnderstand how LLMs can transform unstructured data into ML-ready features\nWrite effective prompts for information extraction tasks\nRecognize the tradeoffs between LLM quality, cost, and speed\nValidate LLM outputs and handle extraction errors\nIntegrate LLM-generated features into traditional ML pipelines\nUnderstand when LLMs add value vs. when simpler approaches suffice\n\n\n\n\n\nDesigning prompts that extract specific information reliably\nEvaluating quality of LLM-generated features\nBuilding complete pipelines: unstructured text → LLM extraction → ML model\nCost-benefit analysis of using LLMs vs. alternatives\n\n\n\n\n\nWriting prompts for extraction tasks (sentiment, categories, numeric values)\nCalling LLM APIs (OpenAI, Anthropic, or local models via Hugging Face)\nParsing and validating LLM JSON responses\nConverting LLM outputs to pandas DataFrames\nIdentifying when LLM extraction fails or is inconsistent\nCalculating API costs and latency for different approaches\nIntegrating LLM-generated features with existing datasets\nTraining ML models on LLM-extracted features\n\n\n\n\n\nBuilding batch processing pipelines for large-scale extraction\nImplementing retry logic and error handling for API calls\nCreating prompt templates for multiple extraction scenarios\nGenerating comprehensive quality reports comparing LLM extraction methods\nBuilding complete pipelines that extract features, train models, and evaluate performance\nTesting multiple LLMs and comparing extraction quality/cost"
  },
  {
    "objectID": "content-overview.html#module-7-neural-networks-and-deep-learning",
    "href": "content-overview.html#module-7-neural-networks-and-deep-learning",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Duration: 3-4 weeks\n\n\n\nPerceptrons and multi-layer perceptrons (MLPs)\nActivation functions: purpose and selection criteria\nBackpropagation algorithm (conceptual understanding)\nLoss functions for neural networks\nRegularization in deep learning (dropout, batch normalization)\nArchitecture design principles\nHyperparameter tuning strategies\n\n\n\n\n\nUnderstand conceptually how neural networks approximate complex functions\nRecognize when deep learning is advantageous vs. traditional ML\nGrasp the role of different activation functions and loss functions\nUnderstand architecture design principles (when to add layers, neurons, etc.)\nLearn to use PyTorch for building and training networks\n\n\n\n\n\nUnderstanding when neural networks are appropriate vs. overkill\nInterpreting training curves and diagnosing overfitting/underfitting\nMaking architecture decisions with justification\n\n\n\n\n\nUnderstanding the conceptual structure of a neural network (layers, neurons, activation functions)\nReading and interpreting PyTorch model definitions\nLoading data and creating PyTorch DataLoaders\nUnderstanding the training loop conceptually (forward pass, loss computation, backward pass, optimization)\nPlotting and interpreting training/validation loss curves\nIdentifying overfitting and underfitting from plots\n\n\n\n\n\nWriting complete PyTorch training loops\nImplementing data augmentation and preprocessing pipelines\nHyperparameter tuning (learning rate, batch size, architecture variations)\nBuilding complex network architectures\nImplementing regularization techniques (dropout, batch normalization)\nRunning extensive experiments with different architectures"
  },
  {
    "objectID": "content-overview.html#module-8-sequence-models-text-analysis-and-computer-vision",
    "href": "content-overview.html#module-8-sequence-models-text-analysis-and-computer-vision",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Duration: 3-4 weeks\n\n\n\nIntroduction to pretrained models and transfer learning\nUsing Hugging Face model hub\nText preprocessing and tokenization\nFine-tuning pretrained language models for classification\nImage data representation and preprocessing\nFine-tuning pretrained vision models\nWhen to use pretrained models vs. training from scratch\nEvaluating fine-tuned models\n\n\n\n\n\nUnderstand the value of pretrained models and when to use them\nNavigate the Hugging Face ecosystem to find appropriate models\nRecognize the difference between using a model out-of-the-box vs. fine-tuning\nGrasp the basics of text and image preprocessing for modern models\nLearn to evaluate model performance on domain-specific tasks\n\n\n\n\n\nSelecting appropriate pretrained models for specific tasks\nFine-tuning projects with justification of model choice\nComparing pretrained model performance across different architectures\nUnderstanding the tradeoffs between model size, speed, and accuracy\n\n\n\n\n\nSearching Hugging Face for appropriate models\nLoading and using pretrained models for inference (text classification, sentiment analysis, image classification)\nUnderstanding model cards and documentation\nPreparing datasets in the format required by pretrained models\nRunning inference on small batches of data\nEvaluating model outputs and interpreting results\n\n\n\n\n\nWriting complete fine-tuning pipelines\nImplementing custom data loaders for text/image data\nSetting up training arguments and hyperparameters for fine-tuning\nRunning extensive experiments with different pretrained models\nCreating comprehensive evaluation reports\nBuilding end-to-end applications that use pretrained models"
  },
  {
    "objectID": "content-overview.html#course-integration-and-assessment-philosophy",
    "href": "content-overview.html#course-integration-and-assessment-philosophy",
    "title": "Introduction to Data Science Course Outline",
    "section": "",
    "text": "Each module builds conceptual understanding before introducing implementation details. Students master the “why” before the “how,” enabling them to effectively use AI coding assistants for implementation while maintaining critical thinking about model choice and evaluation.\n\n\n\nAll modules include case studies and projects using messy, real-world datasets that require students to make modeling decisions under uncertainty—mirroring actual data science work.\n\n\n\nEarly modules emphasize hands-on coding to build foundational skills. Later modules increasingly allow AI coding assistance, but require detailed justification of modeling choices and interpretation of results.\n\n\n\nThe course culminates in a comprehensive project where students select and justify their approach across multiple modeling paradigms, demonstrating mastery of both technical skills and decision-making frameworks."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "Related Assignments:\n\nModule 4 Homework\nModule 4 Quiz"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#module-resources",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#module-resources",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "Related Assignments:\n\nModule 4 Homework\nModule 4 Quiz"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#introduction",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#introduction",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Introduction",
    "text": "Introduction\nYou’ve spent weeks learning how to build machine learning models—linear regression, logistic regression, random forests, SVMs. You know how to tune hyperparameters, evaluate performance, and diagnose problems. But here’s the thing: all those models need one critical ingredient before they can work their magic.\nFeatures.\nAnd not just any features—numeric or categorical features that capture the information hidden in your data. If you have structured data (age, income, house size), you’re set. But what about unstructured data? What about customer reviews, support tickets, job descriptions, social media posts, news articles? Most real-world data lives in text, and traditional machine learning models can’t directly consume text.\nThis is where Large Language Models (LLMs) come in—not as the end goal, but as powerful feature engineering tools. Think of LLMs as intelligent extractors that can read text, understand context, and pull out structured information. Need to know if a review is positive or negative? LLM. Want to extract job requirements from a posting? LLM. Need to categorize thousands of support tickets? LLM.\nThe beautiful part? You don’t need to train these models. You don’t need GPUs. You don’t even need to understand how they work internally. You just call an API, send some text with instructions, and get back structured data ready for your ML pipeline.\nBut LLMs aren’t free, and they aren’t perfect. Every API call costs money. Extraction quality varies. Some tasks work beautifully with simpler versions of LLMs, such as GPT-4o, Gemini Flash, Claude Haiku, while others need more powerful models like GPT-5, Gemini Pro, or Claude Sonnet. Sometimes a simple regex pattern works better than an expensive LLM call. The skill isn’t just using LLMs—it’s knowing when to use them, which one to use, and how to validate the results.\nThis chapter teaches you to use LLMs as practical data science tools. You’ll learn to write prompts that extract information reliably, parse and validate LLM responses, calculate costs, integrate extracted features into ML pipelines, and most importantly—judge when LLMs add value versus when simpler approaches suffice.\nLet’s jump in."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#the-feature-engineering-challenge-from-text-to-numbers",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#the-feature-engineering-challenge-from-text-to-numbers",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "1. The Feature Engineering Challenge: From Text to Numbers",
    "text": "1. The Feature Engineering Challenge: From Text to Numbers\n\n1.1 Why Traditional ML Needs Structured Features\nRemember our housing price predictor from earlier modules? The input was clean: square footage (numeric), number of bedrooms (numeric), neighborhood (categorical). Easy. Train-test split, fit the model, done.\nBut look at this product review:\n\n“This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it’s a bit loud, but I can live with that for this price.”\n\nWhat features can we extract? What information is hidden here that might help predict if other customers will find this review helpful?\n\nimport pandas as pd\nimport numpy as np\n\n# Example: Product review data\nreview_text = \"This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud, but I can live with that for this price.\"\n\n# Traditional ML needs numbers or categories\n# We could count words, but that misses the meaning\nword_count = len(review_text.split())\nprint(f\"Word count: {word_count}\")\n\n# We could look for specific keywords\nhas_positive_words = any(word in review_text.lower() for word in ['amazing', 'great', 'love'])\nhas_negative_words = any(word in review_text.lower() for word in ['bad', 'terrible', 'hate'])\nprint(f\"Has positive words: {has_positive_words}\")\nprint(f\"Has negative words: {has_negative_words}\")\n\n# But we're missing so much: \n# - sentiment nuance\n# - specific features mentioned\n# - overall tone\n# - ...\n\nWord count: 28\nHas positive words: True\nHas negative words: False\n\n\nWord counts and keyword matching capture some information, but they miss the meaning. This review is mostly positive despite mentioning a downside. A keyword approach might rate it as mixed because it has both positive and negative words. But a human reading it understands: this person likes the product.\n\n\n1.2 What Information Is Hidden in Text?\nText contains structured information waiting to be extracted:\nSentiment/Opinion:\n\nOverall positive/negative/neutral\nStrength of sentiment (mildly positive vs. extremely positive)\nSentiment about specific aspects (loves taste, dislikes noise)\n\nCategories/Classification:\n\nProduct category (coffee maker, not coffee beans)\nIssue type in support tickets (billing vs. technical)\nJob seniority level (entry vs. senior)\n\nEntities and Attributes:\n\nBrands mentioned\nSpecific features discussed (speed, taste, noise)\nRequirements (in job postings: “5 years experience”, “Python required”)\n\nNumeric Values:\n\nImplicit ratings (“amazing” = 5 stars, “okay” = 3 stars)\nQuantities mentioned\nPrice ranges\n\n\n\n1.3 Traditional NLP Approaches\nBefore LLMs, we had a few options:\n1. Keyword/Regex Matching:\nThis approach attempts to search for specific keywords in the text. For example, look at the positive_words and negative_words in the code below.\n\nimport re\n\ndef simple_sentiment(text):\n    \"\"\"Basic sentiment using keyword matching\"\"\"\n    positive_words = ['amazing', 'great', 'excellent', 'love', 'perfect']\n    negative_words = ['bad', 'terrible', 'hate', 'awful', 'waste']\n\n    text_lower = text.lower()\n    pos_count = sum(1 for word in positive_words if word in text_lower)\n    neg_count = sum(1 for word in negative_words if word in text_lower)\n\n    if pos_count &gt; neg_count:\n        return \"positive\"\n    elif neg_count &gt; pos_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\nreview = \"This coffee maker is amazing! Brews quickly and the coffee tastes great.\"\nprint(f\"Simple sentiment: {simple_sentiment(review)}\")\n\nSimple sentiment: positive\n\n\nWhen to use this:\n\nWhen you have specific words you’re looking for, such as students mentioning a specific course\n\nWhen this fails:\n\nThis works for simple cases but breaks easily:\n\n“This product is not bad” → Incorrectly classified as negative\n“I expected amazing quality but got terrible service” → Confusing mix\nDoesn’t handle sarcasm, context, or nuance\n\n\n2. Bag-of-Words + ML: “Bag of words” is a simple way to represent text as a vector of word counts. For example, we might assign:\n\n“apple” = 0\n“banana” = 1\n“orange” = 2\n\nThese number represent the index of each word in our vector. So “apple” is at index 0, “banana” is at index 1, and “orange” is at index 2. Then we might count up how many times each word appears in the text. For example, the text “I have an apple and a banana” would be represented as [1, 1, 0] (“apple” apears once as indicated in index zero, and “banana” appears once, but “orange” never appears).\nThis gives us numeric values, which can then be used in any of the machine learning models you’ve learned so far this semester.\nProblems with this approach include:\n\nNeed a large labeled dataset, which is expensive to create\nNeed to retrain for new domains (i.e. the dataset you have may not cover the domain you’re interested in)\nSeparate model for each extraction task (i.e. you need a separate model for each feature you want to extract, such as sentiment, brand, price, etc.)\n\n3. Other approaches: Traditionally, people used many, many different approaches for feature extraction, such as TF-IDF, word embeddings, and more. These approaches were often ad-hoc and required a lot of expertise to implement.\nOnce LLMs arose, people quickly realized that we can simply ask the LLM to extract what we want. While not always perfect, this approach is highly flexible, tuneable, and easily implemented. Because of that, we’ll focus the majority of our efforts on this technique.\n\n\n1.4 The LLM Advantage: Zero-Shot Extraction\nLLMs offer something revolutionary: zero-shot learning, which means that you can extract information without training on your specific task, without labeled data, just by asking clearly.\nHere’s a preview (we’ll implement this properly soon):\n\nPrompt: “What is the sentiment of this review? Answer with just ‘positive’, ‘negative’, or ‘neutral’: This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it’s a bit loud.”\nLLM Response: “positive”\n\nNo training. No labeled data. Just clear instructions and the text. The LLM understands context, handles negation, grasps nuance. It knows “only downside” indicates a minor criticism in an otherwise positive review.\nThis is powerful. But it’s also expensive, sometimes inconsistent, and not always necessary. In addition, it’s clearly a black box, since we have no idea how the LLM decided this should be a positive review. Of course, we could continually tweak instructions to get outcomes closer to what we want. This is the art and science of working with LLMs."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#your-first-llm-extraction-sentiment-analysis",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#your-first-llm-extraction-sentiment-analysis",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "2. Your First LLM Extraction: Sentiment Analysis",
    "text": "2. Your First LLM Extraction: Sentiment Analysis\n\n2.1 Setting Up API Access\nTo use LLMs, you need API access. The main options:\n\nOpenAI (GPT-5): Most popular, good quality, moderate cost\nAnthropic (Claude): High quality, good for long texts, moderate cost\nGoogle (Gemini): Competitive quality, often cheaper\nOpen-source via Hugging Face: Free but requires more setup\n\nFor this chapter, we’ll use OpenAI’s API since it’s widely available. The concepts transfer to other providers.\n\n\n\n\n\n\nNote\n\n\n\nAbout Running These Examples: The code examples in this chapter that call the OpenAI API will require an API key to run. If you don’t have an API key set, the examples will be skipped during rendering. This is intentional - you can still learn from reading the code and understanding the patterns. When you’re ready to run these examples yourself:\n\nSign up for an OpenAI API account at platform.openai.com\nGenerate an API key\nSet it as an environment variable: export OPENAI_API_KEY=\"your-key-here\"\nBe mindful of costs - start with small tests before processing large datasets\n\n\n\n\n# First, install the OpenAI library if needed\n# pip install openai\n\nfrom openai import OpenAI\nimport os\n\n# Set up your API key\n# IMPORTANT: Never hard-code API keys! Use environment variables\n# Set this in your terminal: export OPENAI_API_KEY=\"your-key-here\"\n\n# Check if API key is set\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    print(\"⚠️  API key not found. Set OPENAI_API_KEY environment variable.\")\n    print(\"⚠️  API calls in this chapter will be skipped.\")\n    client = None\nelse:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    print(\"✓ API key is set\")\n\n⚠️  API key not found. Set OPENAI_API_KEY environment variable.\n⚠️  API calls in this chapter will be skipped.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNever commit API keys to GitHub! Use environment variables or config files that are in .gitignore. Exposed keys can lead to unexpected charges or account suspension.\n\n\n\n\n2.2 Making Your First Extraction Call\nLet’s extract sentiment from a product review:\n\ndef extract_sentiment_simple(review_text):\n    \"\"\"\n    Extract sentiment using GPT-3.5\n    Returns: 'positive', 'negative', or 'neutral'\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return \"neutral\"  # Default response\n\n    # Create the prompt\n    prompt = f\"\"\"What is the sentiment of this review?\nAnswer with just one word: 'positive', 'negative', or 'neutral'.\n\nReview: {review_text}\n\nSentiment:\"\"\"\n\n    # Call the API\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # Cheaper, faster model\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0,  # Deterministic output\n        max_tokens=10   # We only need one word\n    )\n\n    # Extract the response\n    sentiment = response.choices[0].message.content.strip().lower()\n    return sentiment\n\n# Test it (only if API key is available)\nif client is not None:\n    review1 = \"This coffee maker is amazing! Brews quickly and the coffee tastes great. Only downside is it's a bit loud.\"\n    review2 = \"Total waste of money. Broke after two uses. Terrible product.\"\n    review3 = \"It's okay. Does the job but nothing special.\"\n\n    print(f\"Review 1 sentiment: {extract_sentiment_simple(review1)}\")\n    print(f\"Review 2 sentiment: {extract_sentiment_simple(review2)}\")\n    print(f\"Review 3 sentiment: {extract_sentiment_simple(review3)}\")\nelse:\n    print(\"Skipping examples - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Review 1 sentiment: positive\")\n    print(\"Review 2 sentiment: negative\")\n    print(\"Review 3 sentiment: neutral\")\n\nSkipping examples - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nReview 1 sentiment: positive\nReview 2 sentiment: negative\nReview 3 sentiment: neutral\n\n\nWe sent a clear instruction, included the text, and got back exactly what we asked for. No training, no labeled data, no complex preprocessing. Notice how the LLM correctly identifies Review 1 as positive despite it mentioning a downside—it understands that “only downside” indicates a minor complaint in an otherwise positive review.\n\n\n2.3 Understanding the API Call\nLet’s break down the important parameters:\nmodel: Which LLM to use\n\ngpt-3.5-turbo: Cheaper ($0.0005 per 1K tokens), faster, good for most tasks\ngpt-4: More expensive ($0.03 per 1K tokens), smarter, better for complex extraction\nStart with 3.5, upgrade to 4 if quality isn’t sufficient\n\ntemperature: Controls randomness (0 to 2)\n\n0: Deterministic, same input → same output\n0.7-1.0: More creative/varied (for generation tasks)\nFor extraction: use 0 for consistency\n\nmax_tokens: Maximum length of response\n\nTokens ≈ words × 1.3 (rough estimate)\nFor simple extraction: 10-50 tokens\nFor detailed extraction: 100-500 tokens\nMore tokens = higher cost\n\n\n# Let's see token usage and cost\nif client is not None:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n        temperature=0\n    )\n\n    usage = response.usage\n    print(f\"Prompt tokens: {usage.prompt_tokens}\")\n    print(f\"Completion tokens: {usage.completion_tokens}\")\n    print(f\"Total tokens: {usage.total_tokens}\")\n\n    # Calculate cost (GPT-3.5-turbo pricing as of 2024)\n    cost_per_1k_tokens = 0.0005\n    cost = (usage.total_tokens / 1000) * cost_per_1k_tokens\n    print(f\"Cost for this call: ${cost:.6f}\")\nelse:\n    print(\"Skipping - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Prompt tokens: 14\")\n    print(\"Completion tokens: 1\")\n    print(\"Total tokens: 15\")\n    print(\"Cost for this call: $0.000008\")\n\nSkipping - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nPrompt tokens: 14\nCompletion tokens: 1\nTotal tokens: 15\nCost for this call: $0.000008\n\n\nFor our simple sentiment extraction, we’re using about 50-100 tokens per review. At $0.0005 per 1K tokens, that’s $0.00005 per review. To process 10,000 reviews: $0.50. Cheap!\n\n\n2.4 Handling Errors and Edge Cases\nLLMs don’t always follow instructions perfectly. Let’s make our extraction more robust:\n\ndef extract_sentiment_robust(review_text):\n    \"\"\"\n    Robust sentiment extraction with error handling\n    \"\"\"\n    try:\n        prompt = f\"\"\"What is the sentiment of this review?\nAnswer with ONLY one of these words: positive, negative, neutral\n\nReview: {review_text}\n\nSentiment:\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=10\n        )\n\n        sentiment = response.choices[0].message.content.strip().lower()\n\n        # Validate the response\n        valid_sentiments = ['positive', 'negative', 'neutral']\n        if sentiment in valid_sentiments:\n            return sentiment\n        else:\n            # Try to extract valid sentiment from response\n            for valid in valid_sentiments:\n                if valid in sentiment:\n                    return valid\n            # If we still can't find it, return None\n            return None\n\n    except Exception as e:\n        print(f\"Error during extraction: {e}\")\n        return None\n\n# Test with various inputs\nreviews = [\n    \"Love it!\",\n    \"Terrible product.\",\n    \"It's fine.\",\n    \"\"  # Empty review - will this break?\n]\n\nif client is not None:\n    for review in reviews:\n        result = extract_sentiment_robust(review)\n        print(f\"'{review}' → {result}\")\nelse:\n    print(\"Skipping examples - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"'Love it!' → positive\")\n    print(\"'Terrible product.' → negative\")\n    print(\"'It's fine.' → neutral\")\n    print(\"'' → None\")\n\nSkipping examples - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\n'Love it!' → positive\n'Terrible product.' → negative\n'It's fine.' → neutral\n'' → None\n\n\nKey improvements:\n\nTry-except block catches API errors\nValidation checks if response matches expected values\nFallback logic tries to extract valid sentiment if response is wordy\nReturns None if extraction fails (rather than crashing)\n\nNotice how the function handles the empty review gracefully by returning None instead of crashing."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#prompt-engineering-for-reliable-extraction",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#prompt-engineering-for-reliable-extraction",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "3. Prompt Engineering for Reliable Extraction",
    "text": "3. Prompt Engineering for Reliable Extraction\n\n3.1 The Anatomy of a Good Extraction Prompt\nA good prompt has four parts:\n\nClear task description: What do you want extracted?\nOutput format specification: Exactly how should the response look?\nThe input data: The text to analyze\nAny constraints or examples: Helps guide the LLM\n\nLet’s compare bad vs. good prompts:\n\n# BAD PROMPT - Vague, no format specified\nbad_prompt = \"\"\"\nTell me about this review:\n{review_text}\n\"\"\"\n\n# GOOD PROMPT - Clear, specific format\ngood_prompt = \"\"\"\nExtract the following information from this product review:\n- Sentiment: positive, negative, or neutral\n- Rating: estimated star rating from 1-5\n- Main complaint: brief description, or \"none\" if no complaints\n\nFormat your response as JSON:\n{{\"sentiment\": \"positive/negative/neutral\", \"rating\": 1-5, \"complaint\": \"text or none\"}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\nreview = \"Great coffee maker! Makes excellent coffee quickly. Wish it was quieter though. 4 stars.\"\n\n# The good prompt will give us structured, parseable output\n\nSee the difference? The good prompt: - Lists exactly what to extract - Specifies valid values for each field - Requests JSON format for easy parsing - Shows the structure we expect\n\n\n3.2 Requesting Structured Output (JSON)\nJSON is your best friend for extraction. It’s easy to parse and works with any number of fields:\n\nimport json\n\ndef extract_review_features(review_text):\n    \"\"\"\n    Extract multiple features from a review using JSON output\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return None\n\n    prompt = f\"\"\"Extract information from this product review.\n\nRespond with ONLY valid JSON in this exact format:\n{{\n    \"sentiment\": \"positive/negative/neutral\",\n    \"rating\": 1-5,\n    \"pros\": [\"list\", \"of\", \"positive\", \"aspects\"],\n    \"cons\": [\"list\", \"of\", \"negative\", \"aspects\"]\n}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=200\n    )\n\n    # Parse the JSON response\n    try:\n        result_text = response.choices[0].message.content.strip()\n        # Sometimes LLMs add markdown formatting, remove it\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n        result = json.loads(result_text)\n        return result\n    except json.JSONDecodeError as e:\n        print(f\"Failed to parse JSON: {e}\")\n        print(f\"Raw response: {result_text}\")\n        return None\n\n# Test it (only if API key is available)\nif client is not None:\n    review = \"\"\"This coffee maker is fantastic! The coffee tastes amazing and\n    it's very fast. Design is sleek. Only complaint is it's somewhat loud\n    during brewing, but that's minor. Highly recommend!\"\"\"\n\n    features = extract_review_features(review)\n    if features:\n        print(\"Extracted features:\")\n        print(json.dumps(features, indent=2))\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Extracted features:\")\n    print(\"\"\"{\n  \"sentiment\": \"positive\",\n  \"rating\": 4,\n  \"pros\": [\n    \"Coffee tastes amazing\",\n    \"Very fast\",\n    \"Sleek design\"\n  ],\n  \"cons\": [\n    \"Somewhat loud during brewing\"\n  ]\n}\"\"\")\n\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nExtracted features:\n{\n  \"sentiment\": \"positive\",\n  \"rating\": 4,\n  \"pros\": [\n    \"Coffee tastes amazing\",\n    \"Very fast\",\n    \"Sleek design\"\n  ],\n  \"cons\": [\n    \"Somewhat loud during brewing\"\n  ]\n}\n\n\nThis gives us structured data we can immediately put into a DataFrame! Notice how the LLM extracted multiple pieces of information from a single review: overall sentiment, an estimated rating, specific positive aspects, and even the one complaint mentioned.\n\n\n3.3 Few-Shot Learning: Teaching by Example\nSometimes clear instructions aren’t enough. LLMs learn better with examples:\n\ndef extract_with_examples(review_text):\n    \"\"\"\n    Use few-shot learning - provide examples of correct extraction\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return \"neutral\"\n\n    prompt = f\"\"\"Extract sentiment from product reviews.\n\nExamples:\n\nReview: \"Best purchase ever! Love this product.\"\nSentiment: positive\n\nReview: \"Broke after one day. Waste of money.\"\nSentiment: negative\n\nReview: \"It's okay. Does what it says.\"\nSentiment: neutral\n\nNow extract sentiment from this review:\n\nReview: {review_text}\nSentiment:\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=10\n    )\n\n    return response.choices[0].message.content.strip().lower()\n\n# Test with tricky example (only if API key is available)\nif client is not None:\n    tricky_review = \"I wanted to love this, but it's just not good enough.\"\n    sentiment = extract_with_examples(tricky_review)\n    print(f\"Tricky review sentiment: {sentiment}\")\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Tricky review sentiment: negative\")\n\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nTricky review sentiment: negative\n\n\nFew-shot prompting (providing 2-5 examples) helps with: - Ambiguous edge cases - Specific formatting requirements - Consistency across similar inputs - Domain-specific language\n\n\n\n\n\n\nTip\n\n\n\nFew-Shot Prompting Best Practices: - Use 2-5 examples (more doesn’t always help) - Examples should cover different scenarios (positive, negative, neutral) - Keep examples concise - Examples cost tokens—balance quality vs. cost\n\n\n\n\n3.4 Iterating on Prompts: A Real Example\nPrompts rarely work perfectly the first time. Let’s iterate:\n\n# V1: First attempt - too vague\nprompt_v1 = \"What category is this job posting?\"\n\n# V2: More specific\nprompt_v2 = \"\"\"What job category is this posting?\nChoose from: Engineering, Marketing, Sales, Customer Support, Other\"\"\"\n\n# V3: Even more specific with format\nprompt_v3 = \"\"\"Classify this job posting into ONE category.\nValid categories: Engineering, Marketing, Sales, Customer Support, Other\nRespond with ONLY the category name, nothing else.\n\nJob posting: {text}\n\nCategory:\"\"\"\n\n# V4: Add examples for edge cases\nprompt_v4 = \"\"\"Classify this job posting into ONE category.\n\nValid categories: Engineering, Marketing, Sales, Customer Support, Other\n\nExamples:\n\"Senior Python Developer needed\" → Engineering\n\"Social Media Manager wanted\" → Marketing\n\"Account Executive\" → Sales\n\nJob posting: {text}\n\nCategory:\"\"\"\n\n# This iterative process is normal and expected\n# Start simple, add specificity based on failures\n\nThe key is testing your prompts on real data and refining based on errors. We’ll see more on validation in the next section."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#parsing-validating-and-converting-to-dataframes",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#parsing-validating-and-converting-to-dataframes",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "4. Parsing, Validating, and Converting to DataFrames",
    "text": "4. Parsing, Validating, and Converting to DataFrames\n\n4.1 Parsing JSON Responses\nLet’s build a robust system for extracting and parsing:\n\ndef extract_job_features(job_text):\n    \"\"\"\n    Extract structured information from job postings\n    Returns a dictionary or None if extraction fails\n    \"\"\"\n    if client is None:\n        print(\"⚠️  Skipping API call (no API key set)\")\n        return None\n\n    prompt = f\"\"\"Extract these fields from the job posting.\n\nRespond with valid JSON:\n{{\n    \"title\": \"job title\",\n    \"category\": \"Engineering/Marketing/Sales/Support/Other\",\n    \"experience_level\": \"Entry/Mid/Senior/Lead\",\n    \"remote_policy\": \"Remote/Hybrid/Onsite\",\n    \"key_skills\": [\"skill1\", \"skill2\", \"skill3\"]\n}}\n\nJob posting: {job_text}\n\nJSON:\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=300\n        )\n\n        # Get response text\n        result_text = response.choices[0].message.content.strip()\n\n        # Clean up markdown formatting if present\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n\n        # Parse JSON\n        result = json.loads(result_text)\n\n        return result\n\n    except json.JSONDecodeError as e:\n        print(f\"JSON parsing error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"API error: {e}\")\n        return None\n\n# Test it (only if API key is available)\nif client is not None:\n    job_posting = \"\"\"\n    Senior Data Scientist - Remote\n    We're seeking an experienced data scientist with 5+ years experience.\n    Required: Python, SQL, machine learning, deep learning.\n    Fully remote position.\n    \"\"\"\n\n    features = extract_job_features(job_posting)\n    if features:\n        print(\"Extracted job features:\")\n        for key, value in features.items():\n            print(f\"  {key}: {value}\")\nelse:\n    print(\"Skipping example - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Extracted job features:\")\n    print(\"  title: Senior Data Scientist\")\n    print(\"  category: Engineering\")\n    print(\"  experience_level: Senior\")\n    print(\"  remote_policy: Remote\")\n    print(\"  key_skills: ['Python', 'SQL', 'machine learning', 'deep learning']\")\n\nSkipping example - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nExtracted job features:\n  title: Senior Data Scientist\n  category: Engineering\n  experience_level: Senior\n  remote_policy: Remote\n  key_skills: ['Python', 'SQL', 'machine learning', 'deep learning']\n\n\n\n\n4.2 Validating Extractions\nAlways validate LLM outputs—they don’t always follow instructions:\n\ndef validate_job_features(features):\n    \"\"\"\n    Validate that extracted features match expected format\n    Returns True if valid, False otherwise\n    \"\"\"\n    if features is None:\n        return False\n\n    # Define expected fields and valid values\n    expected_fields = ['title', 'category', 'experience_level', 'remote_policy', 'key_skills']\n    valid_categories = ['Engineering', 'Marketing', 'Sales', 'Support', 'Other']\n    valid_experience = ['Entry', 'Mid', 'Senior', 'Lead']\n    valid_remote = ['Remote', 'Hybrid', 'Onsite']\n\n    # Check all required fields present\n    if not all(field in features for field in expected_fields):\n        print(\"Missing required fields\")\n        return False\n\n    # Check category is valid\n    if features['category'] not in valid_categories:\n        print(f\"Invalid category: {features['category']}\")\n        return False\n\n    # Check experience level is valid\n    if features['experience_level'] not in valid_experience:\n        print(f\"Invalid experience level: {features['experience_level']}\")\n        return False\n\n    # Check remote policy is valid\n    if features['remote_policy'] not in valid_remote:\n        print(f\"Invalid remote policy: {features['remote_policy']}\")\n        return False\n\n    # Check key_skills is a list\n    if not isinstance(features['key_skills'], list):\n        print(\"key_skills should be a list\")\n        return False\n\n    return True\n\n# Test validation (only if we have features from previous cell)\nif client is not None:\n    # features was defined in previous cell when client is not None\n    is_valid = validate_job_features(features)\n    print(f\"\\nValidation result: {is_valid}\")\nelse:\n    print(\"Skipping validation - set OPENAI_API_KEY to run\")\n    print(\"\\nExpected output when API key is set:\")\n    print(\"Validation result: True\")\n\nSkipping validation - set OPENAI_API_KEY to run\n\nExpected output when API key is set:\nValidation result: True\n\n\nSee what happened? All the extracted fields are in the expected format: category is one of the valid options, experience level is valid, remote policy is valid, and key_skills is a list. The validation passed!\n\n\n\n\n\n\nWarning\n\n\n\nNever Trust LLM Output Blindly LLMs can: - Return invalid categories - Return wrong data types - Hallucinate information not in the text - Miss fields entirely\nAlways validate before using extracted features in ML models.\n\n\n\n\n4.3 Batch Processing Multiple Texts\nReal datasets have hundreds or thousands of texts. Let’s process them efficiently:\n\ndef extract_batch(texts, extract_func, show_progress=True):\n    \"\"\"\n    Extract features from multiple texts\n\n    Args:\n        texts: List of text strings\n        extract_func: Function that extracts features from one text\n        show_progress: Whether to print progress\n\n    Returns:\n        List of extracted features (same length as texts)\n    \"\"\"\n    results = []\n\n    for i, text in enumerate(texts):\n        if show_progress and (i % 10 == 0):\n            print(f\"Processing {i}/{len(texts)}...\")\n\n        result = extract_func(text)\n        results.append(result)\n\n    return results\n\n# Example: Batch process multiple reviews\nreviews = [\n    \"Amazing product! Best purchase ever.\",\n    \"Terrible quality. Broke immediately.\",\n    \"It's fine. Nothing special.\",\n    \"Great value for money!\",\n    \"Disappointed with this purchase.\"\n]\n\n# Note: This would actually call the API multiple times\n# We'll implement a more efficient version with error handling next\nsentiments = extract_batch(reviews, extract_sentiment_robust, show_progress=True)\n\nprint(\"\\nResults:\")\nfor review, sentiment in zip(reviews, sentiments):\n    print(f\"'{review[:30]}...' → {sentiment}\")\n\nProcessing 0/5...\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\nError during extraction: 'NoneType' object has no attribute 'chat'\n\nResults:\n'Amazing product! Best purchase...' → None\n'Terrible quality. Broke immedi...' → None\n'It's fine. Nothing special....' → None\n'Great value for money!...' → None\n'Disappointed with this purchas...' → None\n\n\n\n\n4.4 Converting to pandas DataFrame\nNow the payoff—converting extracted features to a clean DataFrame ready for ML:\n\nimport pandas as pd\n\n# Simulated extraction results (in practice, these come from API calls)\nextraction_results = [\n    {\n        \"title\": \"Senior Data Scientist\",\n        \"category\": \"Engineering\",\n        \"experience_level\": \"Senior\",\n        \"remote_policy\": \"Remote\",\n        \"key_skills\": [\"Python\", \"SQL\", \"ML\"]\n    },\n    {\n        \"title\": \"Marketing Manager\",\n        \"category\": \"Marketing\",\n        \"experience_level\": \"Mid\",\n        \"remote_policy\": \"Hybrid\",\n        \"key_skills\": [\"SEO\", \"Analytics\", \"Content\"]\n    },\n    {\n        \"title\": \"Sales Representative\",\n        \"category\": \"Sales\",\n        \"experience_level\": \"Entry\",\n        \"remote_policy\": \"Onsite\",\n        \"key_skills\": [\"Communication\", \"CRM\"]\n    }\n]\n\n# Convert to DataFrame\ndf = pd.DataFrame(extraction_results)\n\n# Handle the list field (key_skills)\ndf['num_skills'] = df['key_skills'].apply(len)\ndf['skills_str'] = df['key_skills'].apply(lambda x: ', '.join(x))\n\ndf.head()\n\n\n\n\n\n\n\n\ntitle\ncategory\nexperience_level\nremote_policy\nkey_skills\nnum_skills\nskills_str\n\n\n\n\n0\nSenior Data Scientist\nEngineering\nSenior\nRemote\n[Python, SQL, ML]\n3\nPython, SQL, ML\n\n\n1\nMarketing Manager\nMarketing\nMid\nHybrid\n[SEO, Analytics, Content]\n3\nSEO, Analytics, Content\n\n\n2\nSales Representative\nSales\nEntry\nOnsite\n[Communication, CRM]\n2\nCommunication, CRM\n\n\n\n\n\n\n\nNow we have a clean DataFrame! The categorical variables (category, experience_level, remote_policy) are ready for encoding. The skills are processed. This data is ready to be fed into machine learning models."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#integration-with-ml-pipelines",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#integration-with-ml-pipelines",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "5. Integration with ML Pipelines",
    "text": "5. Integration with ML Pipelines\n\n5.1 The Complete Pipeline: Text → Features → Model\nLet’s build a complete example: extract features from reviews, then predict review helpfulness:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Simulated dataset: product reviews with helpfulness labels\nreviews_data = {\n    'review_text': [\n        \"Amazing product! The quality is outstanding. Highly recommend.\",\n        \"Total waste of money. Broke after one use.\",\n        \"It's okay. Does the job but nothing special.\",\n        \"Best purchase I've made! Love everything about it.\",\n        \"Disappointed. Expected better quality for the price.\",\n        \"Works as described. No complaints.\",\n        \"Fantastic! Exceeded my expectations in every way.\",\n        \"Not worth it. Too expensive for what you get.\",\n        \"Pretty good. Would buy again.\",\n        \"Terrible product. Avoid at all costs.\"\n    ],\n    'helpful_votes': [45, 32, 8, 51, 28, 12, 48, 25, 15, 38]  # Number of helpful votes\n}\n\ndf_reviews = pd.DataFrame(reviews_data)\n\n# Create binary target: helpful (&gt;20 votes) or not helpful (≤20 votes)\ndf_reviews['is_helpful'] = (df_reviews['helpful_votes'] &gt; 20).astype(int)\n\nprint(\"Dataset:\")\nprint(df_reviews[['review_text', 'helpful_votes', 'is_helpful']].head())\n\nDataset:\n                                         review_text  helpful_votes  \\\n0  Amazing product! The quality is outstanding. H...             45   \n1         Total waste of money. Broke after one use.             32   \n2       It's okay. Does the job but nothing special.              8   \n3  Best purchase I've made! Love everything about...             51   \n4  Disappointed. Expected better quality for the ...             28   \n\n   is_helpful  \n0           1  \n1           1  \n2           0  \n3           1  \n4           1  \n\n\nNow let’s extract features using our LLM:\n\ndef extract_review_features_for_ml(review_text):\n    \"\"\"\n    Extract features specifically useful for predicting helpfulness\n    \"\"\"\n    if client is None:\n        # Return default values if no API key\n        return {\n            \"sentiment\": \"neutral\",\n            \"is_detailed\": False,\n            \"mentions_specific_features\": False,\n            \"mentions_price_value\": False,\n            \"has_comparison\": False\n        }\n\n    prompt = f\"\"\"Analyze this product review and extract features.\n\nRespond with valid JSON:\n{{\n    \"sentiment\": \"positive/negative/neutral\",\n    \"is_detailed\": true/false,\n    \"mentions_specific_features\": true/false,\n    \"mentions_price_value\": true/false,\n    \"has_comparison\": true/false\n}}\n\nReview: {review_text}\n\nJSON:\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            max_tokens=150\n        )\n\n        result_text = response.choices[0].message.content.strip()\n        result_text = result_text.replace('```json', '').replace('```', '').strip()\n        features = json.loads(result_text)\n        return features\n    except:\n        # Return default values if extraction fails\n        return {\n            \"sentiment\": \"neutral\",\n            \"is_detailed\": False,\n            \"mentions_specific_features\": False,\n            \"mentions_price_value\": False,\n            \"has_comparison\": False\n        }\n\n# Extract features for all reviews\n# NOTE: In practice, this would make API calls\n# For this example, we'll simulate the results\nllm_features = [\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"neutral\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": True, \"mentions_specific_features\": False,\n     \"mentions_price_value\": True, \"has_comparison\": False},\n    {\"sentiment\": \"neutral\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": True, \"mentions_specific_features\": True,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": True, \"mentions_specific_features\": False,\n     \"mentions_price_value\": True, \"has_comparison\": False},\n    {\"sentiment\": \"positive\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n    {\"sentiment\": \"negative\", \"is_detailed\": False, \"mentions_specific_features\": False,\n     \"mentions_price_value\": False, \"has_comparison\": False},\n]\n\n# Add LLM features to DataFrame\ndf_features = pd.DataFrame(llm_features)\ndf_reviews = pd.concat([df_reviews, df_features], axis=1)\n\ndf_reviews.head()\n\n\n\n\n\n\n\n\nreview_text\nhelpful_votes\nis_helpful\nsentiment\nis_detailed\nmentions_specific_features\nmentions_price_value\nhas_comparison\n\n\n\n\n0\nAmazing product! The quality is outstanding. H...\n45\n1\npositive\nTrue\nTrue\nFalse\nFalse\n\n\n1\nTotal waste of money. Broke after one use.\n32\n1\nnegative\nFalse\nFalse\nFalse\nFalse\n\n\n2\nIt's okay. Does the job but nothing special.\n8\n0\nneutral\nFalse\nFalse\nFalse\nFalse\n\n\n3\nBest purchase I've made! Love everything about...\n51\n1\npositive\nTrue\nTrue\nFalse\nFalse\n\n\n4\nDisappointed. Expected better quality for the ...\n28\n1\nnegative\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n\n5.2 Encoding and Training\nNow we have LLM-extracted features. Let’s train a model:\n\n# Encode sentiment\nle = LabelEncoder()\ndf_reviews['sentiment_encoded'] = le.fit_transform(df_reviews['sentiment'])\n\n# Select features for ML model\nfeature_columns = [\n    'sentiment_encoded',\n    'is_detailed',\n    'mentions_specific_features',\n    'mentions_price_value',\n    'has_comparison'\n]\n\nX = df_reviews[feature_columns]\ny = df_reviews['is_helpful']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Train a classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"\\nModel Performance:\")\nprint(classification_report(y_test, y_pred))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': feature_columns,\n    'importance': clf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n\nModel Performance:\n              precision    recall  f1-score   support\n\n           0       1.00      0.50      0.67         2\n           1       0.50      1.00      0.67         1\n\n    accuracy                           0.67         3\n   macro avg       0.75      0.75      0.67         3\nweighted avg       0.83      0.67      0.67         3\n\n\nFeature Importance:\n                      feature  importance\n0           sentiment_encoded    0.589149\n1                 is_detailed    0.295655\n2  mentions_specific_features    0.090456\n3        mentions_price_value    0.024740\n4              has_comparison    0.000000\n\n\nSee what we did? We used LLM to extract features from text, encoded them properly, and trained a traditional ML model. The LLM extracted semantic information (sentiment, detail level, specific mentions) that would be hard to capture with simple word counts or regex patterns.\n\n\n5.3 Avoiding Data Leakage with LLM Features\nImportant consideration: when do you extract features?\n\n# WRONG WAY - Data leakage!\n# Don't do this: extracting features from full dataset before split\n# The LLM might learn patterns from test set during extraction\n\n# RIGHT WAY - Extract after split\n# But wait... LLMs don't \"learn\" from your prompts in real-time\n# So technically, this isn't data leakage in the traditional sense\n\n# However, best practice:\n# 1. Split your data first\n# 2. Develop/test prompts ONLY on training data\n# 3. Once prompt is finalized, apply to train and test separately\n# 4. Never iterate on prompts while looking at test set results\n\nThe principle: don’t use test set information to develop your extraction prompts, just like you wouldn’t use test set to select model hyperparameters.\n\n\n5.4 Comparing With and Without LLM Features\nLet’s see if LLM features actually help:\n\n# Baseline: Just simple features (no LLM)\ndf_reviews['review_length'] = df_reviews['review_text'].apply(len)\ndf_reviews['word_count'] = df_reviews['review_text'].apply(lambda x: len(x.split()))\ndf_reviews['exclamation_count'] = df_reviews['review_text'].apply(lambda x: x.count('!'))\n\n# Model 1: Without LLM features\nX_baseline = df_reviews[['review_length', 'word_count', 'exclamation_count']]\nX_train_base, X_test_base, y_train, y_test = train_test_split(\n    X_baseline, y, test_size=0.3, random_state=42\n)\n\nclf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_baseline.fit(X_train_base, y_train)\nbaseline_score = clf_baseline.score(X_test_base, y_test)\n\n# Model 2: With LLM features\nX_llm = df_reviews[['review_length', 'word_count', 'exclamation_count'] + feature_columns]\nX_train_llm, X_test_llm, y_train, y_test = train_test_split(\n    X_llm, y, test_size=0.3, random_state=42\n)\n\nclf_llm = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_llm.fit(X_train_llm, y_train)\nllm_score = clf_llm.score(X_test_llm, y_test)\n\nprint(f\"Baseline model (no LLM) accuracy: {baseline_score:.3f}\")\nprint(f\"With LLM features accuracy: {llm_score:.3f}\")\nprint(f\"Improvement: {llm_score - baseline_score:.3f}\")\n\nBaseline model (no LLM) accuracy: 0.000\nWith LLM features accuracy: 0.333\nImprovement: 0.333\n\n\nThis comparison tells you whether the LLM extraction was worth the cost. Sometimes it helps significantly. Sometimes simple features work just as well. Always compare!"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#cost-analysis-and-provider-comparison",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#cost-analysis-and-provider-comparison",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "6. Cost Analysis and Provider Comparison",
    "text": "6. Cost Analysis and Provider Comparison\n\n6.1 Understanding Token-Based Pricing\nLLM APIs charge per token. Understanding costs is crucial:\n\ndef estimate_tokens(text):\n    \"\"\"\n    Rough estimate: 1 token ≈ 0.75 words\n    Or approximately: 1 token ≈ 4 characters\n    \"\"\"\n    # Method 1: Based on words\n    word_estimate = len(text.split()) * 1.3\n\n    # Method 2: Based on characters\n    char_estimate = len(text) / 4\n\n    # Average the two methods\n    return int((word_estimate + char_estimate) / 2)\n\n# Example texts\ntexts = [\n    \"Short review.\",\n    \"This is a medium-length review with several sentences about the product.\",\n    \"\"\"This is a long, detailed review that goes into great depth about\n    various aspects of the product including quality, price, features,\n    customer service, shipping speed, packaging, and overall value for money.\n    I would definitely recommend this to anyone considering a purchase.\"\"\"\n]\n\nfor text in texts:\n    estimated = estimate_tokens(text)\n    print(f\"Text length: {len(text)} chars, ~{estimated} tokens\")\n\nText length: 13 chars, ~2 tokens\nText length: 72 chars, ~16 tokens\nText length: 285 chars, ~62 tokens\n\n\n\n\n6.2 Calculating Extraction Costs\nLet’s calculate real costs for a dataset:\n\ndef calculate_extraction_cost(\n    num_texts,\n    avg_text_length,\n    prompt_tokens,\n    response_tokens,\n    model='gpt-3.5-turbo'\n):\n    \"\"\"\n    Calculate total cost for extracting features from a dataset\n\n    Pricing (as of 2024):\n    - GPT-3.5-turbo: $0.0005 per 1K tokens (input and output)\n    - GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens\n    - Claude Sonnet: $0.003 per 1K input tokens, $0.015 per 1K output tokens\n    \"\"\"\n    # Estimate tokens per extraction\n    text_tokens = avg_text_length / 4  # Rough estimate\n    total_input_tokens = (prompt_tokens + text_tokens) * num_texts\n    total_output_tokens = response_tokens * num_texts\n\n    # Pricing\n    if model == 'gpt-3.5-turbo':\n        cost_per_1k_input = 0.0005\n        cost_per_1k_output = 0.0005\n    elif model == 'gpt-4':\n        cost_per_1k_input = 0.03\n        cost_per_1k_output = 0.06\n    elif model == 'claude-sonnet':\n        cost_per_1k_input = 0.003\n        cost_per_1k_output = 0.015\n    else:\n        raise ValueError(f\"Unknown model: {model}\")\n\n    input_cost = (total_input_tokens / 1000) * cost_per_1k_input\n    output_cost = (total_output_tokens / 1000) * cost_per_1k_output\n    total_cost = input_cost + output_cost\n\n    return {\n        'total_input_tokens': int(total_input_tokens),\n        'total_output_tokens': int(total_output_tokens),\n        'input_cost': input_cost,\n        'output_cost': output_cost,\n        'total_cost': total_cost\n    }\n\n# Example: Extract sentiment from 10,000 product reviews\nnum_reviews = 10000\navg_review_length = 200  # characters\nprompt_tokens = 50  # Our prompt\nresponse_tokens = 5  # Just \"positive\"/\"negative\"/\"neutral\"\n\nprint(\"Cost comparison for 10,000 reviews:\\n\")\n\nfor model in ['gpt-3.5-turbo', 'gpt-4', 'claude-sonnet']:\n    cost_info = calculate_extraction_cost(\n        num_reviews, avg_review_length, prompt_tokens, response_tokens, model\n    )\n    print(f\"{model}:\")\n    print(f\"  Total tokens: {cost_info['total_input_tokens'] + cost_info['total_output_tokens']:,}\")\n    print(f\"  Total cost: ${cost_info['total_cost']:.2f}\\n\")\n\nCost comparison for 10,000 reviews:\n\ngpt-3.5-turbo:\n  Total tokens: 1,050,000\n  Total cost: $0.53\n\ngpt-4:\n  Total tokens: 1,050,000\n  Total cost: $33.00\n\nclaude-sonnet:\n  Total tokens: 1,050,000\n  Total cost: $3.75\n\n\n\nThis shows the dramatic cost difference between models. For simple extraction, GPT-3.5 is often sufficient and much cheaper.\n\n\n6.3 When to Use Which Model\nDecision framework:\n\ndef recommend_model(task_complexity, dataset_size, budget):\n    \"\"\"\n    Recommend which LLM to use based on requirements\n\n    Args:\n        task_complexity: 'simple', 'moderate', 'complex'\n        dataset_size: number of texts to process\n        budget: maximum budget in dollars\n\n    Returns:\n        Recommended model and reasoning\n    \"\"\"\n    # Estimate costs (simplified)\n    gpt35_cost_per_item = 0.0001\n    gpt4_cost_per_item = 0.001\n\n    gpt35_total = gpt35_cost_per_item * dataset_size\n    gpt4_total = gpt4_cost_per_item * dataset_size\n\n    recommendations = []\n\n    if task_complexity == 'simple':\n        recommendations.append({\n            'model': 'gpt-3.5-turbo',\n            'reasoning': 'Simple extraction tasks work well with GPT-3.5',\n            'estimated_cost': gpt35_total\n        })\n\n    elif task_complexity == 'moderate':\n        if gpt35_total &lt;= budget:\n            recommendations.append({\n                'model': 'gpt-3.5-turbo (try first)',\n                'reasoning': 'Start with GPT-3.5, upgrade if quality insufficient',\n                'estimated_cost': gpt35_total\n            })\n        if gpt4_total &lt;= budget:\n            recommendations.append({\n                'model': 'gpt-4 (if GPT-3.5 fails)',\n                'reasoning': 'Better accuracy but 10x cost',\n                'estimated_cost': gpt4_total\n            })\n\n    else:  # complex\n        if gpt4_total &lt;= budget:\n            recommendations.append({\n                'model': 'gpt-4',\n                'reasoning': 'Complex tasks need GPT-4 reasoning',\n                'estimated_cost': gpt4_total\n            })\n        else:\n            recommendations.append({\n                'model': 'Consider alternatives',\n                'reasoning': 'Budget insufficient for GPT-4 at this scale. Consider: sampling, fine-tuning smaller model, or traditional NLP',\n                'estimated_cost': None\n            })\n\n    return recommendations\n\n# Examples\nscenarios = [\n    ('simple', 10000, 10),\n    ('moderate', 5000, 5),\n    ('complex', 1000, 50),\n]\n\nfor complexity, size, budget in scenarios:\n    print(f\"\\nScenario: {complexity} task, {size:,} items, ${budget} budget\")\n    recs = recommend_model(complexity, size, budget)\n    for rec in recs:\n        print(f\"  → {rec['model']}: {rec['reasoning']}\")\n        if rec['estimated_cost']:\n            print(f\"    Estimated cost: ${rec['estimated_cost']:.2f}\")\n\n\nScenario: simple task, 10,000 items, $10 budget\n  → gpt-3.5-turbo: Simple extraction tasks work well with GPT-3.5\n    Estimated cost: $1.00\n\nScenario: moderate task, 5,000 items, $5 budget\n  → gpt-3.5-turbo (try first): Start with GPT-3.5, upgrade if quality insufficient\n    Estimated cost: $0.50\n  → gpt-4 (if GPT-3.5 fails): Better accuracy but 10x cost\n    Estimated cost: $5.00\n\nScenario: complex task, 1,000 items, $50 budget\n  → gpt-4: Complex tasks need GPT-4 reasoning\n    Estimated cost: $1.00\n\n\n\n\n\n\n\n\nTip\n\n\n\nCost Optimization Strategies: 1. Start cheap: Try GPT-3.5 first, upgrade only if needed 2. Sample first: Test on 100 examples before processing 10,000 3. Shorten prompts: Every token counts—be concise 4. Cache results: Don’t reprocess the same text twice 5. Consider batching: Some providers offer batch APIs at lower cost\n\n\n\n\n6.4 ROI Analysis: LLM vs. Alternatives\nIs LLM extraction worth it? Compare alternatives:\n\n# Scenario: Classify 10,000 customer support tickets\n\nalternatives = {\n    'Manual labeling': {\n        'cost': 0.50 * 10000,  # $0.50 per ticket\n        'time_hours': 333,  # 2 mins per ticket\n        'accuracy': 0.95,\n        'scalability': 'Poor'\n    },\n    'Traditional ML (train from scratch)': {\n        'cost': 2000,  # Data labeling for training set\n        'time_hours': 40,  # Development time\n        'accuracy': 0.85,\n        'scalability': 'Excellent (after training)'\n    },\n    'LLM extraction (GPT-3.5)': {\n        'cost': 1.0 * 10000 * 0.0001,  # $1.00\n        'time_hours': 1,  # Just API calls\n        'accuracy': 0.90,\n        'scalability': 'Excellent'\n    },\n    'LLM extraction (GPT-4)': {\n        'cost': 10.0 * 10000 * 0.0001,  # $10.00\n        'time_hours': 1,\n        'accuracy': 0.93,\n        'scalability': 'Excellent'\n    }\n}\n\ncomparison = pd.DataFrame(alternatives).T\ncomparison.head()\n\n\n\n\n\n\n\n\ncost\ntime_hours\naccuracy\nscalability\n\n\n\n\nManual labeling\n5000.0\n333\n0.95\nPoor\n\n\nTraditional ML (train from scratch)\n2000\n40\n0.85\nExcellent (after training)\n\n\nLLM extraction (GPT-3.5)\n1.0\n1\n0.9\nExcellent\n\n\nLLM extraction (GPT-4)\n10.0\n1\n0.93\nExcellent\n\n\n\n\n\n\n\nFor this scenario, LLM extraction with GPT-3.5 is clearly the winner: cheap, fast, accurate enough, and scalable. But the best choice depends on your specific constraints."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#quality-control-and-validation",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#quality-control-and-validation",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "7. Quality Control and Validation",
    "text": "7. Quality Control and Validation\n\n7.1 Measuring Extraction Accuracy\nHow do you know if your LLM extraction is working well?\n\ndef calculate_extraction_accuracy(extracted, ground_truth):\n    \"\"\"\n    Compare LLM extractions to ground truth labels\n\n    Args:\n        extracted: List of LLM-extracted labels\n        ground_truth: List of correct labels\n\n    Returns:\n        Accuracy metrics\n    \"\"\"\n    correct = sum(e == g for e, g in zip(extracted, ground_truth))\n    total = len(ground_truth)\n    accuracy = correct / total\n\n    # Breakdown by category\n    from collections import defaultdict\n    category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n\n    for ext, truth in zip(extracted, ground_truth):\n        category_stats[truth]['total'] += 1\n        if ext == truth:\n            category_stats[truth]['correct'] += 1\n\n    return {\n        'overall_accuracy': accuracy,\n        'correct': correct,\n        'total': total,\n        'by_category': dict(category_stats)\n    }\n\n# Example: Sentiment extraction validation\nground_truth_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative']\nllm_extracted_sentiments = ['positive', 'negative', 'neutral', 'positive', 'negative']\n\naccuracy = calculate_extraction_accuracy(llm_extracted_sentiments, ground_truth_sentiments)\nprint(f\"Overall accuracy: {accuracy['overall_accuracy']:.2%}\")\nprint(f\"Correct: {accuracy['correct']}/{accuracy['total']}\")\n\nOverall accuracy: 100.00%\nCorrect: 5/5\n\n\n\n\n7.2 Spot-Checking and Manual Review\nYou can’t manually check everything, but strategic sampling helps:\n\ndef spot_check_extractions(texts, extractions, n_samples=20, random_state=42):\n    \"\"\"\n    Sample extractions for manual review\n\n    Shows random samples + edge cases for human verification\n    \"\"\"\n    np.random.seed(random_state)\n\n    # Random sample\n    indices = np.random.choice(len(texts), min(n_samples, len(texts)), replace=False)\n\n    print(\"Random sample for manual review:\\n\")\n    for i, idx in enumerate(indices, 1):\n        print(f\"{i}. Text: {texts[idx][:80]}...\")\n        print(f\"   Extraction: {extractions[idx]}\")\n        print(f\"   Correct? (Y/N): ___\")\n        print()\n\n    return indices\n\n# Example usage\nsample_reviews = [\n    \"This product is amazing! Love it.\",\n    \"Terrible. Waste of money.\",\n    \"It's okay, I guess.\",\n    \"Best purchase ever made!\",\n    \"Not great, not terrible.\"\n]\nsample_sentiments = ['positive', 'negative', 'neutral', 'positive', 'neutral']\n\nspot_check_extractions(sample_reviews, sample_sentiments, n_samples=3)\n\nRandom sample for manual review:\n\n1. Text: Terrible. Waste of money....\n   Extraction: negative\n   Correct? (Y/N): ___\n\n2. Text: Not great, not terrible....\n   Extraction: neutral\n   Correct? (Y/N): ___\n\n3. Text: It's okay, I guess....\n   Extraction: neutral\n   Correct? (Y/N): ___\n\n\n\narray([1, 4, 2])\n\n\n\n\n7.3 Identifying Systematic Errors\nLook for patterns in failures:\n\ndef analyze_errors(texts, extracted, ground_truth):\n    \"\"\"\n    Find patterns in extraction errors\n    \"\"\"\n    errors = []\n\n    for text, ext, truth in zip(texts, extracted, ground_truth):\n        if ext != truth:\n            errors.append({\n                'text': text,\n                'extracted': ext,\n                'ground_truth': truth,\n                'text_length': len(text),\n                'word_count': len(text.split())\n            })\n\n    if not errors:\n        print(\"No errors found!\")\n        return\n\n    error_df = pd.DataFrame(errors)\n\n    print(f\"Total errors: {len(errors)}/{len(texts)} ({len(errors)/len(texts):.1%})\\n\")\n\n    # Analyze error patterns\n    print(\"Errors by true category:\")\n    print(error_df['ground_truth'].value_counts())\n\n    print(\"\\nAverage length of texts with errors:\")\n    print(f\"  Characters: {error_df['text_length'].mean():.0f}\")\n    print(f\"  Words: {error_df['word_count'].mean():.0f}\")\n\n    print(\"\\nSample errors:\")\n    for _, row in error_df.head(3).iterrows():\n        print(f\"  Text: {row['text'][:60]}...\")\n        print(f\"  Extracted: {row['extracted']}, True: {row['ground_truth']}\\n\")\n\n    return error_df\n\n# Example with some errors\ntexts_with_errors = [\n    \"Love this product!\",\n    \"It's not bad.\",  # Tricky: double negative\n    \"Terrible quality.\",\n    \"I wouldn't say it's good.\",  # Tricky: negation\n    \"Amazing!\"\n]\nextracted_with_errors = ['positive', 'negative', 'negative', 'negative', 'positive']\nground_truth_with_errors = ['positive', 'positive', 'negative', 'negative', 'positive']\n\nerror_analysis = analyze_errors(texts_with_errors, extracted_with_errors, ground_truth_with_errors)\n\nTotal errors: 1/5 (20.0%)\n\nErrors by true category:\nground_truth\npositive    1\nName: count, dtype: int64\n\nAverage length of texts with errors:\n  Characters: 13\n  Words: 3\n\nSample errors:\n  Text: It's not bad....\n  Extracted: negative, True: positive\n\n\n\n\n\n7.4 When Extraction Quality Is “Good Enough”\nPerfect extraction isn’t always necessary:\n\ndef is_quality_sufficient(accuracy, task_requirements):\n    \"\"\"\n    Determine if extraction quality meets requirements\n\n    Args:\n        accuracy: Measured extraction accuracy (0-1)\n        task_requirements: Dict with requirements\n\n    Returns:\n        Boolean and explanation\n    \"\"\"\n    min_accuracy = task_requirements.get('min_accuracy', 0.85)\n    critical_task = task_requirements.get('critical', False)\n\n    if critical_task:\n        # High-stakes tasks need very high accuracy\n        threshold = max(min_accuracy, 0.95)\n        sufficient = accuracy &gt;= threshold\n        msg = f\"Critical task requires ≥{threshold:.0%} accuracy. \"\n    else:\n        # Normal tasks can tolerate some errors\n        threshold = min_accuracy\n        sufficient = accuracy &gt;= threshold\n        msg = f\"Task requires ≥{threshold:.0%} accuracy. \"\n\n    msg += f\"Current accuracy: {accuracy:.1%}. \"\n    msg += \"✓ Sufficient\" if sufficient else \"✗ Insufficient\"\n\n    return sufficient, msg\n\n# Examples\nscenarios_quality = [\n    (0.92, {'min_accuracy': 0.85, 'critical': False}),  # Good enough\n    (0.92, {'min_accuracy': 0.85, 'critical': True}),   # Not good enough (critical)\n    (0.78, {'min_accuracy': 0.85, 'critical': False}),  # Not good enough\n]\n\nfor acc, reqs in scenarios_quality:\n    sufficient, message = is_quality_sufficient(acc, reqs)\n    print(message)\n    print()\n\nTask requires ≥85% accuracy. Current accuracy: 92.0%. ✓ Sufficient\n\nCritical task requires ≥95% accuracy. Current accuracy: 92.0%. ✗ Insufficient\n\nTask requires ≥85% accuracy. Current accuracy: 78.0%. ✗ Insufficient\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuality Thresholds by Task Type: - Medical/Legal: 95%+ accuracy required (often manual review needed) - Financial: 90-95% accuracy - Marketing/Content: 80-85% accuracy often sufficient - Exploratory analysis: 70-80% may be acceptable\nRemember: LLM features are inputs to ML models. Small extraction errors might not significantly hurt final model performance."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#when-to-use-llms-vs.-traditional-nlp",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#when-to-use-llms-vs.-traditional-nlp",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "8. When to Use LLMs vs. Traditional NLP",
    "text": "8. When to Use LLMs vs. Traditional NLP\n\n8.1 Decision Framework\nNot every text problem needs an LLM. Here’s how to decide:\n\ndef should_use_llm(task_description):\n    \"\"\"\n    Decision tree: LLM vs. traditional NLP\n\n    Returns recommendation and reasoning\n    \"\"\"\n    # Simple pattern matching tasks\n    simple_patterns = [\n        'contains keyword',\n        'find email addresses',\n        'extract phone numbers',\n        'detect URLs',\n        'count words'\n    ]\n\n    # Traditional ML appropriate\n    traditional_ml_tasks = [\n        'you have large labeled dataset',\n        'need very fast inference',\n        'extremely cost-sensitive',\n        'offline/no internet'\n    ]\n\n    # LLM appropriate\n    llm_tasks = [\n        'understand context',\n        'extract categories not seen before',\n        'handle nuance',\n        'multiple languages',\n        'complex reasoning',\n        'no labeled data available'\n    ]\n\n    task_lower = task_description.lower()\n\n    # Check each category\n    if any(pattern in task_lower for pattern in simple_patterns):\n        return {\n            'recommendation': 'Use regex or simple string matching',\n            'reasoning': 'Simple pattern matching - no need for expensive LLM',\n            'example_tool': 're module in Python'\n        }\n\n    if any(indicator in task_lower for indicator in traditional_ml_tasks):\n        return {\n            'recommendation': 'Train traditional ML model',\n            'reasoning': 'Your constraints favor traditional ML over LLM APIs',\n            'example_tool': 'scikit-learn text classification'\n        }\n\n    if any(indicator in task_lower for indicator in llm_tasks):\n        return {\n            'recommendation': 'Use LLM extraction',\n            'reasoning': 'Task requires language understanding that LLMs excel at',\n            'example_tool': 'GPT-3.5 or GPT-4 via API'\n        }\n\n    # Default: try simple first\n    return {\n        'recommendation': 'Try simple methods first, then LLM if needed',\n        'reasoning': 'Always start with simplest solution',\n        'example_tool': 'Regex → Traditional ML → LLM (in that order)'\n    }\n\n# Test with different tasks\ntasks = [\n    \"Extract email addresses from text\",\n    \"Determine sentiment of product reviews with nuanced language\",\n    \"Classify support tickets into categories with 10,000 labeled examples\",\n    \"Extract job requirements that vary significantly across postings\",\n    \"Count how many times 'refund' appears in complaints\"\n]\n\nprint(\"Task recommendations:\\n\")\nfor task in tasks:\n    rec = should_use_llm(task)\n    print(f\"Task: {task}\")\n    print(f\"  → {rec['recommendation']}\")\n    print(f\"  Reasoning: {rec['reasoning']}\")\n    print(f\"  Tool: {rec['example_tool']}\\n\")\n\nTask recommendations:\n\nTask: Extract email addresses from text\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Determine sentiment of product reviews with nuanced language\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Classify support tickets into categories with 10,000 labeled examples\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Extract job requirements that vary significantly across postings\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\nTask: Count how many times 'refund' appears in complaints\n  → Try simple methods first, then LLM if needed\n  Reasoning: Always start with simplest solution\n  Tool: Regex → Traditional ML → LLM (in that order)\n\n\n\n\n\n8.2 Simple Regex vs. LLM: A Comparison\nLet’s see both approaches on the same task:\n\nimport re\n\n# Task: Extract product category from reviews\n\n# Approach 1: Regex/keyword matching\ndef extract_category_regex(review_text):\n    \"\"\"Simple keyword matching\"\"\"\n    text_lower = review_text.lower()\n\n    if any(word in text_lower for word in ['coffee', 'brew', 'espresso', 'caffeine']):\n        return 'Coffee Maker'\n    elif any(word in text_lower for word in ['vacuum', 'clean', 'suction', 'dirt']):\n        return 'Vacuum'\n    elif any(word in text_lower for word in ['headphone', 'audio', 'sound', 'music']):\n        return 'Headphones'\n    else:\n        return 'Other'\n\n# Approach 2: LLM extraction\ndef extract_category_llm(review_text):\n    \"\"\"LLM-based category extraction\"\"\"\n    prompt = f\"\"\"What product category is this review about?\nChoose from: Coffee Maker, Vacuum, Headphones, Other\n\nReview: {review_text}\n\nCategory:\"\"\"\n\n    # Simulated LLM response\n    # In practice, this would call the API\n    # For now, we'll simulate based on content\n    text_lower = review_text.lower()\n    if 'coffee' in text_lower or 'brew' in text_lower:\n        return 'Coffee Maker'\n    elif 'vacuum' in text_lower or 'clean' in text_lower:\n        return 'Vacuum'\n    elif 'headphone' in text_lower or 'sound' in text_lower:\n        return 'Headphones'\n    else:\n        return 'Other'\n\n# Test cases\ntest_reviews = [\n    \"This coffee maker brews excellent coffee.\",  # Simple - both work\n    \"Makes great espresso every morning.\",  # Simple - both work\n    \"The audio quality is amazing!\",  # LLM better (understands audio → headphones)\n    \"Keeps my floors spotless.\",  # LLM better (spotless → cleaning → vacuum)\n    \"Battery life could be better but the noise cancellation is top-notch.\"  # LLM much better\n]\n\nprint(\"Category extraction comparison:\\n\")\nfor review in test_reviews:\n    regex_cat = extract_category_regex(review)\n    llm_cat = extract_category_llm(review)\n    print(f\"Review: {review}\")\n    print(f\"  Regex: {regex_cat}\")\n    print(f\"  LLM: {llm_cat}\")\n    print()\n\nCategory extraction comparison:\n\nReview: This coffee maker brews excellent coffee.\n  Regex: Coffee Maker\n  LLM: Coffee Maker\n\nReview: Makes great espresso every morning.\n  Regex: Coffee Maker\n  LLM: Other\n\nReview: The audio quality is amazing!\n  Regex: Headphones\n  LLM: Other\n\nReview: Keeps my floors spotless.\n  Regex: Other\n  LLM: Other\n\nReview: Battery life could be better but the noise cancellation is top-notch.\n  Regex: Other\n  LLM: Other\n\n\n\nThe LLM shines when: - Keywords aren’t explicit (“noise cancellation” → headphones) - Context matters (“spotless” → cleaning → vacuum) - Synonyms and paraphrasing (“audio quality” = sound)\nRegex wins when: - Keywords are explicit and consistent - Speed is critical - Cost must be zero\n\n\n8.3 Cost-Benefit Comparison Table\n\ncomparison_data = {\n    'Approach': ['Regex/Keywords', 'Traditional ML', 'LLM (GPT-3.5)', 'LLM (GPT-4)'],\n    'Setup Cost': ['$0', '$500-5000', '$0', '$0'],\n    'Per-Item Cost': ['$0', '$0', '$0.0001', '$0.001'],\n    'Setup Time': ['1 hour', '1-4 weeks', '1-2 hours', '1-2 hours'],\n    'Accuracy (simple)': ['60-70%', '85-90%', '85-90%', '90-95%'],\n    'Accuracy (complex)': ['40-50%', '75-85%', '80-90%', '88-95%'],\n    'Scalability': ['Excellent', 'Excellent', 'Good', 'Good'],\n    'Flexibility': ['Poor', 'Poor', 'Excellent', 'Excellent']\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df\n\n\n\n\n\n\n\n\nApproach\nSetup Cost\nPer-Item Cost\nSetup Time\nAccuracy (simple)\nAccuracy (complex)\nScalability\nFlexibility\n\n\n\n\n0\nRegex/Keywords\n$0\n$0\n1 hour\n60-70%\n40-50%\nExcellent\nPoor\n\n\n1\nTraditional ML\n$500-5000\n$0\n1-4 weeks\n85-90%\n75-85%\nExcellent\nPoor\n\n\n2\nLLM (GPT-3.5)\n$0\n$0.0001\n1-2 hours\n85-90%\n80-90%\nGood\nExcellent\n\n\n3\nLLM (GPT-4)\n$0\n$0.001\n1-2 hours\n90-95%\n88-95%\nGood\nExcellent\n\n\n\n\n\n\n\n\n\n8.4 Hybrid Approaches\nOften, the best solution combines methods:\n\ndef hybrid_extraction(review_text):\n    \"\"\"\n    Hybrid approach: Use simple rules when possible, LLM for hard cases\n    \"\"\"\n    # Step 1: Try simple keyword matching\n    simple_result = extract_category_regex(review_text)\n\n    # Step 2: Check confidence\n    # If the review explicitly mentions category keywords, trust regex\n    text_lower = review_text.lower()\n    explicit_mentions = sum([\n        any(word in text_lower for word in ['coffee', 'brew', 'espresso']),\n        any(word in text_lower for word in ['vacuum', 'suction']),\n        any(word in text_lower for word in ['headphone', 'audio'])\n    ])\n\n    if explicit_mentions &gt; 0 and simple_result != 'Other':\n        # High confidence - use regex result\n        return {\n            'category': simple_result,\n            'method': 'regex',\n            'cost': 0\n        }\n    else:\n        # Low confidence - use LLM\n        llm_result = extract_category_llm(review_text)\n        return {\n            'category': llm_result,\n            'method': 'llm',\n            'cost': 0.0001\n        }\n\n# Test hybrid approach\nprint(\"Hybrid approach results:\\n\")\nfor review in test_reviews[:3]:\n    result = hybrid_extraction(review)\n    print(f\"Review: {review}\")\n    print(f\"  Category: {result['category']} (via {result['method']})\")\n    print(f\"  Cost: ${result['cost']:.6f}\\n\")\n\nHybrid approach results:\n\nReview: This coffee maker brews excellent coffee.\n  Category: Coffee Maker (via regex)\n  Cost: $0.000000\n\nReview: Makes great espresso every morning.\n  Category: Coffee Maker (via regex)\n  Cost: $0.000000\n\nReview: The audio quality is amazing!\n  Category: Headphones (via regex)\n  Cost: $0.000000\n\n\n\nThis hybrid approach saves money by using free regex when confidence is high, falling back to LLM only when needed.\n\n\n\n\n\n\nTip\n\n\n\nHybrid Strategy Benefits: - Use regex/keywords for 60-80% of cases (free, fast) - Use LLM for ambiguous 20-40% (accuracy boost where it matters) - Best of both worlds: low cost + high accuracy"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practical-considerations-and-best-practices",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practical-considerations-and-best-practices",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "9. Practical Considerations and Best Practices",
    "text": "9. Practical Considerations and Best Practices\n\n9.1 Rate Limiting and API Quotas\nAPIs have limits. Handle them gracefully:\n\nimport time\nfrom datetime import datetime\n\ndef extract_with_rate_limiting(texts, extract_func, requests_per_minute=60):\n    \"\"\"\n    Extract features with rate limiting to avoid API errors\n\n    Args:\n        texts: List of texts to process\n        extract_func: Function that extracts from one text\n        requests_per_minute: Max API calls per minute\n\n    Returns:\n        List of extracted features\n    \"\"\"\n    results = []\n    delay = 60 / requests_per_minute  # Seconds between requests\n\n    for i, text in enumerate(texts):\n        # Extract\n        result = extract_func(text)\n        results.append(result)\n\n        # Progress update\n        if (i + 1) % 10 == 0:\n            print(f\"Processed {i+1}/{len(texts)} texts...\")\n\n        # Rate limit (except for last item)\n        if i &lt; len(texts) - 1:\n            time.sleep(delay)\n\n    return results\n\n# Example usage (simulated)\nprint(\"Processing with rate limiting:\")\nprint(f\"Rate: 60 requests/minute (1 per second)\")\nprint(f\"For 100 texts, this will take ~100 seconds\\n\")\n\n# In practice:\n# results = extract_with_rate_limiting(my_texts, extract_sentiment_robust, requests_per_minute=60)\n\nProcessing with rate limiting:\nRate: 60 requests/minute (1 per second)\nFor 100 texts, this will take ~100 seconds\n\n\n\n\n\n9.2 Error Handling and Retries\nNetworks fail. APIs timeout. Handle it:\n\ndef extract_with_retry(text, extract_func, max_retries=3, backoff=2):\n    \"\"\"\n    Extract with exponential backoff retry logic\n\n    Args:\n        text: Text to extract from\n        extract_func: Extraction function\n        max_retries: Maximum retry attempts\n        backoff: Backoff multiplier (2 = double wait time each retry)\n    \"\"\"\n    wait_time = 1  # Start with 1 second wait\n\n    for attempt in range(max_retries):\n        try:\n            result = extract_func(text)\n            return result\n        except Exception as e:\n            if attempt == max_retries - 1:\n                # Last attempt failed\n                print(f\"Failed after {max_retries} attempts: {e}\")\n                return None\n            else:\n                # Retry with exponential backoff\n                print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\")\n                time.sleep(wait_time)\n                wait_time *= backoff\n\n    return None\n\n# Example usage\nprint(\"Example retry behavior (simulated):\")\nprint(\"Attempt 1 fails → wait 1s\")\nprint(\"Attempt 2 fails → wait 2s\")\nprint(\"Attempt 3 succeeds → return result\")\n\nExample retry behavior (simulated):\nAttempt 1 fails → wait 1s\nAttempt 2 fails → wait 2s\nAttempt 3 succeeds → return result\n\n\n\n\n9.3 Caching Results to Avoid Reprocessing\nNever process the same text twice:\n\nclass LLMCache:\n    \"\"\"Simple cache for LLM extraction results\"\"\"\n\n    def __init__(self):\n        self.cache = {}\n\n    def get(self, text):\n        \"\"\"Get cached result if available\"\"\"\n        # Use hash of text as key\n        key = hash(text)\n        return self.cache.get(key)\n\n    def set(self, text, result):\n        \"\"\"Store result in cache\"\"\"\n        key = hash(text)\n        self.cache[key] = result\n\n    def extract_with_cache(self, text, extract_func):\n        \"\"\"Extract with caching\"\"\"\n        # Check cache first\n        cached = self.get(text)\n        if cached is not None:\n            return cached\n\n        # Not in cache - extract and store\n        result = extract_func(text)\n        self.set(text, result)\n        return result\n\n# Usage example\ncache = LLMCache()\n\ntexts_with_duplicates = [\n    \"This is great!\",\n    \"This is terrible.\",\n    \"This is great!\",  # Duplicate - will use cache\n    \"This is okay.\",\n    \"This is great!\"   # Duplicate - will use cache\n]\n\nprint(\"Processing with cache:\")\nfor text in texts_with_duplicates:\n    # In practice, would call API\n    cached_result = cache.get(text)\n    if cached_result:\n        print(f\"'{text}' → CACHED\")\n    else:\n        # Simulate extraction\n        result = \"positive\"  # Simulated\n        cache.set(text, result)\n        print(f\"'{text}' → EXTRACTED (API call)\")\n\nProcessing with cache:\n'This is great!' → EXTRACTED (API call)\n'This is terrible.' → EXTRACTED (API call)\n'This is great!' → CACHED\n'This is okay.' → EXTRACTED (API call)\n'This is great!' → CACHED\n\n\n\n\n9.4 Logging and Monitoring\nTrack your extractions for debugging:\n\nimport logging\nfrom datetime import datetime\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\ndef extract_with_logging(text, extract_func):\n    \"\"\"Extract with detailed logging\"\"\"\n    start_time = datetime.now()\n\n    logging.info(f\"Starting extraction for text: {text[:50]}...\")\n\n    try:\n        result = extract_func(text)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        logging.info(f\"Extraction successful in {duration:.2f}s: {result}\")\n\n        return result\n\n    except Exception as e:\n        duration = (datetime.now() - start_time).total_seconds()\n        logging.error(f\"Extraction failed after {duration:.2f}s: {e}\")\n        return None\n\n# Example\nprint(\"Extraction with logging:\")\n# result = extract_with_logging(\"Test review\", extract_sentiment_robust)\n\nExtraction with logging:"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#real-world-example-complete-pipeline",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#real-world-example-complete-pipeline",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "10. Real-World Example: Complete Pipeline",
    "text": "10. Real-World Example: Complete Pipeline\nLet’s put it all together with a complete, realistic example:\n\n# Scenario: Extract features from customer support tickets\n# Use those features to predict ticket priority\n\n# Sample support tickets\ntickets_data = {\n    'ticket_id': [1, 2, 3, 4, 5],\n    'ticket_text': [\n        \"My account is locked and I can't access my billing information. This is urgent!\",\n        \"I have a question about your pricing plans. No rush.\",\n        \"The app keeps crashing every time I try to upload a file. Very frustrating!\",\n        \"Can you explain how the export feature works?\",\n        \"CRITICAL: Production server is down! Need immediate assistance!\"\n    ],\n    'actual_priority': ['high', 'low', 'medium', 'low', 'critical']  # Ground truth\n}\n\ndf_tickets = pd.DataFrame(tickets_data)\n\n# Step 1: Define extraction function\ndef extract_ticket_features(ticket_text):\n    \"\"\"\n    Extract features from support ticket\n    \"\"\"\n    # In real implementation, this would call OpenAI API\n    # For this example, we'll simulate the extraction\n\n    # Simulated LLM extraction logic\n    text_lower = ticket_text.lower()\n\n    # Determine urgency\n    if any(word in text_lower for word in ['critical', 'urgent', 'down', 'broken']):\n        urgency = 'high'\n    elif any(word in text_lower for word in ['frustrating', 'issue', 'problem']):\n        urgency = 'medium'\n    else:\n        urgency = 'low'\n\n    # Determine category\n    if any(word in text_lower for word in ['billing', 'account', 'payment']):\n        category = 'Billing'\n    elif any(word in text_lower for word in ['crash', 'bug', 'error', 'down']):\n        category = 'Technical'\n    else:\n        category = 'General'\n\n    # Sentiment\n    if any(word in text_lower for word in ['critical', 'frustrating', 'broken']):\n        sentiment = 'negative'\n    else:\n        sentiment = 'neutral'\n\n    return {\n        'urgency': urgency,\n        'category': category,\n        'sentiment': sentiment,\n        'has_exclamation': '!' in ticket_text,\n        'text_length': len(ticket_text)\n    }\n\n# Step 2: Extract features for all tickets\nprint(\"Extracting features from tickets...\\n\")\nextracted_features = []\n\nfor text in df_tickets['ticket_text']:\n    features = extract_ticket_features(text)\n    extracted_features.append(features)\n\ndf_features = pd.DataFrame(extracted_features)\n\n# Step 3: Combine with original data\ndf_complete = pd.concat([df_tickets, df_features], axis=1)\n\nprint(\"Extracted features:\")\nprint(df_complete[['ticket_id', 'urgency', 'category', 'sentiment']].head())\n\n# Step 4: Encode for ML\nle_urgency = LabelEncoder()\nle_category = LabelEncoder()\nle_sentiment = LabelEncoder()\n\ndf_complete['urgency_encoded'] = le_urgency.fit_transform(df_complete['urgency'])\ndf_complete['category_encoded'] = le_category.fit_transform(df_complete['category'])\ndf_complete['sentiment_encoded'] = le_sentiment.fit_transform(df_complete['sentiment'])\ndf_complete['has_exclamation_int'] = df_complete['has_exclamation'].astype(int)\n\n# Step 5: Train ML model\nfeature_cols = ['urgency_encoded', 'category_encoded', 'sentiment_encoded',\n                'has_exclamation_int', 'text_length']\n\nX = df_complete[feature_cols]\ny = df_complete['actual_priority']\n\n# Note: In practice, you'd have more data and do train-test split\n# This is just a demonstration\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=3, random_state=42)\nclf.fit(X, y)\n\n# Step 6: Make predictions\ndf_complete['predicted_priority'] = clf.predict(X)\n\n# Step 7: Evaluate\nprint(\"\\n\\nResults:\")\nprint(df_complete[['ticket_id', 'actual_priority', 'predicted_priority']])\n\naccuracy = (df_complete['actual_priority'] == df_complete['predicted_priority']).mean()\nprint(f\"\\nAccuracy: {accuracy:.1%}\")\n\nExtracting features from tickets...\n\nExtracted features:\n   ticket_id urgency   category sentiment\n0          1    high    Billing   neutral\n1          2     low    General   neutral\n2          3  medium  Technical  negative\n3          4     low    General   neutral\n4          5    high  Technical  negative\n\n\nResults:\n   ticket_id actual_priority predicted_priority\n0          1            high               high\n1          2             low                low\n2          3          medium             medium\n3          4             low                low\n4          5        critical           critical\n\nAccuracy: 100.0%\n\n\nThis complete pipeline shows the real workflow: 1. Define extraction prompt/function 2. Extract features from text 3. Validate and convert to DataFrame 4. Encode categorical variables 5. Train ML model on extracted features 6. Evaluate performance\nIn production, you’d add: - Error handling and retries - Rate limiting - Caching - Logging - Cost tracking - Quality monitoring\nBut the core pattern remains the same: text → LLM extraction → features → ML model → predictions."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#summary",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#summary",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Summary",
    "text": "Summary\nLLMs are powerful feature engineering tools that transform unstructured text into structured, ML-ready data. The key advantage is zero-shot learning—you can extract information without training data, just clear prompts. But they’re not magic bullets.\nYou’ve learned to write effective extraction prompts: clear task descriptions, specific output formats (JSON), few-shot examples when needed, and iterative refinement based on errors. You know how to parse and validate LLM responses, handle malformed JSON, check for invalid categories, and identify when extraction fails.\nCost matters. GPT-3.5 costs $0.0005 per 1K tokens; GPT-4 costs 60x more. For simple extraction on large datasets, that difference is huge. Start with GPT-3.5, upgrade only if quality demands it. Calculate costs before processing thousands of texts. Compare LLM cost to alternatives—sometimes manual labeling or traditional ML is cheaper.\nQuality control is critical. Spot-check extractions manually. Calculate accuracy against ground truth. Identify systematic errors and refine prompts. Decide when quality is “good enough” based on task requirements. Perfect extraction isn’t always necessary—remember that LLM features are inputs to ML models, and small errors might not hurt final performance.\nKnow when to use LLMs versus alternatives. Simple keyword matching works for pattern detection. Traditional ML works when you have labeled data and need very fast inference. LLMs excel at context understanding, handling nuanced language, extracting categories without training, and zero-shot learning. Often, hybrid approaches work best: use simple rules when confident, fall back to LLM for hard cases.\nIntegration with ML pipelines is straightforward: extract features, convert to DataFrame, encode categorical variables, train traditional ML models. The LLM doesn’t replace your ML workflow—it enhances it by creating better features from text.\nLLMs are tools, not solutions. They cost money, make mistakes, and sometimes overkill. The skill isn’t just using them—it’s knowing when they add value, which model to choose, how to validate output, and how to integrate them into your data science workflow. Use your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practice-exercises",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#practice-exercises",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nPrompt Engineering: Write three different prompts to extract sentiment from movie reviews. Test them on 10 reviews. Which prompt gives the most consistent results? Why?\nCost Analysis: Calculate the cost to extract categories from 50,000 product descriptions using GPT-3.5 vs. GPT-4. Assume average description length of 150 characters, prompt of 60 tokens, response of 10 tokens.\nValidation: Extract job seniority levels (Entry/Mid/Senior) from 20 job postings. Manually label them yourself. Calculate your LLM’s accuracy. Identify which extractions failed and why.\nComplete Pipeline: Build a pipeline that: (a) extracts sentiment and rating from customer reviews, (b) converts to DataFrame, (c) trains a classifier to predict if review is helpful (&gt;10 votes), (d) evaluates performance.\nComparison: Take a simple classification task (e.g., categorizing emails as work/personal/spam). Implement three approaches: (1) keyword matching, (2) LLM extraction, (3) hybrid. Compare accuracy and cost.\nQuality Control: Extract product categories from 100 product titles. Create a validation function that flags suspicious extractions (e.g., category not in predefined list). How many would need manual review?"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#additional-resources",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/chapter-4-llms-feature-engineering.html#additional-resources",
    "title": "Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOpenAI API Documentation - Complete API reference and guides\nAnthropic Claude API - Alternative LLM provider with good context windows\nPrompt Engineering Guide - Comprehensive guide to writing effective prompts\nLangChain Documentation - Framework for building LLM applications (more advanced)\nOpenAI Cookbook - Practical examples and code snippets\nToken Counting Tool - See how text converts to tokens\nBest Practices for Prompt Engineering - Official OpenAI guide"
  },
  {
    "objectID": "Textbook/Module-2-Regression/images/placeholder-info.html",
    "href": "Textbook/Module-2-Regression/images/placeholder-info.html",
    "title": "Image Placeholders for Module 4 - Regression Models",
    "section": "",
    "text": "Description: Common patterns in residual plots and what they indicate Suggested content: - 2x3 grid showing six residual vs. fitted plots: - Top Left - Good: Random scatter around zero (no pattern) - Title: “Assumptions Met” - Top Middle - Non-linearity: Curved pattern (U-shaped or inverted-U) - Title: “Non-linear Relationship” - Top Right - Heteroscedasticity: Funnel shape (variance increases) - Title: “Non-constant Variance” - Bottom Left - Outliers: Most points near zero, few far away - Title: “Outliers Present” - Bottom Middle - Clusters: Distinct groups of points - Title: “Missing Categorical Variable” - Bottom Right - Systematic Pattern: Clear trend (upward or downward) - Title: “Missing Variable” - Each plot: X-axis = “Fitted Values”, Y-axis = “Residuals” - Horizontal line at y=0 on each plot - Red points for emphasis on problem areas\nDimensions: ~1200x800px\n\n\n\n\nDescription: Visualization showing multicollinearity and its effects Suggested content: - Two side-by-side visualizations: - Left Panel - Correlation Heatmap: - 5x5 correlation matrix - Color-coded (dark blue = -1, white = 0, dark red = +1) - Highlight one pair with correlation ~0.95 - Labels: “Feature 1”, “Feature 2”, “Feature 3”, “Feature 4”, “Feature 5” - Right Panel - Coefficient Instability: - Bar plot showing coefficients for the same model fit on slightly different data - Two sets of bars (Data Sample 1 vs Data Sample 2) - Large differences between the two samples for highly correlated features - Stable coefficients for uncorrelated features - Title: “Unstable Coefficients with Multicollinearity”\nDimensions: ~1000x500px\n\n\n\n\nDescription: Flowchart showing complete regression analysis workflow Suggested content: - Flowchart with boxes and arrows: 1. Start: “Fit Linear Regression” 2. Check: “Create Diagnostic Plots” (residuals, Q-Q, etc.) 3. Decision Diamond: “Assumptions Met?” - Yes arrow → “Validate Performance” → “Report Results” - No arrow → “Identify Problem” 4. Problem boxes: - “Non-linear?” → “Add Polynomial Features” - “Multicollinearity?” → “Use Ridge/Lasso” - “Outliers?” → “Investigate/Transform” 5. All problem solutions loop back to “Fit Model” (step 1) - Use different colors for different stages (blue=fit, orange=diagnose, green=fix) - Arrows showing the iterative nature\nDimensions: ~800x1000px\n\n\n\n\nDescription: Visual comparison of Ridge, Lasso, and Linear Regression coefficients Suggested content: - Three bar plots side-by-side: - Left: Linear Regression coefficients (some very large positive/negative) - Middle: Ridge coefficients (all shrunk toward zero, none exactly zero) - Right: Lasso coefficients (some shrunk to exactly zero) - Same features on x-axis for all three - Y-axis: coefficient value - Different colors for each bar - Clear labels showing which features Lasso set to zero - Title above each: “Linear Regression”, “Ridge (α=1)”, “Lasso (α=1)”\nDimensions: ~1200x400px\n\n\n\n\nDescription: Regularization path showing how coefficients change with alpha Suggested content: - Line plot with: - X-axis: log(alpha) from small (0.001) to large (1000) - Y-axis: coefficient value - Multiple lines, one for each feature (different colors) - Lines shrinking toward zero as alpha increases (Ridge) or dropping to exactly zero at different alphas (Lasso) - Legend identifying each feature - Vertical dashed line showing optimal alpha selected by cross-validation - Title: “Lasso Regularization Path”\nDimensions: ~800x600px\n\n\n\n\n\n\nImages should be placed in this /images/ folder\nUse .png format for diagrams and screenshots\nUse descriptive filenames\nEnsure images are readable when embedded in Quarto HTML output\nFor diagnostic plots, use realistic-looking synthetic data\nColor code consistently (e.g., red for problems, green for good)\nInclude clear labels and titles on all plots"
  },
  {
    "objectID": "Textbook/Module-2-Regression/images/placeholder-info.html#required-images",
    "href": "Textbook/Module-2-Regression/images/placeholder-info.html#required-images",
    "title": "Image Placeholders for Module 4 - Regression Models",
    "section": "",
    "text": "Description: Common patterns in residual plots and what they indicate Suggested content: - 2x3 grid showing six residual vs. fitted plots: - Top Left - Good: Random scatter around zero (no pattern) - Title: “Assumptions Met” - Top Middle - Non-linearity: Curved pattern (U-shaped or inverted-U) - Title: “Non-linear Relationship” - Top Right - Heteroscedasticity: Funnel shape (variance increases) - Title: “Non-constant Variance” - Bottom Left - Outliers: Most points near zero, few far away - Title: “Outliers Present” - Bottom Middle - Clusters: Distinct groups of points - Title: “Missing Categorical Variable” - Bottom Right - Systematic Pattern: Clear trend (upward or downward) - Title: “Missing Variable” - Each plot: X-axis = “Fitted Values”, Y-axis = “Residuals” - Horizontal line at y=0 on each plot - Red points for emphasis on problem areas\nDimensions: ~1200x800px\n\n\n\n\nDescription: Visualization showing multicollinearity and its effects Suggested content: - Two side-by-side visualizations: - Left Panel - Correlation Heatmap: - 5x5 correlation matrix - Color-coded (dark blue = -1, white = 0, dark red = +1) - Highlight one pair with correlation ~0.95 - Labels: “Feature 1”, “Feature 2”, “Feature 3”, “Feature 4”, “Feature 5” - Right Panel - Coefficient Instability: - Bar plot showing coefficients for the same model fit on slightly different data - Two sets of bars (Data Sample 1 vs Data Sample 2) - Large differences between the two samples for highly correlated features - Stable coefficients for uncorrelated features - Title: “Unstable Coefficients with Multicollinearity”\nDimensions: ~1000x500px\n\n\n\n\nDescription: Flowchart showing complete regression analysis workflow Suggested content: - Flowchart with boxes and arrows: 1. Start: “Fit Linear Regression” 2. Check: “Create Diagnostic Plots” (residuals, Q-Q, etc.) 3. Decision Diamond: “Assumptions Met?” - Yes arrow → “Validate Performance” → “Report Results” - No arrow → “Identify Problem” 4. Problem boxes: - “Non-linear?” → “Add Polynomial Features” - “Multicollinearity?” → “Use Ridge/Lasso” - “Outliers?” → “Investigate/Transform” 5. All problem solutions loop back to “Fit Model” (step 1) - Use different colors for different stages (blue=fit, orange=diagnose, green=fix) - Arrows showing the iterative nature\nDimensions: ~800x1000px\n\n\n\n\nDescription: Visual comparison of Ridge, Lasso, and Linear Regression coefficients Suggested content: - Three bar plots side-by-side: - Left: Linear Regression coefficients (some very large positive/negative) - Middle: Ridge coefficients (all shrunk toward zero, none exactly zero) - Right: Lasso coefficients (some shrunk to exactly zero) - Same features on x-axis for all three - Y-axis: coefficient value - Different colors for each bar - Clear labels showing which features Lasso set to zero - Title above each: “Linear Regression”, “Ridge (α=1)”, “Lasso (α=1)”\nDimensions: ~1200x400px\n\n\n\n\nDescription: Regularization path showing how coefficients change with alpha Suggested content: - Line plot with: - X-axis: log(alpha) from small (0.001) to large (1000) - Y-axis: coefficient value - Multiple lines, one for each feature (different colors) - Lines shrinking toward zero as alpha increases (Ridge) or dropping to exactly zero at different alphas (Lasso) - Legend identifying each feature - Vertical dashed line showing optimal alpha selected by cross-validation - Title: “Lasso Regularization Path”\nDimensions: ~800x600px"
  },
  {
    "objectID": "Textbook/Module-2-Regression/images/placeholder-info.html#notes",
    "href": "Textbook/Module-2-Regression/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Module 4 - Regression Models",
    "section": "",
    "text": "Images should be placed in this /images/ folder\nUse .png format for diagrams and screenshots\nUse descriptive filenames\nEnsure images are readable when embedded in Quarto HTML output\nFor diagnostic plots, use realistic-looking synthetic data\nColor code consistently (e.g., red for problems, green for good)\nInclude clear labels and titles on all plots"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html",
    "title": "Chapter 3: Classification Models",
    "section": "",
    "text": "Related Assignments:\n\nModule 3 Homework\nModule 3 Quiz"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#module-resources",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#module-resources",
    "title": "Chapter 3: Classification Models",
    "section": "",
    "text": "Related Assignments:\n\nModule 3 Homework\nModule 3 Quiz"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#introduction",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#introduction",
    "title": "Chapter 3: Classification Models",
    "section": "Introduction",
    "text": "Introduction\nYou’ve spent weeks learning regression—predicting continuous values like house prices or income. But what happens when you need to predict categories instead? Will this customer churn or stay? Is this email spam or legitimate? Does this patient have the disease or not?\nThis is classification, and it’s everywhere. Classification powers the spam filter in your email, the fraud detection on your credit card, the recommendation systems suggesting what you should watch next, and the medical diagnostics helping doctors identify diseases. If you’ve ever wondered “will this happen or not?” or “which category does this belong to?”—that’s a classification problem.\nJust like with regression, there isn’t one “best” classification algorithm. Logistic regression is fast and interpretable. Decision trees are easy to explain to non-technical stakeholders. Random forests are robust and powerful. Support vector machines handle high-dimensional data elegantly. k-Nearest neighbors is beautifully simple but computationally expensive. Each has strengths, weaknesses, and situations where it shines.\nAnd the evaluation is completely different from regression. You can’t use R² or MSE. Instead, you’ll need to navigate confusion matrices, ROC curves, precision-recall tradeoffs, and decide whether false positives or false negatives are more costly for your specific problem. A model that’s 99% accurate might be completely useless if you’re trying to detect a rare disease.\nThis chapter will teach you not just how to fit classification models, but how to think like a data scientist choosing between them. You’ll learn:\n\nHow five major classification algorithms work and when to use each\nHow to interpret confusion matrices and choose the right metrics\nThe precision-recall tradeoff and why it matters\nHow to handle imbalanced datasets (the most common real-world scenario)\nHow to visualize decision boundaries to understand what your model is actually doing\nHow to use ROC curves to compare model performance\n\nLet’s jump in."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#machine-learning-paradigms-supervised-vs-unsupervised-learning",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#machine-learning-paradigms-supervised-vs-unsupervised-learning",
    "title": "Chapter 3: Classification Models",
    "section": "1. Machine Learning Paradigms: Supervised vs Unsupervised Learning",
    "text": "1. Machine Learning Paradigms: Supervised vs Unsupervised Learning\nBefore we dive into specific classification algorithms, let’s step back and understand a fundamental distinction in machine learning: supervised versus unsupervised learning. Understanding this paradigm will help you recognize when classification is the right approach and when other techniques might be more appropriate.\n\n1.1 Supervised Learning: Learning from Labels\nSupervised learning is what we’ve been doing throughout this course: you give the model input data (features) and the correct answers (labels/targets), and it learns to predict the right answer for new inputs.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Load Spotify data (supervised classification example)\nspotify_df = pd.read_csv('../data/spotify.csv')\n\n# We have features (X) and a target (y)\n# Let's predict whether a song will be popular (binary classification)\nX = spotify_df[['danceability', 'energy', 'valence', 'tempo']]\ny = (spotify_df['popularity'] &gt; 50).astype(int)  # 1 if popular, 0 if not\n\nprint(\"Supervised learning setup:\")\nprint(f\"Features (X) shape: {X.shape}\")\nprint(f\"Target (y) shape: {y.shape}\")\nprint(f\"Class distribution: {y.value_counts().to_dict()}\")\nprint(\"\\nFirst few rows of features:\")\n\nX.head()\n\nSupervised learning setup:\nFeatures (X) shape: (42663, 4)\nTarget (y) shape: (42663,)\nClass distribution: {0: 33578, 1: 9085}\n\nFirst few rows of features:\n\n\n\n\n\n\n\n\n\ndanceability\nenergy\nvalence\ntempo\n\n\n\n\n0\n0.792\n0.7310\n0.8380\n113.007\n\n\n1\n0.418\n0.3280\n0.6800\n164.315\n\n\n2\n0.199\n0.0957\n0.0391\n77.722\n\n\n3\n0.862\n0.5210\n0.3730\n154.983\n\n\n4\n0.168\n0.1960\n0.0554\n83.898\n\n\n\n\n\n\n\n\nprint(\"\\nCorresponding targets (1 = popular, 0 = not popular):\")\ny.head()\n\n\nCorresponding targets (1 = popular, 0 = not popular):\n\n\n0    0\n1    0\n2    0\n3    1\n4    0\nName: popularity, dtype: int64\n\n\nThe key to supervised learning is that we have labels. We know what the correct answer is for each training example. The model learns by comparing its predictions to the true labels and adjusting to get closer.\n\n\n\n\n\n\nNote\n\n\n\nBoth regression (from Chapter 2) and classification are supervised learning tasks. The difference is that regression predicts continuous values while classification predicts discrete categories. But both require labeled training data.\n\n\n\n\n1.2 Unsupervised Learning: Finding Patterns Without Labels\nUnsupervised learning is different: you only have input data (X), no labels (y). The model’s job is to find patterns, structure, or groupings in the data on its own.\nFor example, we can use unsupervised learning to find natural groupings of flowers based on their measurements. We don’t tell the model what the groupings should be—it discovers them on its own. This is what clustering algorithms like KMeans do.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Using Iris dataset, but ignoring the labels (species)\n# Can we find natural groupings of flowers based on their measurements?\niris = load_iris(as_frame=True)\niris_df = iris.frame\n\nX_unlabeled = iris_df[['petal length (cm)', 'petal width (cm)']]\n\n# K-Means clustering: find groups in the data\nkmeans = KMeans(n_clusters=3, random_state=1, n_init=10)\nclusters = kmeans.fit_predict(X_unlabeled)\n\n# Visualize the clusters\nplt.figure(figsize=(10, 6))\nplt.scatter(X_unlabeled['petal length (cm)'], X_unlabeled['petal width (cm)'],\n            c=clusters, cmap='viridis', alpha=0.6)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c='red', marker='X', s=200, edgecolors='black', label='Centroids')\nplt.xlabel('Petal Length (cm)')\nplt.ylabel('Petal Width (cm)')\nplt.title('K-Means Clustering: Discovering Flower Groups (Unsupervised)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice what happened: we didn’t tell the model which flowers were which species. We just said “find 3 groups” and it discovered natural clusters based on petal measurements alone.\nCommon unsupervised learning tasks include:\n\nClustering: Grouping similar items together (customer segmentation, document clustering)\nDimensionality Reduction: Reducing many features to a few key ones (PCA, t-SNE for visualization)\nAnomaly Detection: Finding unusual examples (fraud detection, manufacturing defects)\n\n\n\n1.3 When to Use Each Paradigm\nUse supervised learning when:\n\nYou have labeled data (you know the correct answers)\nYou want to predict something specific\nYou can define success (accuracy, error rate, etc.)\n\nUse unsupervised learning when:\n\nYou don’t have labels (or getting labels is too expensive)\nYou want to explore data structure\nYou’re looking for patterns you don’t know exist yet\n\n\n\n\n\n\n\nNote\n\n\n\nMost real-world applications use supervised learning, because prediction is usually the goal. Unsupervised learning is powerful for exploration and preprocessing, but harder to evaluate (how do you know if clusters are “good”?). Unsupervised learning is often used as an intermediate step to generate new features, which are then used in a supervised learning model.\n\n\nFor the rest of this chapter, we’ll focus on supervised classification, since that’s where you’ll spend most of your time as a data scientist."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#classification-vs-regression-whats-different",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#classification-vs-regression-whats-different",
    "title": "Chapter 3: Classification Models",
    "section": "2. Classification vs Regression: What’s Different?",
    "text": "2. Classification vs Regression: What’s Different?\n\n2.1 The Fundamental Difference\nIn regression, we predict a continuous value. In classification, we predict a category. Seems simple, but this fundamental difference changes everything about how we build, train, and evaluate models.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Let's load a classification dataset - Titanic survival\n# This is a classic binary classification problem: survived or didn't survive\ntitanic = pd.read_csv('../data/titanic.csv')\n\n# Look at the target variable\nprint(\"Target variable (Survived) value counts:\")\nprint(titanic['Survived'].value_counts())\nprint(f\"\\nProportion survived: {titanic['Survived'].mean():.2%}\")\n\nTarget variable (Survived) value counts:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nProportion survived: 38.38%\n\n\nInstead of predicting a number on a continuous scale, we’re predicting one of two discrete outcomes: 0 (didn’t survive) or 1 (survived). This is binary classification—the most common type.\n\n\n1.2 Types of Classification Problems\nBinary Classification: Two possible outcomes (yes/no, spam/ham, fraud/legitimate)\n\nTitanic survival\nEmail spam detection\nLoan default prediction\nDisease diagnosis\n\nMulti-class Classification: More than two categories\n\nIris flower species (setosa, versicolor, virginica)\nHandwritten digit recognition (0-9)\nCustomer segment classification\nImage classification (cat, dog, bird, etc.)\n\nMost of this chapter focuses on binary classification since it’s simpler to visualize and understand. But the techniques extend naturally to multi-class problems.\n\n\n1.3 Why We Can’t Use Linear Regression\nYou might be tempted to use linear regression for classification. Just predict the category as a number, right? Let’s see why that breaks:\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nimport seaborn as sns\n\n# Prepare simple features\ntitanic_clean = titanic[['Age', 'Survived', 'Pclass', 'Fare']].dropna()\nX = titanic_clean[['Age', 'Pclass', 'Fare']].values\ny = titanic_clean['Survived'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Try linear regression (WRONG!)\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\nlinear_pred = linear_model.predict(X_test)\n\nprint('Predictions:')\nprint(linear_pred[:5])\n\nprint('Classes:')\nprint(y_test[:5])\n\nPredictions:\n[0.33588326 0.70372944 0.4655869  0.77416664 0.0881573 ]\nClasses:\n[0 1 1 1 0]\n\n\nSee the problem? Linear regression gives us predictions like 0.73 or -0.15 or 1.42. But we need 0 or 1! We could threshold at 0.5, but linear regression makes no guarantee that predictions will be between 0 and 1. It’s using the wrong tool for the job.\nClassification models are designed to output probabilities (values between 0 and 1) or direct class predictions. That’s why we need specialized algorithms."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#logistic-regression-the-foundation",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#logistic-regression-the-foundation",
    "title": "Chapter 3: Classification Models",
    "section": "3. Logistic Regression: The Foundation",
    "text": "3. Logistic Regression: The Foundation\n\n2.1 From Linear to Logistic\nLogistic regression might sound like a regression technique, but don’t be fooled—it’s pure classification. The name comes from its history: it takes linear regression and transforms it to work for classification.\nHere’s the key insight: instead of predicting the outcome directly, logistic regression predicts the probability of the positive class. It does this by taking a linear combination of features (just like linear regression) and passing it through the sigmoid function:\n\\[\n\\text{probability} = \\frac{1}{1 + e^{-z}}\n\\]\nwhere \\(z\\) is the linear combination: \\(z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\)\nThe sigmoid function has a beautiful property: it squashes any real number into the range (0, 1), making it perfect for probabilities.\n\n# Visualize the sigmoid function\nz = np.linspace(-10, 10, 100)\nsigmoid = 1 / (1 + np.exp(-z))\n\nplt.figure(figsize=(10, 6))\nplt.plot(z, sigmoid, 'b-', linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Decision threshold (0.5)')\nplt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\nplt.xlabel('z (linear combination of features)', fontsize=12)\nplt.ylabel('Probability of positive class', fontsize=12)\nplt.title('The Sigmoid Function: Turning Linear into Probability', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWhen \\(z = 0\\), the probability is exactly 0.5. As \\(z\\) increases, the probability approaches 1. As \\(z\\) decreases, the probability approaches 0. The sigmoid smoothly transitions between these extremes.\n\n\n2.2 Fitting Logistic Regression\nLet’s fit a logistic regression model to predict Titanic survival:\n\n# Fit logistic regression\nlog_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_model.fit(X_train, y_train)\n\n# Get predictions - both probabilities and classes\ny_pred_proba = log_model.predict_proba(X_test)[:, 1]  # Probability of class 1\ny_pred_class = log_model.predict(X_test)  # Predicted class (0 or 1)\n\n# Show the difference\ncomparison = pd.DataFrame({\n    'True': y_test,\n    'Prob_Survived': y_pred_proba,\n    'Predicted': y_pred_class\n})\n\ncomparison.head(10)\n\n\n\n\n\n\n\n\nTrue\nProb_Survived\nPredicted\n\n\n\n\n0\n0\n0.302603\n0\n\n\n1\n1\n0.741984\n1\n\n\n2\n1\n0.459317\n0\n\n\n3\n1\n0.791706\n1\n\n\n4\n0\n0.118771\n0\n\n\n5\n1\n0.707625\n1\n\n\n6\n1\n0.183988\n0\n\n\n7\n1\n0.202158\n0\n\n\n8\n0\n0.217621\n0\n\n\n9\n0\n0.261196\n0\n\n\n\n\n\n\n\nNotice the two types of predictions:\n\nProbabilities (from predict_proba): Values between 0 and 1 representing confidence\nClasses (from predict): Hard 0/1 decisions using a threshold (default 0.5)\n\nIf the probability is above 0.5, we predict class 1 (survived). Otherwise, class 0 (didn’t survive). But you can adjust this threshold based on your problem—more on that later.\nHow can we evaluate this model’s performance? One simple way is to ask about the average probability for each different true class (i.e. average probabibility of survival for those who actually survived vs. those who didn’t).\n\n# Calculate average probability for each true class\navg_prob_survived = y_pred_proba[y_test == 1].mean()\navg_prob_not_survived = y_pred_proba[y_test == 0].mean()\n\nprint(f\"Average probability for those who survived: {avg_prob_survived:.3f}\")\nprint(f\"Average probability for those who didn't survive: {avg_prob_not_survived:.3f}\")\n\nAverage probability for those who survived: 0.505\nAverage probability for those who didn't survive: 0.339\n\n\nThis is good, but it fails to capture the variability in predictions. A better approach would be to look at the distribution of probabilities for each class.\nLet’s visualize this:\n\nimport seaborn as sns\nimport pandas as pd\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=pd.DataFrame({'prob': y_pred_proba[y_test == 1], 'class': 'Survived'}), x='prob', alpha=0.7, label='Survived', bins=20, color='blue', stat='density')\nsns.histplot(data=pd.DataFrame({'prob': y_pred_proba[y_test == 0], 'class': 'Did not survive'}), x='prob', alpha=0.7, label='Did not survive', bins=20, color='red', stat='density')\nplt.xlabel('Predicted Probability of Survival')\nplt.ylabel('Density')\nplt.title('Distribution of Predicted Probabilities by True Outcome')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe see that there are more people who survived with higher predicted probability of survival, and more people who died with lower predicted probability. This shows that our model is somewhat calibrated - it tends to give higher probabilities to those who actually survived and lower probabilities to those who didn’t.\nWe’ll learn more advanced techniques later, but this is a good starting point.\n\n\n3.3 Interpreting Coefficients\nJust like linear regression, logistic regression has coefficients. But the interpretation is different. In particular, we’re primarily concerned with the odds ratio of a coefficient, which is calculated as \\(e^{\\beta}\\) where \\(\\beta\\) is the coefficient. This tells us how the odds of the outcome change for a one-unit increase in the predictor.\n\n# Let's use more features to make interpretation interesting\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ntitanic_features = titanic[features + ['Survived']].dropna()\n\nX_full = titanic_features[features].values\ny_full = titanic_features['Survived'].values\n\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n    X_full, y_full, test_size=0.2, random_state=42\n)\n\n# Fit the model\nlog_model_full = LogisticRegression(random_state=42, max_iter=1000)\nlog_model_full.fit(X_train_full, y_train_full)\n\n# Display coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': log_model_full.coef_[0],\n    'Odds_Ratio': np.exp(log_model_full.coef_[0])\n})\n\ncoef_df\n\n\n\n\n\n\n\n\nFeature\nCoefficient\nOdds_Ratio\n\n\n\n\n0\nPclass\n-1.129795\n0.323100\n\n\n1\nAge\n-0.051874\n0.949448\n\n\n2\nSibSp\n-0.295579\n0.744101\n\n\n3\nParch\n0.241379\n1.273004\n\n\n4\nFare\n0.003802\n1.003810\n\n\n\n\n\n\n\nOdds ratio measures how much the odds of the outcome change for a one-unit increase in the predictor. An odds ratio greater than 1 indicates a positive association with the outcome, while an odds ratio less than 1 indicates a negative association.\nFor example, if Fare has an odds ratio of 1.5, it means that for every one-unit increase in fare, the odds of survival increase by 50%. If Pclass has an odds ratio of 0.7, it means that for every one-unit increase in class (higher class number), the odds of survival decrease by 30%. Here we see that:\n\nIncreasing passenger class by 1 (e.g. 3rd class -&gt; 2nd class) decreased the predicted probability of survival by about 68% \\((1-0.323 = 0.677)\\).\nIncreasing the age by 1 year slightly decreased (~5%) the predicted probability of survival.\nIncreasing the number of parents/children in the family (Parch) increased the predicted probability of survival by a whopping 27%!\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful about collinearity! Remember that last chapter we discussed how collinearity can lead to non-interpretable coefficients. Since logistic regression is simply performing linear regression under the hood, all those same caveats apply here. For example, it seems weird to say that people in higher passenger classes are less likely to survive. What’s probably going on is collinearity between Pclass and Fare, since both are essentially measuring the same thing (how much the ticket cost)."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#the-confusion-matrix-understanding-errors",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#the-confusion-matrix-understanding-errors",
    "title": "Chapter 3: Classification Models",
    "section": "4. The Confusion Matrix: Understanding Errors",
    "text": "4. The Confusion Matrix: Understanding Errors\n\n3.1 What Is a Confusion Matrix?\nWhen you make predictions, you’ll make mistakes. The confusion matrix breaks down exactly what kinds of mistakes you’re making. It’s a 2×2 table for binary classification:\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Get predictions\ny_pred = log_model_full.predict(X_test_full)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test_full, y_pred)\n\n# Visualize it\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Not Survived', 'Survived'],\n            yticklabels=['Not Survived', 'Survived'])\nplt.title('Confusion Matrix', fontsize=14)\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.show()\n\nprint(f\"\\nTrue Negatives (TN): {cm[0, 0]} people died and we predicted they would die\")\nprint(f\"False Positives (FP): {cm[0, 1]} people survived but we predicted they would die\")\nprint(f\"False Negatives (FN): {cm[1, 0]} people died but we predicted they would survive\")\nprint(f\"True Positives (TP): {cm[1, 1]} people survived and we predicted they would survive\")\n\n\n\n\n\n\n\n\n\nTrue Negatives (TN): 74 people died and we predicted they would die\nFalse Positives (FP): 13 people survived but we predicted they would die\nFalse Negatives (FN): 30 people died but we predicted they would survive\nTrue Positives (TP): 26 people survived and we predicted they would survive\n\n\nThe four quadrants:\n\nTrue Negatives (TN): Correctly predicted “didn’t survive”\nFalse Positives (FP): Predicted “survived” but actually didn’t (Type I error)\nFalse Negatives (FN): Predicted “didn’t survive” but actually did (Type II error)\nTrue Positives (TP): Correctly predicted “survived”\n\nHere we have shown the confusion matrix with the actual counts from our logistic regression model, such as 74 people, 13 people, etc. However, we’re often more interested in understanding how well our model predicted the correct values. For example, of the people who actually survived, what percentage did we predict will and won’t survive? In other words, we’re normalizing along the rows so that each rows adds up to 100%.\n\n# Calculate row-wise percentages (normalize by actual positives)\nrow_sums = cm.sum(axis=1)\nrecall_precision = cm.astype(float) / row_sums.reshape(-1, 1)\n\n# Display as a confusion matrix, with numbers formatted as percentages\nplt.figure(figsize=(8, 6))\nsns.heatmap(recall_precision, annot=True, fmt='.0%', cmap='Blues', cbar=False,\n            xticklabels=['Not Survived', 'Survived'],\n            yticklabels=['Not Survived', 'Survived'])\nplt.title('Normalized Confusion Matrix (Row-wise)', fontsize=14)\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nHere we see that, of the people who actually died, 87% were correctly predicted as having died (True Negative rate), while 13% were incorrectly predicted as having survived (False Positive rate). Similarly, of the people who actually survived, 92% were correctly predicted as having survived (True Positive rate), while 8% were incorrectly predicted as having died (False Negative rate).\n\n\n\n\n\n\nTip\n\n\n\nDisplaying the data normalized relative to the true values is typically more useful than looking at the raw numbers. When converting to percentages we see that our model did quite well predicting people who will die, but very poorly predicting people who will survive. This suggests our model may be biased towards predicting death, which could be important to consider for real-world applications.\n\n\n\n\n3.2 Computing Metrics from the Confusion Matrix\nAll the important classification metrics come directly from these four numbers:\nAccuracy: What percentage of all predictions were correct? \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\n\naccuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\nprint(f\"Accuracy: {accuracy:.3f}\")\n\nAccuracy: 0.699\n\n\nPrecision: When you predict positive, how often are you right? \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\n\nprecision = cm[1, 1] / (cm[1, 1] + cm[0, 1])\nprint(f\"Precision: {precision:.3f}\")\n\nPrecision: 0.667\n\n\nRecall (Sensitivity): Of all actual positives, how many did you find? \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\n\nrecall = cm[1, 1] / (cm[1, 1] + cm[1, 0])\nprint(f\"Recall: {recall:.3f}\")\n\nRecall: 0.464\n\n\nF1 Score: Harmonic mean of precision and recall \\[\n\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\nf1 = 2 * (precision * recall) / (precision + recall)\nprint(f\"F1 Score: {f1:.3f}\")\n\nF1 Score: 0.547\n\n\n\n\n3.3 When Each Metric Matters\nDifferent problems care about different metrics:\nUse Accuracy when:\n\nClasses are balanced\nFalse positives and false negatives are equally costly\nExample: Predicting coin flips (50/50 balanced, no asymmetric cost)\n\nUse Precision when:\n\nFalse positives are very costly\nYou want to be confident when you predict positive\nExample: Spam detection (marking legitimate email as spam is very annoying, but one or two spam emails slipping through is acceptable)\n\nUse Recall when:\n\nFalse negatives are very costly\nYou want to catch all positive cases, even if it means some false alarms\nExample: Disease screening (missing a sick patient with a life-threatening condition is much worse than telling a healthy patient that they are sick)\n\nUse F1 Score when:\n\nYou want a balance between precision and recall\nClasses are imbalanced\nExample: Fraud detection (imbalanced, and both FP and FN have significant consequences)\n\n\n\n\n\n\n\nNote\n\n\n\nThere’s almost always a tradeoff between precision and recall. Increase one, and the other goes down. You need to decide which matters more for your specific problem.\n\n\nLet’s see the full classification report:\n\nprint(classification_report(y_test_full, y_pred,\n                          target_names=['Not Survived', 'Survived']))\n\n              precision    recall  f1-score   support\n\nNot Survived       0.71      0.85      0.77        87\n    Survived       0.67      0.46      0.55        56\n\n    accuracy                           0.70       143\n   macro avg       0.69      0.66      0.66       143\nweighted avg       0.69      0.70      0.69       143\n\n\n\nThis report shows precision, recall, and F1 for both classes, plus overall accuracy."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#decision-trees-interpretable-non-linear-classifiers",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#decision-trees-interpretable-non-linear-classifiers",
    "title": "Chapter 3: Classification Models",
    "section": "5. Decision Trees: Interpretable Non-Linear Classifiers",
    "text": "5. Decision Trees: Interpretable Non-Linear Classifiers\n\n4.1 How Decision Trees Work\nDecision trees make predictions by asking a series of yes/no questions about the features. They split the data recursively based on feature values, creating a tree structure.\nHere’s the beautiful part: decision trees are incredibly interpretable. You can literally draw out the decision-making process and explain it to anyone.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n# Fit a simple decision tree\ntree_model = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_model.fit(X_train_full, y_train_full)\n\n# Visualize the tree\nplt.figure(figsize=(20, 10))\nplot_tree(tree_model, feature_names=features, class_names=['Not Survived', 'Survived'],\n          filled=True, fontsize=10, rounded=True)\nplt.title('Decision Tree for Titanic Survival', fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\nEach box shows:\n\nThe question being asked (e.g., “Fare &lt;= 26.27?”)\nThe Gini impurity (measure of how mixed the classes are, discussed in the next section)\nThe number of samples reaching this node\nThe class distribution (number of zeros and ones)\nThe predicted class\n\n\n\n4.2 Splitting Criteria: Gini vs Entropy\nDecision trees decide where to split by maximizing information gain. The most common criteria is the Gini impurity.\nGini Impurity: Measures how often a randomly chosen element would be incorrectly classified \\[\n\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2\n\\]\nwhere \\(p_i\\) is the proportion of samples in class \\(i\\).\n\n\n\n\n\n\nNote\n\n\n\nAnother criteria you’ll sometimes see is the Entropy, which is a measure of the amount of “disorder” or uncertainty. It is defined as \\[\n\\text{Entropy} = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n\\] The actual values of Gini and entropy are often extremely similar, and are measuring similar things. Therefore, we’ll focus on Gini impurity in this textbook.\n\n\nBut what exactly is Gini computing? Let’s look at the tree we just created. In the first node, we see that there are 308 samples with class 0 (survived) and 154 samples with class 1 (did not survive). The Gini impurity is computed as follows: \\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=1}^2 p_i^2 = 1 - \\left(\\frac{308}{462}\\right)^2 - \\left(\\frac{154}{462}\\right)^2 = \\frac{4}{9} = 0.444\n\\]\nNotice that this is exactly the Gini imprutiy stated in that node in the tree.\nIn general, how should we think about the value for Gini impurity?\nImagine that, in one of the nodes, there were 100 samples, and all of the samples were people who survived. Then the Gini impurity would be\n\\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{100}{100}\\right)^2 - \\left(\\frac{0}{100}\\right)^2 = 0\n\\]\nSimilarly, if all of the samples were people who did not survive, the Gini impurity would be\n\\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{0}{100}\\right)^2 - \\left(\\frac{100}{100}\\right)^2 = 0\n\\]\nOn the other hand, suppose there were a 50-50 split of the samples, with 50 samples of each class. Then the Gini impurity would be\n\\[\n\\text{Gini} = 1 - \\displaystyle\\sum_{i=0}^1 p_i^2 = 1 - \\left(\\frac{50}{100}\\right)^2 - \\left(\\frac{50}{100}\\right)^2 = \\frac{1}{2} = 0.5\n\\]\nNote how Gini impurity is small at the extremes (all samples have the same class), and bigger when the classes are more balanced. We can see this in the graph below, where the x-axis represents the proportion of class 1, and the y-axis represents the Gini impurity.\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0, 1, 100, endpoint=True)\ngini = 1 - (p**2 + (1-p)**2)\n\ngini_df = pd.DataFrame({\n    'p': p,\n    'gini': gini\n})\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='p', y='gini', data=gini_df)\nplt.xlabel('Proportion of Class 1')\nplt.ylabel('Gini Impurity')\nplt.title('Gini Impurity in a Binary Classification Problem')\nplt.show()\n\n\n\n\n\n\n\n\nSo, how is Gini impurity actually used to determine the splitting in decision trees? Our goal is to choose a split (i.e. a question) that best splits the data. For example, if we wanted to determine how well students will do in a class, asking whether they own a dog is a valid question, but won’t give me any additional information. On the other hand, if we ask if they regularly attend tutoring, we will get a much better idea of their performance.\nIn decision trees we use Gini impurity by calculating the Gini impurity for each possible split, and then choosing the split that has the smallest Gini impurity. This is because a smaller Gini impurity means that the split is better at separating the classes.\n\n\n4.3 Controlling Tree Depth: The Overfitting Problem\nTrees have a dangerous tendency: if you let them grow without limits, they’ll memorize the training data.\n\n# Compare different tree depths\ndepths = [1, 3, 5, 10, 20, None]  # None means no limit\ntrain_scores = []\ntest_scores = []\n\nfor depth in depths:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train_full, y_train_full)\n    train_scores.append(tree.score(X_train_full, y_train_full))\n    test_scores.append(tree.score(X_test_full, y_test_full))\n\n# Plot\ndepth_labels = [str(d) if d is not None else 'Unlimited' for d in depths]\nplt.figure(figsize=(10, 6))\nplt.plot(depth_labels, train_scores, 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(depth_labels, test_scores, 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('Maximum Tree Depth', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('Tree Depth vs Performance: The Overfitting Story', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nClassic overfitting! Training accuracy keeps increasing with depth, but test accuracy peaks and then plateaus or even decreases. The tree is memorizing noise in the training data.\nWe can see this concretely by printing out one of the decision paths in the deep trees:\n\n# Print one of the decision paths in the deep trees\ntree = DecisionTreeClassifier(max_depth=20, random_state=42)\ntree.fit(X_train_full, y_train_full)\n\nsample_id = 0\nsingle_sample = X_test_full[sample_id].reshape(1, -1) # Reshape for single sample input\n\n# Get the decision path for the single sample\nnode_indicator = tree.decision_path(single_sample)\nnode_indices = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n\n# Get tree structure information for interpreting the path\nchildren_left = tree.tree_.children_left\nchildren_right = tree.tree_.children_right\nfeature = tree.tree_.feature\nthreshold = tree.tree_.threshold\nfeature_names = features\n\nprint(f\"Decision path for sample {sample_id}:\")\nfor node_id in node_indices:\n    if children_left[node_id] != children_right[node_id]:  # Check if it's a split node\n        feature_index = feature[node_id]\n        threshold_value = threshold[node_id]\n        sample_feature_value = single_sample[0, feature_index]\n\n        if sample_feature_value &lt;= threshold_value:\n            decision = f\"{feature_names[feature_index]} ({sample_feature_value:.2f}) &lt;= {threshold_value:.2f}\"\n        else:\n            decision = f\"{feature_names[feature_index]} ({sample_feature_value:.2f}) &gt; {threshold_value:.2f}\"\n        print(f\"  Node {node_id}: Split on {decision}\")\n    else:  # It's a leaf node\n        print(f\"  Node {node_id}: Leaf node reached (prediction: {tree.predict(single_sample)[0]})\")\n\nDecision path for sample 0:\n  Node 0: Split on Fare (13.00) &lt;= 52.28\n  Node 1: Split on Age (42.00) &gt; 5.50\n  Node 11: Split on Pclass (2.00) &lt;= 2.50\n  Node 12: Split on Age (42.00) &lt;= 42.50\n  Node 13: Split on Fare (13.00) &gt; 12.31\n  Node 23: Split on Pclass (2.00) &gt; 1.50\n  Node 43: Split on Age (42.00) &gt; 15.00\n  Node 45: Split on Age (42.00) &gt; 39.50\n  Node 103: Split on Fare (13.00) &lt;= 26.50\n  Node 104: Leaf node reached (prediction: 1)\n\n\nWe start by asking reasonable seeming questions, but then asking more and more specific questions that seem unnecessary. For example, we ask if the persons age is:\n\nOver 5.5\nLess than 42.5\nOver 15\nOver 39.5\n\nPutting these together, we’ve asked if the person is between the ages of 39.5 and 42.5. That’s unnecessarily specific; do we really believe that a person being exactly 40 or 41 years old is so important? Instead, what’s happening is that our model is overfitting to the training data, and the results don’t generalize to the test data.\nCommon hyperparameters to control overfitting:\n\nmax_depth: Maximum depth of the tree\nmin_samples_split: Minimum samples required to split a node\nmin_samples_leaf: Minimum samples required at a leaf node\nmax_features: Number of features to consider for each split\n\n\n\n5.4 Feature Importance\nTrees can tell you which features are most important for making predictions:\n\n# Get feature importances\ntree_final = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_final.fit(X_train_full, y_train_full)\n\nimportance_df = pd.DataFrame({\n    'Feature': features,\n    'Importance': tree_final.feature_importances_\n}).sort_values('Importance', ascending=False)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance from Decision Tree', fontsize=14)\nplt.gca().invert_yaxis()\nplt.show()\n\n\n\n\n\n\n\n\nFeature importance represents how much each feature contributes to reducing impurity across all splits. Higher values mean more important features.\n\n\n\n\n\n\nNote\n\n\n\nFeature importance can be calculated a number of different ways. One way is to take a column, randomly shuffle the values, and see how the impurity changes. If the impurity decreases, then the feature is important. This is called permutation feature importance.\nFor DecisionTreeClassifier, feature importance is calculated using impurity decrease. The impurity decrease is the difference in impurity before and after a split. The higher the impurity decrease, the more important the feature. For example, if a feature can be used to end up with nodes with small impurity, that means that feature can split the data into groups where one class is highly represented. Recall from above that impurity is a measure of how mixed up the classes are in a node. So if a feature can be used to end up with nodes with small impurity, that means that feature can split the data into groups where one class is highly represented. This is why the feature is important.\n\n\n\n\n\n\n\n\nTip\n\n\n\nDifferent models calculate feature importance differently. However, they’re all trying to answer the same question: how much does this feature matter in terms of making predictions?\nDon’t treat feature importance like a gold standard. It’s a tool for understanding your model, but it’s not a substitute for domain knowledge. Instead, you can often use feature importance to double-check your own understanding of the problem. If you believe a feature should be important, but it’s not showing up in the feature importance, then why not? Is it a problem with the model/data, or a problem with your understanding?"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#random-forests-ensemble-power",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#random-forests-ensemble-power",
    "title": "Chapter 3: Classification Models",
    "section": "6. Random Forests: Ensemble Power",
    "text": "6. Random Forests: Ensemble Power\n\n5.1 Why Ensembles Work\nHere’s a powerful idea: what if instead of training one tree, we trained many trees and let them vote?\nRandom Forests are one of the most successful examples. The key insight: many weak learners can combine to create a strong learner.\n\n\n\n\n\n\nNote\n\n\n\nEnsemble learning is the idea of combining many weak learners to create a strong learner. The key insight is that many weak learners can combine to create a strong learner.\nBy a weak learner we typically mean a very simple model, such as a decision tree with a depth of two. By a strong learner we typically mean a more complex model, such as a decision tree with a depth of ten.\nThe idea is that one large complex model may overfit to the training data, but many small simple models can combine to create a strong learner that generalizes well to the test data.\nRandom forests are one example of ensemble models, but we’ll learn more. Ensemble models are typically the gold standard in classical machine learning.\n\n\nHow Random Forests work:\n\nCreate many decision trees (e.g., 100 trees)\nFor each tree:\n\nSample a random subset of the data (bootstrapping)\nAt each split, only consider a random subset of features\n\nMake predictions by majority vote (classification), or by averaging probabilities (regression)\n\nThe randomness in both samples and features ensures that trees are different from each other. After all, if we took all rows and all columns, each weak learner would likely be exactly the same. By sampling a random subset of the data and features, we ensure that each tree is different from the others.\nWhen weak learners disagree, it’s often because they’re focusing on different aspects of the data. When they agree, you can be more confident.\n\n\n5.2 Fitting a Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Fit random forest\nrf_model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\nrf_model.fit(X_train_full, y_train_full)\n\n# Compare to single tree\ntree_comparison = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_comparison.fit(X_train_full, y_train_full)\n\nprint(\"Single Decision Tree:\")\nprint(f\"  Training Accuracy: {tree_comparison.score(X_train_full, y_train_full):.3f}\")\nprint(f\"  Test Accuracy: {tree_comparison.score(X_test_full, y_test_full):.3f}\")\n\nprint(\"\\nRandom Forest (50 trees):\")\nprint(f\"  Training Accuracy: {rf_model.score(X_train_full, y_train_full):.3f}\")\nprint(f\"  Test Accuracy: {rf_model.score(X_test_full, y_test_full):.3f}\")\n\nSingle Decision Tree:\n  Training Accuracy: 0.765\n  Test Accuracy: 0.678\n\nRandom Forest (50 trees):\n  Training Accuracy: 0.788\n  Test Accuracy: 0.699\n\n\nRandom forests typically achieve better generalization than single trees. The ensemble reduces overfitting through diversity.\n\n\n\n\n\n\nTip\n\n\n\nRandom forests are most useful for complex data with many columns, and with complex relationships between columns. For simple data with few columns, a single decision tree is often sufficient.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou may feel like “if one tree is good, then more trees are better!” This is true to an extent, but it comes with a tradeoff. When you train an ensemble model such as a random forest, you’re training many models. Each model takes time to train, and each model uses memory and compute time. Imagine training a single tree which takes ten seconds to train.\nNow imagine training 50 trees, which would take 500 seconds to train, or nearly ten minutes! Combine this with hyperparameter tuning such as through a grid search, and you could easily be looking at an hour or more of training time. If the improvement in performance is large this may be worth it. But if your data is simple enough to get by with a simpler model, you can save hours of compute by going with a simpler model.\n\n\n\n\n5.3 Feature Importance in Random Forests\nRandom forests also provide feature importances, but they’re generally more reliable than single trees because they average across many trees:\n\n# Get feature importances from both models\ntree_importance = pd.DataFrame({\n    'Feature': features,\n    'Decision Tree': tree_comparison.feature_importances_,\n    'Random Forest': rf_model.feature_importances_\n})\n\n# Melt the dataframe for plotting\nimportance_melted = tree_importance.melt(\n    id_vars='Feature',\n    var_name='Model',\n    value_name='Importance'\n)\n\n# Sort by average importance across both models\navg_importance = tree_importance.set_index('Feature').mean(axis=1).sort_values(ascending=False)\nimportance_melted['Feature'] = pd.Categorical(\n    importance_melted['Feature'],\n    categories=avg_importance.index,\n    ordered=True\n)\n\n# Create dodged bar chart\nplt.figure(figsize=(10, 6))\nsns.barplot(data=importance_melted, x='Importance', y='Feature', hue='Model', palette='Set2')\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance: Decision Tree vs Random Forest', fontsize=14)\nplt.legend(title='Model', fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRandom forest importances tend to be more stable because they’re averaged across many trees with different random subsets of data and features.\n\n\n6.4 Hyperparameter Tuning\nRandom forests have several important hyperparameters:\n\n# Test different numbers of trees\nn_trees_list = [25, 50, 75, 100, 150, 200]\nscores = []\n\nfor n_trees in n_trees_list:\n    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n    rf.fit(X_train_full, y_train_full)\n    scores.append(rf.score(X_test_full, y_test_full))\n\nplt.figure(figsize=(10, 6))\nplt.plot(n_trees_list, scores, 'o-', linewidth=2, markersize=8)\nplt.xlabel('Number of Trees', fontsize=12)\nplt.ylabel('Test Accuracy', fontsize=12)\nplt.title('Random Forest Performance vs Number of Trees', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nPerformance typically improves with more trees, but you get diminishing returns. After a certain point (often 100-500 trees), adding more trees barely helps (or even leads to overfitting)but makes training slower. Use hyperparameter tuning to find the right number of trees. A typically starting point is 50 to 100 trees.\n\n\n\n\n\n\nTip\n\n\n\nRandom forests are often a great default choice for classification. They’re robust, handle non-linear relationships, require minimal hyperparameter tuning, and rarely overfit badly. When in doubt, try a random forest!"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#support-vector-machines-maximum-margin-classifiers",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#support-vector-machines-maximum-margin-classifiers",
    "title": "Chapter 3: Classification Models",
    "section": "7. Support Vector Machines: Maximum Margin Classifiers",
    "text": "7. Support Vector Machines: Maximum Margin Classifiers\n\n7.1 The Margin Concept\nSupport Vector Machines (SVMs) have a beautiful geometric intuition: find the decision boundary that maximizes the distance to the nearest points from each class.\nThink of it like this: if you’re drawing a line to separate two groups of points, you want it to be as far as possible from both groups. This gives you more confidence that future points will be classified correctly.\nThe “support vectors” are the points closest to the decision boundary—these are the critical points that define where the boundary goes.\n\nfrom sklearn.svm import SVC\n\n# Fit SVM with linear kernel\nsvm_linear = SVC(kernel='linear', random_state=42)\nsvm_linear.fit(X_train_full, y_train_full)\n\nprint(f\"SVM Linear Kernel Accuracy: {svm_linear.score(X_test_full, y_test_full):.3f}\")\nprint(f\"Number of support vectors: {len(svm_linear.support_)}\")\n\nSVM Linear Kernel Accuracy: 0.678\nNumber of support vectors: 374\n\n\n\n\n7.2 The Kernel Trick\nHere’s where SVMs get really powerful: the kernel trick. By using different kernel functions, SVMs can create non-linear decision boundaries while still solving a linear problem in a higher-dimensional space.\nCommon kernels:\n\nLinear: Creates straight boundaries (like logistic regression)\nRBF (Radial Basis Function): Creates circular/curved boundaries\nPolynomial: Creates polynomial curves as boundaries\n\n\n# Compare different kernels\nkernels = ['linear', 'rbf', 'poly']\nsvm_models = {}\n\nfor kernel in kernels:\n    svm = SVC(kernel=kernel, random_state=42)\n    svm.fit(X_train_full, y_train_full)\n    svm_models[kernel] = svm\n    print(f\"{kernel:8s} kernel - Test Accuracy: {svm.score(X_test_full, y_test_full):.3f}\")\n\nlinear   kernel - Test Accuracy: 0.678\nrbf      kernel - Test Accuracy: 0.615\npoly     kernel - Test Accuracy: 0.629\n\n\n\n\n6.3 Visualizing SVM Decision Boundaries\nLet’s see how different kernels create different boundaries. We’ll use California housing data, where different regions of the state have vastly different housing prices—a perfect example of non-linear geographic clustering:\n\n# Load California housing data\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\ncalifornia = fetch_california_housing(as_frame=True)\nca_df = california.frame\n\n# Create binary classification: expensive (&gt;$3) vs affordable (&lt;=$3) houses\n# Median house value is in $100,000s, so 3 = $300,000\nca_df['expensive'] = (ca_df['MedHouseVal'] &gt; 3.0).astype(int)\n\n# Use longitude and median house value for visualization\n# Different parts of California have very different price patterns\nX_viz = ca_df[['Latitude', 'MedHouseVal']].values\ny_viz = ca_df['expensive'].values\n\n# Sample for faster visualization (full dataset is 20k+ points)\nnp.random.seed(42)\nsample_idx = np.random.choice(len(X_viz), size=2000, replace=False)\nX_viz_raw = X_viz[sample_idx]\ny_viz = y_viz[sample_idx]\n\n# Scale features for SVM (IMPORTANT: SVMs are sensitive to feature scales)\nscaler = StandardScaler()\nX_viz = scaler.fit_transform(X_viz_raw)\n\nexpensive = y_viz == 1\n\n# Create a mesh for plotting decision boundaries (in scaled space)\nlon_range = np.linspace(X_viz[:, 0].min(), X_viz[:, 0].max(), 100)\nprice_range = np.linspace(X_viz[:, 1].min(), X_viz[:, 1].max(), 100)\nlon_mesh, price_mesh = np.meshgrid(lon_range, price_range)\nmesh_points = np.c_[lon_mesh.ravel(), price_mesh.ravel()]\n\n# Fit SVMs with different kernels on scaled data\nsvm_linear_viz = SVC(kernel='linear', random_state=42)\nsvm_rbf_viz = SVC(kernel='rbf', gamma='auto', random_state=42)\n\nsvm_linear_viz.fit(X_viz, y_viz)\nsvm_rbf_viz.fit(X_viz, y_viz)\n\n# Create predictions on mesh\nmesh_linear = svm_linear_viz.predict(mesh_points).reshape(lon_mesh.shape)\nmesh_rbf = svm_rbf_viz.predict(mesh_points).reshape(lon_mesh.shape)\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor ax, mesh_pred, title in zip(axes, [mesh_linear, mesh_rbf],\n                                 ['SVM - Linear Kernel', 'SVM - RBF Kernel']):\n    ax.contourf(lon_mesh, price_mesh, mesh_pred, levels=1, cmap='RdYlGn', alpha=0.4)\n    ax.scatter(X_viz[expensive, 0], X_viz[expensive, 1], c='darkgreen', marker='o',\n               s=20, edgecolors='black', alpha=0.5, label='Expensive (&gt;$300k)')\n    ax.scatter(X_viz[~expensive, 0], X_viz[~expensive, 1], c='darkred', marker='x',\n               s=20, alpha=0.5, label='Affordable (≤$300k)')\n    ax.set_xlabel('Latitude', fontsize=12)\n    ax.set_ylabel('Median House Value ($100k)', fontsize=12)\n    ax.set_title(title, fontsize=14)\n    ax.legend()\n    ax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSee the difference? The linear kernel creates a straight boundary—it tries to separate expensive from affordable homes with a single line. But the RBF kernel creates smooth, curved boundaries that adapt to the geographic clustering of housing prices.\nNotice how the RBF kernel captures the reality that certain geographic regions (coastal areas, Bay Area) command higher prices regardless of other factors. The curved decision boundary wraps around these high-value clusters much more naturally than a straight line ever could.\n\n\n7.4 When to Use SVMs\nStrengths:\n\nEffective in high-dimensional spaces\nMemory efficient (only stores support vectors)\nFlexible with different kernels\nWorks well with clear margin of separation\n\nWeaknesses:\n\nSlow to train on large datasets (doesn’t scale well beyond ~10,000 samples)\nRequires feature scaling (sensitive to feature magnitudes)\nChoosing the right kernel and hyperparameters can be tricky\nLess interpretable than trees or logistic regression\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways scale your features before using SVMs! They’re very sensitive to feature magnitudes. Use StandardScaler or MinMaxScaler from scikit-learn."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#k-nearest-neighbors-simple-but-powerful",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#k-nearest-neighbors-simple-but-powerful",
    "title": "Chapter 3: Classification Models",
    "section": "8. k-Nearest Neighbors: Simple but Powerful",
    "text": "8. k-Nearest Neighbors: Simple but Powerful\n\n7.1 The k-NN Algorithm\nk-Nearest Neighbors might be the simplest classification algorithm: to classify a new point, find the k closest training points and let them vote.\nThat’s it. No training phase. No learning parameters. Just store the data and compute distances when you need to make predictions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Fit k-NN with k=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_full, y_train_full)\n\nprint(f\"k-NN (k=5) Test Accuracy: {knn.score(X_test_full, y_test_full):.3f}\")\n\nk-NN (k=5) Test Accuracy: 0.650\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKNN models sound like they should be simple. You simply find the k closest training points and let them vote. But in practice, they can be devilishly complex. For example, how do you measure “closeness”? In some data that may be simple, such as closeness in position or time. But what about data on students? What do we mean by the “closest students”? Closest in age? Same/similar major? Same/similar year in college? Are all of these equally important? Is one more important than another? How about closeness in courses taken? When should we consider two courses “close”?\nAll of these questions are enormously important in building effective KNN models, but they don’t have easy answers. KNN models, more than many others, require extensive testing to determine what works best.\n\n\n\n\n7.2 Choosing k: The Bias-Variance Tradeoff Again\nThe value of k controls the bias-variance tradeoff:\n\nSmall k (e.g., k=1): Very flexible, low bias, high variance (overfitting)\nLarge k (e.g., k=100): Smoother boundaries, high bias, low variance (underfitting)\n\n\n# Test different values of k\nk_values = [1, 3, 5, 10, 20, 50, 100]\ntrain_scores_knn = []\ntest_scores_knn = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_full, y_train_full)\n    train_scores_knn.append(knn.score(X_train_full, y_train_full))\n    test_scores_knn.append(knn.score(X_test_full, y_test_full))\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, train_scores_knn, 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(k_values, test_scores_knn, 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('k (number of neighbors)', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('k-NN: Choosing k', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nNotice the pattern: small k gives high training accuracy but may overfit. Moderate k (often 3-10) tends to work best, but hyperparameter tuning is needed to determine the best choice.\n\n\n8.3 Distance Metrics: How Do We Measure “Closeness”?\nThe entire k-NN algorithm hinges on one question: how do you measure which points are “closest”? This isn’t just a technical detail—it fundamentally changes how your model behaves. Scikit-learn supports several distance metrics, and choosing the right one can dramatically affect performance.\nThe most common distance metrics:\n\nEuclidean distance (default): The straight-line distance between two points \\[d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\]\nManhattan distance: Sum of absolute differences (think of it as the number of city blocks between two locations, you can only walk horizontally or vertically) \\[d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\\]\nMinkowski distance: A mixture between Euclidean and Manhattan (p=1 is Manhattan, p=2 is Euclidean, but you can set p to any positive real number) \\[d(x, y) = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}\\]\nCosine distance: Measures angle between vectors (ignores the length of the vectors, and only cares about how far apart the directions they point are) \\[d(x, y) = 1 - \\frac{x \\cdot y}{||x|| \\cdot ||y||}\\]\nHamming distance: Checks whether two values are equal or not, and computes the average number of features where two samples are equal \\[d(x, y) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{1}(x_i \\neq y_i)\\] where \\(\\mathbb{1}(x_i \\neq y_i)\\) equals 1 if \\(x_i \\neq y_i\\) and 0 otherwise.\n\nLet’s see how different metrics perform on our Titanic data:\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Test different distance metrics\nmetrics = ['euclidean', 'manhattan', 'minkowski', 'cosine', 'hamming']\nmetric_scores = {}\n\nfor metric in metrics:\n    # Some metrics need additional parameters\n    if metric == 'minkowski':\n        knn = KNeighborsClassifier(n_neighbors=5, metric=metric, p=3)\n    else:\n        knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n\n    knn.fit(X_train_full, y_train_full)\n    train_score = knn.score(X_train_full, y_train_full)\n    test_score = knn.score(X_test_full, y_test_full)\n    metric_scores[metric] = {'train': train_score, 'test': test_score}\n\n    # print(f\"{metric:12s} - Train: {train_score:.3f}, Test: {test_score:.3f}\")\n\n# Plot the metric and train/test score\nplt.figure(figsize=(10, 6))\nplt.plot(metrics, [scores['train'] for scores in metric_scores.values()], 'o-', label='Training Accuracy', linewidth=2)\nplt.plot(metrics, [scores['test'] for scores in metric_scores.values()], 's-', label='Test Accuracy', linewidth=2)\nplt.xlabel('Distance Metric', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('k-NN: Choosing Distance Metric', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nDifferent metrics can give meaningfully different results! But which should you use?\nWhen to use each metric:\nEuclidean distance (the default) works well when:\n\nAll features have similar scales and units\nYou care about the actual geometric distance\nFeatures are continuous numeric values\nExample: Geographic coordinates (latitude/longitude), physical measurements\n\nManhattan distance works well when:\n\nFeatures represent different units that shouldn’t be combined quadratically\nYou have grid-like data (think city blocks, not “as the crow flies”)\nYou want to reduce the influence of outliers (no squaring!)\nExample: Recommender systems, routing/navigation problems\n\nCosine distance works well when:\n\nYou care about direction/orientation, not magnitude\nData is high-dimensional and sparse\nFeature scales vary wildly\nExample: Text data (word counts), recommendation systems with user ratings\n\nHamming distance works well when:\n\nYou have categorical or binary features\nAll features are equally important (no scaling needed)\nYou want to count how many features differ, not by how much\nExample: DNA sequences, binary feature vectors, categorical data (after one-hot encoding)\n\n\n\n\n\n\n\nNote\n\n\n\nHamming distance treats all feature differences equally. If Feature A differs by 0.1 and Feature B differs by 10, Hamming sees both as “different.” It’s perfect for categorical data where “different is different” regardless of magnitude, but not ideal for continuous numeric features where the size of the difference matters.\n\n\nLet’s visualize how these different metrics create different neighborhoods. We’ll use a simple 2D example:\n\nfrom sklearn.neighbors import NearestNeighbors\n\n# Create a simple 2D point to query\nquery_point = np.array([[2.0, 3.0]])\n\n# Create some sample points\nnp.random.seed(42)\nsample_points = np.random.rand(20, 2) * 5\n\n# Find 5 nearest neighbors using different metrics\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor ax, metric in zip(axes, ['euclidean', 'manhattan', 'cosine']):\n    # Find neighbors\n    nbrs = NearestNeighbors(n_neighbors=5, metric=metric)\n    nbrs.fit(sample_points)\n    distances, indices = nbrs.kneighbors(query_point)\n\n    # Plot\n    ax.scatter(sample_points[:, 0], sample_points[:, 1], c='lightgray',\n               s=100, alpha=0.6, label='Other points')\n    ax.scatter(sample_points[indices[0], 0], sample_points[indices[0], 1],\n               c='blue', s=100, edgecolors='black', linewidth=2, label='5 nearest neighbors')\n    ax.scatter(query_point[0, 0], query_point[0, 1], c='red', s=200,\n               marker='*', edgecolors='black', linewidth=2, label='Query point')\n\n    # Draw lines to nearest neighbors\n    for idx in indices[0]:\n        ax.plot([query_point[0, 0], sample_points[idx, 0]],\n                [query_point[0, 1], sample_points[idx, 1]],\n                'b--', alpha=0.3, linewidth=1)\n\n    ax.set_title(f'{metric.capitalize()} Distance', fontsize=14)\n    ax.set_xlabel('Feature 1', fontsize=12)\n    ax.set_ylabel('Feature 2', fontsize=12)\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSee how the same query point has different nearest neighbors depending on the metric? Euclidean forms circular neighborhoods, Manhattan forms diamond-shaped neighborhoods, and cosine focuses on angular similarity.\n\n\n7.4 Mixing Metrics: Different Features Need Different Distances\nHere’s a critical insight that’s often overlooked: real datasets have different types of features, and each type needs its own distance metric.\nThink about internet service provider (ISP) customer data and predicting churn:\n\nInternet Service: Categorical (DSL, Fiber optic, No internet). We want to know if two customers have the same service type—not treat “DSL” as somehow numerically between “No internet” and “Fiber optic”\nContract type: Categorical (Month-to-month, One year, Two year). Either the same or different.\nGender: Categorical (Male, Female). Same or different.\nMonthly Charges: Continuous numeric variable. A customer paying $50/month is more similar to one paying $55 than to one paying $100.\nTenure: Continuous numeric variable. The actual difference in months matters.\n\nThe problem? When you call KNeighborsClassifier(metric='euclidean'), it treats ALL features the same way! It computes Euclidean distance on internet service type and contract (treating categorical values as if they were numbers) just like it does on monthly charges and tenure.\nThe solution: Create a custom distance metric that treats different feature types appropriately.\nFor example, you could define a custom distance function that:\n\nUses Hamming distance (equality check) for categorical features (Internet Service, Contract, Gender)\nUses Euclidean distance for continuous features (Monthly Charges, Tenure)\n\nThese can be difficult to implement by hand, so working together with an AI coding assistant is the way to go.\n\n\n\n\n\n\nNote\n\n\n\nWhy does this matter?\nImagine comparing two ISP customers: - Customer A: DSL, Month-to-month contract, Male, $50/month, 12 months tenure - Customer B: Fiber optic, Month-to-month contract, Male, $52/month, 14 months tenure\nWith pure Euclidean distance, if internet service is encoded as DSL=0 and Fiber optic=1, the difference in service type contributes “1” to the distance calculation, just like a $1/month price difference. But internet service type is categorical! Having DSL vs Fiber optic is a fundamental categorical difference—not a numeric one.\nWith the mixed metric, we recognize that internet service differs (Hamming distance = 1), contract and gender are the same (Hamming distance = 0 for each), and then we properly compute Euclidean distance for the continuous features (monthly charges, tenure) where the magnitude of difference actually matters.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen building custom distance metrics:\n\nIdentify feature types first: Which are categorical? Which are continuous?\nScale continuous features: Use StandardScaler before computing distances\nDon’t scale categorical features: They represent discrete categories, not magnitudes\nTest your metric: Does it give sensible distances for sample pairs?\nWeight carefully: You might want to weight categorical and continuous distances differently\n\nThe custom metric approach requires more work, but it’s often worth it for datasets with mixed feature types!\n\n\n\n\n\n\n\n\nTip\n\n\n\nHow to choose a distance metric:\n\nStart with Euclidean (the default) - it works well in most cases\nTry Hamming if you have categorical features that have no obvious ordering\nTry Manhattan if you have outliers or features on very different scales\nTry Cosine if your data is high-dimensional or sparse (like text data)\nUse cross-validation to compare metrics on your specific dataset\nAlways scale your features before using distance-based methods!\n\nThe “best” metric depends on your data and problem. Don’t just accept the default—experiment and use validation performance to guide your choice.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFeature scaling is critical for k-NN! If one feature ranges from 0-1 and another ranges from 0-1000, the second feature will dominate the distance calculation. Always use StandardScaler or MinMaxScaler before fitting k-NN models.\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n\n\n\n7.5 When to Use k-NN\nStrengths:\n\nSimple to understand and implement\nNo training phase (though this is also a weakness)\nNaturally handles multi-class problems\nCan capture complex patterns\n\nWeaknesses:\n\nSlow prediction (has to compute distances to all training points)\nMemory intensive (stores all training data)\nRequires feature scaling\nChoosing k can be tricky\nDetermining appropriate distance metric can be complex"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#roc-curves-and-auc-comparing-models",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#roc-curves-and-auc-comparing-models",
    "title": "Chapter 3: Classification Models",
    "section": "9. ROC Curves and AUC: Comparing Models",
    "text": "9. ROC Curves and AUC: Comparing Models\n\n9.1 The ROC Curve\nSo far we’ve been using a fixed threshold (0.5) to convert probabilities to class predictions. But what if we tried different thresholds?\nThe ROC curve (Receiver Operating Characteristic) shows model performance across all possible thresholds. It plots:\n\nTrue Positive Rate (TPR) = Recall = TP / (TP + FN)\nFalse Positive Rate (FPR) = FP / (FP + TN)\n\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Get probability predictions from logistic regression\ny_proba_log = log_model_full.predict_proba(X_test_full)[:, 1]\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y_test_full, y_proba_log)\n\n# Compute AUC (Area Under the Curve)\nauc = roc_auc_score(y_test_full, y_proba_log)\n\n# Plot\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Logistic Regression (AUC = {auc:.3f})')\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier (AUC = 0.5)')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate (Recall)', fontsize=12)\nplt.title('ROC Curve', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nInterpreting the ROC curve:\n\nThe diagonal line represents a random classifier (flip a coin)\nThe closer your curve sticks to the top-left corner, the better\nAUC (Area Under the Curve) summarizes performance in one number\nAUC = 1.0: Perfect classifier\nAUC = 0.5: Random guessing\nAUC &lt; 0.5: Worse than random (you’re predicting backwards!)\n\n\n\n\n\n\n\nNote\n\n\n\nWhy is an ROC curve “sticking to the top-left corner” a good thing? The top-left corner means we have essentially zero false positives, and high true positives.\n\n\n\n\n8.2 Comparing Multiple Models with ROC\nLet’s compare all our models on the same ROC plot:\n\n# Get probabilities from all models\nmodels_for_roc = {\n    'Logistic Regression': log_model_full,\n    'Decision Tree': tree_final,\n    'Random Forest': rf_model,\n    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42).fit(X_train_full, y_train_full),\n    'k-NN': knn\n}\n\nplt.figure(figsize=(10, 8))\n\nfor name, model in models_for_roc.items():\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test_full)[:, 1]\n    else:\n        # SVM without probability=True would fail here\n        y_proba = model.predict_proba(X_test_full)[:, 1]\n\n    fpr, tpr, _ = roc_curve(y_test_full, y_proba)\n    auc = roc_auc_score(y_test_full, y_proba)\n    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves: Model Comparison', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nThis visualization makes it easy to compare models at a glance. The model with the highest AUC is typically performing best across all thresholds.\n\n\n8.3 When to Use ROC/AUC\nUse ROC/AUC when:\n\nYou want threshold-independent evaluation\nClasses are relatively balanced\nYou care about ranking (who’s more likely to be positive?)\n\nDon’t use ROC/AUC when:\n\nClasses are severely imbalanced\nYou have a specific threshold constraint\nYou care more about absolute performance at one threshold"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#class-imbalance-the-real-world-problem",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#class-imbalance-the-real-world-problem",
    "title": "Chapter 3: Classification Models",
    "section": "10. Class Imbalance: The Real-World Problem",
    "text": "10. Class Imbalance: The Real-World Problem\n\n9.1 Why Class Imbalance Matters\nThe truth is, most real-world classification problems have imbalanced classes. Fraud detection? Maybe 0.1% of transactions are fraud. Disease diagnosis? Most patients who show up a a hospital don’t have a specific disease. Email spam? While we all get spam, that’s not the majority of our email.\nClass imbalance breaks naive approaches. For example, suppose we had data where 1% of the people had a given rare disease. If we built a model that predicted everyone as not having the disease, we would be 99% accurate. But that’s not a very useful model!\n\n\n9.2 Detecting Class Imbalance\nAlways check class balance before building models. Let’s look at a real example: predicting whether a song on Spotify will become a “viral hit” (popularity score above 80). Most songs don’t go viral, so this is naturally imbalanced:\n\n# Load Spotify data and create an imbalanced classification problem\nspotify_df = pd.read_csv('../data/spotify.csv')\n\n# Create binary target: viral hit (popularity &gt; 80) vs not\nspotify_df['viral_hit'] = (spotify_df['popularity'] &gt; 80).astype(int)\n\n# Select numeric features for our model\nspotify_features = ['danceability', 'energy', 'loudness', 'speechiness',\n                    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n\n# Drop rows with missing values\nspotify_clean = spotify_df[spotify_features + ['viral_hit']].dropna()\n\nX_imbalanced = spotify_clean[spotify_features].values\ny_imbalanced = spotify_clean['viral_hit'].values\n\n# Split imbalanced data\nX_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n    X_imbalanced, y_imbalanced, test_size=0.2, random_state=42\n)\n\nprint(f\"Loaded Spotify dataset with {len(y_imbalanced)} songs\")\nprint(f\"Viral hits (popularity &gt; 90): {y_imbalanced.sum()}\")\nprint(f\"Regular songs: {(y_imbalanced == 0).sum()}\")\n\nLoaded Spotify dataset with 42663 songs\nViral hits (popularity &gt; 90): 125\nRegular songs: 42538\n\n\nNow let’s check the class balance:\n\n# Check balance with seaborn\ndef check_class_balance(y, name=\"Dataset\"):\n    counts = pd.Series(y).value_counts()\n    df = pd.DataFrame({'Class': ['Class 0', 'Class 1'], 'Count': counts})\n    sns.barplot(x='Class', y='Count', data=df)\n    plt.title(name)\n    plt.show()\n\ncheck_class_balance(y_imbalanced, \"Spotify Viral Hit Dataset - Highly imbalanced\")\ncheck_class_balance(y_full, \"Titanic Dataset - Fairly balanced\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRules of thumb:\n\nRatio &lt; 2:1 → Probably not a problem\nRatio 2:1 to 10:1 → Moderate imbalance, be careful with metrics\nRatio &gt; 10:1 → Severe imbalance, definitely needs special handling\n\n\n\n9.3 Handling Class Imbalance: Class Weights\nOne simple approach: tell the model to weight the minority class more heavily during training. Many models have the ability to assign weights to different classes, using the class_weight parameter with a value of 'balanced'.\nFirst, let’s fit a “naive” model that ignores the imbalance:\n\n# Fit a naive model (ignores imbalance)\nnaive_model = LogisticRegression(random_state=42, max_iter=1000)\nnaive_model.fit(X_train_imb, y_train_imb)\n\ny_pred_naive = naive_model.predict(X_test_imb)\nconf_mat = confusion_matrix(y_test_imb, y_pred_naive)\n\n# Plot normalized confusion matrix (normalize by row)\nplt.figure(figsize=(5, 5))\nconf_mat_norm = conf_mat.astype('float') / conf_mat.sum(axis=1, keepdims=True)\nsns.heatmap(conf_mat_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=['Not Viral', 'Viral'],\n            yticklabels=['Not Viral', 'Viral'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Naive Model Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nNotice how every single song is predicted as being not popular. That’s because the model gets “better” predictions by predicting the majority class (not viral) for every song.\nNow let’s compare with a model that uses class weights:\n\n# Fit with class weights\nweighted_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\nweighted_model.fit(X_train_imb, y_train_imb)\n\ny_pred_weighted = weighted_model.predict(X_test_imb)\n\n# Plot normalized confusion matrix (normalize by row)\nplt.figure(figsize=(5, 5))\nconf_mat_weighted = confusion_matrix(y_test_imb, y_pred_weighted)\nconf_mat_weighted_norm = conf_mat_weighted.astype('float') / conf_mat_weighted.sum(axis=1, keepdims=True)\nsns.heatmap(conf_mat_weighted_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=['Not Viral', 'Viral'],\n            yticklabels=['Not Viral', 'Viral'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Model with balanced class weights Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nThe class_weight='balanced' parameter automatically weights classes inversely proportional to their frequency. This forces the model to pay more attention to minority class errors, and results in a fairly strong model with good precision and recall.\n\n\n9.4 Handling Class Imbalance: Resampling\nAnother approach: change the data itself so that it’s balanced.\nUndersampling: Remove examples from majority class\nOversampling: Duplicate examples from minority class\nSMOTE (Synthetic Minority Over-sampling Technique): Create synthetic minority class examples\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Apply SMOTE\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_imb, y_train_imb)\n\nprint(\"Original training set:\")\ncheck_class_balance(y_train_imb, \"Before SMOTE\")\n\nprint(\"\\nAfter SMOTE:\")\ncheck_class_balance(y_train_smote, \"After SMOTE\")\n\n# Train on balanced data\nsmote_model = LogisticRegression(random_state=42, max_iter=1000)\nsmote_model.fit(X_train_smote, y_train_smote)\n\n# Evaluate on original imbalanced test set\ny_pred_smote = smote_model.predict(X_test_imb)\n\nprint(\"\\nModel trained on SMOTE data:\")\nconf_mat_smote = confusion_matrix(y_test_imb, y_pred_smote)\nconf_mat_smote_norm = conf_mat_smote.astype('float') / conf_mat_smote.sum(axis=1, keepdims=True)\nplt.figure(figsize=(5, 5))\nsns.heatmap(conf_mat_smote_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=['Not Viral', 'Viral'],\n            yticklabels=['Not Viral', 'Viral'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('SMOTE Model Confusion Matrix')\nplt.show()\n\nOriginal training set:\n\n\n\n\n\n\n\n\n\n\nAfter SMOTE:\n\n\n\n\n\n\n\n\n\n\nModel trained on SMOTE data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using resampling techniques like SMOTE, only resample the training data! Never resample the test set—you want to evaluate on the natural class distribution.\n\n\n\n\n9.5 Choosing Metrics for Imbalanced Data\nWith imbalanced data, accuracy is almost always misleading. Use:\n\nPrecision: When false positives are costly\nRecall: When false negatives are costly\nF1 Score: When you want a balance\nAUC-ROC: Threshold-independent, but can be optimistic with severe imbalance"
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#comparing-all-models-a-practical-guide",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#comparing-all-models-a-practical-guide",
    "title": "Chapter 3: Classification Models",
    "section": "10. Comparing All Models: A Practical Guide",
    "text": "10. Comparing All Models: A Practical Guide\n\n10.1 Model Selection Framework\nWith so many classification algorithms, how do you choose? Here’s a practical framework:\nStart with logistic regression if:\n\nYou need interpretability (coefficients matter)\nYou want fast training and prediction\nYou suspect linear decision boundaries\nYou have limited data\n\nUse decision trees if:\n\nYou need maximum interpretability (show the tree to stakeholders)\nFeatures are on different scales (trees don’t need scaling)\nYou have non-linear relationships\nYou’re okay with potential overfitting\n\nUse random forests if:\n\nYou want robust performance without much tuning\nYou have enough data (hundreds or thousands of samples)\nYou don’t need interpretability\nYou want feature importance estimates\n\nUse SVMs if:\n\nYou have high-dimensional data (many features)\nYou have clear margin of separation\nYou’re willing to spend time tuning hyperparameters\nDataset is not too large (&lt; 10,000 samples)\n\nUse k-NN if:\n\nYou have small datasets\nYou don’t need fast predictions\nYou have low-to-moderate dimensions\nDecision boundaries are very irregular\n\n\n\n10.2 Complete Model Comparison\nLet’s do a comprehensive comparison:\n\nfrom sklearn.model_selection import cross_val_score\n\n# Define models\ncomparison_models = {\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Compare on Titanic data using cross-validation\nresults = []\n\nfor name, model in comparison_models.items():\n    # Cross-validation scores\n    cv_scores = cross_val_score(model, X_full, y_full, cv=5, scoring='accuracy')\n\n    # Fit and evaluate\n    model.fit(X_train_full, y_train_full)\n    y_pred_comp = model.predict(X_test_full)\n\n    # Compute metrics\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n    results.append({\n        'Model': name,\n        'CV Accuracy (mean)': cv_scores.mean(),\n        'CV Accuracy (std)': cv_scores.std(),\n        'Test Accuracy': accuracy_score(y_test_full, y_pred_comp),\n        'Precision': precision_score(y_test_full, y_pred_comp),\n        'Recall': recall_score(y_test_full, y_pred_comp),\n        'F1 Score': f1_score(y_test_full, y_pred_comp)\n    })\n\nresults_df = pd.DataFrame(results).set_index('Model')\nresults_df\n\n\n\n\n\n\n\n\nCV Accuracy (mean)\nCV Accuracy (std)\nTest Accuracy\nPrecision\nRecall\nF1 Score\n\n\nModel\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n0.689077\n0.042540\n0.699301\n0.666667\n0.464286\n0.547368\n\n\nDecision Tree\n0.715719\n0.041416\n0.678322\n0.619048\n0.464286\n0.530612\n\n\nRandom Forest\n0.703103\n0.037517\n0.664336\n0.600000\n0.428571\n0.500000\n\n\nSVM (RBF)\n0.666699\n0.072339\n0.615385\n0.513514\n0.339286\n0.408602\n\n\nk-NN (k=5)\n0.659795\n0.059218\n0.650350\n0.565217\n0.464286\n0.509804\n\n\n\n\n\n\n\n\n\n10.3 Visualizing Model Performance\n\n# Plot comparison\nmetrics = ['Test Accuracy', 'Precision', 'Recall', 'F1 Score']\nresults_df[metrics].plot(kind='bar', figsize=(12, 6), rot=45)\nplt.ylabel('Score', fontsize=12)\nplt.title('Model Comparison Across Metrics', fontsize=14)\nplt.legend(loc='lower right')\nplt.ylim(0.5, 1.0)\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.4 The Importance of Context\nThere’s no single “best” model. The right choice depends on:\n\nProblem requirements: Speed? Interpretability? Accuracy?\nData characteristics: Size? Dimensionality? Imbalance?\nComputational resources: Training time? Prediction time? Memory?\nBusiness context: Cost of errors? Regulatory requirements?\n\nA model with 90% accuracy might be useless if it misses the 10% that actually matters. A model with 75% accuracy might be perfect if it catches the critical cases."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#summary",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#summary",
    "title": "Chapter 3: Classification Models",
    "section": "Summary",
    "text": "Summary\nYou’ve learned the fundamentals of classification and explored five major approaches. Let’s recap the key insights.\nClassification is fundamentally different from regression. You’re predicting categories, not continuous values. This changes everything: the algorithms, the evaluation metrics, the challenges you’ll face. Linear regression is the wrong tool. You need classifiers designed to output probabilities or discrete predictions.\nEach algorithm has a sweet spot. Logistic regression for speed and interpretability with linear boundaries. Decision trees for maximum explainability. Random forests for robust performance without much tuning. SVMs for high-dimensional data with clear margins. k-NN for small datasets with complex local patterns. There’s no universal best—context matters.\nThe confusion matrix is your diagnostic tool. True positives, false positives, true negatives, false negatives—these four numbers tell you exactly where your model succeeds and fails. Every metric (accuracy, precision, recall, F1) derives from them. Master confusion matrices and you can navigate any classification problem.\nAccuracy alone is almost always insufficient. Especially with imbalanced data, accuracy can be completely misleading. You need to understand precision (when I predict positive, am I usually right?) and recall (of all actual positives, how many do I catch?). The tradeoff between them depends on your specific problem’s costs. Medical diagnosis? Maximize recall. Spam detection? Maybe maximize precision. There’s no one-size-fits-all answer.\nROC curves let you compare models across all thresholds. Instead of committing to 0.5 as your decision threshold, ROC curves show performance across all possible thresholds. AUC summarizes this in one number. Higher is better.\nClass imbalance is the norm, not the exception. Fraud detection, disease diagnosis, rare event prediction—most interesting real-world problems have imbalanced classes. Naive models will just predict the majority class and claim victory with high accuracy. You need to detect imbalance (check value counts!), use appropriate metrics (forget accuracy, use F1 or AUC), and handle it properly (class weights, SMOTE, or other resampling techniques).\nVisualization helps build intuition. Decision boundaries, ROC curves, confusion matrix heatmaps—these aren’t just pretty pictures. They help you understand what your model is actually doing. A model might have great accuracy but terrible decision boundaries. Visualization helps you see problems that metrics alone might hide.\nClassification is a core data science skill. You’ll use it constantly: predicting customer churn, detecting fraud, diagnosing diseases, filtering spam, recommending products, identifying images. The algorithms you’ve learned here are the foundation. Master them, understand their tradeoffs, and you’ll be equipped to tackle real classification problems.\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#practice-exercises",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#practice-exercises",
    "title": "Chapter 3: Classification Models",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nBuild and Compare Classifiers: Using the Titanic dataset (or another binary classification dataset), fit all five classifier types (Logistic Regression, Decision Tree, Random Forest, SVM, k-NN). Create confusion matrices for each and compare their precision, recall, and F1 scores. Which performs best? Why do you think that is?\nROC Curve Comparison: Using the same dataset from Exercise 1, plot ROC curves for all five models on the same figure. Which model has the highest AUC? Does this match the model with the best accuracy? Why or why not?\nHyperparameter Tuning: Take a Decision Tree classifier and experiment with different values of max_depth (try 1, 3, 5, 10, 20, None). Plot training and test accuracy vs depth. At what depth does overfitting become apparent? How can you tell?\nFeature Importance Analysis: Fit a Random Forest on a classification dataset with multiple features. Extract and visualize feature importances. Which features are most predictive? Now remove the top feature and retrain. How much does performance drop?\nClass Imbalance Challenge: Create an imbalanced dataset (90% class 0, 10% class 1) using make_classification. Fit a naive logistic regression and check its confusion matrix. Then try three approaches to handle the imbalance: class weights, random oversampling, and SMOTE. Which works best? Use F1 score to compare.\nThreshold Tuning: Using logistic regression with predict_proba(), manually try different classification thresholds (0.3, 0.5, 0.7, 0.9). For each threshold, compute precision and recall. Plot precision vs recall as you vary the threshold. Explain the tradeoff you observe."
  },
  {
    "objectID": "Textbook/Module-3-Classification/chapter-3-classification.html#additional-resources",
    "href": "Textbook/Module-3-Classification/chapter-3-classification.html#additional-resources",
    "title": "Chapter 3: Classification Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nScikit-learn Classification Documentation - Official docs for all classifiers\nUnderstanding the ROC Curve - Google’s ML crash course on ROC/AUC\nImbalanced-learn Documentation - Handling imbalanced datasets with Python\nConfusion Matrix Guide - Clear explanation of TP, FP, TN, FN\nPrecision vs Recall - When to optimize for which metric\nRandom Forests Explained - Original paper by Leo Breiman (creator of random forests)\nSVM Visualization - StatQuest video explaining SVMs visually"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html",
    "href": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html",
    "title": "Image Placeholders for Chapter 7: Neural Networks and Deep Learning",
    "section": "",
    "text": "This document describes the images needed for Chapter 7. These should be created and placed in this directory.\n\n\n\n\nLocation in chapter: Section 1.3 “What’s Actually Happening Inside?”\nDescription: A clear diagram showing a multi-layer perceptron (MLP) architecture with: - Input layer with 10 nodes (representing features) - First hidden layer with 5 nodes - Output layer with 1 node - Lines/edges connecting all nodes between layers showing full connectivity - Labels indicating “Input Layer”, “Hidden Layer”, “Output Layer” - Optional: Show activation function symbol (like “ReLU”) between layers - Clean, professional style suitable for educational content\nPurpose: Help students visualize how layers connect and how information flows through the network\nSuggested tools: Draw.io, Lucidchart, PowerPoint, or Python (matplotlib/networkx)\n\n\n\n\nLocation in chapter: Section 6.2 “Overfitting: When Your Model Memorizes”\nDescription: A line plot showing: - X-axis: Epochs (0-100) - Y-axis: Loss - Two lines: - Training loss (blue): Continuously decreasing, reaching very low values - Validation loss (red): Initially decreasing, then increasing after epoch ~30 - Clear divergence between the two curves showing the “overfitting gap” - Title: “Overfitting: Training vs. Validation Loss” - Legend clearly labeling both lines - Grid lines for readability\nPurpose: Show students what overfitting looks like in practice\nSuggested approach: Generate synthetic data or use actual training results\n\n\n\n\nLocation in chapter: Section 6.3 “Underfitting: When Your Model Is Too Simple”\nDescription: A line plot showing: - X-axis: Epochs (0-100) - Y-axis: Loss - Two lines: - Training loss (blue): High, plateaus quickly around epoch 10 - Validation loss (red): High, tracks training loss closely - Both curves plateau at a relatively high loss value (~3000-5000) - Very little gap between the two curves - Title: “Underfitting: Model Too Simple” - Legend clearly labeling both lines - Grid lines for readability\nPurpose: Show students what underfitting looks like—both losses high and plateaued\nSuggested approach: Train an overly simple model (e.g., 1-2 neurons) on complex data\n\n\n\n\nLocation in chapter: Section 6.4 “The Goldilocks Zone: Just Right”\nDescription: A line plot showing: - X-axis: Epochs (0-100) - Y-axis: Loss - Two lines: - Training loss (blue): Smoothly decreasing, plateaus at low value - Validation loss (red): Tracks training loss, plateaus at similar low value - Small but acceptable gap between curves - Both curves level off together around epoch 60-70 - Title: “Healthy Training: Good Generalization” - Legend clearly labeling both lines - Grid lines for readability\nPurpose: Show students what successful training looks like\nSuggested approach: Use the actual California housing training results from the chapter\n\n\n\n\nLocation in chapter: Section 7.1 “Dropout: Randomly Forgetting”\nDescription: Two side-by-side neural network diagrams: - Left diagram (“Normal Training”): - Shows a 3-layer network with all neurons active (filled circles) - All connections visible - Right diagram (“With Dropout”): - Same network structure - Some neurons shown as faded/crossed out (randomly disabled) - Connections to disabled neurons also faded/removed - Approximately 30-40% of neurons should appear disabled - Clear labels: “Normal Training” and “With Dropout (p=0.3)” - Arrow between diagrams showing this happens randomly during training\nPurpose: Visually explain how dropout works by randomly disabling neurons\nSuggested tools: Draw.io, PowerPoint, or illustration software\n\n\n\n\nLocation in chapter: Section 5.4 “Understanding the Loss Landscape”\nDescription: A 3D surface plot showing: - X and Y axes: Two weight parameters (w1, w2) - Z axis: Loss value (height) - Surface with hills and valleys representing loss landscape - A path/trajectory showing gradient descent steps moving toward a valley (minimum) - Color gradient from red (high loss) to blue (low loss) - One or more valley regions (local minima) - Viewpoint that clearly shows the 3D structure\nPurpose: Give students intuition about the optimization landscape\nSuggested approach: Use matplotlib or plotly to create 3D surface plot with synthetic function\n\n\n\n\n\nAll images should follow these guidelines:\n\nResolution: At least 1200px wide for diagrams, 1000px wide for plots\nFormat: PNG with transparent background where appropriate\nColors: Use colorblind-friendly palettes\n\nBlue (#3498db) for primary elements\nRed/Orange (#e74c3c) for secondary elements\nGreen (#2ecc71) for positive/correct elements\n\nFonts: Use clear, readable fonts (Arial, Helvetica, or similar)\nStyle: Professional but not overly formal—match the textbook’s casual tone\nLabels: All axes, lines, and elements should be clearly labeled\nSize: Large enough text that it’s readable when embedded in the document\n\n\n\n\nIf time is limited, create images in this priority order:\n\ntraining-curves-good.png - Most important for learning to diagnose models\ntraining-curves-overfitting.png - Critical for understanding overfitting\nmlp-architecture.png - Helps with basic understanding of structure\ndropout-visualization.png - Explains a complex concept visually\ntraining-curves-underfitting.png - Completes the training curves set\nloss-landscape.png - Nice to have, but less critical\n\n\n\n\n\nTraining curve plots can be generated directly from actual PyTorch training runs\nThe architecture diagrams need to be created manually or with diagramming tools\nAll plots should be generated with matplotlib using the seaborn style for consistency with other chapters\nConsider creating a Python script to generate all the training curve plots programmatically"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#required-images",
    "href": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#required-images",
    "title": "Image Placeholders for Chapter 7: Neural Networks and Deep Learning",
    "section": "",
    "text": "Location in chapter: Section 1.3 “What’s Actually Happening Inside?”\nDescription: A clear diagram showing a multi-layer perceptron (MLP) architecture with: - Input layer with 10 nodes (representing features) - First hidden layer with 5 nodes - Output layer with 1 node - Lines/edges connecting all nodes between layers showing full connectivity - Labels indicating “Input Layer”, “Hidden Layer”, “Output Layer” - Optional: Show activation function symbol (like “ReLU”) between layers - Clean, professional style suitable for educational content\nPurpose: Help students visualize how layers connect and how information flows through the network\nSuggested tools: Draw.io, Lucidchart, PowerPoint, or Python (matplotlib/networkx)\n\n\n\n\nLocation in chapter: Section 6.2 “Overfitting: When Your Model Memorizes”\nDescription: A line plot showing: - X-axis: Epochs (0-100) - Y-axis: Loss - Two lines: - Training loss (blue): Continuously decreasing, reaching very low values - Validation loss (red): Initially decreasing, then increasing after epoch ~30 - Clear divergence between the two curves showing the “overfitting gap” - Title: “Overfitting: Training vs. Validation Loss” - Legend clearly labeling both lines - Grid lines for readability\nPurpose: Show students what overfitting looks like in practice\nSuggested approach: Generate synthetic data or use actual training results\n\n\n\n\nLocation in chapter: Section 6.3 “Underfitting: When Your Model Is Too Simple”\nDescription: A line plot showing: - X-axis: Epochs (0-100) - Y-axis: Loss - Two lines: - Training loss (blue): High, plateaus quickly around epoch 10 - Validation loss (red): High, tracks training loss closely - Both curves plateau at a relatively high loss value (~3000-5000) - Very little gap between the two curves - Title: “Underfitting: Model Too Simple” - Legend clearly labeling both lines - Grid lines for readability\nPurpose: Show students what underfitting looks like—both losses high and plateaued\nSuggested approach: Train an overly simple model (e.g., 1-2 neurons) on complex data\n\n\n\n\nLocation in chapter: Section 6.4 “The Goldilocks Zone: Just Right”\nDescription: A line plot showing: - X-axis: Epochs (0-100) - Y-axis: Loss - Two lines: - Training loss (blue): Smoothly decreasing, plateaus at low value - Validation loss (red): Tracks training loss, plateaus at similar low value - Small but acceptable gap between curves - Both curves level off together around epoch 60-70 - Title: “Healthy Training: Good Generalization” - Legend clearly labeling both lines - Grid lines for readability\nPurpose: Show students what successful training looks like\nSuggested approach: Use the actual California housing training results from the chapter\n\n\n\n\nLocation in chapter: Section 7.1 “Dropout: Randomly Forgetting”\nDescription: Two side-by-side neural network diagrams: - Left diagram (“Normal Training”): - Shows a 3-layer network with all neurons active (filled circles) - All connections visible - Right diagram (“With Dropout”): - Same network structure - Some neurons shown as faded/crossed out (randomly disabled) - Connections to disabled neurons also faded/removed - Approximately 30-40% of neurons should appear disabled - Clear labels: “Normal Training” and “With Dropout (p=0.3)” - Arrow between diagrams showing this happens randomly during training\nPurpose: Visually explain how dropout works by randomly disabling neurons\nSuggested tools: Draw.io, PowerPoint, or illustration software\n\n\n\n\nLocation in chapter: Section 5.4 “Understanding the Loss Landscape”\nDescription: A 3D surface plot showing: - X and Y axes: Two weight parameters (w1, w2) - Z axis: Loss value (height) - Surface with hills and valleys representing loss landscape - A path/trajectory showing gradient descent steps moving toward a valley (minimum) - Color gradient from red (high loss) to blue (low loss) - One or more valley regions (local minima) - Viewpoint that clearly shows the 3D structure\nPurpose: Give students intuition about the optimization landscape\nSuggested approach: Use matplotlib or plotly to create 3D surface plot with synthetic function"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#image-style-guidelines",
    "href": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#image-style-guidelines",
    "title": "Image Placeholders for Chapter 7: Neural Networks and Deep Learning",
    "section": "",
    "text": "All images should follow these guidelines:\n\nResolution: At least 1200px wide for diagrams, 1000px wide for plots\nFormat: PNG with transparent background where appropriate\nColors: Use colorblind-friendly palettes\n\nBlue (#3498db) for primary elements\nRed/Orange (#e74c3c) for secondary elements\nGreen (#2ecc71) for positive/correct elements\n\nFonts: Use clear, readable fonts (Arial, Helvetica, or similar)\nStyle: Professional but not overly formal—match the textbook’s casual tone\nLabels: All axes, lines, and elements should be clearly labeled\nSize: Large enough text that it’s readable when embedded in the document"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#priority-order",
    "href": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#priority-order",
    "title": "Image Placeholders for Chapter 7: Neural Networks and Deep Learning",
    "section": "",
    "text": "If time is limited, create images in this priority order:\n\ntraining-curves-good.png - Most important for learning to diagnose models\ntraining-curves-overfitting.png - Critical for understanding overfitting\nmlp-architecture.png - Helps with basic understanding of structure\ndropout-visualization.png - Explains a complex concept visually\ntraining-curves-underfitting.png - Completes the training curves set\nloss-landscape.png - Nice to have, but less critical"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#notes",
    "href": "Textbook/Module-7-Neural-Networks/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Chapter 7: Neural Networks and Deep Learning",
    "section": "",
    "text": "Training curve plots can be generated directly from actual PyTorch training runs\nThe architecture diagrams need to be created manually or with diagramming tools\nAll plots should be generated with matplotlib using the seaborn style for consistency with other chapters\nConsider creating a Python script to generate all the training curve plots programmatically"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "",
    "text": "Related Assignments:\n\nModule 1 Homework\nModule 1 Quiz"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#module-resources",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#module-resources",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "",
    "text": "Related Assignments:\n\nModule 1 Homework\nModule 1 Quiz"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#introduction",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#introduction",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is EDA?\nYou can’t build effective models if you don’t understand your data. Exploratory Data Analysis (EDA) is the foundation of data science - it’s where you explore and understand your data before building models. In this first chapter we’ll learn the tools and techniques needed to do effective EDA.\n\n\nAI Coding Assistants\nThroughout this course we’ll learn both how to do work by hand, and how to use AI tools to scale our work. In today’s landscape, LLMs are exceptionally powerful, and can act as a “calculator for words”. However, it’s important to remember that LLMs are tools, not replacements for understanding.\nGiven the stochastic (unpredictable, non-deterministic) nature of LLMs, it’s important to learn how to work with them effectively as partners, rather than human replacements. It’s also important to learn how to do certain things by hand, such as calculating averages, making quick graphs, and selecting subsets of your data to analyze. If you’re reliant on LLMs to do every time step for you, you’ll take significantly longer to build models, and you’ll also be less likely to catch errors in your work. By learning to work with LLMs, instead of treating LLMs as a replacement for your own work, you’ll be able to build models more quickly and with greater accuracy."
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#getting-started-with-ai-coding-assistants",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#getting-started-with-ai-coding-assistants",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "1. Getting Started with AI Coding Assistants",
    "text": "1. Getting Started with AI Coding Assistants\n\n1.1 Why Use AI for Coding?\nImagine the following situation: You have a data set with five columns of data, one of which is a target variable you want to predict. For example, you might be trying to predict the price of a house based on its size, number of bedrooms, number of bathrooms, and location. You want to build a model to predict the price of a house based on these features. This data is small enough that you could do the EDA by hand, including: - Making graphs for each column - Calculating summary statistics - Checking for missing values - Checking for outliers - Checking for data quality issues\nHowever, what if your data had three hundred columns? Now you move well beyond what you could do by hand, and what could be analyzed by a human. This is where AI coding assistants come in.\nThis is exactly how we’ll use LLMs in this class. We’ll first learn the topics by hand at a foundational level and a small scale. Then, we’ll learn how to effectively use AI to scale our work.\n\n\n\n\n\n\nNote\n\n\n\nAI won’t take your job. Someone who knows how to use AI more effectively than you do will take your job. Be that person.\n\n\n\n\n1.2 Writing Effective Prompts\nLearning to write good prompts is like learning to communicate clearly with a colleague. You wouldn’t walk up to a coworker and say “make graph” and expect them to know exactly what you want. The same goes for AI coding assistants.\nHere are the key principles for effective prompting:\n1. Be specific about what you want\nDon’t just ask for “a visualization” when you know you want a histogram. Tell the AI exactly what type of output you’re looking for.\n2. Provide context\nLet the AI know what data you’re working with, what columns exist, and what you’re trying to accomplish. The more context you provide, the better the results.\n3. Start simple, then iterate\nDon’t try to get everything perfect in one prompt. Start with a basic request, see what you get, and refine from there. This is much faster than trying to write the “perfect” prompt.\n4. Specify the details that matter\nIf you care about axis labels, titles, colors, or specific parameter values, say so. If you don’t specify, the AI will make assumptions that might not match what you need.\nLet’s look at some examples. Here’s a poor prompt:\n\n“Make a graph of the data”\n\nWhat’s wrong with this? There’s no context about what data we’re working with, what type of graph we want, or what we’re trying to show. The AI has to guess everything.\nHere’s a better prompt:\n\n“I’m working with a housing dataset that has columns for price, square_feet, bedrooms, and location. Create a histogram showing the distribution of house prices. Use 30 bins, add a title ‘Distribution of House Prices’, and label the x-axis as ‘Price ($)’ and y-axis as ‘Frequency’. I’m working in Python and using matplotlib for graphing.”\n\nSee the difference? This prompt tells the AI:\n\nWhat data we have\nWhat visualization we want (histogram)\nWhat variable to plot (price)\nSpecific parameters (30 bins)\nFormatting details (title, axis labels)\n\nLet’s see this in action. Here’s what you might get from that better prompt:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming you have a DataFrame called 'housing_df'\nplt.figure(figsize=(10, 6))\nplt.hist(housing_df['price'], bins=30, edgecolor='black')\nplt.title('Distribution of House Prices')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.show()\nNotice that even with a good prompt, you might need to iterate. Maybe you look at this histogram and realize you want different bin sizes, or you want to add grid lines. That’s fine! Just refine your prompt: “Great, now add grid lines to make it easier to read the values.”\nThe key is that you’re building on what works, not starting from scratch each time. This iterative approach is exactly how you should work with AI assistants throughout this course.\n\n\n1.3 When to Use AI vs. Coding By Hand\nHow do you decide when to use AI, and when to do things by hand? You should start by asking yourself the following question: “How well do I understand my data and the task I’m trying to accomplish?”\nIf you understand it deeply and know exactly what you want to accomplish, then AI can be a great tool. However, if you’re just starting to understand your data, and/or the task isn’t clear, you should be working by hand.\nBecause LLMs take time to generate code, and because they often don’t do quite what you had in mind, at the early stages you can spend more time modifying/re-prompting an LLM than you would if you were doing things by hand. You want a quick feedback loop between your brain and your screen. As soon as a question pops into your head (“I wonder what the relationship is between…”) you should be able to answer it quickly. Changing your focus to work with an LLM will likely take too long, potentially result in more errors, and take you away from the task at hand.\n\n\n\n\n\n\nNote\n\n\n\nAs you become more comfortable with your data and the task at hand, you can start to use AI to scale your work. You can use AI to generate code for you, and then modify it to fit your needs. You can also use AI to generate code for you, and then modify it to fit your needs. When you look at your work and say “I know I’m on the right track, now I just need to do more”, then you should be using AI to scale your work.\n\n\n\n\n\nPlaceholder for AI workflow diagram"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#data-manipulation-with-pandas",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#data-manipulation-with-pandas",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "2. Data Manipulation with Pandas",
    "text": "2. Data Manipulation with Pandas\nPandas is the workhorse library for data manipulation in Python. If you’re going to be a data scientist, you need to know Pandas inside and out. The good news is that once you learn the basics, everything else follows a similar pattern.\n\n2.1 Loading Data\nLet’s start with the most basic task: loading data into Python. Most of the time, your data will be stored in a CSV (comma-separated values) file. Pandas makes this incredibly easy.\n\nimport pandas as pd\n\n# Load the California housing dataset\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Take a look at the first few rows\nhousing_df.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nWhat’s happening here? We’re importing the pandas library (always abbreviated as pd), then using the read_csv() function to load our data. The result is a DataFrame, which you can think of as a table with rows and columns, similar to an Excel spreadsheet.\nThe .head() method shows us the first 5 rows. This is always a good first step when you load data—you want to see what you’re working with.\nNow we can see our columns and get a sense of what the data looks like. This is California housing data with information about different districts. Notice that each row has an index (0, 1, 2, 3, 4) on the left side. Pandas automatically creates this for us.\nLet’s explore our data a bit more:\n\n# How many rows and columns do we have?\nprint(f'df shape: {housing_df.shape}\\n')\n\n# What are the column names?\nprint(f'df columns: {housing_df.columns}\\n')\n\n# What data types are in each column?\nprint('df dtypes:')\nprint(housing_df.dtypes)\n\ndf shape: (20640, 10)\n\ndf columns: Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income',\n       'median_house_value', 'ocean_proximity'],\n      dtype='object')\n\ndf dtypes:\nlongitude             float64\nlatitude              float64\nhousing_median_age    float64\ntotal_rooms           float64\ntotal_bedrooms        float64\npopulation            float64\nhouseholds            float64\nmedian_income         float64\nmedian_house_value    float64\nocean_proximity        object\ndtype: object\n\n\nThe .shape attribute tells us the dimensions of our DataFrame. The .columns attribute gives us the column names. And .dtypes tells us what type of data is in each column (integers, floating point numbers, strings, etc.).\nWhy do we care about data types? Because you can’t calculate the average of text data, and you can’t use numbers as categories. Making sure your data types are correct is a crucial early step in any analysis. Even things that seem like numbers may not be. For example, suppose that we had a row in our housing data which looked like this:\nmedian_house_value  total_rooms  ...\nUnknown             1500.0       ...\nIf we try to calculate the average house value, we’ll get an error:\nhousing_df['median_house_value'].mean()\nbecause the median_house_value column isn’t numerical (it has a string value in it).\n\n\n\n\n\n\nNote\n\n\n\nYou should always start by looking at your data using df.head()! This will help you catch any data type issues early. Following up with basic data checks (column names, data types, etc.) is also best practices.\n\n\n\n\n2.2 Selecting and Filtering Data\nNow that we have data loaded, we need to know how to slice it up and look at specific parts. This is where Pandas really shines.\nSelecting columns:\n\n# Select a single column (returns a Series)\nhouse_values = housing_df['median_house_value']\n\n# Select multiple columns\nmultiple_cols = ['median_house_value', 'ocean_proximity']\n\n# Select multiple columns (returns a DataFrame)\nvalue_and_location = housing_df[multiple_cols]\n\nNotice that, when selecting multiple columns, you should enclose them in a list, like ['median_house_value', 'ocean_proximity']. Here we did this in two steps: 1) Write down the columns we want, 2) Extract them from the data. However, there’s no reason you can’t do them in a single step:\nvalue_and_location = housing_df[\n    ['median_house_value', \n    'ocean_proximity']]\nSometimes this can be confusing, because people think that double brackets [['median_house_value', 'ocean_proximity']] are some kind of special syntax. They’re not. They’re just a list of column names.\nFiltering rows:\nHere’s where things get really useful. Let’s say we only want to look at districts near the bay. We’ll break this out into multiple steps, and then show how to combine it in a single step.\n\n# Check if each row is near the bay by creating a \"mask\"\nnear_bay_mask = housing_df['ocean_proximity'] == 'NEAR BAY'\n# Output: [True, True, True, True, True, False, False, ...]\n\n# Use this \"mask\" to filter the DataFrame\nnear_bay_districts = housing_df[near_bay_mask]\n\nnear_bay_districts.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nWhat’s going on here? The expression housing_df['ocean_proximity'] == 'NEAR BAY' creates a True/False (boolean) value for each row—True if the district is near the bay, False otherwise. Then we use that boolean mask to filter the DataFrame.\nHere we split it out into two steps, where we first created the mask near_bay_mask = housing_df['ocean_proximity'] == 'NEAR BAY', and then used that mask to filter the DataFrame housing_df[near_bay_mask]. This is a common pattern when filtering data. However, you can also write this in just one step:\nnear_bay_districts = housing_df[housing_df['ocean_proximity'] == 'NEAR BAY']\nOnce again, people sometimes find this confusing because of the housing_df inside the outer housing_df. However, as we’ve seen, all that’s going on is two steps: 1) Create a mask to select what you want (e.g. districts near the bay), 2) Use that mask to filter the DataFrame.\nLet’s try some more complex filters:\n\n# Districts with more than 1000 total rooms\nlarge_districts = housing_df[housing_df['total_rooms'] &gt; 1000]\n\n# Districts that are both near the bay AND have expensive houses (&gt; $400,000)\nexpensive_bay_area = housing_df[(housing_df['ocean_proximity'] == 'NEAR BAY') &\n                                 (housing_df['median_house_value'] &gt; 400000)]\n\n# Districts that are either very cheap OR very expensive\nextreme_values = housing_df[(housing_df['median_house_value'] &lt; 150000) |\n                             (housing_df['median_house_value'] &gt; 400000)]\n\nexpensive_bay_area.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n89\n-122.27\n37.80\n52.0\n249.0\n78.0\n396.0\n85.0\n1.2434\n500001.0\nNEAR BAY\n\n\n128\n-122.21\n37.83\n40.0\n4991.0\n674.0\n1616.0\n654.0\n7.5544\n411500.0\nNEAR BAY\n\n\n140\n-122.18\n37.81\n30.0\n292.0\n38.0\n126.0\n52.0\n6.3624\n483300.0\nNEAR BAY\n\n\n155\n-122.23\n37.81\n52.0\n2315.0\n292.0\n861.0\n258.0\n8.8793\n410300.0\nNEAR BAY\n\n\n\n\n\n\n\nNotice a few things: - We use & for “and” and | for “or” - We need parentheses around each condition when combining them - The comparison operators (&gt;, &lt;, ==) work just like you’d expect\nHere’s a practical example. Let’s say you’re analyzing housing affordability and you want to find all inland districts with median incomes above 5 and median house values under $300,000:\n\n# Your target districts\naffordable_inland = housing_df[(housing_df['ocean_proximity'] == 'INLAND') &\n                               (housing_df['median_income'] &gt; 5) &\n                               (housing_df['median_house_value'] &lt; 300000)]\n\nprint(f\"Found {len(affordable_inland)} districts matching criteria\")\n\naffordable_inland.head()\n\nFound 534 districts matching criteria\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n968\n-121.88\n37.67\n25.0\n2244.0\n301.0\n937.0\n324.0\n6.4524\n296900.0\nINLAND\n\n\n969\n-121.89\n37.67\n20.0\n2948.0\n471.0\n1181.0\n474.0\n6.0604\n247900.0\nINLAND\n\n\n975\n-121.87\n37.66\n52.0\n775.0\n134.0\n315.0\n123.0\n5.0677\n233300.0\nINLAND\n\n\n979\n-121.87\n37.67\n10.0\n4337.0\n800.0\n1813.0\n743.0\n5.5000\n247200.0\nINLAND\n\n\n981\n-121.85\n37.68\n4.0\n4719.0\n741.0\n1895.0\n742.0\n6.8132\n282500.0\nINLAND\n\n\n\n\n\n\n\nThis kind of filtering is something you’ll do constantly in data science. You’ll often want to analyze specific subsets of your data to understand patterns.\n\n\n\n\n\n\nNote\n\n\n\nBreaking complex queries, such as the affordable inland districts example above, into smaller steps can make it easier to understand and debug. Compare the query above with the same query written on a single line:\naffordable_inland = housing_df[(housing_df['ocean_proximity'] == 'INLAND') & (housing_df['median_income'] &gt; 5) & (housing_df['median_house_value'] &lt; 300000)]\nThis is much more difficult to read and debug!\n\n\n\n\n2.3 Data Cleaning Basics\nReal-world data is messy. You’ll have missing values, duplicates, wrong data types—all sorts of problems. Let’s learn how to spot and fix them.\nChecking for missing values:\n\n# How many missing values in each column?\nhousing_df.isnull().sum()\n\nlongitude               0\nlatitude                0\nhousing_median_age      0\ntotal_rooms             0\ntotal_bedrooms        207\npopulation              0\nhouseholds              0\nmedian_income           0\nmedian_house_value      0\nocean_proximity         0\ndtype: int64\n\n\nIn this command, housing_df.isnull() is returning True/False for each value in the DataFrame. Then we use .sum() to count up the number of True values in each column.\nSo we have 207 missing values in the total_bedrooms column. What do we do about this?\nOption 1: Drop rows with missing values\n# Drop any row that has at least one missing value\nhousing_clean = housing_df.dropna()\n\n# Drop only rows where a specific column (total_bedrooms) is missing a value\n# This is useful if this column is extremely important, and your analysis wouldn't make sense without it\nhousing_clean = housing_df.dropna(subset=['total_bedrooms'])\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with this approach! If you have many columns and missing values are scattered throughout, you might end up dropping most of your data. This is especially true if you’re working with data with lots of columns and/or columns which aren’t especially important. For example, a store may have a rewards number column. However, not all customers are reward customers. If we dropped all rows with any missing value, then all non-rewards customers would disappear from our data!\n\n\nOption 2: Fill missing values\n# Fill with a specific value (e.g. zero)\nhousing_df['total_bedrooms'] = housing_df['total_bedrooms'].fillna(0)\n\n# Fill missing values with the median (for numeric columns)\nmedian_bedrooms = housing_df['total_bedrooms'].median()\nhousing_df['total_bedrooms'] = housing_df['total_bedrooms'].fillna(median_bedrooms)\nWhich approach should you use? It depends on your data and your analysis. If you have lots of data and relatively few missing values, dropping might be fine. If missing values are common, you’ll need to fill them thoughtfully.\n\nBe careful with filling missing values! Sometimes a missing value is a signal that something is wrong. For example, if a house is missing a price, it might be because it’s not for sale. Or it may indicate something, such as a lack of sale price meaning the home wasn’t sold. When you fill missing values with other values, you are making assumptions about the data that may not be true and run the risk of corrupting your data.\n\nChecking for duplicates:\nWe can check for duplicates using the .duplicated() method. As with other methods we’ve seen today, it returns True/False values according to whether the row is a duplicate or not. By using .sum() we add up (i.e. count) the number of duplicate rows.\n\n# Are there any duplicate rows?\nprint(f'Duplicates: {housing_df.duplicated().sum()}')\n\n# Remove duplicates\nhousing_clean = housing_df.drop_duplicates()\n\n# Remove duplicates based on specific columns\n# (e.g., if you only care about unique combinations of location and ocean proximity)\nhousing_unique = housing_df.drop_duplicates(subset=['longitude', 'latitude', 'ocean_proximity'])\n\nprint(f'Duplicates after removing: {housing_unique.duplicated().sum()}')\n\nDuplicates: 0\nDuplicates after removing: 0\n\n\n\nDuplicates aren’t necessarily bad! If I’m a customer at a store and I return multiple times in the same day, depending on the info the store saves above me (e.g. name, date of visit), this may look like a duplicate, when it was really just me shopping multiple times.\n\nFixing data types:\nSometimes Pandas doesn’t guess the right data type when loading your data. For example, a column of numbers might be loaded as strings:\n\n# Check current data type\nprint(housing_df['median_house_value'].dtype)  # Shows float64\n\n# Convert a column to categorical (useful for columns with few unique values)\nhousing_df['ocean_proximity'] = housing_df['ocean_proximity'].astype('category')\n\nfloat64\n\n\nSample data cleaning workflow:\nHere’s a complete example of a basic data cleaning workflow:\n\nimport pandas as pd\n\n# Load the data\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Check for issues\nprint(\"Shape:\", housing_df.shape)\nprint(\"\\nMissing values:\")\nprint(housing_df.isnull().sum())\nprint(\"\\nDuplicates:\", housing_df.duplicated().sum())\nprint(\"\\nData types:\")\nprint(housing_df.dtypes)\n\n# Clean the data\nhousing_clean = housing_df.copy()  # Make a copy so we don't modify the original\n\n# Remove duplicates (if any)\nhousing_clean = housing_clean.drop_duplicates()\n\n# Fill missing total_bedrooms with the median\nmedian_bedrooms = housing_clean['total_bedrooms'].median()\nhousing_clean['total_bedrooms'] = housing_clean['total_bedrooms'].fillna(median_bedrooms)\n\n# Convert ocean_proximity to categorical\nhousing_clean['ocean_proximity'] = housing_clean['ocean_proximity'].astype('category')\n\n# Verify the cleaning worked\nprint(\"\\nAfter cleaning:\")\nprint(\"Shape:\", housing_clean.shape)\nprint(\"Missing values:\", housing_clean.isnull().sum().sum())\n\nShape: (20640, 10)\n\nMissing values:\nlongitude               0\nlatitude                0\nhousing_median_age      0\ntotal_rooms             0\ntotal_bedrooms        207\npopulation              0\nhouseholds              0\nmedian_income           0\nmedian_house_value      0\nocean_proximity         0\ndtype: int64\n\nDuplicates: 0\n\nData types:\nlongitude             float64\nlatitude              float64\nhousing_median_age    float64\ntotal_rooms           float64\ntotal_bedrooms        float64\npopulation            float64\nhouseholds            float64\nmedian_income         float64\nmedian_house_value    float64\nocean_proximity        object\ndtype: object\n\nAfter cleaning:\nShape: (20640, 10)\nMissing values: 0\n\n\nThis workflow checks for issues, fixes them, and verifies the fixes worked. You’ll use patterns like this constantly in your EDA work.\nThe key takeaway: always inspect your data before you analyze it. You need to know what you’re working with, spot problems early, and fix them before they cause issues down the line. Don’t just assume your data is clean—check!"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#statistical-summaries-and-data-profiling",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#statistical-summaries-and-data-profiling",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "3. Statistical Summaries and Data Profiling",
    "text": "3. Statistical Summaries and Data Profiling\nOnce your data is loaded and cleaned, the next step is understanding what it contains. You need to know the typical values, the spread of the data, and whether there are any weird patterns. This is where statistical summaries come in.\n\n3.1 Descriptive Statistics\nPandas makes it incredibly easy to get summary statistics for your data. Let’s start with the simplest approach:\n\n# Get summary statistics for all numeric columns\nhousing_df.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\nWhat is all this telling us? Let’s break it down row by row:\n\ncount: How many non-missing values exist\nmean: The average value\nstd: Standard deviation (how spread out the values are)\nmin: The smallest value\n25%: The 25th percentile (25% of values are below this)\n50%: The median (middle value when sorted)\n75%: The 75th percentile (75% of values are below this)\nmax: The largest value\n\nBut what do these numbers actually mean for our housing data? Let’s interpret:\nThe median house value is about $206,856 on average, with a standard deviation of $115,396. That’s a pretty big spread—prices vary a lot across California! The median (50th percentile) is $179,700, which is lower than the mean. What does that tell us? It suggests the distribution might be right-skewed, meaning there are some very expensive districts pulling the average up.\nFor total rooms, the typical district has around 2,636 rooms, with most districts falling between 1,448 and 3,148 rooms (the 25th to 75th percentile range). But look at that max value: 39,320 rooms! That’s a huge outlier that we might want to investigate.\nSometimes you want statistics for a single column:\n\n# Mean of a specific column\navg_value = housing_df['median_house_value'].mean()\nprint(f\"Average house value: ${avg_value:,.0f}\")\n\n# Median\nmedian_value = housing_df['median_house_value'].median()\nprint(f\"Median house value: ${median_value:,.0f}\")\n\n# Standard deviation\nvalue_std = housing_df['median_house_value'].std()\nprint(f\"House value standard deviation: ${value_std:,.0f}\")\n\n# Specific percentiles\npercentile_90 = housing_df['median_house_value'].quantile(0.90)\nprint(f\"90th percentile: ${percentile_90:,.0f}\")\n\nAverage house value: $206,856\nMedian house value: $179,700\nHouse value standard deviation: $115,396\n90th percentile: $376,600\n\n\nWhy would you care about the median vs. the mean? The median is more robust to outliers. If you have one very expensive district in an area of typical districts, the mean will be pulled way up, but the median will stay reasonable. When you’re trying to understand “typical” values, the median is often more useful.\nHere’s a practical example. Let’s say you want to understand house values in different proximity to the ocean:\n\n# Average value by ocean proximity\nvalue_by_proximity = housing_df.groupby('ocean_proximity')['median_house_value'].agg(['mean', 'median', 'count'])\nprint(value_by_proximity)\n\n                          mean    median  count\nocean_proximity                                \n&lt;1H OCEAN        240084.285464  214850.0   9136\nINLAND           124805.392001  108500.0   6551\nISLAND           380440.000000  414700.0      5\nNEAR BAY         259212.311790  233800.0   2290\nNEAR OCEAN       249433.977427  229450.0   2658\n\n\nNow we’re getting somewhere! Districts near the ocean (especially islands!) are significantly more expensive on average. This kind of breakdown is crucial for understanding your data—overall statistics can hide important patterns in subgroups.\n\n\n\n\n\n\nTip\n\n\n\nPandas can automatically do this (and more) for you with the .describe() method. Try running housing_df.describe() on your data and see what it gives you.\n\n\n\n\n3.2 Outliers\nOutliers are another important aspect of understanding your data. These are values that are far from the typical range. The typical way to identify outliers is to find data that is in the top and bottom 1% of the data.\n\n# Find the 1st and 99th percentiles\nbottom_1_percent = housing_df['median_house_value'].quantile(0.01)\ntop_1_percent = housing_df['median_house_value'].quantile(0.99)\n\nprint(f\"Bottom 1%: ${bottom_1_percent:,.0f}\")\nprint(f\"Top 1%: ${top_1_percent:,.0f}\")\n\n# Find potential outliers\noutliers = housing_df[(housing_df['median_house_value'] &lt; bottom_1_percent) |\n                       (housing_df['median_house_value'] &gt; top_1_percent)]\n\nprint(f\"\\nFound {len(outliers)} potential outliers\\n\")\n\nprint(outliers[['median_house_value', 'ocean_proximity', 'median_income']].head())\n\nBottom 1%: $50,000\nTop 1%: $500,001\n\nFound 199 potential outliers\n\n      median_house_value ocean_proximity  median_income\n1175             42500.0          INLAND         0.8252\n1176             45100.0          INLAND         1.0585\n1177             39400.0          INLAND         1.3289\n1181             41800.0          INLAND         2.2045\n1188             49000.0          INLAND         1.7727\n\n\nThere’s nothing particular about the top and bottom 1%. You could certainly look at the top and bottom 1%, or 2%, or 5%, or 10%, or any other percentage you want. The key is to look at the data and see if it makes sense. Primarily what you’re looking for is data that doesn’t make sense. If a district has very high house values, that could be totally reasonable, and of course there will always be some data in the top/bottom 1%. The problem is when those values don’t make sense. For example, if house values are shown as negative, that’s not real. You need to investigate before deciding what to do. Similarly, if values are impossibly high, that also doesn’t make sense.\n\n\n\n\n\n\nTip\n\n\n\nShould you remove outliers? Not automatically! They might be legitimate data (yes, some houses really are that expensive), or they might be errors (someone entered $5,000,000 instead of $500,000). You need to investigate before deciding what to do."
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#data-visualization-principles",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#data-visualization-principles",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "4. Data Visualization Principles",
    "text": "4. Data Visualization Principles\nNumbers and summary statistics are useful, but humans are visual creatures. A good visualization can reveal patterns that would take hours to find in tables of numbers. But here’s the thing: not all visualizations are created equal. You need to match the right type of plot to the question you’re asking.\n\n4.1 Choosing the Right Visualization\nThe type of visualization you choose depends on what you’re trying to show. Here are the most common scenarios:\nWant to see the distribution of a single variable? Use a histogram or box plot.\nWant to see the relationship between two numeric variables? Use a scatter plot.\nWant to compare values across categories? Use a bar plot.\nWant to see how something changes over time? Use a line plot.\nLet’s break these down with examples.\n\n4.1.1 Histograms\nHistograms show you how data is distributed. They’re perfect for answering questions like “Are house values normally distributed?” or “How many districts fall into different value ranges?”\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of median house values\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value', bins=30)\nplt.title('Distribution of Median House Values')\nplt.xlabel('Median House Value ($)')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\nThis shows you the shape of your data. Is it symmetric? Skewed? Are there outliers? All of this becomes immediately visible.\n\n\n4.1.2 Box plots\nBox plots are another way to visualize distributions, especially useful for comparing across groups:\n\n# Compare house value distributions across ocean proximity categories\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value')\nplt.title('House Values by Ocean Proximity')\nplt.xlabel('Ocean Proximity')\nplt.ylabel('Median House Value ($)')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe box shows the 25th to 75th percentile (the middle 50% of data), the line in the middle is the median, and the “whiskers” extend to show the range. Points beyond the whiskers are potential outliers.\n\n\n\n\n\n\nTip\n\n\n\nBox plots are excellent for quickly comparing distributions across multiple groups. You can instantly see which group has higher medians, more variability, or more outliers.\n\n\n\n\n4.1.3 Scatter plots\nScatter plots reveal relationships between two numeric variables:\n\n# Relationship between median income and house value\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n\n\n\n\n\n\n\n\nIf you see points trending upward from left to right, that’s a positive relationship—higher incomes are associated with more expensive houses. If points are scattered randomly, there’s no clear relationship.\nYou can add a third variable using color:\n\n# Add ocean proximity as color\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', alpha=0.6)\nplt.title('House Value vs. Median Income by Ocean Proximity')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n\n\n\n\n\n\n\n\nNow you can see if the relationship between income and house values differs by ocean proximity. Maybe coastal districts are consistently more expensive for the same income level.\n\n\n4.1.4 Bar plots\nBar plots compare values across categories:\n\n# Average house value by ocean proximity\nplt.figure(figsize=(10, 6))\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', estimator='mean')\nplt.title('Average House Value by Ocean Proximity')\nplt.xlabel('Ocean Proximity')\nplt.ylabel('Average House Value ($)')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis makes comparisons obvious at a glance. You can immediately see which proximity category has the highest average house values.\n\n\n\n\n\n\nNote\n\n\n\nThe key to choosing visualizations: think about what question you’re asking. “How is this variable distributed?” → histogram. “Is there a relationship between these two things?” → scatter plot. “Which group is highest?” → bar plot. Match the visualization to the question.\n\n\n\n\n\n4.2 Creating Visualizations with Seaborn\nSeaborn is built on top of matplotlib and makes creating statistical visualizations much easier. It has sensible defaults, nice-looking styles, and works seamlessly with pandas DataFrames.\nLet’s walk through the basic visualizations you’ll use constantly:\n\n4.2.1 Histograms\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Basic histogram\nsns.histplot(data=housing_df, x='median_house_value')\nplt.show()\n\n# With less bins\nsns.histplot(data=housing_df, x='median_house_value', bins=10)\nplt.show()\n\n# With KDE (smooth density curve) overlay\nsns.histplot(data=housing_df, x='median_house_value', kde=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bins parameter controls how many bars you see. Too few bins and you lose detail. Too many and it gets noisy. Usually 20-50 bins is a good starting point.\n\n\n4.2.2 Scatter plots\n\n# Basic scatter plot\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n# With color by category\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plots are your go-to tool for finding relationships. If you’re doing EDA and wondering whether two variables are related, make a scatter plot. It takes 2 seconds and can reveal patterns that summary statistics miss.\n\n\n\n\n\n\nTip\n\n\n\nWhen exploring relationships, always make a scatter plot first. You might have the same correlation coefficient but completely different patterns. The classic example is Anscombe’s quartet—four datasets with identical statistics but totally different patterns when plotted.\n\n\n\n\n4.2.3 Bar plots\n\n# Count of observations by category\nsns.countplot(data=housing_df, x='ocean_proximity')\nplt.xticks(rotation=45)\nplt.show()\n\n# Average value by category\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', estimator='mean')\nplt.xticks(rotation=45)\nplt.show()\n\n# Grouped bar plot - create age groups first\nhousing_df['age_group'] = pd.cut(housing_df['housing_median_age'],\n                                  bins=[0, 20, 35, 100],\n                                  labels=['New', 'Mid', 'Old'])\nsns.barplot(data=housing_df, x='ocean_proximity', y='median_house_value', hue='age_group')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBar plots are perfect for comparing across categories. The estimator parameter lets you choose what to show—mean, median, sum, etc.\n\n\n4.2.4 Box plots\n\n# Distribution by category\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value')\nplt.xticks(rotation=45)\nplt.show()\n\n# Horizontal (sometimes easier to read with long labels)\nsns.boxplot(data=housing_df, y='ocean_proximity', x='median_house_value')\nplt.show()\n\n# Multiple categories\nhousing_df['income_bracket'] = pd.cut(housing_df['median_income'],\n                                       bins=[0, 2.5, 4.5, 20],\n                                       labels=['Low', 'Medium', 'High'])\nsns.boxplot(data=housing_df, x='ocean_proximity', y='median_house_value', hue='income_bracket')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBox plots pack a lot of information into a small space. You see the median, the spread (IQR), and outliers all at once.\n\n\n4.2.5 Pair plots (bonus)\nWhen you want to see relationships between multiple variables at once:\n\n# Scatter plots for all numeric variables\nsns.pairplot(housing_df[['median_house_value', 'median_income', 'housing_median_age', 'total_rooms']])\nplt.show()\n\n# Color by category\nsns.pairplot(housing_df[['median_house_value', 'median_income', 'housing_median_age',\n                         'total_rooms', 'ocean_proximity']],\n             hue='ocean_proximity')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis creates a grid of scatter plots showing every pair of variables. It’s incredibly useful for initial exploration—you can spot all the relationships at once.\n\n\n\n4.3 Best Practices for Effective Visualizations\nA visualization without labels is just decorative art. You need to make your plots readable and informative. Here are the key principles:\n1. Always add titles and axis labels\n\nprint('Bad: no labels')\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\nprint('Good: clear labels')\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.title('House Value vs. Median Income', fontsize=14, fontweight='bold')\nplt.xlabel('Median Income (in $10k)', fontsize=12)\nplt.ylabel('Median House Value ($)', fontsize=12)\nplt.show()\n\nBad: no labels\n\n\n\n\n\n\n\n\n\nGood: clear labels\n\n\n\n\n\n\n\n\n\nYour title should answer “What am I looking at?” Your axis labels should include units where relevant (dollars, income brackets, etc.).\n2. Make the plot big enough to read\n\n# Too small (default size is often cramped)\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n# Better: specify figure size\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figsize parameter takes (width, height) in inches. A good starting point is (10, 6) for most plots.\n3. Use color meaningfully\nColor should convey information, not just look pretty:\n\n# Color by category to show groups\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', palette='Set2')  # Use a colorblind-friendly palette\nplt.show()\n\n\n\n\n\n\n\n\nSeaborn has many built-in palettes: ‘Set2’, ‘colorblind’, ‘husl’, etc. Choose one that’s easy to distinguish and colorblind-safe.\n\n\n\n\n\n\nWarning\n\n\n\nAvoid using red and green together as your only color distinction. About 8% of men have some form of color blindness that makes red-green distinctions difficult. Use colorblind-friendly palettes like ‘colorblind’ or ‘Set2’.\n\n\n4. Add grid lines for easier reading\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.grid(True, alpha=0.3)  # alpha makes it subtle\nplt.title('House Value vs. Median Income')\nplt.xlabel('Median Income (in $10k)')\nplt.ylabel('Median House Value ($)')\nplt.show()\n\n\n\n\n\n\n\n\nSubtle grid lines make it easier to read values from your plot.\n5. Format numbers appropriately\nLarge numbers benefit from formatting:\n\nimport matplotlib.ticker as mtick\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value')\n\n# Format y-axis as integers\nplt.gca().yaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))\n\n# Format x-axis as dollars\nplt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))\n\nplt.title('Distribution of Median House Values')\nplt.xlabel('Median House Value')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\nThis adds dollar signs and comma separators, making the plot much easier to read.\n6. Adjust plot limits when needed\nSometimes outliers make it hard to see the main pattern:\n\n# If you have extreme outliers\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value')\nplt.xlim(0, 10)  # Focus on median incomes under 10\nplt.ylim(0, 500000)  # Focus on house values under $500k\nplt.show()\n\n\n\n\n\n\n\n\nUse this carefully—you’re choosing to hide data. But sometimes it’s necessary to see the pattern in the majority of your data.\nComplete example with all best practices:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Set the style for all plots\nsns.set_style('whitegrid')\n\n# Create the plot\nplt.figure(figsize=(12, 7))\nsns.scatterplot(data=housing_df, x='median_income', y='median_house_value',\n                hue='ocean_proximity', palette='Set2', s=100, alpha=0.6)\n\n# Add labels and title\nplt.title('House Value vs. Median Income by Ocean Proximity',\n          fontsize=16, fontweight='bold', pad=20)\nplt.xlabel('Median Income (in $10k)', fontsize=13)\nplt.ylabel('Median House Value ($)', fontsize=13)\n\n# Format the y-axis as currency\nplt.gca().yaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))\n\n# Adjust legend\nplt.legend(title='Ocean Proximity', fontsize=11, title_fontsize=12)\n\n# Add subtle grid\nplt.grid(True, alpha=0.3)\n\n# Tight layout to prevent label cutoff\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis creates a professional-looking plot that’s easy to read and interpret.\n\n\n\n\n\n\nTip\n\n\n\nSave your well-formatted plotting code as a template. When you find styling that works well, reuse it. You don’t want to be tweaking font sizes every time you make a plot.\n\n\nThe key takeaway: visualizations are tools for understanding, not just decoration. Choose the right type of plot for your question, label everything clearly, and make it easy to read. When someone looks at your visualization, they should immediately understand what they’re seeing and why it matters."
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#testing-your-data-analysis-code",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#testing-your-data-analysis-code",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "5. Testing Your Data Analysis Code",
    "text": "5. Testing Your Data Analysis Code\n\n5.1 Why Testing Matters\nHere’s a scenario that happens all the time: You write some code to filter your data, run your analysis, get results, and present them to your team. Then someone asks “wait, why are there only 5 data points?”. Your entire analysis is now useless.\nTesting catches these errors before they become embarrassing mistakes. And in data science, testing doesn’t have to be complicated. Simple checks can save you from major headaches.\nThe goal is simple: make sure your code is doing what you think it’s doing. If you filter data, did you actually get the rows you expected? If you drop missing values, did the count of missing values actually drop? These are the kinds of things you should be checking.\n\n\n5.2 Using assert Statements\nThe simplest way to test your data analysis code is with assert statements. An assert is like a sanity check: you state what should be true, and if it’s not, Python stops and tells you something is wrong.\nHere’s the basic idea:\n\n# This will pass (no error)\nassert 5 &gt; 3\n\n# This will fail and raise an error\nassert 2 &gt; 3\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[32], line 5\n      2 assert 5 &gt; 3\n      4 # This will fail and raise an error\n----&gt; 5 assert 2 &gt; 3\n\nAssertionError: \n\n\n\nNotice that when an assertion fails, your code stops. That’s the point! You want to know immediately when something is wrong.\n\n\n\n\n\n\nTip\n\n\n\nYou can (and should!) add a message to the assert statement to help you understand what went wrong.\n\nassert 2 &gt; 3, \"Two is not greater than three!\"\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[33], line 1\n----&gt; 1 assert 2 &gt; 3, \"Two is not greater than three!\"\n\nAssertionError: Two is not greater than three!\n\n\n\n\n\nCommon mistake: Case sensitivity\nLet’s say you want to filter for districts near the bay. The code below looks good at first glance:\n\n# Intentional typo: \"near bay\" instead of \"NEAR BAY\"\nnear_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n\nHowever, later you decide to print out how many districts are near the bay.\n\nprint(f\"Found {len(near_bay)} districts near the bay\")\n\nFound 0 districts near the bay\n\n\nZero districts! But your code didn’t crash, so you might not notice. This is where asserts help:\n\n# Filter the data\nnear_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n\n# Assert that we got some results\nassert len(near_bay) &gt; 0, \"No districts found! Check your filter condition\"\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[36], line 5\n      2 near_bay = housing_df[housing_df['ocean_proximity'] == 'near bay']\n      4 # Assert that we got some results\n----&gt; 5 assert len(near_bay) &gt; 0, \"No districts found! Check your filter condition\"\n\nAssertionError: No districts found! Check your filter condition\n\n\n\nNow the code fails loudly, and the error message tells you exactly what went wrong. The problem is that the values in that column are capitalized, and you forgot to capitalize them. By using an assert you immediately catch the problem and can investigate it, before it becomes a bigger issue.\nChecking data after operations\nUse asserts after every major operation to verify it did what you expected:\n\n# Start with the full dataset\nprint(f\"Starting with {len(housing_df)} rows\")\n\n# Drop missing values\nhousing_clean = housing_df.dropna()\n\n# Assert that we actually dropped some rows (since we know there were missing values)\nassert len(housing_clean) &lt; len(housing_df), \"Expected to drop some rows with missing values\"\n\nprint(f\"After dropping missing values: {len(housing_clean)} rows\")\n\nStarting with 20640 rows\nAfter dropping missing values: 20433 rows\n\n\nChecking ranges and values\nAfter computing statistics, verify they make sense:\n\n# Calculate median house value\nmedian_value = housing_df['median_house_value'].median()\nprint(f\"Median house value: ${median_value:,.0f}\")\n\n# Sanity checks\nassert median_value &gt; 0, \"House values should be positive\"\nassert median_value &lt; 10000000, \"Median seems unrealistically high - check your data\"\n\nMedian house value: $179,700\n\n\nChecking for duplicates\nAfter removing duplicates, verify they’re actually gone:\n\n# Remove duplicates\nhousing_no_dupes = housing_df.drop_duplicates()\n\n# Verify no duplicates remain\nassert housing_no_dupes.duplicated().sum() == 0, \"Duplicates still exist after dropping\"\n\nprint(f\"Successfully removed duplicates. {len(housing_df) - len(housing_no_dupes)} rows removed\")\n\nSuccessfully removed duplicates. 0 rows removed\n\n\nA practical example: Complete data cleaning with asserts\nHere’s how you might use asserts throughout a real data cleaning workflow:\n\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('../data/housing.csv')\n\n# Assert basic structure\nassert len(df) &gt; 0, \"DataFrame is empty\"\nassert 'median_house_value' in df.columns, \"Missing expected column\"\n\nprint(f\"Starting with {len(df)} rows\")\n\n# Check for missing values before cleaning\nmissing_before = df.isnull().sum().sum()\nprint(f\"Missing values before cleaning: {missing_before}\")\n\n# Fill missing values\ndf_clean = df.copy()\nmedian_bedrooms = df_clean['total_bedrooms'].median()\ndf_clean['total_bedrooms'] = df_clean['total_bedrooms'].fillna(median_bedrooms)\n\n# Assert that we reduced missing values\nmissing_after = df_clean.isnull().sum().sum()\nassert missing_after &lt; missing_before, \"Should have fewer missing values after cleaning\"\n\nprint(f\"Missing values after cleaning: {missing_after}\")\n\n# Filter for expensive houses\nexpensive = df_clean[df_clean['median_house_value'] &gt; 400000]\n\n# Assert we got some results\nassert len(expensive) &gt; 0, \"No expensive houses found - check threshold\"\nassert len(expensive) &lt; len(df_clean), \"All houses are expensive? Something is wrong\"\n\nprint(f\"Found {len(expensive)} expensive districts\")\n\nStarting with 20640 rows\nMissing values before cleaning: 207\nMissing values after cleaning: 0\nFound 1744 expensive districts\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse assert statements liberally during development. They’re like guardrails that keep your analysis on track. Once your code is working correctly, you can leave them in—they serve as documentation of what your code expects to be true.\n\n\nWhat about “real” testing?\nAssert statements are great for quick checks during data exploration. In later chapters, we’ll learn about proper unit tests, which are essential when you’re building production data pipelines or reusable analysis code. But for now, assert statements will catch 90% of your mistakes with 10% of the effort.\nThe key principle: Don’t trust your code blindly. Check that it’s doing what you think it’s doing. Your future self (and your team) will thank you."
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#scaling-eda-with-ai",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#scaling-eda-with-ai",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "6. Scaling EDA with AI",
    "text": "6. Scaling EDA with AI\nYou now know how to do EDA by hand: load data, clean it, compute statistics, make visualizations, test your assumptions. This is crucial foundational knowledge. But here’s the thing: what if you have 50 columns instead of 10? What if you want to compare patterns across 20 different categories? What if you want to check 100 different hypotheses?\nThis is where AI coding assistants shine. Once you understand what you’re doing and why, you can use AI to scale your work massively. The key is that you already know what to look for, so you can verify the AI’s output and catch mistakes.\n\n6.1 Automating Exploratory Visualizations\nLet’s say you want to create histograms for every numeric column in your dataset. Doing this by hand would be tedious:\n# By hand: repetitive and error-prone\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_house_value')\nplt.title('Distribution of Median House Value')\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=housing_df, x='median_income')\nplt.title('Distribution of Median Income')\nplt.show()\n\n# ... repeat for every column ...\nInstead, you can use AI to generate this systematically. Here’s an effective prompt:\n\n“I have a pandas DataFrame called housing_df with these numeric columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value.\nWrite Python code using Pandas and Seaborn that creates a histogram for each numeric column. Use seaborn’s histplot, make each figure 10x6 inches, add appropriate titles (formatted nicely with spaces instead of underscores), and save each plot as a PNG file named after the column.\nUse a loop to avoid repetitive code.”\n\nThe AI will generate something like this:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# List of numeric columns\nnumeric_cols = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n                'total_bedrooms', 'population', 'households', 'median_income',\n                'median_house_value']\n\nfor col in numeric_cols:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=housing_df, x=col, bins=30)\n\n    # Format title nicely\n    title = col.replace('_', ' ').title()\n    plt.title(f'Distribution of {title}')\n    plt.xlabel(title)\n    plt.ylabel('Count')\n\n    # Save the plot\n    plt.savefig(f'{col}_histogram.png', dpi=300, bbox_inches='tight')\n    plt.close()  # Close to avoid memory issues\n\nprint(f\"Created {len(numeric_cols)} histograms\")\n\n\n\n\n\n\nNote\n\n\n\nNotice how the prompt was specific about what you wanted. You told the AI the column names, the figure size, the formatting requirements, and the output format. Good prompts get good results.\n\n\nComparing distributions across groups\nWant to see how house values differ across all ocean proximity categories? That’s a perfect job for AI:\n\n“Using my housing_df DataFrame with columns ‘ocean_proximity’ and ‘median_house_value’, create box plots comparing house values across all ocean proximity categories. Make the plot readable with rotated x-axis labels. Also calculate summary statistics (mean, median, count) for each category and save them all in a dataframe.”\n\nCreating a comprehensive visualization suite\nFor a complete EDA report, you might prompt:\n\n“Create a comprehensive visualization suite for the California housing dataset. For each numeric variable, create: 1. A histogram showing the distribution 2. A box plot showing the distribution by ocean_proximity 3. A scatter plot against median_house_value (the target variable)\nOrganize these into subplots so each variable gets a row with 3 plots. Save the output as a single multi-page PDF.”\n\nThe AI will generate code with subplots, proper layout management, and file handling—saving you hours of work.\n\n\n\n\n\n\nNote\n\n\n\nNotice how, in all of these cases, you told the AI precisely what to do. In addition, you used the things you have learned this chapter to prompt it precisely. We do not want to give AI overly vague instructions like “investigate my data” or “find patterns”. It’s our job as the data scientist to guide the AI to help us, not to turn over the entire thought process to it.\n\n\n\n\n6.2 Testing Across Data Subsets\nImagine you want to analyze how the relationship between income and house values differs across different regions. By hand, you’d need to:\n\nFilter for each ocean proximity category\nCompute correlations for each\nCreate scatter plots for each\nCompare the results\n\nThat’s tedious. Instead, prompt the AI:\n\n“For the housing_df DataFrame, I want to analyze how the relationship between ‘median_income’ and ‘median_house_value’ varies by ‘ocean_proximity’.\nFor each ocean proximity category:\n\nCalculate the correlation coefficient\nCreate a scatter plot with a regression line\nPrint summary statistics\n\nStore the summary statistics and correlation coefficients in a dataframe. Save the scatter plots with a filename that includes the ocean proximity category.”\n\n\n\n\n\n\n\nNote\n\n\n\nOnce again, we give the AI precise instructions. Think of it as an assistant who is eager to help, but needs careful direction at every step.\n\n\nThe AI will generate code that loops through each category, performs the analysis, and presents it clearly. You then review the results with your understanding of the data, catching any issues.\nSystematic subset analysis\nYou can scale this to any grouping:\n# AI-generated code for systematic analysis\nfor proximity in housing_df['ocean_proximity'].unique():\n    subset = housing_df[housing_df['ocean_proximity'] == proximity]\n\n    print(f\"\\n=== Analysis for {proximity} ===\")\n    print(f\"Number of districts: {len(subset)}\")\n    print(f\"Median house value: ${subset['median_house_value'].median():,.0f}\")\n    print(f\"Median income: ${subset['median_income'].median():.2f}\")\n\n    # Correlation\n    corr = subset['median_income'].corr(subset['median_house_value'])\n    print(f\"Income-Value correlation: {corr:.3f}\")\n\n    # Visualization\n    plt.figure(figsize=(8, 5))\n    sns.regplot(data=subset, x='median_income', y='median_house_value',\n                scatter_kws={'alpha':0.5})\n    plt.title(f'Income vs Value: {proximity}')\n    plt.show()\nThe point isn’t that you couldn’t write this code—you absolutely can now! The point is that AI lets you analyze 10 subsets as easily as 1 subset. You’re scaling your analysis, not replacing your understanding.\n\n\n\n\n\n\nWarning\n\n\n\nAI is excellent at generating repetitive analysis code, but it cannot make judgment calls about your data. You need to understand what the analyses mean, whether the results make sense, and what actions to take. AI scales your work; it doesn’t replace your thinking.\n\n\nThe key pattern: Do it by hand first, scale with AI second. Learn to check one column carefully, then use AI to check all 50 columns. Learn to analyze one subset, then scale to all subsets. Your understanding guides the AI; the AI amplifies your productivity."
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#the-iterative-nature-of-eda",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#the-iterative-nature-of-eda",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "7. The Iterative Nature of EDA",
    "text": "7. The Iterative Nature of EDA\n\n7.1 EDA as a Process, Not a Checklist\nHere’s a mistake beginners make: they treat EDA like a todo list. Load data ✓, check missing values ✓, make histogram ✓, done!\nReal EDA doesn’t work like that. EDA is a conversation with your data. You ask a question, look at the answer, and that answer suggests new questions. You follow the thread wherever it leads.\nLet’s see this in action with our housing data:\nStarting question: “What affects house values?”\n\n# Initial exploration: which variables correlate with house value?\nnumeric_cols = ['housing_median_age', 'total_rooms', 'total_bedrooms',\n                'population', 'households', 'median_income']\n\nfor col in numeric_cols:\n    corr = housing_df[col].corr(housing_df['median_house_value'])\n    print(f\"{col:20s}: {corr:6.3f}\")\n\nhousing_median_age  :  0.106\ntotal_rooms         :  0.134\ntotal_bedrooms      :  0.050\npopulation          : -0.025\nhouseholds          :  0.066\nmedian_income       :  0.688\n\n\nInsight: Median income has the strongest correlation (0.688). That makes sense—wealthier areas have more expensive houses.\nNew question: “Does this relationship hold across all regions?”\n\n# Check if income-value relationship varies by ocean proximity\nfor proximity in housing_df['ocean_proximity'].unique():\n    subset = housing_df[housing_df['ocean_proximity'] == proximity]\n    corr = subset['median_income'].corr(subset['median_house_value'])\n    print(f\"{proximity:15s}: {corr:.3f} ({len(subset):5d} districts)\")\n\nNEAR BAY       : 0.633 ( 2290 districts)\n&lt;1H OCEAN      : 0.679 ( 9136 districts)\nINLAND         : 0.699 ( 6551 districts)\nNEAR OCEAN     : 0.704 ( 2658 districts)\nISLAND         : -0.540 (    5 districts)\n\n\nInsight: The correlation is weakest for islands (only 5 districts though—small sample!). Strongest for inland areas.\nNew question: “Why is the island correlation so different? Let’s look at those islands.”\n\n# Investigate the island districts\nislands = housing_df[housing_df['ocean_proximity'] == 'ISLAND']\nprint(islands[['median_income', 'median_house_value', 'housing_median_age']].to_string())\n\n      median_income  median_house_value  housing_median_age\n8314         2.1579            450000.0                27.0\n8315         2.8333            414700.0                52.0\n8316         3.3906            300000.0                52.0\n8317         2.7361            450000.0                52.0\n8318         2.6042            287500.0                29.0\n\n\nInsight: Only 5 island districts total, with a narrow range of incomes. The low correlation might just be due to small sample size and limited variation.\nSee what happened? One question led to another, which led to another. We started with “what affects house values?” and ended up investigating the peculiarities of island properties. This is normal. This is good.\nEDA is not linear. You don’t march through a predetermined set of steps. You explore, find something interesting, investigate it, find something else interesting, investigate that. Some paths lead nowhere. Some paths lead to important discoveries. You won’t know until you follow them.\n\n\n\n\n\n\nNote\n\n\n\nThe best data scientists are curious. When they see something unusual in the data, they don’t just note it and move on—they dig deeper. Why is this unusual? What does it mean? What else should I check?\n\n\n\n\n7.2 Documenting Your Findings\nHere’s a problem: you do all this exploration, find interesting patterns, make decisions about the data, and then… a week later you can’t remember what you found or why you made those decisions.\nDocumentation isn’t just for other people. It’s for future you.\nWhat to document:\n\nInteresting patterns you discovered\n\n“House values show clear geographic clustering—coastal areas are consistently more expensive”\n“Total_bedrooms has 207 missing values (1%), appears random, not systematically related to other variables”\n\nDecisions you made and why\n\n“Filled missing bedrooms with median rather than dropping rows—only 1% missing, no pattern suggesting systematic missingness”\n“Kept all outliers in house values—verified these are real (e.g., expensive areas like San Francisco), not data errors”\n\nQuestions that need follow-up\n\n“Island category has only 5 districts—may need to group with NEAR_OCEAN for modeling”\n“Longitude/latitude show clear clustering—should we create region categories?”\n\nHypotheses for modeling\n\n“Median_income will likely be the strongest predictor”\n“May need to include interactions between location and other features”\n\n\nHow to document:\nThe simplest approach is to use markdown cells in your Jupyter notebook or Quarto document:\n# Bad: No context\nhousing_df = housing_df.dropna()\n\n# Better: Explain your reasoning\n# Dropping 207 rows with missing total_bedrooms (1% of data)\n# Missing values appear random (no correlation with other variables)\n# Median imputation would be valid alternative, but with 20k+ rows,\n# dropping 1% has minimal impact\nhousing_df = housing_df.dropna()\nEven better, use markdown cells to write full explanations:\n## Data Cleaning Decisions\n\n### Missing Values\n- `total_bedrooms`: 207 missing (1%)\n  - Dropped these rows rather than imputing\n  - Rationale: Small percentage, appears random, large dataset can afford the loss\n  - Alternative considered: median imputation (would change results minimally)\n\n### Outliers\n- Kept all outliers in `median_house_value`\n  - Verified these are real (checked against known expensive areas)\n  - Removing would bias model toward typical houses\n  - Will monitor model performance on these points\nWhy this matters:\nThree months from now, someone (maybe you!) will ask “why did we drop those missing values instead of imputing them?” If you documented your reasoning, you can answer immediately. If you didn’t, you’ll have to re-do the analysis to remember.\n\n\n\n\n\n\nTip\n\n\n\nDocument as you go, not at the end. When you make a decision, write down why while it’s fresh in your mind. Your future self will thank you."
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#summary",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#summary",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Summary",
    "text": "Summary\nYou’ve now learned the foundations of exploratory data analysis. Let’s recap the key points:\nThe Fundamentals: - Data manipulation with Pandas: loading, selecting, filtering, and cleaning data - Statistical summaries: understanding distributions, detecting outliers, and computing meaningful statistics - Visualization: choosing the right plots, creating them with Seaborn, and following best practices for readability - Testing: using assert statements to catch errors early and verify your assumptions\nWorking with AI: - Write specific, detailed prompts that include context and requirements - Start simple and iterate—don’t try to get everything perfect in one prompt - Use AI to scale your work after you understand the fundamentals by hand - Always verify AI-generated code and results with your own understanding\nThe EDA Mindset: - EDA is iterative, not linear—follow insights wherever they lead - Document your findings and decisions as you go - Let EDA guide your modeling choices—every decision should be informed by what you learned about the data - Stay curious and ask “why?” when you see something interesting\nThe Core Philosophy:\nEDA is foundational to everything else in data science. You can’t build good models without understanding your data. AI is a powerful tool for scaling your work, but it’s not a replacement for understanding. Learn to do things by hand first, then use AI to do them at scale.\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#practice-exercises",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#practice-exercises",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nComplete these exercises using the California housing dataset to reinforce what you’ve learned:\n1. Prompt Engineering Practice\nWrite three prompts for an AI coding assistant to perform the following tasks with the housing dataset. Make your prompts specific and detailed:\n\nCreate a comprehensive correlation analysis between all numeric features and median_house_value, sorted by correlation strength, with a visualization\nAnalyze how housing age affects house values across different ocean proximity categories, including statistical tests and visualizations\nGenerate a data quality report that identifies potential issues (outliers, missing values, suspicious patterns) and suggests remediation strategies\n\n2. Visualization Design\nFor each scenario below, choose the appropriate visualization type and explain why:\n\nYou want to show how the distribution of house ages differs between coastal and inland areas\nYou want to investigate whether there’s a relationship between population density (population/households) and median income\nYou want to compare the median house values across all five ocean proximity categories\nYou want to show how house values have changed across different housing ages, looking for trends\n\n3. Data Quality Investigation\nUsing assert statements and pandas methods:\n\nWrite code to verify that after cleaning, no column has more than 5% missing values\nCreate assertions to check that all house values are positive and less than $10 million\nWrite a test to ensure that total_bedrooms is always less than or equal to total_rooms\nVerify that each ocean_proximity category has at least 100 samples\n\n4. Complete EDA Workflow\nPerform a complete exploratory data analysis on a subset of the housing data:\n\nFilter for districts with median_income between 3 and 6\nDocument three interesting patterns you discover in this subset\nCreate at least three visualizations that reveal insights\nWrite down two hypotheses about what would make a good predictive model for this subset\nUse assert statements to verify your filtering worked correctly\n\n5. Scaling with AI\nWithout actually running the code, write detailed prompts that would generate:\n\nCode to create scatter plots of median_house_value vs. each numeric feature, all on a single figure with subplots\nA function that calculates and reports summary statistics (mean, median, std, min, max) for any numeric column, grouped by any categorical column\nCode that identifies all pairs of numeric features with correlation &gt; 0.7 and creates scatter plots for each pair"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/chapter-1-eda.html#additional-resources",
    "href": "Textbook/Module 1 - EDA/chapter-1-eda.html#additional-resources",
    "title": "Chapter 1: Exploratory Data Analysis and AI-Assisted Coding",
    "section": "Additional Resources",
    "text": "Additional Resources\nPandas Documentation: - Pandas User Guide - Comprehensive guide to Pandas functionality - 10 Minutes to Pandas - Quick-start tutorial - Pandas Cookbook - Common recipes for data manipulation\nVisualization: - Seaborn Gallery - Examples of every Seaborn plot type - Matplotlib Tutorials - Deep dive into matplotlib - From Data to Viz - Guide to choosing the right visualization\nData Quality and Cleaning: - Data Cleaning with Python - Comprehensive guide - Pandas Missing Data - Official guide to handling missing values\nWorking with AI: - Effective Prompting Guide - Principles for writing good prompts - Gemini for Developers - Google’s AI documentation for developers\nStatistics and EDA: - Think Stats (Free Book) - Statistical thinking for programmers - Statistics for Hackers - Practical statistical approaches\nPractice Datasets: - UCI Machine Learning Repository - Hundreds of datasets for practice - Kaggle Datasets - Real-world datasets with community analyses - Data.gov - US government public datasets"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/images/placeholder-info.html",
    "href": "Textbook/Module 1 - EDA/images/placeholder-info.html",
    "title": "Image Placeholders for Module 1 - EDA",
    "section": "",
    "text": "Description: Diagram showing the workflow of using AI coding assistants Suggested content: - Box 1: “Understand the problem/task” - Box 2: “Write specific prompt” - Box 3: “AI generates code” - Box 4: “Review and verify output” - Box 5: “Test and iterate” - Arrows connecting the boxes\nDimensions: ~800x400px\n\n\n\n\nDescription: Grid of example visualizations showing good vs. poor practices Suggested content: - 2x2 grid showing: - Top left: Poorly labeled histogram - Top right: Well-labeled histogram with title, axis labels - Bottom left: Scatter plot with unclear colors - Bottom right: Scatter plot with clear legend and labels\nDimensions: ~1000x800px\n\n\n\n\n\n\nImages should be placed in this /images/ folder\nUse .png format for diagrams and screenshots\nUse descriptive filenames\nEnsure images are readable when embedded in Quarto HTML output"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/images/placeholder-info.html#required-images",
    "href": "Textbook/Module 1 - EDA/images/placeholder-info.html#required-images",
    "title": "Image Placeholders for Module 1 - EDA",
    "section": "",
    "text": "Description: Diagram showing the workflow of using AI coding assistants Suggested content: - Box 1: “Understand the problem/task” - Box 2: “Write specific prompt” - Box 3: “AI generates code” - Box 4: “Review and verify output” - Box 5: “Test and iterate” - Arrows connecting the boxes\nDimensions: ~800x400px\n\n\n\n\nDescription: Grid of example visualizations showing good vs. poor practices Suggested content: - 2x2 grid showing: - Top left: Poorly labeled histogram - Top right: Well-labeled histogram with title, axis labels - Bottom left: Scatter plot with unclear colors - Bottom right: Scatter plot with clear legend and labels\nDimensions: ~1000x800px"
  },
  {
    "objectID": "Textbook/Module 1 - EDA/images/placeholder-info.html#notes",
    "href": "Textbook/Module 1 - EDA/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Module 1 - EDA",
    "section": "",
    "text": "Images should be placed in this /images/ folder\nUse .png format for diagrams and screenshots\nUse descriptive filenames\nEnsure images are readable when embedded in Quarto HTML output"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "",
    "text": "Related Assignments:\n\nModule 7 Homework\nModule 7 Quiz"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#module-resources",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#module-resources",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "",
    "text": "Related Assignments:\n\nModule 7 Homework\nModule 7 Quiz"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#introduction",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#introduction",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "Introduction",
    "text": "Introduction\nYou’ve learned linear regression, logistic regression, decision trees, random forests, SVMs, and k-nearest neighbors. Each of these models has strengths and weaknesses. Each makes specific assumptions about your data. And each has a ceiling—a limit to the complexity of patterns it can learn.\nNeural networks shatter that ceiling.\nThink about it this way: linear regression draws a straight line through your data. Polynomial regression adds some curves. Decision trees create rectangular decision boundaries. Random forests combine many rectangles. But what if your data has patterns that can’t be captured by lines, curves, or rectangles? What if the relationship between your features and target is deeply complex, involving intricate interactions between dozens or hundreds of variables?\nThat’s where neural networks come in. They’re universal function approximators—given enough data and the right architecture, they can learn virtually any pattern. This is why neural networks power image recognition, natural language processing, speech recognition, game playing, and countless other applications that seemed impossible just a decade ago.\nBut here’s the thing: with great power comes great complexity. Neural networks have more hyperparameters, require more data, take longer to train, and are harder to interpret than the models you’ve worked with so far. You can’t just throw a neural network at every problem and expect it to work. You need to understand when to use them, how to build them, and most importantly—how to diagnose and fix the inevitable problems that arise during training.\nThis chapter will teach you not just how to build neural networks in PyTorch, but how to think about them. You’ll learn what makes them different from traditional machine learning models, when they’re worth the added complexity, and how to recognize and fix common training problems. By the end, you’ll understand why neural networks are revolutionary—and why sometimes a simple logistic regression is still the better choice.\nLet’s jump in."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#what-are-neural-networks-the-building-blocks",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#what-are-neural-networks-the-building-blocks",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "1. What Are Neural Networks? The Building Blocks",
    "text": "1. What Are Neural Networks? The Building Blocks\n\n1.1 The Perceptron: Where It All Begins\nHere’s a secret: you already understand the building block of neural networks. Remember linear regression? A perceptron is basically the same thing.\nA perceptron takes multiple inputs, multiplies each by a weight, sums everything up, adds a bias term, and passes the result through an activation function. Sound familiar? It should—this is exactly what logistic regression does.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# A simple perceptron in action\ndef perceptron(inputs, weights, bias):\n    \"\"\"\n    A basic perceptron: weighted sum + bias + activation\n    \"\"\"\n    # Weighted sum\n    z = np.dot(inputs, weights) + bias\n\n    # Activation function (sigmoid for this example)\n    activation = 1 / (1 + np.exp(-z))\n\n    return activation\n\n# Example with 3 inputs\ninputs = np.array([0.5, -0.3, 0.8])\nweights = np.array([0.4, 0.6, -0.2])\nbias = 0.1\n\noutput = perceptron(inputs, weights, bias)\nprint(f\"Inputs: {inputs}\")\nprint(f\"Weights: {weights}\")\nprint(f\"Bias: {bias}\")\nprint(f\"Output: {output:.4f}\")\n\nplt.show()\n\nInputs: [ 0.5 -0.3  0.8]\nWeights: [ 0.4  0.6 -0.2]\nBias: 0.1\nOutput: 0.4900\n\n\nThe perceptron computes: output = activation(w₁x₁ + w₂x₂ + w₃x₃ + b). This is exactly the same structure as logistic regression. So what makes neural networks different?\n\n\n1.2 Stacking Layers: The Magic of Depth\nThe magic happens when you stack perceptrons into layers. Take the outputs from one layer and feed them as inputs to the next layer. Do this multiple times. Suddenly, you have a neural network.\nHere’s why this matters: each layer learns increasingly complex representations of your data. The first layer might learn simple patterns (edges in images, common words in text). The second layer combines those patterns into more complex features (shapes from edges, phrases from words). Deeper layers learn even more abstract representations (objects from shapes, concepts from phrases).\n\nimport torch\nimport torch.nn as nn\n\n# A simple multi-layer perceptron (MLP)\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNN, self).__init__()\n\n        # First layer: input to hidden\n        self.layer1 = nn.Linear(input_size, hidden_size)\n\n        # Second layer: hidden to output\n        self.layer2 = nn.Linear(hidden_size, output_size)\n\n        # Activation function\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Pass through first layer\n        x = self.layer1(x)\n\n        # Apply activation\n        x = self.relu(x)\n\n        # Pass through second layer\n        x = self.layer2(x)\n\n        return x\n\n# Create a network: 10 inputs -&gt; 5 hidden neurons -&gt; 1 output\nmodel = SimpleNN(input_size=10, hidden_size=5, output_size=1)\nprint(model)\n\nSimpleNN(\n  (layer1): Linear(in_features=10, out_features=5, bias=True)\n  (layer2): Linear(in_features=5, out_features=1, bias=True)\n  (relu): ReLU()\n)\n\n\nThis network has two layers of weights (called “layers”) and one hidden layer. Don’t let the terminology confuse you: “hidden layer” just means it’s between the input and output. The network takes 10 input features, transforms them into 5 hidden representations, then produces 1 output.\n\n\n1.3 What’s Actually Happening Inside?\nLet’s trace what happens when data flows through this network:\n\nInput layer: You feed in your features (e.g., 10 numbers representing house characteristics)\nFirst transformation: The network computes 5 different weighted sums of those 10 inputs—each hidden neuron sees all inputs but weights them differently\nActivation: Apply ReLU to each of those 5 values (we’ll explain ReLU soon)\nSecond transformation: Combine those 5 hidden values into a final output\nOutput: A single prediction\n\nHere’s the key insight: by learning the right weights, the network can create useful intermediate representations in that hidden layer. Those 5 hidden neurons might learn to represent things like “overall house quality,” “neighborhood desirability,” “age-related factors,” etc.—concepts that help predict house price but aren’t explicitly in your data.\n\n# Let's see it in action with random data\nrandom_house_features = torch.randn(1, 10)  # 1 house, 10 features\nprediction = model(random_house_features)\n\nprint(f\"Input shape: {random_house_features.shape}\")\nprint(f\"Output shape: {prediction.shape}\")\nprint(f\"Predicted value: {prediction.item():.4f}\")\n\nInput shape: torch.Size([1, 10])\nOutput shape: torch.Size([1, 1])\nPredicted value: 0.1822\n\n\nThe network takes in our features and produces a prediction. Right now it’s random nonsense because we haven’t trained it—but the structure is there.\n\n\n\nPlaceholder: Multi-layer perceptron architecture diagram"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#activation-functions-adding-non-linearity",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#activation-functions-adding-non-linearity",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "2. Activation Functions: Adding Non-Linearity",
    "text": "2. Activation Functions: Adding Non-Linearity\n\n2.1 Why We Need Activation Functions\nHere’s a problem: if you stack linear transformations without activation functions, you just get another linear transformation. No matter how many layers you add, the network can only learn linear patterns. It’s like stacking multiple straight rulers end-to-end—you still just get a straight line.\nActivation functions add non-linearity. They allow the network to learn curves, corners, complex decision boundaries—patterns that can’t be captured by straight lines.\n\n\n2.2 Common Activation Functions\nLet’s look at the most important activation functions you’ll encounter:\n\n# Visualize common activation functions\nx = np.linspace(-5, 5, 100)\n\n# ReLU: Rectified Linear Unit\nrelu = np.maximum(0, x)\n\n# Sigmoid: Squashes to (0, 1)\nsigmoid = 1 / (1 + np.exp(-x))\n\n# Tanh: Squashes to (-1, 1)\ntanh = np.tanh(x)\n\n# Plot them\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].plot(x, relu, linewidth=2, color='#e74c3c')\naxes[0].set_title('ReLU', fontsize=14)\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].axvline(x=0, color='black', linewidth=0.5)\n\naxes[1].plot(x, sigmoid, linewidth=2, color='#3498db')\naxes[1].set_title('Sigmoid', fontsize=14)\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(y=0, color='black', linewidth=0.5)\naxes[1].axvline(x=0, color='black', linewidth=0.5)\n\naxes[2].plot(x, tanh, linewidth=2, color='#2ecc71')\naxes[2].set_title('Tanh', fontsize=14)\naxes[2].grid(True, alpha=0.3)\naxes[2].axhline(y=0, color='black', linewidth=0.5)\naxes[2].axvline(x=0, color='black', linewidth=0.5)\n\nfor ax in axes:\n    ax.set_xlabel('Input', fontsize=12)\n    ax.set_ylabel('Output', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nReLU (Rectified Linear Unit): The most commonly used activation function. It’s dead simple: f(x) = max(0, x). If the input is positive, pass it through. If negative, output zero. Why is this so popular? It’s computationally cheap, doesn’t suffer from vanishing gradients, and works remarkably well in practice.\nSigmoid: Squashes any input to a value between 0 and 1. This is what logistic regression uses. It’s still useful for the output layer when you need probabilities, but rarely used in hidden layers anymore because gradients vanish when inputs are far from zero.\nTanh: Like sigmoid but squashes to (-1, 1) instead. Centers the data around zero, which can help with training. Still suffers from vanishing gradients.\n\n\n2.3 Choosing the Right Activation\nHere’s a practical guide:\n\nHidden layers: Use ReLU. It’s simple, fast, and effective. Start here.\nBinary classification output: Use Sigmoid to get probabilities between 0 and 1\nMulti-class classification output: Use Softmax (we’ll see this later)\nRegression output: No activation (or use linear activation)—you want the network to output any real number\n\n\n# Building a network with proper activations\nclass BetterNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(BetterNN, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()  # ReLU for hidden layer\n        self.layer2 = nn.Linear(hidden_size, output_size)\n        # No activation on output for regression\n        # Would add Sigmoid for binary classification\n        # Would add Softmax for multi-class classification\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)  # Apply ReLU after first layer\n        x = self.layer2(x)  # No activation after output layer\n        return x\n\nmodel = BetterNN(input_size=10, hidden_size=20, output_size=1)\nprint(model)\n\nBetterNN(\n  (layer1): Linear(in_features=10, out_features=20, bias=True)\n  (relu): ReLU()\n  (layer2): Linear(in_features=20, out_features=1, bias=True)\n)\n\n\nSee how we only apply ReLU after the hidden layer, not the output? That’s because we’re doing regression—we want any real number as output. For binary classification, we’d add torch.sigmoid() to the output."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#the-training-process-forward-and-backward",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#the-training-process-forward-and-backward",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "3. The Training Process: Forward and Backward",
    "text": "3. The Training Process: Forward and Backward\n\n3.1 Forward Pass: Making Predictions\nThe forward pass is straightforward: data flows from input through each layer to produce a prediction. You’ve already seen this in the forward() method. Let’s trace it explicitly:\n\n# Create some fake data\nX = torch.randn(5, 10)  # 5 samples, 10 features each\ny = torch.randn(5, 1)   # 5 target values\n\nmodel = BetterNN(input_size=10, hidden_size=20, output_size=1)\n\n# Forward pass: get predictions\npredictions = model(X)\n\nprint(f\"Input shape: {X.shape}\")\nprint(f\"Predictions shape: {predictions.shape}\")\nprint(f\"Predictions:\\n{predictions.detach().numpy()}\")\n\nInput shape: torch.Size([5, 10])\nPredictions shape: torch.Size([5, 1])\nPredictions:\n[[-0.17862587]\n [-0.27817434]\n [-0.23991197]\n [ 0.01879379]\n [ 0.12353444]]\n\n\nThe model takes 5 examples with 10 features each and produces 5 predictions. Right now they’re garbage because the weights are random—but the machinery works.\n\n\n3.2 Loss Calculation: Measuring Error\nJust like in Module 3, we need a loss function to measure how wrong our predictions are. For regression, we typically use Mean Squared Error (MSE):\n\n# Calculate MSE loss\ncriterion = nn.MSELoss()\nloss = criterion(predictions, y)\n\nprint(f\"Loss (MSE): {loss.item():.4f}\")\n\nLoss (MSE): 0.2008\n\n\nThe loss is a single number telling us how bad our predictions are on average. During training, we want to minimize this number.\n\n\n3.3 Backward Pass: Computing Gradients\nHere’s where neural networks get interesting. Remember gradient descent from Module 3? We need the gradient of the loss with respect to every weight in the network. For a network with thousands or millions of weights, computing these gradients by hand would be impossible.\nThis is where backpropagation comes in. The beautiful thing is: you don’t need to understand the details. PyTorch handles it automatically. But conceptually, here’s what’s happening:\n\nStart at the loss (the final output)\nUse the chain rule to compute how much each weight contributed to that loss\n“Propagate” this information backward through the network\nStore the gradient for each weight\n\nIn PyTorch, this is one line:\n\n# Backward pass: compute gradients\nloss.backward()\n\n# Now every parameter has a gradient stored\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name}: gradient shape = {param.grad.shape}\")\n\nlayer1.weight: gradient shape = torch.Size([20, 10])\nlayer1.bias: gradient shape = torch.Size([20])\nlayer2.weight: gradient shape = torch.Size([1, 20])\nlayer2.bias: gradient shape = torch.Size([1])\n\n\nSee? PyTorch computed gradients for every weight in the network. The layer1.weight has shape (20, 10) because there are 20 hidden neurons, each connected to 10 inputs. The gradients have the same shape—one gradient value per weight.\n\n\n3.4 Optimization: Updating Weights\nNow we use those gradients to update the weights. Remember gradient descent? Same idea:\nnew_weight = old_weight - learning_rate * gradient\nPyTorch optimizers handle this for us:\n\n# Create an optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Zero out old gradients (important!)\noptimizer.zero_grad()\n\n# Forward pass\npredictions = model(X)\nloss = criterion(predictions, y)\n\n# Backward pass\nloss.backward()\n\n# Update weights\noptimizer.step()\n\nprint(f\"Loss after one update: {loss.item():.4f}\")\n\nLoss after one update: 0.2008\n\n\nThat’s one complete training step: forward pass → compute loss → backward pass → update weights.\n\n\n3.5 The Training Loop Pattern\nIn practice, we repeat this process many times over the entire dataset. Here’s the standard pattern:\n\n# Training loop pattern (won't actually train anything useful with random data)\nnum_epochs = 3\n\nfor epoch in range(num_epochs):\n    # Forward pass\n    predictions = model(X)\n    loss = criterion(predictions, y)\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear old gradients\n    loss.backward()        # Compute new gradients\n    optimizer.step()       # Update weights\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n\nEpoch 1/3, Loss: 0.1926\nEpoch 2/3, Loss: 0.1849\nEpoch 3/3, Loss: 0.1775\n\n\nSee the pattern? This is the foundation of training every neural network. The specifics might change (different optimizers, different loss functions, validation loops, etc.) but this basic structure remains the same."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#pytorch-fundamentals-building-your-first-real-network",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#pytorch-fundamentals-building-your-first-real-network",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "4. PyTorch Fundamentals: Building Your First Real Network",
    "text": "4. PyTorch Fundamentals: Building Your First Real Network\n\n4.1 Tensors: The Fundamental Data Structure\nPyTorch uses tensors instead of NumPy arrays. Tensors are like arrays but can run on GPUs and support automatic differentiation (the backward pass we just saw).\n\n# Creating tensors\na = torch.tensor([1, 2, 3])\nb = torch.randn(3, 4)  # 3x4 random tensor\nc = torch.zeros(2, 2)  # 2x2 tensor of zeros\n\nprint(f\"a: {a}\")\nprint(f\"b shape: {b.shape}\")\nprint(f\"c:\\n{c}\")\n\n# Tensors work like NumPy arrays\nd = a * 2\ne = b + 1\nprint(f\"a * 2: {d}\")\n\na: tensor([1, 2, 3])\nb shape: torch.Size([3, 4])\nc:\ntensor([[0., 0.],\n        [0., 0.]])\na * 2: tensor([2, 4, 6])\n\n\nFor most purposes, tensors behave exactly like NumPy arrays. The main difference is that tensors track gradients automatically when you tell them to.\n\n\n4.2 Loading Real Data: California Housing\nLet’s work with the California housing dataset we’ve used in previous modules. We’ll build a neural network to predict house prices—a regression problem.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\nhousing = pd.read_csv('../data/housing.csv')\n\n# Convert ocean_proximity categorical variable to dummy variables\nhousing = pd.get_dummies(housing, columns=['ocean_proximity'], drop_first=True)\n\n# Prepare features and target\nX = housing.drop('median_house_value', axis=1)\ny = housing['median_house_value'].values\n\nprint(f\"Features shape after encoding: {X.shape}\")\nprint(f\"Feature names: {list(X.columns)}\")\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# IMPORTANT: Scale features for neural networks\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Training set: {X_train_scaled.shape}\")\nprint(f\"Test set: {X_test_scaled.shape}\")\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train_scaled)\ny_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\nX_test_tensor = torch.FloatTensor(X_test_scaled)\ny_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n\nprint(f\"Tensor shapes: X_train={X_train_tensor.shape}, y_train={y_train_tensor.shape}\")\n\nFeatures shape after encoding: (20640, 12)\nFeature names: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity_INLAND', 'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY', 'ocean_proximity_NEAR OCEAN']\nTraining set: (16512, 12)\nTest set: (4128, 12)\nTensor shapes: X_train=torch.Size([16512, 12]), y_train=torch.Size([16512, 1])\n\n\nTwo important preprocessing steps here. First, we convert ocean_proximity from categorical text values (like “NEAR BAY”, “INLAND”) to dummy variables using pd.get_dummies(). Neural networks need numeric inputs, so we convert each category into a binary (0/1) column. Using drop_first=True prevents multicollinearity by dropping one category as a reference.\nSecond, notice the scaling step—this is crucial for neural networks. Features with different scales can cause training problems. Neural networks work best when input features are normalized to similar ranges.\n\n\n4.3 Creating DataLoaders for Efficient Training\nFor large datasets, we don’t pass the entire dataset through the network at once. Instead, we use mini-batches. PyTorch’s DataLoader handles this automatically:\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Create dataset\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n# Create DataLoader with batch size of 64\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True  # Shuffle data each epoch\n)\n\n# See how it works\nfor batch_X, batch_y in train_loader:\n    print(f\"Batch X shape: {batch_X.shape}\")\n    print(f\"Batch y shape: {batch_y.shape}\")\n    break  # Just show one batch\n\nBatch X shape: torch.Size([64, 12])\nBatch y shape: torch.Size([64, 1])\n\n\nThe DataLoader automatically splits our data into batches of 64 examples each. During training, we’ll process one batch at a time, computing gradients and updating weights after each batch.\n\n\n4.4 Building a Housing Price Predictor\nLet’s build a proper neural network for this task:\n\nclass HousingNN(nn.Module):\n    def __init__(self, input_size):\n        super(HousingNN, self).__init__()\n\n        # Network architecture\n        self.layer1 = nn.Linear(input_size, 64)  # Input to 64 hidden neurons\n        self.layer2 = nn.Linear(64, 32)          # 64 to 32 hidden neurons\n        self.layer3 = nn.Linear(32, 1)           # 32 to 1 output\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Forward pass through three layers\n        x = self.layer1(x)\n        x = self.relu(x)\n\n        x = self.layer2(x)\n        x = self.relu(x)\n\n        x = self.layer3(x)\n        # No activation on output for regression\n\n        return x\n\n# Create the model\ninput_size = X_train_scaled.shape[1]  # Number of features\nmodel = HousingNN(input_size)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model architecture:\")\nprint(model)\nprint(f\"\\nTotal parameters: {total_params:,}\")\n\nModel architecture:\nHousingNN(\n  (layer1): Linear(in_features=12, out_features=64, bias=True)\n  (layer2): Linear(in_features=64, out_features=32, bias=True)\n  (layer3): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n)\n\nTotal parameters: 2,945\n\n\nThis network has three layers with 64, 32, and 1 neurons respectively. Why these numbers? There’s no perfect formula—architecture design is part science, part art, part experimentation. These are reasonable starting points for a dataset this size.\n\n\n4.5 Training the Network\nNow let’s actually train it:\n\n# Setup\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer, not SGD\n\n# Training loop\nnum_epochs = 50\ntrain_losses = []\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    epoch_loss = 0\n\n    for batch_X, batch_y in train_loader:\n        # Forward pass\n        predictions = model(batch_X)\n        loss = criterion(predictions, batch_y)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    # Average loss for this epoch\n    avg_loss = epoch_loss / len(train_loader)\n    train_losses.append(avg_loss)\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.2f}\")\n\n# Plot training loss\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Training Loss (MSE)', fontsize=12)\nplt.title('Training Loss Over Time', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\nEpoch 10/50, Loss: 13785384616.68\nEpoch 20/50, Loss: 7671714087.69\nEpoch 30/50, Loss: 5662778961.36\nEpoch 40/50, Loss: 4936042822.45\nEpoch 50/50, Loss: 4676608069.46\n\n\n\n\n\n\n\n\n\nLook at that loss curve—it starts high and drops quickly, then gradually levels off. This is what healthy training looks like. The model is learning!\n\n\n4.6 Evaluating the Model\nLet’s see how well it predicts on the test set:\n\n# Evaluation mode\nmodel.eval()\n\nwith torch.no_grad():  # Don't compute gradients during evaluation\n    test_predictions = model(X_test_tensor)\n    test_loss = criterion(test_predictions, y_test_tensor)\n\nprint(f\"Test Loss (MSE): {test_loss.item():.2f}\")\n\n# Look at some predictions\ncomparison = pd.DataFrame({\n    'Actual': y_test_tensor.squeeze().numpy()[:10],\n    'Predicted': test_predictions.squeeze().numpy()[:10]\n})\ncomparison['Error'] = comparison['Actual'] - comparison['Predicted']\ncomparison.head(10)\n\nTest Loss (MSE): nan\n\n\n\n\n\n\n\n\n\nActual\nPredicted\nError\n\n\n\n\n0\n47700.0\nNaN\nNaN\n\n\n1\n45800.0\nNaN\nNaN\n\n\n2\n500001.0\nNaN\nNaN\n\n\n3\n218600.0\nNaN\nNaN\n\n\n4\n278000.0\nNaN\nNaN\n\n\n5\n158700.0\nNaN\nNaN\n\n\n6\n198200.0\nNaN\nNaN\n\n\n7\n157500.0\nNaN\nNaN\n\n\n8\n340000.0\nNaN\nNaN\n\n\n9\n446600.0\nNaN\nNaN\n\n\n\n\n\n\n\nThe predictions aren’t perfect, but they’re in the right ballpark. This is a working neural network!"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#loss-functions-for-neural-networks",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#loss-functions-for-neural-networks",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "5. Loss Functions for Neural Networks",
    "text": "5. Loss Functions for Neural Networks\n\n5.1 Connecting to What You Know\nRemember loss functions from Module 3? Neural networks use the exact same ones. The only difference is we’re computing them on neural network predictions instead of linear model predictions.\nFor regression problems (predicting continuous values): - MSE (Mean Squared Error): Penalizes large errors heavily - MAE (Mean Absolute Error): Treats all errors equally - Huber Loss: Compromise between MSE and MAE\nFor classification problems (predicting categories): - Binary Cross-Entropy: For two-class problems (spam/not spam) - Cross-Entropy: For multi-class problems (digit recognition)\n\n\n5.2 Regression Loss Functions\nWe just used MSE. Let’s compare it to MAE:\n\n# MSE vs MAE\ncriterion_mse = nn.MSELoss()\ncriterion_mae = nn.L1Loss()  # L1 loss is MAE\n\n# Make some predictions\nwith torch.no_grad():\n    predictions = model(X_test_tensor[:100])  # First 100 test examples\n    actual = y_test_tensor[:100]\n\n    mse_loss = criterion_mse(predictions, actual)\n    mae_loss = criterion_mae(predictions, actual)\n\nprint(f\"MSE Loss: {mse_loss.item():.2f}\")\nprint(f\"MAE Loss: {mae_loss.item():.2f}\")\n\nMSE Loss: nan\nMAE Loss: nan\n\n\nMSE is larger because it squares errors—a prediction that’s off by 10,000 contributes 100,000,000 to MSE but only 10,000 to MAE. Choose MSE when large errors are especially bad. Choose MAE when you want to treat all errors equally.\n\n\n5.3 Classification Loss Functions\nFor classification, we use cross-entropy. Let’s build a quick binary classifier to see it in action:\n\n# Binary classification example: high-value homes (&gt;300k) vs. low-value\ny_binary = (y_train &gt; 300000).astype(int)\ny_binary_tensor = torch.FloatTensor(y_binary).reshape(-1, 1)\n\n# Binary classification network\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.layer1 = nn.Linear(input_size, 32)\n        self.layer2 = nn.Linear(32, 16)\n        self.layer3 = nn.Linear(16, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.sigmoid(self.layer3(x))  # Output probability\n        return x\n\nbinary_model = BinaryClassifier(input_size)\n\n# Binary cross-entropy loss\ncriterion_bce = nn.BCELoss()\n\n# Quick prediction\nwith torch.no_grad():\n    probs = binary_model(X_train_tensor[:5])\n    print(f\"Predicted probabilities: {probs.squeeze().numpy()}\")\n    print(f\"Actual labels: {y_binary[:5]}\")\n\nPredicted probabilities: [0.42973122 0.4555295  0.45162523 0.4445805  0.46844554]\nActual labels: [0 1 0 0 0]\n\n\nThe model outputs probabilities between 0 and 1 (thanks to sigmoid). Binary cross-entropy measures how confident the model is when it’s wrong. A confident wrong prediction gets penalized heavily.\n\n\n5.4 Understanding the Loss Landscape\nHere’s a helpful mental model: imagine a landscape where height represents loss. Training is like rolling a ball downhill—you’re trying to reach the lowest point. Gradient descent tells you which direction is downhill.\nFor simple models like linear regression, the landscape is bowl-shaped with one clear minimum. For neural networks, the landscape is complex with many hills and valleys. But typically, many of those valleys are “good enough”—your model doesn’t need to find the global best to work well.\n\n\n\n\n\n\nNote\n\n\n\nWhy the loss goes down: Each gradient descent step pushes the weights slightly downhill. Over many steps, you reach a valley. You might not reach the lowest possible valley in the entire landscape, but you’ll reach a good-enough valley where the loss is low."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#diagnosing-training-reading-the-curves",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#diagnosing-training-reading-the-curves",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "6. Diagnosing Training: Reading the Curves",
    "text": "6. Diagnosing Training: Reading the Curves\n\n6.1 Training vs. Validation Loss\nThe most important skill in deep learning: reading loss curves. Here’s what you need to plot:\n\nTraining loss: Loss computed on the training data\nValidation loss: Loss computed on held-out validation data\n\nThese two curves tell you everything about how training is going.\n\n# Let's train a new model and track both losses\nfrom sklearn.model_selection import train_test_split\n\n# Split training data into train and validation\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_train_scaled, y_train, test_size=0.2, random_state=42\n)\n\n# Convert to tensors\nX_tr_tensor = torch.FloatTensor(X_tr)\ny_tr_tensor = torch.FloatTensor(y_tr).reshape(-1, 1)\nX_val_tensor = torch.FloatTensor(X_val)\ny_val_tensor = torch.FloatTensor(y_val).reshape(-1, 1)\n\n# Create new model\nmodel_diag = HousingNN(input_size)\noptimizer = torch.optim.Adam(model_diag.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# Training with validation tracking\ntrain_dataset = TensorDataset(X_tr_tensor, y_tr_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nnum_epochs = 100\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model_diag.train()\n    epoch_train_loss = 0\n\n    for batch_X, batch_y in train_loader:\n        predictions = model_diag(batch_X)\n        loss = criterion(predictions, batch_y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_train_loss += loss.item()\n\n    avg_train_loss = epoch_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # Validation phase\n    model_diag.eval()\n    with torch.no_grad():\n        val_predictions = model_diag(X_val_tensor)\n        val_loss = criterion(val_predictions, y_val_tensor)\n        val_losses.append(val_loss.item())\n\n# Plot both losses\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label='Training Loss', linewidth=2)\nplt.plot(val_losses, label='Validation Loss', linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss (MSE)', fontsize=12)\nplt.title('Training vs. Validation Loss', fontsize=14)\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nSee how both curves decrease? That’s good—the model is learning patterns that generalize to new data. The validation loss is slightly higher than training loss, which is normal and expected.\n\n\n6.2 Overfitting: When Your Model Memorizes\nHere’s what overfitting looks like:\n\n\n\nPlaceholder: Training curves showing overfitting\n\n\nSigns of overfitting: - Training loss keeps decreasing - Validation loss starts increasing (or stops decreasing) - Large gap between training and validation loss\nWhat’s happening? The model is learning patterns specific to the training data—including noise—that don’t generalize to new data. It’s memorizing instead of learning.\nHow to fix it: 1. Get more training data 2. Use regularization (dropout, L2 penalty) 3. Simplify the model (fewer layers, fewer neurons) 4. Stop training earlier (early stopping)\n\n\n6.3 Underfitting: When Your Model Is Too Simple\nUnderfitting looks different:\n\n\n\nPlaceholder: Training curves showing underfitting\n\n\nSigns of underfitting: - Both training and validation loss are high - Both losses plateau quickly - Small gap between training and validation loss\nThe model isn’t complex enough to learn the patterns in your data. It’s like trying to fit a curve with a straight line.\nHow to fix it: 1. Make the model more complex (more layers, more neurons) 2. Train longer 3. Decrease regularization 4. Add better features\n\n\n6.4 The Goldilocks Zone: Just Right\nThis is what you want:\n\n\n\nPlaceholder: Healthy training curves\n\n\nSigns of good fit: - Both losses decrease steadily - Validation loss tracks training loss closely - Losses plateau together at a low value - Small but acceptable gap between them\nThis model has learned generalizable patterns without memorizing noise. This is your target.\n\n\n6.5 Using Training Curves to Debug\nWhen your model isn’t working, look at the curves first:\nProblem: Validation loss is much higher than training loss - Diagnosis: Overfitting - Solution: Add regularization, get more data, or simplify model\nProblem: Both losses are high and not decreasing - Diagnosis: Underfitting or learning rate too low - Solution: Make model more complex or increase learning rate\nProblem: Loss is erratic, jumping up and down - Diagnosis: Learning rate too high - Solution: Decrease learning rate\nProblem: Loss starts decreasing then suddenly explodes - Diagnosis: Learning rate way too high - Solution: Significantly decrease learning rate"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#regularization-in-deep-learning",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#regularization-in-deep-learning",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "7. Regularization in Deep Learning",
    "text": "7. Regularization in Deep Learning\n\n7.1 Dropout: Randomly Forgetting\nDropout is beautifully simple: during training, randomly turn off some neurons. This forces the network to learn redundant representations—it can’t rely on any single neuron.\n\nclass RegularizedNN(nn.Module):\n    def __init__(self, input_size):\n        super(RegularizedNN, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 64)\n        self.dropout1 = nn.Dropout(p=0.3)  # Drop 30% of neurons\n\n        self.layer2 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(p=0.3)\n\n        self.layer3 = nn.Linear(32, 1)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.dropout1(x)  # Apply dropout after activation\n\n        x = self.relu(self.layer2(x))\n        x = self.dropout2(x)\n\n        x = self.layer3(x)\n        return x\n\nmodel_dropout = RegularizedNN(input_size)\nprint(model_dropout)\n\nRegularizedNN(\n  (layer1): Linear(in_features=12, out_features=64, bias=True)\n  (dropout1): Dropout(p=0.3, inplace=False)\n  (layer2): Linear(in_features=64, out_features=32, bias=True)\n  (dropout2): Dropout(p=0.3, inplace=False)\n  (layer3): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n)\n\n\nDuring training, dropout randomly zeros out 30% of activations. During evaluation (when you set model.eval()), dropout is turned off—all neurons are active.\nWhy does this help? By randomly disabling neurons, you prevent any single neuron from becoming too specialized. The network learns more robust representations.\n\n\n\nPlaceholder: Dropout visualization showing neurons being randomly disabled\n\n\n\n\n7.2 Batch Normalization: Stabilizing Training\nBatch normalization normalizes the activations within each layer. This helps gradients flow better and allows higher learning rates.\n\nclass BatchNormNN(nn.Module):\n    def __init__(self, input_size):\n        super(BatchNormNN, self).__init__()\n\n        self.layer1 = nn.Linear(input_size, 64)\n        self.bn1 = nn.BatchNorm1d(64)  # Batch norm after first layer\n\n        self.layer2 = nn.Linear(64, 32)\n        self.bn2 = nn.BatchNorm1d(32)\n\n        self.layer3 = nn.Linear(32, 1)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.bn1(x)  # Normalize before activation\n        x = self.relu(x)\n\n        x = self.layer2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.layer3(x)\n        return x\n\nmodel_bn = BatchNormNN(input_size)\nprint(model_bn)\n\nBatchNormNN(\n  (layer1): Linear(in_features=12, out_features=64, bias=True)\n  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer2): Linear(in_features=64, out_features=32, bias=True)\n  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer3): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n)\n\n\nBatch normalization is a bit like standardizing your input features, but applied within the network. It often speeds up training and can improve final performance.\n\n\n7.3 L2 Regularization: Penalizing Large Weights\nRemember L2 regularization from Module 4? We can use it in neural networks too:\n\n# L2 regularization via weight decay in optimizer\noptimizer_l2 = torch.optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    weight_decay=0.01  # L2 regularization strength\n)\n\nThe weight_decay parameter adds a penalty for large weights, encouraging the model to keep weights small. This discourages overfitting.\n\n\n7.4 Early Stopping: Knowing When to Quit\nThe simplest regularization technique: stop training when validation loss stops improving.\n\n# Early stopping example\nbest_val_loss = float('inf')\npatience = 10  # Stop if no improvement for 10 epochs\npatience_counter = 0\n\nfor epoch in range(200):  # Max 200 epochs\n    # ... training code ...\n\n    # Check validation loss (using our previous val_loss)\n    if epoch &gt; 0:  # Skip first epoch\n        current_val_loss = val_losses[-1]\n\n        if current_val_loss &lt; best_val_loss:\n            best_val_loss = current_val_loss\n            patience_counter = 0\n            # Save model checkpoint here\n        else:\n            patience_counter += 1\n\n        if patience_counter &gt;= patience:\n            print(f\"Early stopping at epoch {epoch}\")\n            break\n\nEarly stopping at epoch 11\n\n\nEarly stopping prevents the model from overfitting by stopping before validation performance degrades.\n\n\n7.5 Comparing Regularization Techniques\nLet’s see the impact:\n\n\n\n\n\n\nTip\n\n\n\nRegularization Quick Guide: - Dropout: Use when you have overfitting. Start with p=0.3-0.5 - Batch Normalization: Almost always helps. Use it. - L2 Regularization: Use with weight_decay=0.001-0.01. Helps with overfitting. - Early Stopping: Always monitor validation loss and stop when it plateaus.\n\n\nYou can combine multiple regularization techniques. In fact, that’s common—use batch normalization + dropout + early stopping together."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#architecture-design-how-many-layers-how-many-neurons",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#architecture-design-how-many-layers-how-many-neurons",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "8. Architecture Design: How Many Layers? How Many Neurons?",
    "text": "8. Architecture Design: How Many Layers? How Many Neurons?\n\n8.1 The Capacity Question\nNetwork architecture determines the model’s capacity—its ability to learn complex patterns. More layers and more neurons = more capacity. But more capacity also means more risk of overfitting.\nThe fundamental question: Does your model have enough capacity to learn the patterns in your data, but not so much that it memorizes noise?\n\n\n8.2 Going Deeper vs. Going Wider\nTwo ways to increase capacity:\nDeeper networks (more layers): - Can learn more abstract, hierarchical representations - Better for very complex patterns - Harder to train (vanishing gradients)\nWider networks (more neurons per layer): - More capacity without as much depth - Easier to train - May not learn hierarchical features as effectively\n\n\n8.3 Rules of Thumb for Architecture Design\nHere are practical starting points:\nFor tabular data (like our housing dataset): - Start with 2-3 hidden layers - Each layer has 32-128 neurons - Make layers smaller as you go deeper (e.g., 128 → 64 → 32)\nFor image data: - Convolutional networks (not covered here) - Many layers (10-100+) - Specialized architectures (ResNet, VGG, etc.)\nFor text data: - Recurrent or transformer networks (not covered here) - Architecture depends heavily on task\nGeneral principle: Start simple, add complexity only if needed.\n\n\n8.4 Experimenting with Architecture\nLet’s try different architectures on our housing data:\n\n# Three different architectures\nclass SmallNN(nn.Module):\n    def __init__(self, input_size):\n        super(SmallNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 16)\n        self.fc2 = nn.Linear(16, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\nclass MediumNN(nn.Module):\n    def __init__(self, input_size):\n        super(MediumNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass LargeNN(nn.Module):\n    def __init__(self, input_size):\n        super(LargeNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 32)\n        self.fc4 = nn.Linear(32, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        return self.fc4(x)\n\n# Count parameters in each\nfor name, ModelClass in [('Small', SmallNN), ('Medium', MediumNN), ('Large', LargeNN)]:\n    model_temp = ModelClass(input_size)\n    params = sum(p.numel() for p in model_temp.parameters())\n    print(f\"{name} model: {params:,} parameters\")\n\nSmall model: 225 parameters\nMedium model: 2,945 parameters\nLarge model: 12,033 parameters\n\n\nFor this dataset, the medium model is probably best. The small model might underfit (not enough capacity). The large model might overfit (too much capacity for the amount of data we have).\n\n\n8.5 How to Choose\nHere’s a decision process:\n\nStart with a simple architecture (2 layers, 32-64 neurons)\nTrain and evaluate - Look at training curves\nDiagnose:\n\nIf underfitting (high training loss) → Increase capacity\nIf overfitting (training loss &lt;&lt; validation loss) → Decrease capacity or add regularization\n\nIterate - Adjust and retrain\n\nDon’t try to guess the perfect architecture upfront. Experiment systematically."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#hyperparameter-tuning-the-art-of-training-neural-networks",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#hyperparameter-tuning-the-art-of-training-neural-networks",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "9. Hyperparameter Tuning: The Art of Training Neural Networks",
    "text": "9. Hyperparameter Tuning: The Art of Training Neural Networks\n\n9.1 Learning Rate: The Most Important Hyperparameter\nLearning rate controls how big a step we take during gradient descent. Too small = slow training. Too large = unstable training or divergence.\n\n# Learning rate experiment\nlearning_rates = [0.0001, 0.001, 0.01, 0.1]\nresults = {}\n\nfor lr in learning_rates:\n    model_lr = HousingNN(input_size)\n    optimizer = torch.optim.Adam(model_lr.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    # Train for 20 epochs\n    losses = []\n    for epoch in range(20):\n        model_lr.train()\n        epoch_loss = 0\n\n        for batch_X, batch_y in train_loader:\n            predictions = model_lr(batch_X)\n            loss = criterion(predictions, batch_y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        losses.append(epoch_loss / len(train_loader))\n\n    results[lr] = losses\n\n# Plot learning curves for different learning rates\nplt.figure(figsize=(10, 6))\nfor lr, losses in results.items():\n    plt.plot(losses, label=f'LR = {lr}', linewidth=2)\n\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Training Loss', fontsize=12)\nplt.title('Effect of Learning Rate', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.yscale('log')  # Log scale to see all curves\nplt.show()\n\n\n\n\n\n\n\n\nSee how learning rate affects training? Too small (0.0001) and loss decreases slowly. Too large (0.1) and training is unstable. The sweet spot is around 0.001-0.01 for this problem.\n\n\n9.2 Batch Size and Its Effects\nBatch size determines how many examples we use to compute each gradient:\n\nSmall batches (8-32): Noisy gradients, slower per epoch, but can help generalization\nLarge batches (128-512): Smooth gradients, faster per epoch, but might overfit\n\nCommon practice: start with 32 or 64, adjust based on your GPU memory and dataset size.\n\n\n9.3 Number of Epochs and Early Stopping\nHow many epochs to train? There’s no fixed answer. Use early stopping:\n\nMonitor validation loss\nIf it hasn’t improved for N epochs (patience), stop\nSave the best model (lowest validation loss)\n\nThis prevents overfitting and saves time.\n\n\n9.4 Learning Rate Scheduling\nSometimes it helps to decrease learning rate during training:\n\n# Learning rate scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',     # Minimize loss\n    factor=0.5,     # Multiply LR by 0.5\n    patience=5      # Wait 5 epochs\n)\n\n# During training:\n# val_loss = ... (compute validation loss)\n# scheduler.step(val_loss)  # Adjust LR based on validation loss\n\nThis automatically reduces learning rate when validation loss plateaus, helping the model fine-tune.\n\n\n9.5 Systematic Experimentation\nDon’t guess-and-check randomly. Be systematic:\n\nFix all but one hyperparameter\nTry several values for that hyperparameter\nPick the best value\nMove to next hyperparameter\n\nThis is where AI assistants shine—they can help you run many experiments in parallel and organize results.\n\n\n\n\n\n\nTip\n\n\n\nHyperparameter Priority: 1. Learning rate (most important) 2. Architecture (number of layers/neurons) 3. Batch size 4. Regularization strength 5. Everything else\nTune in this order. Learning rate has the biggest impact."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#when-to-use-neural-networks-and-when-not-to",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#when-to-use-neural-networks-and-when-not-to",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "10. When to Use Neural Networks (and When Not To)",
    "text": "10. When to Use Neural Networks (and When Not To)\n\n10.1 Problems Where Neural Networks Excel\nNeural networks are worth the complexity when:\nYou have lots of data: Neural networks need data to shine. With only 1000 examples, logistic regression might work better. With 100,000+ examples, neural networks can learn patterns that simpler models miss.\nThe patterns are complex: Images, text, speech, video—these have complex, hierarchical patterns that neural networks capture well. Tabular data with simple relationships? Maybe not.\nYou need flexibility: Neural networks are universal function approximators. If you don’t know what mathematical form your relationship takes, neural networks can learn it.\nExamples: - Image classification (faces, medical images, objects) - Natural language processing (translation, sentiment, generation) - Speech recognition - Game playing (Go, Chess, video games) - Recommendation systems at scale\n\n\n10.2 Problems Where Traditional ML Is Better\nUse simpler models when:\nYou have limited data: With 500 examples, logistic regression or random forests will likely outperform neural networks. Neural networks need more data to tune their many parameters.\nInterpretability matters: Need to explain your model to stakeholders or regulators? Linear models and decision trees are far more interpretable than neural networks.\nThe patterns are simple: If your relationship is roughly linear, why use a neural network? Linear regression is faster, simpler, and easier to interpret.\nYou need fast predictions: Neural networks are slower at inference than simpler models. For real-time applications with tight latency requirements, simpler models might be better.\nExamples: - Small tabular datasets (housing prices with 5000 examples) - Problems requiring explanation (loan decisions, medical diagnosis) - Linear relationships (basic price prediction) - Real-time prediction with tight latency constraints\n\n\n10.3 A Decision Framework\n\n\n\n\n\n\n\n\n\nConsideration\nUse Neural Networks When...\nUse Traditional ML When...\n\n\n\n\n0\nDataset size\n10,000+ examples\n&lt; 5,000 examples\n\n\n1\nPattern complexity\nHighly complex (images, text, etc.)\nModerate (tabular with clear features)\n\n\n2\nInterpretability\nNot critical\nMust explain decisions\n\n\n3\nTraining time\nCan afford hours/days\nNeed results in minutes\n\n\n4\nInference speed\nCan afford milliseconds\nNeed microsecond response\n\n\n5\nHyperparameter tuning\nCan experiment extensively\nNeed simple, quick tuning\n\n\n\n\n\n\n\n\n\n10.4 The Pragmatic Approach\nIn practice, you should try both:\n\nStart with a simple baseline (linear regression, logistic regression, random forest)\nIf performance is insufficient, try a neural network\nCompare results and complexity\nChoose based on the tradeoff between performance and complexity\n\nSometimes a random forest gives you 92% accuracy and a neural network gives 93%—but the neural network takes 10x longer to train and tune. Is 1% worth it? It depends on your problem.\n\n\n10.5 Neural Networks as Part of Your Toolkit\nDon’t think of neural networks as replacing traditional ML. Think of them as a powerful addition to your toolkit. For some problems, they’re the best tool. For others, they’re overkill.\nThe key is knowing which tool to reach for—and that comes from understanding what each tool does well."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#summary",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#summary",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "Summary",
    "text": "Summary\nNeural networks are revolutionary, but they’re not magic. They’re universal function approximators built from simple building blocks: perceptrons stacked into layers with activation functions providing non-linearity. Training happens through the forward pass (make predictions), loss calculation (measure error), backward pass (compute gradients via backpropagation), and optimization (update weights).\nThe fundamental workflow is always the same: load data → build model → define loss → create optimizer → train loop → evaluate. PyTorch handles the complex parts (automatic differentiation, gradient computation) while you focus on architecture, hyperparameters, and diagnosis.\nTraining curves tell you everything. Validation loss tracking training loss? Good. Validation loss increasing while training decreases? Overfitting—add regularization. Both losses high? Underfitting—increase capacity. Losses erratic? Learning rate too high. Reading these curves is your most valuable skill.\nRegularization prevents overfitting: dropout randomly disables neurons, batch normalization stabilizes training, L2 regularization penalizes large weights, and early stopping quits when validation performance plateaus. Use them together.\nArchitecture design is part science, part art. Start simple—2-3 layers with 32-64 neurons. If underfitting, add capacity (more layers or neurons). If overfitting, reduce capacity or add regularization. Don’t guess the perfect architecture—experiment systematically.\nLearning rate is your most important hyperparameter. Too low and training is slow. Too high and training explodes. Find the sweet spot through experimentation. Batch size, number of epochs, and regularization strength matter too, but learning rate matters most.\nFinally, know when to use neural networks and when not to. They excel with large datasets, complex patterns, and problems requiring flexibility. They struggle with small data, when interpretability matters, and for simple patterns. Try both neural networks and traditional ML, then choose based on the performance-complexity tradeoff.\nNeural networks are a powerful tool—but they’re just one tool. The art is knowing when to use them and when to reach for something simpler. Use your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#practice-exercises",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#practice-exercises",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nBuild and train an MLP: Create a 3-layer neural network for the Titanic survival dataset. Use 64 neurons in the first hidden layer, 32 in the second. Train for 50 epochs and plot training/validation loss curves. Identify whether your model is overfitting, underfitting, or well-fit.\nActivation function experiment: Build three identical networks but use different activation functions (ReLU, Sigmoid, Tanh) in the hidden layers. Train all three on the same data and compare final validation performance. Which works best? Why?\nRegularization comparison: Take a network that’s overfitting and fix it three different ways: (a) add dropout, (b) add batch normalization, (c) use early stopping. Compare the validation loss curves. Which technique helps most?\nLearning rate search: Train the same model with learning rates [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]. Plot all training curves on one graph. Identify the optimal learning rate range.\nArchitecture design: You have a dataset with 50 features and 20,000 examples for binary classification. Design three different architectures (small, medium, large) and justify your choices. Implement and compare them.\nWhen to use neural networks: For each scenario, decide whether you’d use a neural network or traditional ML, and explain why:\n\nPredicting apartment rent from 8 features with 2,000 examples\nClassifying dog breeds from images with 50,000 examples\nPredicting customer churn from 30 features with 5,000 examples\nDetecting faces in photos"
  },
  {
    "objectID": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#additional-resources",
    "href": "Textbook/Module-7-Neural-Networks/chapter-7-neural-networks.html#additional-resources",
    "title": "Chapter 7: Neural Networks and Deep Learning",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPyTorch Official Tutorial - Comprehensive tutorials from basics to advanced topics\nNeural Networks and Deep Learning (Michael Nielsen) - Free online book with excellent intuitive explanations\nCS231n: Convolutional Neural Networks - Stanford course notes with fantastic explanations\nDistill.pub - Beautiful interactive articles explaining neural network concepts\nPyTorch Documentation - Official API documentation\nDeep Learning Book (Goodfellow et al.) - Comprehensive but more mathematical treatment\nfast.ai Practical Deep Learning - Practical course focused on getting results quickly"
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "This document describes the images/diagrams that should be created for Chapter 5 on Classification Models.\n\n\nPurpose: A clean, annotated diagram explaining the structure of a confusion matrix\nDescription: - 2x2 grid showing Predicted (columns) vs Actual (rows) - Clearly labeled quadrants: - Top-left: True Negative (TN) - predicted negative, actually negative - Top-right: False Positive (FP) - predicted positive, actually negative (Type I error) - Bottom-left: False Negative (FN) - predicted negative, actually positive (Type II error) - Bottom-right: True Positive (TP) - predicted positive, actually positive - Color coding: greens for correct predictions (TN, TP), reds for errors (FP, FN) - Arrows showing how metrics are calculated: - Accuracy = (TP + TN) / Total - Precision = TP / (TP + FP) - Recall = TP / (TP + FN)\nStyle: Clean, professional diagram with clear labels. Use icons or simple graphics to make it visually appealing.\n\n\n\n\nPurpose: Side-by-side comparison of decision boundaries for different classifiers\nDescription: - 2x3 grid of scatter plots (or 3x2) - Each subplot shows the same 2D classification dataset (e.g., two classes in different colors) - Six subplots showing decision boundaries for: 1. Logistic Regression (linear boundary) 2. Decision Tree depth=3 (rectangular/axis-aligned splits) 3. Decision Tree depth=10 (more complex rectangular regions) 4. Random Forest (smooth but irregular boundary) 5. SVM with linear kernel (straight line) 6. SVM with RBF kernel (curved boundary) 7. k-NN with k=1 (very jagged, local boundary) 8. k-NN with k=20 (smoother boundary) - Each subplot clearly labeled with the model type - Decision boundary shown as a bold line or colored region - Data points shown as scattered dots with class colors\nStyle: Consistent color scheme across all subplots. Make it easy to compare the shapes of different boundaries.\n\n\n\n\nPurpose: Annotated ROC curve explaining how to interpret it\nDescription: - Classic ROC curve plot (True Positive Rate vs False Positive Rate) - Shows three example curves: 1. Perfect classifier (hugs the top-left corner, AUC = 1.0) 2. Good classifier (bow-shaped curve above diagonal, AUC ≈ 0.85) 3. Random classifier (diagonal line, AUC = 0.5) - Annotations pointing out: - “Best possible performance” at top-left corner (TPR=1, FPR=0) - “Random guessing” along the diagonal - Shaded area under curve with “AUC = Area Under Curve” - Arrow showing “Better models” direction (toward top-left) - Optionally show a few threshold values along the good classifier curve\nStyle: Clear, educational diagram. Use different line styles (solid, dashed, dotted) for the three curves.\n\n\n\n\nPurpose: Visualize the precision-recall tradeoff with threshold adjustment\nDescription: - Two panels side-by-side or stacked: - Left/Top: Precision-Recall curve showing how precision and recall change as threshold varies - Right/Bottom: Line plot showing precision and recall as separate lines vs threshold value - Annotations showing: - “High threshold → High precision, Low recall” - “Low threshold → Low precision, High recall” - The optimal F1 score point - Visual example showing why the tradeoff exists (e.g., small threshold = predict positive more often = catch more positives but more false positives too)\nStyle: Clear, intuitive visualization. Use contrasting colors for precision vs recall lines.\n\n\n\n\nPurpose: Illustrate why class imbalance is problematic\nDescription: - Side-by-side comparison of balanced vs imbalanced datasets - Left panel: Balanced dataset (50/50 split) - Pie chart or bar chart showing class distribution - Example confusion matrix from a reasonable classifier - Metrics: Accuracy = 85%, Precision = 83%, Recall = 87% - Right panel: Imbalanced dataset (95/5 split) - Pie chart or bar chart showing class distribution - Example confusion matrix from a naive classifier (predicts majority class) - Metrics: Accuracy = 95% (misleading!), Precision = 0%, Recall = 0% - Visual highlighting showing “95% accuracy but useless for minority class!”\nStyle: Use visual contrast to show the problem. Make it clear that high accuracy doesn’t mean good performance on imbalanced data.\n\n\n\n\nPurpose: Conceptual diagram showing how Random Forests combine multiple decision trees\nDescription: - Show the Random Forest process: 1. Original dataset (shown as a grid or table icon) 2. Arrows pointing to multiple bootstrap samples (3-5 copies with slight variations) 3. Each sample trains a decision tree (show simplified tree diagrams) 4. Trees make individual predictions (show thumbs up/down or class labels) 5. Final prediction combines via voting (show majority vote mechanism) - Annotations explaining: - “Bootstrap sampling (with replacement)” - “Random feature subset at each split” - “Majority vote for final prediction” - Visual emphasis on diversity: show that trees are different from each other\nStyle: Flow diagram with clear arrows showing the process. Use icons and simple graphics to make it intuitive.\n\n\n\n\n\n\n\nDiagram showing support vectors and the maximum margin\nTwo parallel lines showing the margin\nSupport vectors highlighted\nDecision boundary in the middle\n\n\n\n\n\nScatter plot showing a new point to classify\nCircles of different radii showing k=1, k=3, k=5 neighbors\nVisual showing how the class is determined by majority vote\n\n\n\n\n\nPlot of sigmoid function\nAnnotations showing asymptotic behavior (approaches 0 and 1)\nConnection to logistic regression (linear combination → sigmoid → probability)\n\n\n\n\n\n\n\nAll images should use a consistent color palette throughout the chapter\nUse colorblind-friendly colors (avoid red-green only distinctions)\nInclude clear labels and legends on all plots\nKeep diagrams simple and focused on one concept at a time\nUse professional fonts and consistent styling\nSave images as PNG with transparent backgrounds where appropriate\nRecommended size: 1200-2000 pixels wide for main diagrams"
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#confusion-matrix-diagram.png",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#confusion-matrix-diagram.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "Purpose: A clean, annotated diagram explaining the structure of a confusion matrix\nDescription: - 2x2 grid showing Predicted (columns) vs Actual (rows) - Clearly labeled quadrants: - Top-left: True Negative (TN) - predicted negative, actually negative - Top-right: False Positive (FP) - predicted positive, actually negative (Type I error) - Bottom-left: False Negative (FN) - predicted negative, actually positive (Type II error) - Bottom-right: True Positive (TP) - predicted positive, actually positive - Color coding: greens for correct predictions (TN, TP), reds for errors (FP, FN) - Arrows showing how metrics are calculated: - Accuracy = (TP + TN) / Total - Precision = TP / (TP + FP) - Recall = TP / (TP + FN)\nStyle: Clean, professional diagram with clear labels. Use icons or simple graphics to make it visually appealing."
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#decision-boundary-comparison.png",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#decision-boundary-comparison.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "Purpose: Side-by-side comparison of decision boundaries for different classifiers\nDescription: - 2x3 grid of scatter plots (or 3x2) - Each subplot shows the same 2D classification dataset (e.g., two classes in different colors) - Six subplots showing decision boundaries for: 1. Logistic Regression (linear boundary) 2. Decision Tree depth=3 (rectangular/axis-aligned splits) 3. Decision Tree depth=10 (more complex rectangular regions) 4. Random Forest (smooth but irregular boundary) 5. SVM with linear kernel (straight line) 6. SVM with RBF kernel (curved boundary) 7. k-NN with k=1 (very jagged, local boundary) 8. k-NN with k=20 (smoother boundary) - Each subplot clearly labeled with the model type - Decision boundary shown as a bold line or colored region - Data points shown as scattered dots with class colors\nStyle: Consistent color scheme across all subplots. Make it easy to compare the shapes of different boundaries."
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#roc-curve-interpretation.png",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#roc-curve-interpretation.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "Purpose: Annotated ROC curve explaining how to interpret it\nDescription: - Classic ROC curve plot (True Positive Rate vs False Positive Rate) - Shows three example curves: 1. Perfect classifier (hugs the top-left corner, AUC = 1.0) 2. Good classifier (bow-shaped curve above diagonal, AUC ≈ 0.85) 3. Random classifier (diagonal line, AUC = 0.5) - Annotations pointing out: - “Best possible performance” at top-left corner (TPR=1, FPR=0) - “Random guessing” along the diagonal - Shaded area under curve with “AUC = Area Under Curve” - Arrow showing “Better models” direction (toward top-left) - Optionally show a few threshold values along the good classifier curve\nStyle: Clear, educational diagram. Use different line styles (solid, dashed, dotted) for the three curves."
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#precision-recall-tradeoff.png",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#precision-recall-tradeoff.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "Purpose: Visualize the precision-recall tradeoff with threshold adjustment\nDescription: - Two panels side-by-side or stacked: - Left/Top: Precision-Recall curve showing how precision and recall change as threshold varies - Right/Bottom: Line plot showing precision and recall as separate lines vs threshold value - Annotations showing: - “High threshold → High precision, Low recall” - “Low threshold → Low precision, High recall” - The optimal F1 score point - Visual example showing why the tradeoff exists (e.g., small threshold = predict positive more often = catch more positives but more false positives too)\nStyle: Clear, intuitive visualization. Use contrasting colors for precision vs recall lines."
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#class-imbalance-problem.png",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#class-imbalance-problem.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "Purpose: Illustrate why class imbalance is problematic\nDescription: - Side-by-side comparison of balanced vs imbalanced datasets - Left panel: Balanced dataset (50/50 split) - Pie chart or bar chart showing class distribution - Example confusion matrix from a reasonable classifier - Metrics: Accuracy = 85%, Precision = 83%, Recall = 87% - Right panel: Imbalanced dataset (95/5 split) - Pie chart or bar chart showing class distribution - Example confusion matrix from a naive classifier (predicts majority class) - Metrics: Accuracy = 95% (misleading!), Precision = 0%, Recall = 0% - Visual highlighting showing “95% accuracy but useless for minority class!”\nStyle: Use visual contrast to show the problem. Make it clear that high accuracy doesn’t mean good performance on imbalanced data."
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#ensemble-method-illustration.png",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#ensemble-method-illustration.png",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "Purpose: Conceptual diagram showing how Random Forests combine multiple decision trees\nDescription: - Show the Random Forest process: 1. Original dataset (shown as a grid or table icon) 2. Arrows pointing to multiple bootstrap samples (3-5 copies with slight variations) 3. Each sample trains a decision tree (show simplified tree diagrams) 4. Trees make individual predictions (show thumbs up/down or class labels) 5. Final prediction combines via voting (show majority vote mechanism) - Annotations explaining: - “Bootstrap sampling (with replacement)” - “Random feature subset at each split” - “Majority vote for final prediction” - Visual emphasis on diversity: show that trees are different from each other\nStyle: Flow diagram with clear arrows showing the process. Use icons and simple graphics to make it intuitive."
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#optional-additional-images",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#optional-additional-images",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "Diagram showing support vectors and the maximum margin\nTwo parallel lines showing the margin\nSupport vectors highlighted\nDecision boundary in the middle\n\n\n\n\n\nScatter plot showing a new point to classify\nCircles of different radii showing k=1, k=3, k=5 neighbors\nVisual showing how the class is determined by majority vote\n\n\n\n\n\nPlot of sigmoid function\nAnnotations showing asymptotic behavior (approaches 0 and 1)\nConnection to logistic regression (linear combination → sigmoid → probability)"
  },
  {
    "objectID": "Textbook/Module-3-Classification/images/placeholder-info.html#notes",
    "href": "Textbook/Module-3-Classification/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Chapter 5: Classification Models",
    "section": "",
    "text": "All images should use a consistent color palette throughout the chapter\nUse colorblind-friendly colors (avoid red-green only distinctions)\nInclude clear labels and legends on all plots\nKeep diagrams simple and focused on one concept at a time\nUse professional fonts and consistent styling\nSave images as PNG with transparent backgrounds where appropriate\nRecommended size: 1200-2000 pixels wide for main diagrams"
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html",
    "title": "Chapter 2: Regression Models",
    "section": "",
    "text": "Related Assignments:\n\nModule 2 Homework\nModule 2 Quiz"
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#module-resources",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#module-resources",
    "title": "Chapter 2: Regression Models",
    "section": "",
    "text": "Related Assignments:\n\nModule 2 Homework\nModule 2 Quiz"
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#introduction",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#introduction",
    "title": "Chapter 2: Regression Models",
    "section": "Introduction",
    "text": "Introduction\nLinear regression is perhaps the most important algorithm in machine learning. Not because it’s the most powerful—it’s not. Not because it always works—it doesn’t. Linear regression matters because understanding it deeply gives you the foundation to understand almost everything else in machine learning.\nHere’s the thing: most machine learning is just sophisticated regression. Neural networks? Stacked regressions with non-linear activation functions. Logistic regression? Linear regression passed through a special function. Even tree-based methods are essentially breaking the data into regions and fitting constants (which are just simple regressions) in each region.\nBut linear regression has a dirty secret: it only works well when certain assumptions are met. Violate those assumptions, and your beautiful model with a high R² might be making terrible predictions. This is where many beginners get burned. They fit a model, see a nice R² value, and think they’re done. Then they deploy it to production and watch it fail spectacularly.\nThis chapter is about becoming a regression expert. Not just someone who can call LinearRegression().fit(), but someone who knows when regression will work, when it won’t, and how to fix it when it breaks. We’ll cover the assumptions behind linear regression, how to check if they’re violated, what to do when they are, and how regularization techniques can save you when things get messy.\n\n\n\n\n\n\nTip\n\n\n\nYou may assume that linear regression is only useful when your data has a linear relationship. But this is not true! Even data with complex relationships can be modeled using linear regression with the right features, transformations, and regularization techniques.\n\n\nBy the end of this chapter, you’ll understand:\n\nWhat assumptions linear regression makes and why they matter\nHow to diagnose problems using residual plots\nWhen and how to use polynomial features for non-linear relationships\nHow multicollinearity breaks coefficient interpretation\nHow Ridge, Lasso, and Elastic Net regularization fix overfitting and multicollinearity\nHow to choose between different regression approaches\n\nLet’s jump in."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#trainvalidationtest-methodology",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#trainvalidationtest-methodology",
    "title": "Chapter 2: Regression Models",
    "section": "1. Train/Validation/Test Methodology",
    "text": "1. Train/Validation/Test Methodology\nBefore we dive into specific regression models, we need to establish a crucial foundation: how to properly split and evaluate our data. Understanding train/validation/test methodology is essential for building models that actually generalize to new data.\n\n1.1 Why Three Sets?\nYou’ve seen train/test splits. But proper ML methodology requires three sets. Here’s why:\n\nTraining set: Used to fit the model (learn parameters)\nValidation set: Used to tune hyperparameters and select models\nTest set: Used ONCE at the very end to get an unbiased estimate of performance\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Load California housing data for demonstration\nhousing_df = pd.read_csv('../data/housing.csv')\n\n# Select features and target\nX = housing_df[['median_income', 'housing_median_age', 'total_rooms', 'population']]\ny = housing_df['median_house_value']\n\n# Remove missing values\nX_clean = X.dropna()\ny_clean = y[X_clean.index]\n\n# Proper three-way split\ndef train_val_test_split(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=1):\n    \"\"\"Split data into train, validation, and test sets\"\"\"\n\n    # First split: separate test set\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    # Second split: separate train and validation\n    # val_size needs to be recalculated relative to remaining data\n    val_size_adjusted = val_size / (train_size + val_size)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n    )\n\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\n# Split the data\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X_clean, y_clean)\n\nprint(\"Dataset sizes (California housing):\")\nprint(f\"Training: {len(X_train)} samples ({len(X_train)/len(X_clean)*100:.0f}%)\")\nprint(f\"Validation: {len(X_val)} samples ({len(X_val)/len(X_clean)*100:.0f}%)\")\nprint(f\"Test: {len(X_test)} samples ({len(X_test)/len(X_clean)*100:.0f}%)\")\nprint(f\"Total: {len(X_clean)} samples\")\n\nDataset sizes (California housing):\nTraining: 12384 samples (60%)\nValidation: 4128 samples (20%)\nTest: 4128 samples (20%)\nTotal: 20640 samples\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNever use the test set for model selection or hyperparameter tuning. If you evaluate multiple models on the test set and pick the best, you’ve contaminated it. The test set should be used exactly once, at the very end, to get an honest estimate of how your chosen model will perform on new data.\n\n\n\n\n1.2 Cross-Validation: Getting More Reliable Estimates\nA single train/validation split can be unlucky—maybe the validation set happens to be easy or hard. Cross-validation solves this by using multiple validation sets.\nIn cross-validation, we split the training data into multiple “folds”. The model is fit on some folds and validated on the remaining fold. This process is repeated multiple times, with each fold serving as the validation set once. The final cross-validation score is the average of all validation scores.\nThe benefits of this are that:\n\nWe can better understand the variability of our predictions, because we can see how much the model’s performance changes when we use different validation sets.\nWe can keep our test set “clean”, and only use it once at the very end.\n\nLet’s implement this in code. Here we’ll use cross-validation to find the optimal number of neighbors for a KNN regression model. We’ll choose between 2, 5, 10, 20, 30, 40, and 50 neighbors. Rather than doing a single fit on the training set and then validating on the validation set, we’ll use cross-validation. Doing so allows us to only use the test set once at the very end, and also to better understand the variability of our predictions, which we’ll show as error bars.\n\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\n# Use cross-validation to find the optimal number of neighbors\n# Test different values of n_neighbors\nn_neighbors_options = [2, 5, 10, 20, 30, 40, 50]\n\nresults = []\nfor n in n_neighbors_options:\n    # Create model with this value of n_neighbors\n    model = KNeighborsRegressor(n_neighbors=n)\n\n    # Perform 5-fold cross-validation\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5,\n                                 scoring='neg_mean_squared_error')\n\n    # Convert to MSE (scores are negative)\n    mse_scores = -cv_scores\n    mean_mse = mse_scores.mean()\n    std_mse = mse_scores.std()\n\n    results.append({\n        'n_neighbors': n,\n        'mean_mse': mean_mse,\n        'std_mse': std_mse\n    })\n\n# Find the best n_neighbors\nbest_result = min(results, key=lambda x: x['mean_mse'])\n\n# Visualize the results\nplt.figure(figsize=(10, 6))\nn_values = [r['n_neighbors'] for r in results]\nmean_mses = [r['mean_mse'] for r in results]\nstd_mses = [r['std_mse'] for r in results]\n\nplt.errorbar(n_values, mean_mses, yerr=std_mses, marker='o', linewidth=2,\n             capsize=5, capthick=2, markersize=8, alpha=0.75)\nplt.xlabel('Number of Neighbors', fontsize=12)\nplt.ylabel('Mean Squared Error', fontsize=12)\nplt.title('Cross-Validation: Finding Optimal n_neighbors', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nWe see that 10 neighbors appears optimal, as it has the lowest mean squared error with reasonably low variation. This means that it generalizes well to new data.\nCross-validation gives you:\n\nMore reliable estimates: Averages over multiple validation sets\nUncertainty quantification: Standard deviation tells you how variable performance is\nBetter use of data: Every example gets to be in the validation set once\n\n\n\n\n\n\n\nTip\n\n\n\nUse cross-validation for model selection when you have limited data. For very large datasets, a single train/val/test split is often sufficient and much faster."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#linear-regression-revisited",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#linear-regression-revisited",
    "title": "Chapter 2: Regression Models",
    "section": "2. Linear Regression Revisited",
    "text": "2. Linear Regression Revisited\n\n2.1 The Line of Best Fit\nYou’ve probably heard linear regression described as “finding the line of best fit.” But what does “best” actually mean? Best according to what criteria?\nThe answer: best means the line that minimizes the sum of squared errors. For each data point, we calculate how far the prediction is from the actual value (the error), square it, and add up all these squared errors. We can either minimize this sum, or minimize the average, they’re equivalent. The line that makes this value as small as possible is our “best fit” line. The equation is\n\\[\n\\frac{1}{n}\\displaystyle\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nDoes this look familiar? It’s just mean squared error (MSE), which is the loss function for linear regression. The goal of linear regression is to find the line that minimizes this loss.\nLet’s see this in action with the NYC census data:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Load NYC census data\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Filter to complete cases for income prediction\nnyc_clean = nyc_census[['IncomePerCap', 'Income']].dropna()\n\n# Let's predict median household income from per-capita income\n# Start with just one feature to visualize easily\nX = nyc_clean[['IncomePerCap']].values  # Income per capita\ny = nyc_clean['Income'].values  # Median household income\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit linear regression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\n# Visualize the fit\nplt.figure(figsize=(10, 6))\nplt.scatter(X_train, y_train, alpha=0.5, s=20, label='Training Data')\nplt.plot(sorted(X_train.flatten()), model.predict(sorted(X_train.reshape(-1, 1))),\n         'r-', linewidth=2, label='Best Fit Line')\nplt.xlabel('Income Per Capita ($)', fontsize=12)\nplt.ylabel('Median Household Income ($)', fontsize=12)\nplt.title('Linear Regression: Per Capita Income vs Household Income', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Slope (coefficient): {model.coef_[0]:.4f}\")\nprint(f\"Intercept: {model.intercept_:.4f}\")\nprint(f\"Training R²: {model.score(X_train, y_train):.4f}\")\nprint(f\"Test R²: {model.score(X_test, y_test):.4f}\")\n\n\n\n\n\n\n\n\nSlope (coefficient): 0.9894\nIntercept: 27540.5059\nTraining R²: 0.6878\nTest R²: 0.7202\n\n\nThe regression line tries to find the balance that minimizes total squared error across all points. The slope and y-intercept are learned (machine learning) such that the sum of squared residuals, or MSE, is as small as possible.\n\n\n2.2 The Assumptions Behind Linear Regression\nLinear regression makes four key assumptions. Violate them, and your model might give you terrible predictions even if the R² looks good.\nThe Four Assumptions:\n\nLinearity: The relationship between X and y is actually linear. If it’s curved, a straight line won’t fit well (however, we’ll handle those cases later in this chapter).\nIndependence: Each observation is independent. One house’s price doesn’t directly affect another’s. (This can be violated with time series or spatial data.)\nHomoscedasticity: The “spread” of points around the line should be roughly the same everywhere. If errors are tiny for low X values but huge for high X values, that’s a problem. (The fancy term means “constant variance.”)\nNormality: The residuals (errors) are normally distributed. This matters more for statistical inference than prediction.\n\nWhat happens when you violate these?\n\nViolate linearity: Your predictions will be systematically wrong in certain ranges\nViolate independence: Your confidence intervals and p-values become unreliable\nViolate homoscedasticity: Some predictions are much more uncertain than others (but you won’t know which ones!)\nViolate normality: Your confidence intervals might be wrong, but predictions can still be okay\n\nWe’ll learn how to check these assumptions using diagnostic plots in Section 3.\n\n\n2.3 Interpreting Coefficients\nCoefficients tell you the relationship between features and the target. Let’s extract and interpret them:\n\n# Fit a model with multiple features\nfeatures = ['TotalPop', 'Professional', 'Poverty', 'Unemployment']\nnyc_multi = nyc_census[features + ['Income']].dropna()\nX_multi = nyc_multi[features].values\ny_multi = nyc_multi['Income'].values\n\nX_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n    X_multi, y_multi, test_size=0.2, random_state=42\n)\n\n# Fit model\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_train_multi, y_train_multi)\n\n# Display coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': model_multi.coef_\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"Regression Coefficients:\")\nprint(coef_df)\nprint(f\"\\nIntercept: {model_multi.intercept_:.4f}\")\n\nRegression Coefficients:\n        Feature  Coefficient\n1  Professional   777.977464\n0      TotalPop    -0.184356\n3  Unemployment  -191.478521\n2       Poverty -1036.991744\n\nIntercept: 53635.7278\n\n\nHow to interpret these:\n\nProfessional coefficient: For every 1% increase in the percentage of professionals, median household income changes by the coefficient amount, holding all other features constant.\nPoverty coefficient: For every 1% increase in the poverty rate, median household income changes by the coefficient amount (likely negative), holding all other features constant.\nIntercept: The predicted median household income when all features are zero. Often not meaningful in practice (what census tract has zero population or zero unemployment?).\n\nNotice the key phrase: “holding all other features constant.” That’s crucial. It means the coefficient shows the isolated effect of that one feature, assuming everything else stays the same. Change one feature while keeping others fixed, and the coefficient tells you how much the prediction changes.\n\n\n\n\n\n\nWarning\n\n\n\nCoefficient interpretation breaks down when features are highly correlated (multicollinearity). We’ll address this in Section 5 when we discuss multicollinearity."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#regression-evaluation-metrics",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#regression-evaluation-metrics",
    "title": "Chapter 2: Regression Models",
    "section": "3. Regression Evaluation Metrics",
    "text": "3. Regression Evaluation Metrics\nHow do you know if your regression model is any good? You need metrics. R² is popular, but it can be misleading. Let’s look at the most important metrics, what they mean, and when to use each one.\n\n3.1 Mean Squared Error (MSE)\nMSE is the average of squared errors. Remember, an error (or residual) is just actual value minus predicted value. Square those, average them, and you have MSE.\nWhy does MSE matter? It directly reflects what linear regression minimizes during training. When you fit a linear regression, you’re literally finding the line that gives you the smallest possible MSE on the training data.\nBut here’s the catch: squaring errors means big errors get penalized heavily. If one prediction is off by 100 and another is off by 10, the first error contributes 10,000 to MSE while the second contributes only 100. That’s 100 times worse, not 10 times worse. This makes MSE sensitive to outliers.\n\n\n3.2 Mean Absolute Error (MAE)\nMAE is simpler: just the average of the absolute values of errors. No squaring involved.\nMAE vs MSE: What’s the difference?\nMAE treats all errors proportionally. An error of 100 is exactly 10 times worse than an error of 10. With MSE, that error of 100 is 100 times worse.\n\n\n\n\n\n\nTip\n\n\n\nWhile MSE does not preserve units (since it squares the errors), you can convert back to the original units by taking a square root, such as\n\\[\n\\text{RMSE} = \\sqrt{\\text{MSE}} = \\displaystyle\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\]\nThis is called “root mean squared error”, or RMSE. It’s in the same units as the target variable, making it more interpretable.\n\n\n\n\n\n\n\n\nTip\n\n\n\nUnless there’s an extremely compelling reason to do otherwise, you should always start with MSE or RMSE as your metric. It’s the most natural metric for linear regression, and it’s what linear regression optimizes.\nHowever, it’s often beneficial to also compute MAE, as it’s more interpretable and less sensitive to outliers. It’s typical to report both values, as they each have their own strengths.\n\n\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Calculate MAE and MSE\nmse_sklearn = mean_squared_error(y_test, y_pred_test)\nmae = mean_absolute_error(y_test, y_pred_test)\n\n# Calculate Root Mean Squared Error (RMSE) for comparison\nrmse = np.sqrt(mse_sklearn)\n\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\n\nprint(f\"\\nInterpretation:\")\nprint(f\"- On average, predictions are off by ${mae:.0f} (MAE)\")\nprint(f\"- MAE is lower than RMSE because MSE penalizes large errors more heavily\")\n\nMAE: 12478.0785\nRMSE: 15670.5818\n\nInterpretation:\n- On average, predictions are off by $12478 (MAE)\n- MAE is lower than RMSE because MSE penalizes large errors more heavily\n\n\nMAE is lower than RMSE (square root of MSE). That’s typical—RMSE is always at least as large as MAE, and the gap tells you something about outliers. A big gap means you have some really bad predictions pulling up the RMSE.\n\n\n3.3 R² (R-Squared)\nR² tells you the proportion of variance in the target variable that your model explains. It ranges from 0 to 1, where:\n\nR² = 1: Perfect predictions (your model explains 100% of the variance)\nR² = 0: Your model is no better than just predicting the mean every time\nR² &lt; 0: Your model is worse than predicting the mean (yes, this is possible!)\n\nThe formula is: R² = 1 - (Sum of Squared Residuals / Total Sum of Squares)\nBut what does that actually mean? Think of it this way: imagine you know nothing about the features and just predict the mean house value for every house. That’s your baseline. R² tells you how much better (or worse!) your model is than that naive baseline.\n\nfrom sklearn.metrics import r2_score\n\n# Calculate R² manually to understand it\n# Total Sum of Squares: how far each point is from the mean\ny_mean = y_test.mean()\ntotal_ss = ((y_test - y_mean) ** 2).sum()\n\n# Residual Sum of Squares: how far predictions are from actual values\nresidual_ss = ((y_test - y_pred_test) ** 2).sum()\n\n# R² = 1 - (residual variation / total variation)\nr2_manual = 1 - (residual_ss / total_ss)\n\n# Compare to sklearn\nr2_sklearn = r2_score(y_test, y_pred_test)\n\nprint(f\"R² (manual): {r2_manual:.4f}\")\nprint(f\"R² (sklearn): {r2_sklearn:.4f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"The model explains {r2_sklearn*100:.2f}% of the variance in house prices\")\nprint(f\"That means {(1-r2_sklearn)*100:.2f}% of the variance is unexplained\")\n\n# Visualize what R² means\nfig = plt.figure(figsize=(10, 6))\n\n# Baseline plot\nplt.scatter(X_test, y_test, alpha=0.3, s=10, label='Actual Data')\nplt.axhline(y_mean, color='red', linestyle='--', linewidth=2, label=f'Baseline: Predict Mean = {y_mean:.2f}')\n\n# Regression line\nplt.plot(X_test, y_pred_test, 'b-', linewidth=2, label=f'Regression Line (R² = {r2_sklearn:.4f})')\n\nplt.xlabel('Median Income (in $10,000s)', fontsize=12)\nplt.ylabel('Median House Value (in $100,000s)', fontsize=12)\nplt.title('Baseline Model (R² = 0)', fontsize=14)\n\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nR² (manual): 0.7202\nR² (sklearn): 0.7202\n\nInterpretation:\nThe model explains 72.02% of the variance in house prices\nThat means 27.98% of the variance is unexplained\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how our regression line is a significantly better fit than the baseline line. That leads to an R² value significantly above zero.\n\n\nR² is intuitive and easy to explain to non-technical audiences. But it has problems:\nProblem 1: R² always increases when you add features, even if those features are random noise. Add 100 random columns to your data, and R² will go up, even though the model hasn’t actually improved.\nProblem 2: R² doesn’t tell you if your predictions are biased. R² tells you how close to a line your actual vs predicted values are. What’s one easy way to get a straight line? Just predict the same y-value every single time! Congrats, you have a perfectly straight line, and thus have an R² of 100%! R² doesn’t tell you if your predictions are biased. You could have a high R² but still systematically overpredict or underpredict in certain ranges.\nProblem 3: R² can be negative on test data if your model is terrible. That’s a good warning sign, actually.\n\n\n3.4 Adjusted R²\nAdjusted R² fixes the “adding features always increases R²” problem. It penalizes you for adding features that don’t actually help.\nThe formula adjusts R² based on the number of features (p) and number of observations (n):\nAdjusted R² = \\(1 - \\displaystyle\\biggl[\\frac{(1 - R^2)(n - 1)}{(n - p - 1)}\\biggr]\\)\nAdd a useless feature? Regular R² might go from 0.75 to 0.751. But adjusted R² might drop from 0.75 to 0.748 because the penalty for adding a feature outweighs the tiny improvement.\nWhen to use Adjusted R²: - When comparing models with different numbers of features - When you’re worried about overfitting - When presenting results to stakeholders (it’s more honest)\nWhen regular R² is fine: - When comparing models with the same number of features - When you’re more concerned about prediction accuracy than interpretation\nReal-World Example: When Many Features Mislead\nLet’s see this with real census data from Staten Island. We’ll predict income using many demographic features:\n\n# Load NYC census tract data\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Filter to just Staten Island\nstaten_island_data = nyc_census[nyc_census['Borough'] == 'Staten Island'].copy()\n\n# Remove rows with missing income (our target)\nstaten_island_data = staten_island_data.dropna(subset=['Income'])\n\n# Select features (exclude identifiers and the target)\nfeature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',\n                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',\n                'ChildPoverty', 'Professional', 'Service', 'Office',\n                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',\n                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',\n                'Unemployment']\n\n# Prepare data (drop rows with any missing values)\nstaten_island_clean = staten_island_data[feature_cols + ['Income']].dropna()\n\nX_si = staten_island_clean[feature_cols].values\ny_si = staten_island_clean['Income'].values\n\nprint(f\"\\n{len(staten_island_clean)} observations\")\nprint(f\"Number of features: {len(feature_cols)}\")\nprint(f\"Ratio of features to observations (p/n): {len(feature_cols)/len(staten_island_clean):.3f}\")\n\n# Split data\nX_train_si, X_test_si, y_train_si, y_test_si = train_test_split(\n    X_si, y_si, test_size=0.3, random_state=42\n)\n\n# Fit model\nmodel_si = LinearRegression()\nmodel_si.fit(X_train_si, y_train_si)\n\n# Calculate metrics\ntrain_r2_si = model_si.score(X_train_si, y_train_si)\ntest_r2_si = model_si.score(X_test_si, y_test_si)\n\n# Calculate Adjusted R² for training set\nn_train = len(X_train_si)\np_si = X_train_si.shape[1]\nadj_r2_train_si = 1 - ((1 - train_r2_si) * (n_train - 1) / (n_train - p_si - 1))\n\n# Calculate Adjusted R² for test set\nn_test = len(X_test_si)\nadj_r2_test_si = 1 - ((1 - test_r2_si) * (n_test - 1) / (n_test - p_si - 1))\n\n# Visualize the comparison\nmetrics_si = pd.DataFrame({\n    'Metric': ['R²', 'Adjusted R²', 'R²', 'Adjusted R²'],\n    'Dataset': ['Training', 'Training', 'Test', 'Test'],\n    'Value': [train_r2_si, adj_r2_train_si, test_r2_si, adj_r2_test_si]\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx_pos = [0, 1, 3, 4]\ncolors = ['skyblue', 'lightcoral', 'skyblue', 'lightcoral']\nbars = ax.bar(x_pos, metrics_si['Value'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n\n# Add value labels on bars (handle negative values)\nfor i, (pos, val) in enumerate(zip(x_pos, metrics_si['Value'])):\n    y_offset = 0.02 if val &gt;= 0 else -0.05\n    ax.text(pos, val + y_offset, f'{val:.3f}', ha='center', fontsize=10, weight='bold')\n\nax.set_ylabel('Score', fontsize=12)\nax.set_title(f'R² vs Adjusted R² with {p_si} Features', fontsize=14)\nax.set_xticks(x_pos)\nax.set_xticklabels(['R²\\n\\nTraining', 'Adj R²\\n\\nTraining', 'R²\\n\\nTest', 'Adj R²\\n\\nTest'], fontsize=11)\n# Set y-limits to accommodate negative adjusted R² values\ny_min = min(metrics_si['Value'].min() - 0.1, -0.1)\nax.set_ylim(y_min, 1.1)\nax.axhline(y=0, color='black', linewidth=0.8, linestyle='-')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n107 observations\nNumber of features: 30\nRatio of features to observations (p/n): 0.280\n\n\n\n\n\n\n\n\n\nSee the problem? With 30 features and a small number of Staten Island census tracts, the picture is alarming:\n\nAdjusted R² can be negative - When test adjusted R² is negative, the model performs worse than just predicting the mean after accounting for model complexity. The penalty for using so many features overwhelms any predictive value. Remember, this is possible because the model learned from the training data, but is evaluated on the test data. What was learned from the training data is leading the model astray on the test data.\nTest R² drops significantly from training - the model doesn’t generalize well\nThe high p/n ratio is the culprit - too many features relative to observations\n\nThis is a realistic scenario where you might look at training R² and think “Great, decent fit!” but adjusted R² and test performance reveal the truth: the model is overfitted and unreliable.\n\n\n\n\n\n\nWarning\n\n\n\nRule of thumb: When your p/n ratio exceeds 0.10 (1 feature per 10 observations), be very skeptical of regular R². Always check Adjusted R² and test set performance. Better yet, use cross-validation to get a more honest assessment."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#residual-analysis-your-diagnostic-tool",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#residual-analysis-your-diagnostic-tool",
    "title": "Chapter 2: Regression Models",
    "section": "4. Residual Analysis: Your Diagnostic Tool",
    "text": "4. Residual Analysis: Your Diagnostic Tool\nMetrics like R² and MSE tell you how much error your model makes. But residual plots tell you where and why the model is making mistakes. This is where you catch problems before they bite you in production.\n\n4.1 What Are Residuals?\nA residual is simple: residual = actual value - predicted value.\nIf your model predicts a house is worth $250,000 but it’s actually worth $300,000, the residual is $50,000. Positive residual means you underpredicted. Negative residual means you overpredicted.\nWhy do residuals matter more than just looking at errors? Because the pattern of residuals reveals whether your model’s assumptions are violated. Random residuals? Great. Systematic patterns? Problem.\n\n# Load NYC census tract data (all boroughs this time)\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Remove rows with missing income (our target)\nnyc_census = nyc_census.dropna(subset=['Income'])\n\n# Select features (exclude identifiers and the target)\nfeature_cols = ['TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black',\n                'Native', 'Asian', 'Citizen', 'IncomePerCap', 'Poverty',\n                'ChildPoverty', 'Professional', 'Service', 'Office',\n                'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed',\n                'PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork',\n                'Unemployment']\n\n# Prepare data (drop rows with any missing values)\nnyc_census = nyc_census[feature_cols + ['Income']].dropna()\n\nX_nyc = nyc_census[feature_cols].values\ny_nyc = nyc_census['Income'].values\n\n# Split data\nX_train_nyc, X_test_nyc, y_train_nyc, y_test_nyc = train_test_split(\n    X_nyc, y_nyc, test_size=0.3, random_state=1\n)\n\n# Fit model\nmodel_nyc = LinearRegression()\nmodel_nyc.fit(X_train_nyc, y_train_nyc)\n\n# Calculate residuals from our simple model\ny_pred_test = model_nyc.predict(X_test_nyc)\nresiduals = y_test_nyc - y_pred_test\n\n# Create a DataFrame for easy viewing\nresidual_df = pd.DataFrame({\n    'Actual': y_test_nyc.flatten(),\n    'Predicted': y_pred_test.flatten(),\n    'Residual': residuals.flatten()\n})\n\nresidual_df.head()\n\n\n\n\n\n\n\n\nActual\nPredicted\nResidual\n\n\n\n\n0\n87708.0\n89123.902250\n-1415.902250\n\n\n1\n21577.0\n20985.111768\n591.888232\n\n\n2\n51595.0\n59802.080768\n-8207.080768\n\n\n3\n102625.0\n72263.008289\n30361.991711\n\n\n4\n51045.0\n58308.513690\n-7263.513690\n\n\n\n\n\n\n\nNotice that the residuals all fluctuate around zero? That’s expected for linear regression—the errors cancel out on average. But that doesn’t mean the errors are acceptable! We need to look at the pattern.\n\n\n4.2 Residuals vs. Fitted Values Plot\nThis is the single most important diagnostic plot you’ll make. It plots residuals (y-axis) against predicted values (x-axis).\nWhat you want to see: Random scatter around zero. No patterns, no trends, just noise.\nWhat you don’t want to see: Curves, funnels, or systematic patterns. These indicate problems.\n\n# Create the residuals vs fitted plot\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred_test, residuals, alpha=0.5, s=20)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.xlabel('Fitted Values (Predictions)', fontsize=12)\nplt.ylabel('Residuals', fontsize=12)\nplt.title('Residuals vs. Fitted Values', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Also show distribution of residuals\nplt.figure(figsize=(10, 5))\nplt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Residual', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.title('Distribution of Residuals', fontsize=14)\nplt.axvline(0, color='red', linestyle='--', linewidth=2)\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee what’s happening? The residuals are somewhat randomly scattered, but there’s two small issues:\n\nThe model tends to underpredict (positive residuals) for very low predictions.\nThe model tends to overpredict (negative residuals) for very high predictions.\n\nThis suggests the relationship might not be perfectly linear. However, despite this, this shows a reasonably good fit overall, with normally distributed residuals centered around zero.\n\n\n4.3 What Patterns Tell You\nLet’s look at specific patterns and what they mean:\nPattern 1: Curved Residuals (Non-Linearity)\nIf residuals form a U-shape or inverted U-shape, your relationship isn’t linear. The model systematically underpredicts in some regions and overpredicts in others.\nFix: Try polynomial features, log transforms, or use a non-linear model.\nPattern 2: Funnel Shape (Heteroscedasticity)\nIf residuals spread out as predictions increase (or decrease), you have non-constant variance. Maybe small houses have predictable prices, but mansion prices vary wildly.\nFix: Transform the target variable (log, square root), use weighted regression, or use a model that handles heteroscedasticity.\nPattern 3: Outliers\nPoints far from zero are outliers. A few outliers are normal. Many outliers mean something’s wrong with your model or data.\nFix: Investigate outliers (data errors? special cases?), consider robust regression, or use regularization.\nLet’s demonstrate these patterns with synthetic data, comparing good vs. bad residual plots:\n\n# Create three datasets: good fit, non-linear, and heteroscedastic\nnp.random.seed(42)\n\n# GOOD: Linear relationship with constant variance\nX_good = np.linspace(0, 10, 200).reshape(-1, 1)\ny_good = 5 * X_good.flatten() + np.random.normal(0, 5, 200)\n\nmodel_good = LinearRegression()\nmodel_good.fit(X_good, y_good)\ny_pred_good = model_good.predict(X_good)\nresiduals_good = y_good - y_pred_good\n\n# BAD: Non-linear relationship\nX_nonlinear = np.linspace(0, 10, 200).reshape(-1, 1)\ny_nonlinear = 2 * X_nonlinear.flatten()**2 + np.random.normal(0, 10, 200)\n\nmodel_nonlinear = LinearRegression()\nmodel_nonlinear.fit(X_nonlinear, y_nonlinear)\ny_pred_nonlinear = model_nonlinear.predict(X_nonlinear)\nresiduals_nonlinear = y_nonlinear - y_pred_nonlinear\n\n# BAD: Heteroscedasticity (funnel pattern)\nX_hetero = np.linspace(1, 10, 200).reshape(-1, 1)\ny_hetero = 5 * X_hetero.flatten() + np.random.normal(0, X_hetero.flatten(), 200)\n\nmodel_hetero = LinearRegression()\nmodel_hetero.fit(X_hetero, y_hetero)\ny_pred_hetero = model_hetero.predict(X_hetero)\nresiduals_hetero = y_hetero - y_pred_hetero\n\n# Create comparison plot\nfig, axes = plt.subplots(3, 2, figsize=(14, 15))\n\n# Row 1: GOOD - Linear data with constant variance\naxes[0, 0].scatter(X_good, y_good, alpha=0.5, s=10, color='green')\naxes[0, 0].plot(X_good, y_pred_good, 'r-', linewidth=2)\naxes[0, 0].set_title('GOOD: Linear Data, Constant Variance', fontsize=12, fontweight='bold', color='green')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('y')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].scatter(y_pred_good, residuals_good, alpha=0.5, s=10, color='green')\naxes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[0, 1].set_title('✓ Random Scatter (Good!)', fontsize=12, fontweight='bold', color='green')\naxes[0, 1].set_xlabel('Fitted Values')\naxes[0, 1].set_ylabel('Residuals')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Row 2: BAD - Non-linearity\naxes[1, 0].scatter(X_nonlinear, y_nonlinear, alpha=0.5, s=10, color='orange')\naxes[1, 0].plot(X_nonlinear, y_pred_nonlinear, 'r-', linewidth=2)\naxes[1, 0].set_title('BAD: Non-Linear Data with Linear Fit', fontsize=12, fontweight='bold', color='orange')\naxes[1, 0].set_xlabel('X')\naxes[1, 0].set_ylabel('y')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].scatter(y_pred_nonlinear, residuals_nonlinear, alpha=0.5, s=10, color='orange')\naxes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[1, 1].set_title('✗ Curved Pattern (Bad!)', fontsize=12, fontweight='bold', color='orange')\naxes[1, 1].set_xlabel('Fitted Values')\naxes[1, 1].set_ylabel('Residuals')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Row 3: BAD - Heteroscedasticity\naxes[2, 0].scatter(X_hetero, y_hetero, alpha=0.5, s=10, color='red')\naxes[2, 0].plot(X_hetero, y_pred_hetero, 'r-', linewidth=2)\naxes[2, 0].set_title('BAD: Data with Increasing Variance', fontsize=12, fontweight='bold', color='red')\naxes[2, 0].set_xlabel('X')\naxes[2, 0].set_ylabel('y')\naxes[2, 0].grid(True, alpha=0.3)\n\naxes[2, 1].scatter(y_pred_hetero, residuals_hetero, alpha=0.5, s=10, color='red')\naxes[2, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[2, 1].set_title('✗ Funnel Pattern (Bad!)', fontsize=12, fontweight='bold', color='red')\naxes[2, 1].set_xlabel('Fitted Values')\naxes[2, 1].set_ylabel('Residuals')\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Compare the three residual plots (right column):\")\nprint(\"  TOP (Green): Random scatter around zero = GOOD\")\nprint(\"  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)\")\nprint(\"  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)\")\n\n\n\n\n\n\n\n\nCompare the three residual plots (right column):\n  TOP (Green): Random scatter around zero = GOOD\n  MIDDLE (Orange): U-shaped curve = BAD (non-linearity)\n  BOTTOM (Red): Funnel/cone shape = BAD (heteroscedasticity)\n\n\nSee the difference?\n\nTop row (GOOD): Residuals are randomly scattered around zero with constant spread. This is what you want!\nMiddle row (BAD - Curved): Residuals show a clear U-shaped pattern—the model systematically underpredicts at the extremes and overpredicts in the middle.\nBottom row (BAD - Funnel): Residuals fan out as predictions increase—variance is not constant.\n\n\n\n\n\n\n\nWarning\n\n\n\nA high R² doesn’t mean your assumptions are met! You can have R² = 0.95 but still have terrible residual patterns that indicate the model will fail on new data.\n\n\nThese plots are your early warning system. Learn to read them, and you’ll catch problems before they become disasters.\n\n\n4.4 What to Do When Assumptions Fail\nOkay, you’ve identified a problem in your residual plots. Now what?\nProblem: Non-Linearity (curved residual plot)\nSolutions:\n\nPolynomial features: Add X², X³, etc. (covered in Section 4)\nTransform features: Try log(X), √X, or 1/X\nTransform target: Try log(y) or √y\nUse a non-linear model: Tree-based models, neural networks, etc.\n\nProblem: Heteroscedasticity (funnel residual plot)\nSolutions:\n\nTransform target variable: log(y) often stabilizes variance\nWeighted least squares: Give less weight to high-variance observations\nUse robust standard errors: Adjust your confidence intervals\nJust accept it: If you only care about predictions (not inference), heteroscedasticity matters less\n\nProblem: Outliers\nSolutions:\n\nInvestigate: Are they data errors? Real but unusual observations?\nRemove them: Only if justified (e.g., data entry errors)\nUse robust regression: Huber regression, RANSAC, etc.\nUse regularization: Ridge and Lasso reduce outlier influence (covered in Sections 6-7)\n\n\n\n\n\n\n\nTip\n\n\n\nStart with the simplest fix first. A log transformation of the target variable often fixes multiple problems at once: non-linearity and heteroscedasticity."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#polynomial-regression-handling-non-linearity",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#polynomial-regression-handling-non-linearity",
    "title": "Chapter 2: Regression Models",
    "section": "5. Polynomial Regression: Handling Non-Linearity",
    "text": "5. Polynomial Regression: Handling Non-Linearity\nWhat do you do when your residual plot shows a clear curve? The relationship isn’t linear, so a straight line won’t work. This is where polynomial regression saves you.\n\n5.1 When Linear Isn’t Enough\nReal relationships are rarely perfectly linear. Temperature and ice cream sales? Definitely curved (sales don’t go negative when it’s cold, and they plateau when it’s hot).\nPolynomial regression lets you fit curves while still using linear regression. The trick? Create new features that are powers of your original features: X², X³, etc. Then fit a linear model to these polynomial features.\nHere’s the key insight: polynomial regression is still linear regression. It’s linear in the coefficients, even though the relationship with X is non-linear. The model is y = β₀ + β₁X + β₂X² + β₃X³, which is a linear combination of the features [X, X², X³].\n\n\n5.2 Creating Polynomial Features\nLet’s see this in action. We’ll create polynomial features and fit them to data with a clear non-linear relationship.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Use data with a non-linear relationship\n# Let's explore Unemployment vs Income (often a non-linear relationship)\nnyc_poly = nyc_census[['Unemployment', 'Income']].dropna()\n\n# Filter to only unemployment under 30% (ignore outliers)\nnyc_poly = nyc_poly[nyc_poly['Unemployment'] &lt; 30]\n\nX_unemp = nyc_poly[['Unemployment']].values\ny_income_poly = nyc_poly['Income'].values\n\n# Split the data\nX_unemp_train, X_unemp_test, y_income_poly_train, y_income_poly_test = train_test_split(\n    X_unemp, y_income_poly, test_size=0.2, random_state=42\n)\n\n# First, try linear regression\nmodel_linear_unemp = LinearRegression()\nmodel_linear_unemp.fit(X_unemp_train, y_income_poly_train)\ny_pred_linear_unemp = model_linear_unemp.predict(X_unemp_test)\n\nprint(\"Linear Model:\")\nprint(f\"R² = {model_linear_unemp.score(X_unemp_test, y_income_poly_test):.4f}\")\n\n# Now try polynomial regression (degree 2)\npoly_2 = PolynomialFeatures(degree=2, include_bias=False)\nX_unemp_train_poly2 = poly_2.fit_transform(X_unemp_train)\nX_unemp_test_poly2 = poly_2.transform(X_unemp_test)\n\nprint(f\"\\nOriginal features shape: {X_unemp_train.shape}\")\nprint(f\"Polynomial features shape: {X_unemp_train_poly2.shape}\")\nprint(f\"\\nFeature names: {poly_2.get_feature_names_out(['Unemployment'])}\")\n\nmodel_poly2 = LinearRegression()\nmodel_poly2.fit(X_unemp_train_poly2, y_income_poly_train)\ny_pred_poly2 = model_poly2.predict(X_unemp_test_poly2)\n\nprint(\"\\nDegree-2 Polynomial Model:\")\nprint(f\"R² = {model_poly2.score(X_unemp_test_poly2, y_income_poly_test):.4f}\")\nprint(f\"Coefficients: {model_poly2.coef_}\")\n\n# Visualize the difference\nplt.figure(figsize=(10, 6))\n# Sort for smooth plotting\nsort_idx = np.argsort(X_unemp_test.flatten())\nX_test_sorted = X_unemp_test.flatten()[sort_idx]\ny_pred_linear_sorted = y_pred_linear_unemp.flatten()[sort_idx]\ny_pred_poly2_sorted = y_pred_poly2.flatten()[sort_idx]\n\nplt.scatter(X_unemp_test, y_income_poly_test, alpha=0.3, s=10, label='Actual Data')\nplt.plot(X_test_sorted, y_pred_linear_sorted, 'r-', linewidth=2, label='Linear Model')\nplt.plot(X_test_sorted, y_pred_poly2_sorted, 'g-', linewidth=2, label='Degree-2 Polynomial')\nplt.xlabel('Unemployment Rate (%)', fontsize=12)\nplt.ylabel('Median Household Income ($)', fontsize=12)\nplt.title('Linear vs Polynomial Regression', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nLinear Model:\nR² = 0.2180\n\nOriginal features shape: (1670, 1)\nPolynomial features shape: (1670, 2)\n\nFeature names: ['Unemployment' 'Unemployment^2']\n\nDegree-2 Polynomial Model:\nR² = 0.2339\nCoefficients: [-5763.42735409   114.23513525]\n\n\n\n\n\n\n\n\n\nSee what happened? PolynomialFeatures(degree=2) created a new feature: Unemployment². The linear model then fits: y = β₀ + β₁(Unemployment) + β₂(Unemployment²). This gives us a parabola, which can capture curvature that a straight line can’t.\n\n\n\n\n\n\nTip\n\n\n\nHere we’re using the PolynomialFeatures pipeline from scikit-learn. However, you can also just square (or cube, or whatever you want) the column, such as df['unemp_poly2'] = df['unemp']**2.\n\n\n\n\n5.3 Choosing Polynomial Degree\nHow do you know what degree to use? Degree 1 is linear. Degree 2 adds curvature. Degree 3 adds an S-curve. Higher degrees add more wiggles.\nThe problem: Higher degree doesn’t always mean better. You can overfit spectacularly with high-degree polynomials.\nThe solution: Try multiple degrees and use a validation set (or cross-validation) to pick the best one.\n\n# Try polynomials of degree 1 through 10\ndegrees = range(1, 11)\ntrain_scores = []\ntest_scores = []\n\nfor degree in degrees:\n    # Create polynomial features\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_unemp_train)\n    X_test_poly = poly.transform(X_unemp_test)\n\n    # Fit model\n    model = LinearRegression()\n    model.fit(X_train_poly, y_income_poly_train)\n\n    # Score\n    train_score = model.score(X_train_poly, y_income_poly_train)\n    test_score = model.score(X_test_poly, y_income_poly_test)\n\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n\n    print(f\"Degree {degree}: Train R² = {train_score:.4f}, Test R² = {test_score:.4f}\")\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, train_scores, 'o-', linewidth=2, label='Training R²')\nplt.plot(degrees, test_scores, 's-', linewidth=2, label='Test R²')\nplt.xlabel('Polynomial Degree', fontsize=12)\nplt.ylabel('R²', fontsize=12)\nplt.title('Model Performance vs Polynomial Degree', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(degrees)\nplt.show()\n\n# Find optimal degree\noptimal_degree = degrees[np.argmax(test_scores)]\nprint(f\"\\nOptimal polynomial degree: {optimal_degree} (Test R² = {max(test_scores):.4f})\")\n\nDegree 1: Train R² = 0.2718, Test R² = 0.2180\nDegree 2: Train R² = 0.2890, Test R² = 0.2339\nDegree 3: Train R² = 0.2904, Test R² = 0.2329\nDegree 4: Train R² = 0.2904, Test R² = 0.2317\nDegree 5: Train R² = 0.2923, Test R² = 0.2294\nDegree 6: Train R² = 0.2953, Test R² = 0.2225\nDegree 7: Train R² = 0.2955, Test R² = 0.2185\nDegree 8: Train R² = 0.2965, Test R² = 0.2265\nDegree 9: Train R² = 0.2960, Test R² = 0.2290\nDegree 10: Train R² = 0.2969, Test R² = 0.2255\n\n\n\n\n\n\n\n\n\n\nOptimal polynomial degree: 2 (Test R² = 0.2339)\n\n\nSee that? Training R² keeps increasing with degree—the model just memorizes the training data. But test R² peaks and then starts decreasing. That’s overfitting. The model gets so wiggly it fits training noise instead of the true pattern.\n\n\n\n\n\n\nWarning\n\n\n\nNever choose polynomial degree based on training performance alone! Always use validation data or cross-validation. Otherwise, you’ll pick a high degree that overfits.\n\n\n\n\n5.4 The Overfitting Risk\nLet’s visualize what high-degree polynomials do. They create absurd wiggles that fit every bump in the training data but fail on new data.\n\n# Compare degree 2 vs degree 10\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Create small dataset to exaggerate overfitting\nnp.random.seed(42)\nX_small = np.linspace(0, 10, 30).reshape(-1, 1)\ny_small = 2 * X_small.flatten() + np.random.normal(0, 5, 30)\n\n# Degree 2\npoly_2_small = PolynomialFeatures(degree=2, include_bias=False)\nX_small_poly2 = poly_2_small.fit_transform(X_small)\nmodel_poly2_small = LinearRegression()\nmodel_poly2_small.fit(X_small_poly2, y_small)\n\n# Create smooth line for plotting\nX_plot = np.linspace(0, 10, 200).reshape(-1, 1)\nX_plot_poly2 = poly_2_small.transform(X_plot)\ny_plot_poly2 = model_poly2_small.predict(X_plot_poly2)\n\naxes[0].scatter(X_small, y_small, s=50, label='Training Data')\naxes[0].plot(X_plot, y_plot_poly2, 'r-', linewidth=2, label='Degree 2')\naxes[0].set_xlabel('X', fontsize=12)\naxes[0].set_ylabel('y', fontsize=12)\naxes[0].set_title('Degree 2: Reasonable Fit', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Degree 10\npoly_10_small = PolynomialFeatures(degree=10, include_bias=False)\nX_small_poly10 = poly_10_small.fit_transform(X_small)\nmodel_poly10_small = LinearRegression()\nmodel_poly10_small.fit(X_small_poly10, y_small)\n\nX_plot_poly10 = poly_10_small.transform(X_plot)\ny_plot_poly10 = model_poly10_small.predict(X_plot_poly10)\n\naxes[1].scatter(X_small, y_small, s=50, label='Training Data')\naxes[1].plot(X_plot, y_plot_poly10, 'r-', linewidth=2, label='Degree 10')\naxes[1].set_xlabel('X', fontsize=12)\naxes[1].set_ylabel('y', fontsize=12)\naxes[1].set_title('Degree 10: Overfitting!', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_ylim(axes[0].get_ylim())  # Same y-axis for comparison\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Degree 2 - Training R²: {model_poly2_small.score(X_small_poly2, y_small):.4f}\")\nprint(f\"Degree 10 - Training R²: {model_poly10_small.score(X_small_poly10, y_small):.4f}\")\n\n\n\n\n\n\n\n\nDegree 2 - Training R²: 0.5739\nDegree 10 - Training R²: 0.6720\n\n\nLook at that degree-10 model! It wiggles wildly to pass through training points. Training R² is higher, but the model is useless for prediction. It learned noise, not signal.\nThe Extreme Case: More Features Than Observations (p &gt; n)\nNow let’s see what happens when you push this to the limit: more features than data points. This creates an overdetermined system where you can get perfect training fit (R² = 1.0) that means absolutely nothing.\n\nnyc_census = pd.read_csv('../data/nyc_census_tracts.csv')\n\n# Load our Bronx census data again\nbronx_subset = nyc_census[nyc_census['Borough'] == 'Bronx'].copy()\nbronx_subset = bronx_subset.dropna(subset=['Income'])\n\n# Select just a few features to start\nbase_features = ['TotalPop', 'IncomePerCap', 'Poverty', 'Professional', 'Unemployment']\nbronx_tiny = bronx_subset[base_features + ['Income']].dropna()\n\n# Take only 30 census tracts (small sample)\nbronx_tiny_sample = bronx_tiny.sample(n=30, random_state=42)\n\nX_tiny = bronx_tiny_sample[base_features].values\ny_tiny = bronx_tiny_sample['Income'].values\n\nprint(f\"Starting with: {len(bronx_tiny_sample)} observations, {len(base_features)} features\")\n\n# Now create polynomial features to blow up the feature count\npoly_extreme = PolynomialFeatures(degree=4, include_bias=False)\nX_tiny_poly = poly_extreme.fit_transform(X_tiny)\n\nprint(f\"After polynomial expansion (degree 4): {X_tiny_poly.shape[0]} observations, {X_tiny_poly.shape[1]} features\")\nprint(f\"Features &gt; Observations: {X_tiny_poly.shape[1] &gt; X_tiny_poly.shape[0]}\")\n\n# Hold out just ONE observation for \"testing\"\nX_train_tiny = X_tiny_poly[:-1]\ny_train_tiny = y_tiny[:-1]\nX_test_tiny = X_tiny_poly[-1:]\ny_test_tiny = y_tiny[-1:]\n\nprint(f\"\\nTraining: {X_train_tiny.shape[0]} observations, {X_train_tiny.shape[1]} features\")\nprint(f\"p &gt; n? {X_train_tiny.shape[1] &gt; X_train_tiny.shape[0]}\")\n\n# Fit the model\nmodel_extreme = LinearRegression()\nmodel_extreme.fit(X_train_tiny, y_train_tiny)\n\n# Check performance\ntrain_r2_extreme = model_extreme.score(X_train_tiny, y_train_tiny)\ny_pred_train_extreme = model_extreme.predict(X_train_tiny)\ny_pred_test_extreme = model_extreme.predict(X_test_tiny)\n\ntrain_mse_extreme = mean_squared_error(y_train_tiny, y_pred_train_extreme)\ntest_error_extreme = abs(y_test_tiny[0] - y_pred_test_extreme[0])\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RESULTS: {X_train_tiny.shape[1]} features, {X_train_tiny.shape[0]} observations\")\nprint(f\"{'='*60}\")\nprint(f\"Training R²: {train_r2_extreme:.10f}\")\nprint(f\"Training MSE: {train_mse_extreme:.10f}\")\nprint(f\"\\nActual income (held-out): ${y_test_tiny[0]:,.0f}\")\nprint(f\"Predicted income: ${y_pred_test_extreme[0]:,.0f}\")\nprint(f\"Prediction error: ${test_error_extreme:,.0f}\")\nprint(f\"Prediction error (%): {100 * test_error_extreme / y_test_tiny[0]:.1f}%\")\n\n# Visualize the \"perfect\" fit\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Training \"perfection\"\naxes[0].scatter(y_train_tiny, y_pred_train_extreme, s=50, alpha=0.7)\naxes[0].plot([y_train_tiny.min(), y_train_tiny.max()],\n             [y_train_tiny.min(), y_train_tiny.max()],\n             'r--', linewidth=2, label='Perfect Prediction')\naxes[0].set_xlabel('Actual Income', fontsize=12)\naxes[0].set_ylabel('Predicted Income', fontsize=12)\naxes[0].set_title(f'Training: R² = {train_r2_extreme:.6f}', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Right: Test disaster\naxes[1].scatter([y_test_tiny[0]], [y_pred_test_extreme[0]], s=200, c='red',\n                marker='X', label='Test Prediction', zorder=3)\naxes[1].plot([y_train_tiny.min(), y_train_tiny.max()],\n             [y_train_tiny.min(), y_train_tiny.max()],\n             'r--', linewidth=2, label='Perfect Prediction')\naxes[1].set_xlabel('Actual Income', fontsize=12)\naxes[1].set_ylabel('Predicted Income', fontsize=12)\naxes[1].set_title(f'Test: Error = ${test_error_extreme:,.0f}', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nStarting with: 30 observations, 5 features\nAfter polynomial expansion (degree 4): 30 observations, 125 features\nFeatures &gt; Observations: True\n\nTraining: 29 observations, 125 features\np &gt; n? True\n\n============================================================\nRESULTS: 125 features, 29 observations\n============================================================\nTraining R²: 1.0000000000\nTraining MSE: 0.0000000000\n\nActual income (held-out): $52,344\nPredicted income: $167,147\nPrediction error: $114,803\nPrediction error (%): 219.3%\n\n\n\n\n\n\n\n\n\nWhat just happened?\nWhen p &gt; n (more features than observations), the system becomes underdetermined. The model can perfectly fit every training point because it has enough degrees of freedom to pass through all of them. R² = 1.0 is guaranteed mathematically—but it’s meaningless!\nThis is like drawing a curve through 5 points with a 10th-degree polynomial. You have so much flexibility that you can hit every point exactly. But the curve between points? Pure nonsense.\n\n\n\n\n\n\nWarning\n\n\n\nDANGER: The Perfect R² Trap\nIf you ever see R² = 1.0000 (or extremely close) on training data, be immediately suspicious:\n\nCheck if p ≥ n (features ≥ observations)\nCheck if you accidentally included the target variable as a feature\nCheck for data leakage (future information in features)\nCheck if you have duplicate rows\n\nA “perfect” fit is almost never real. It’s almost always a problem.\n\n\nReal-world implications:\nThis happens more often than you think:\n\nSmall datasets: Medical studies with 50 patients but 200 genetic markers\nHigh-dimensional data: Images, text, genomics where features vastly outnumber samples\nTime series: Predicting tomorrow with 100 technical indicators but only 30 days of data\n\nThe solution? Regularization (covered in the next sections) or getting more data. Never trust a model where p/n &gt; 1.0, and be very cautious when p/n &gt; 0.3.\nKey takeaways:\n\nPolynomial features let you model non-linear relationships with linear regression\nChoose degree using validation data, not training data\nLower degree often generalizes better than higher degree\nWhen p ≥ n, you get perfect training fit but meaningless predictions\nRegularization (covered next) can help control overfitting in polynomial models"
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#multicollinearity-when-features-are-too-similar",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#multicollinearity-when-features-are-too-similar",
    "title": "Chapter 2: Regression Models",
    "section": "6. Multicollinearity: When Features Are Too Similar",
    "text": "6. Multicollinearity: When Features Are Too Similar\nImagine trying to predict house prices using both “square footage” and “square meters” as separate features. They contain basically the same information! This creates multicollinearity, and it breaks coefficient interpretation in sneaky ways.\n\n6.1 What Is Multicollinearity?\nMulticollinearity means your features are highly correlated with each other. One feature can be predicted fairly well from others.\nWhy is this a problem? Linear regression tries to isolate the effect of each feature while “holding others constant.” But if two features move together, you can’t hold one constant while changing the other—they’re tied together!\nThe result: coefficient estimates become unstable. Add or remove one observation, and coefficients swing wildly. Even worse, a feature that’s clearly important might show up with a tiny coefficient (or even the wrong sign!) because its effect is “stolen” by correlated features.\nLet’s create an example to see this:\n\n# Create dataset with multicollinearity\nnp.random.seed(42)\nn = 1000\n\n# X1 is random\nX1 = np.random.normal(0, 1, n)\n\n# X2 is highly correlated with X1\nX2 = X1 + np.random.normal(0, 0.1, n)  # Almost identical to X1\n\n# X3 is independent\nX3 = np.random.normal(0, 1, n)\n\n# True relationship: y = 5*X1 + 0*X2 + 3*X3 + noise\n# Note: X2 has NO effect, but it's correlated with X1\ny_multi = 5*X1 + 3*X3 + np.random.normal(0, 1, n)\n\n# Create DataFrame\ndf_multi = pd.DataFrame({\n    'X1': X1,\n    'X2': X2,\n    'X3': X3,\n    'y': y_multi\n})\n\n# Check correlation matrix\nprint(\"Correlation Matrix:\")\nprint(df_multi.corr())\n\n# Visualize correlations\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_multi.corr(), annot=True, cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Feature Correlation Matrix', fontsize=14)\nplt.show()\n\nCorrelation Matrix:\n          X1        X2        X3         y\nX1  1.000000  0.994818  0.022129  0.843973\nX2  0.994818  1.000000  0.020966  0.838088\nX3  0.022129  0.020966  1.000000  0.525535\ny   0.843973  0.838088  0.525535  1.000000\n\n\n\n\n\n\n\n\n\nSee that? X1 and X2 have a correlation of about 0.995. They’re nearly identical. Now watch what happens when we fit a regression:\n\n# Fit model with all features\nX_multi = df_multi[['X1', 'X2', 'X3']].values\ny_multi_target = df_multi['y'].values\n\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_multi, y_multi_target)\n\nprint(\"Coefficients with multicollinearity:\")\nfor i, name in enumerate(['X1', 'X2', 'X3']):\n    print(f\"  {name}: {model_multi.coef_[i]:.4f}\")\n\nprint(f\"\\nRemember: True coefficients are X1=5, X2=0, X3=3\")\nprint(\"But the model can't tell X1 and X2 apart!\")\n\n# Fit model with only X1 and X3 (no multicollinearity)\nX_clean = df_multi[['X1', 'X3']].values\nmodel_clean = LinearRegression()\nmodel_clean.fit(X_clean, y_multi_target)\n\nprint(\"\\nCoefficients without multicollinearity:\")\nfor i, name in enumerate(['X1', 'X3']):\n    print(f\"  {name}: {model_clean.coef_[i]:.4f}\")\n\nprint(\"\\nMuch closer to the true values!\")\n\nCoefficients with multicollinearity:\n  X1: 5.5507\n  X2: -0.5675\n  X3: 3.0223\n\nRemember: True coefficients are X1=5, X2=0, X3=3\nBut the model can't tell X1 and X2 apart!\n\nCoefficients without multicollinearity:\n  X1: 4.9855\n  X3: 3.0229\n\nMuch closer to the true values!\n\n\n\n\n6.2 Why It’s a Problem\nLet me be blunt: multicollinearity doesn’t hurt prediction accuracy. Your R² will be fine. Your predictions will be fine. So why do we care?\nProblem 1: Coefficient interpretation becomes meaningless\nIf X1 and X2 are highly correlated, the model might give X1 a coefficient of 10 and X2 a coefficient of -5. Or it might do the opposite: X1 = -5, X2 = 10. Or X1 = 2.5, X2 = 2.5. All three give similar predictions! But which feature is “really” important? You can’t tell.\n\n\n\n\n\n\nNote\n\n\n\nOne of the main benefits of linear regression models is their interpretability. You can look at the coefficients and read off what they tell you. If you lose that, then you might as well not use a linear model!\n\n\nProblem 2: Coefficient instability\nSmall changes in the data cause huge swings in coefficients. Add 10 new observations? Coefficients might flip signs. This makes the model untrustworthy for understanding relationships.\nProblem 3: Hard to select features\nIf X1 and X2 are correlated, dropping one might barely hurt performance, but dropping both kills it. This makes feature selection confusing.\nWhen to care:\n\nYou need to interpret coefficients (e.g., explaining to stakeholders)\nYou want to identify the “most important” features\nYou’re making causal claims\n\n\n\n6.3 Detecting Multicollinearity\nMethod 1: Correlation Matrix\nThe simplest approach. Look for correlations close to ±1 (say, above 0.8 or 0.9).\n\n# Use NYC census data\nfeatures_for_vif = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']\nnyc_vif = nyc_census[features_for_vif].dropna()\nX_vif = nyc_vif\n\n# Correlation matrix\ncorr_matrix = X_vif.corr()\n\n# Visualize\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, fmt='.3f')\nplt.title('Feature Correlation Matrix - NYC Census Data', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Identify high correlations\nhigh_corr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if abs(corr_matrix.iloc[i, j]) &gt; 0.7:\n            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\nprint(\"\\nHighly correlated feature pairs (|correlation| &gt; 0.7):\")\nfor feat1, feat2, corr in high_corr_pairs:\n    print(f\"  {feat1} & {feat2}: {corr:.3f}\")\n\n\n\n\n\n\n\n\n\nHighly correlated feature pairs (|correlation| &gt; 0.7):\n  Professional & IncomePerCap: 0.782\n  Poverty & ChildPoverty: 0.911\n\n\nMethod 2: Variance Inflation Factor (VIF)\nVIF measures how much the variance of a coefficient is “inflated” due to multicollinearity.\nVIF interpretation: - VIF = 1: No multicollinearity - VIF = 1-5: Moderate multicollinearity (usually okay) - VIF = 5-10: High multicollinearity (concerning) - VIF &gt; 10: Severe multicollinearity (definitely a problem)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = features_for_vif\nvif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(features_for_vif))]\n\n# Visualize VIF\nplt.figure(figsize=(10, 6))\nplt.barh(vif_data[\"Feature\"], vif_data[\"VIF\"])\nplt.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='VIF = 5 (Moderate)')\nplt.axvline(x=10, color='red', linestyle='--', linewidth=2, label='VIF = 10 (Severe)')\nplt.xlabel('VIF', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Variance Inflation Factors', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3, axis='x')\nplt.show()\n\n\n\n\n\n\n\n\nSee features with high VIF? Those are the ones tangled up with others. The solution? Remove one of the correlated features, combine them, or use regularization (which we’ll cover next)."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#ridge-regression-l2-regularization",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#ridge-regression-l2-regularization",
    "title": "Chapter 2: Regression Models",
    "section": "7. Ridge Regression: L2 Regularization",
    "text": "7. Ridge Regression: L2 Regularization\nMulticollinearity makes coefficients unstable. Polynomial features risk overfitting. The solution to both? Regularization. It’s one of the most important ideas in machine learning.\n\n6.1 The Regularization Idea\nHere’s the core insight: penalize large coefficients. Instead of just minimizing error, also minimize the size of coefficients. The model has to balance two goals:\n\nFit the training data well (low error)\nKeep coefficients small (low complexity)\n\nWhy does this help? Large coefficients make the model sensitive to small changes in features. By shrinking coefficients, you make the model more stable. We want to discourage the model from making wild swings in predictions from very small changes in the data. Imagine if you had a coefficient of 10,000,000. Then a one unit change in x would add ten million to your predicted value. That’s almost certainly not desirable.\nYes, you’re intentionally introducing bias (the model won’t fit training data perfectly). But you reduce variance (the model generalizes better to new data).\nThis is the famous bias-variance tradeoff. A little bias for a lot less variance is usually a great deal.\n\n\n6.2 How Ridge Works\nRidge regression adds a penalty term to the loss function:\n\\[\n\\text{Loss} = \\text{MSE} + \\alpha \\displaystyle\\sum\\text{coefficients}^2\n\\]\nThat second term is the L2 penalty (sum of squared coefficients). The α (alpha) parameter controls how much you penalize:\n\nα = 0: No penalty, just regular linear regression\nSmall α: Light penalty, coefficients shrink a little\nLarge α: Heavy penalty, coefficients shrink toward zero (but never reach exactly zero)\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that we want as small a loss as possible. The goal of machine learning algorithms are to find the parameters (e.g. coefficients of the regression model) that minimize the loss function. Regularization adds the coefficients themselves (squared) to the loss function, meaning the algorithm is penalized for choosing large coefficients.\n\n\nLet’s see it in action using our synthetic data with extreme multicollinearity from section 5.1. Remember: X1 and X2 are nearly identical (correlation ≈ 0.995), and the true model is y = 5X1 + 3X3.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\n# Use the multicollinearity data we created earlier\n# X1 and X2 are highly correlated; true coefficients are X1=5, X2=0, X3=3\nX_ridge = df_multi[['X1', 'X2', 'X3']].values\ny_ridge = df_multi['y'].values\n\n# Split data\nX_ridge_train, X_ridge_test, y_ridge_train, y_ridge_test = train_test_split(\n    X_ridge, y_ridge, test_size=0.2, random_state=42\n)\n\n# IMPORTANT: Always scale features before regularization!\n# Features with larger scales get penalized more\nscaler = StandardScaler()\nX_ridge_train_scaled = scaler.fit_transform(X_ridge_train)\nX_ridge_test_scaled = scaler.transform(X_ridge_test)\n\n# Fit regular linear regression\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_ridge_train_scaled, y_ridge_train)\nprint(\"Linear Regression (no regularization):\")\nprint(f\"  Train R²: {model_lr.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lr.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"  Coefficients: {model_lr.coef_}\")\nprint(f\"  Coefficient magnitudes: {np.abs(model_lr.coef_).sum():.4f}\")\nprint(f\"\\n  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)\")\nprint(f\"  True coefficients: X1=5, X2=0, X3=3\")\nprint(f\"  But with multicollinearity, coefficients are unstable!\")\n\n# Fit Ridge with alpha=1\nmodel_ridge = Ridge(alpha=1.0)\nmodel_ridge.fit(X_ridge_train_scaled, y_ridge_train)\nprint(\"\\nRidge Regression (alpha=1.0):\")\nprint(f\"  Train R²: {model_ridge.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_ridge.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"  Coefficients: {model_ridge.coef_}\")\nprint(f\"  Coefficient magnitudes: {np.abs(model_ridge.coef_).sum():.4f}\")\nprint(f\"\\n  Ridge distributes the X1 effect across X1 and X2 more evenly\")\nprint(f\"  Coefficients are more stable!\")\n\n# Visualize coefficient shrinkage\nfeature_names = ['X1', 'X2', 'X3']\ncoef_comparison = pd.DataFrame({\n    'Feature': feature_names,\n    'Linear Regression': model_lr.coef_,\n    'Ridge (α=1)': model_ridge.coef_,\n    'True Value': [5, 0, 3]\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\nax.bar(x - width, coef_comparison['True Value'], width, label='True Coefficients', alpha=0.8, color='green')\nax.bar(x + width, coef_comparison['Ridge (α=1)'], width, label='Ridge (α=1)', alpha=0.8, color='red')\nax.bar(x, coef_comparison['Linear Regression'], width, label='Linear Regression', alpha=0.8, color='blue')\nax.set_xlabel('Feature', fontsize=12)\nax.set_ylabel('Coefficient Value', fontsize=12)\nax.set_title('Ridge Handles Multicollinearity Better Than OLS', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(feature_names)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\nLinear Regression (no regularization):\n  Train R²: 0.9700\n  Test R²: 0.9665\n  Coefficients: [ 5.71502205 -0.82461178  3.00187963]\n  Coefficient magnitudes: 9.5415\n\n  Remember: X1 and X2 are nearly identical (corr ≈ 0.995)\n  True coefficients: X1=5, X2=0, X3=3\n  But with multicollinearity, coefficients are unstable!\n\nRidge Regression (alpha=1.0):\n  Train R²: 0.9699\n  Test R²: 0.9670\n  Coefficients: [ 5.07662499 -0.1891781   2.99844018]\n  Coefficient magnitudes: 8.2642\n\n  Ridge distributes the X1 effect across X1 and X2 more evenly\n  Coefficients are more stable!\n\n\n\n\n\n\n\n\n\nSee what happened? With regular linear regression, the coefficients for X1 and X2 were far from the true values. Ridge regression distributes the effect more evenly between the correlated features. While Ridge coefficients still aren’t perfect (X2 should be 0), they’re much more stable and reasonable.\n\n\n\n\n\n\nWarning\n\n\n\nAlways scale features before using Ridge! Features with large scales get penalized more heavily than features with small scales. Standardizing (mean=0, std=1) ensures all features are treated equally. Imagine if I wrote your salary in pennies. Each year you get a raise, and the raise would look enormous to the model!\n\n\n\n\n6.3 Choosing Alpha\nHow do you pick α? Try many values and use cross-validation to see which generalizes best. Let’s continue with our synthetic multicollinearity data:\n\nfrom sklearn.model_selection import cross_val_score\n\n# Try many alpha values\nalphas = np.logspace(-2, 2, 50)  # From 0.01 to 100\ntrain_scores_ridge = []\ntest_scores_ridge = []\ncv_scores_ridge = []\ncoef_sum_ridge = []\n\nfor alpha in alphas:\n    model = Ridge(alpha=alpha)\n\n    # Train score\n    model.fit(X_ridge_train_scaled, y_ridge_train)\n    train_scores_ridge.append(model.score(X_ridge_train_scaled, y_ridge_train))\n\n    # Test score\n    test_scores_ridge.append(model.score(X_ridge_test_scaled, y_ridge_test))\n\n    # Cross-validation score (5-fold)\n    cv_score = cross_val_score(model, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    cv_scores_ridge.append(cv_score)\n\n    # Track total coefficient magnitude\n    coef_sum_ridge.append(np.abs(model.coef_).sum())\n\n# Plot results\nfig = plt.figure(figsize=(8, 5))\n\n# Plot 1: R² scores\nplt.plot(alphas, train_scores_ridge, label='Training R²', linewidth=2)\nplt.plot(alphas, test_scores_ridge, label='Test R²', linewidth=2)\nplt.plot(alphas, cv_scores_ridge, label='CV R² (5-fold)', linewidth=2, linestyle='--')\nplt.xscale('log')\nplt.xlabel('Alpha (log scale)', fontsize=12)\nplt.ylabel('R²', fontsize=12)\nplt.title('Ridge Regression: Choosing Alpha', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\noptimal_alpha = alphas[np.argmax(cv_scores_ridge)]\nplt.axvline(optimal_alpha, color='red', linestyle=':', linewidth=2, label='Optimal Alpha')\n\nplt.tight_layout()\nplt.show()\n\n# Best alpha\nprint(f\"Optimal alpha (by cross-validation): {optimal_alpha:.4f}\")\nprint(f\"Best CV R²: {max(cv_scores_ridge):.4f}\")\n\n\n\n\n\n\n\n\nOptimal alpha (by cross-validation): 0.0100\nBest CV R²: 0.9692\n\n\nAs α increases:\n\nTraining R² decreases (more bias, less fit to training data), however…\nTest/CV R² are still high, and even increase for a while (multicollinearity handled), then sharply decreases (too much bias)\n\nThe optimal α is where test/CV performance peaks. For this synthetic data with severe multicollinearity, Ridge helps stabilize coefficients even with small α values."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#lasso-regression-l1-regularization",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#lasso-regression-l1-regularization",
    "title": "Chapter 2: Regression Models",
    "section": "8. Lasso Regression: L1 Regularization",
    "text": "8. Lasso Regression: L1 Regularization\nRidge is great, but it never says “this feature is useless.” Lasso does. It performs automatic feature selection by setting some coefficients to exactly zero.\n\n8.1 How Lasso Differs from Ridge\nLasso uses L1 regularization instead of L2:\n\\[\n\\text{Loss} = \\text{MSE} + \\alpha \\sum |\\text{coefficients}|\n\\]\nThe difference? Instead of squaring coefficients (Ridge), we take their absolute value (Lasso). This small change has a huge impact: Lasso can set coefficients to exactly zero.\nWhy? It’s geometry. The L1 penalty creates “corners” where the optimal solution often has some coefficients at exactly zero. Ridge’s L2 penalty is smooth, so coefficients approach zero but never arrive.\n\n\n7.2 Lasso for Feature Selection\nLasso is feature selection built into the regression. As α increases, Lasso zeroes out features one by one, keeping only the most important.\n\nfrom sklearn.linear_model import Lasso\n\nalphas_path = np.logspace(-2, 2, 100)\n\n# Fit Lasso with alpha=0.01\nmodel_lasso = Lasso(alpha=0.01, max_iter=10000)\nmodel_lasso.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(\"Lasso Regression (alpha=0.01):\")\nprint(f\"  Train R²: {model_lasso.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lasso.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"\\nCoefficients:\")\nfor feature, coef in zip(feature_names, model_lasso.coef_):\n    if abs(coef) &lt; 0.0001:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED!\")\n    else:\n        print(f\"  {feature}: {coef:.6f}\")\n\n# Compare with larger alpha\nmodel_lasso_strong = Lasso(alpha=0.1, max_iter=10000)\nmodel_lasso_strong.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(\"\\nLasso Regression (alpha=0.1):\")\nprint(f\"  Train R²: {model_lasso_strong.score(X_ridge_train_scaled, y_ridge_train):.4f}\")\nprint(f\"  Test R²: {model_lasso_strong.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\nprint(f\"\\nCoefficients:\")\nfor feature, coef in zip(feature_names, model_lasso_strong.coef_):\n    if abs(coef) &lt; 0.0001:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED!\")\n    else:\n        print(f\"  {feature}: {coef:.6f}\")\n\n# Visualize feature selection\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\nax.bar(x - width, model_lr.coef_, width, label='Linear Regression', alpha=0.8)\nax.bar(x, model_lasso.coef_, width, label='Lasso (α=0.01)', alpha=0.8)\nax.bar(x + width, model_lasso_strong.coef_, width, label='Lasso (α=0.1)', alpha=0.8)\nax.set_xlabel('Feature', fontsize=12)\nax.set_ylabel('Coefficient Value', fontsize=12)\nax.set_title('Lasso Feature Selection', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(feature_names)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\nLasso Regression (alpha=0.01):\n  Train R²: 0.9698\n  Test R²: 0.9671\n\nCoefficients:\n  X1: 4.884947\n  X2: 0.000000 → ELIMINATED!\n  X3: 2.992327\n\nLasso Regression (alpha=0.1):\n  Train R²: 0.9692\n  Test R²: 0.9680\n\nCoefficients:\n  X1: 4.797478\n  X2: 0.000000 → ELIMINATED!\n  X3: 2.904858\n\n\n\n\n\n\n\n\n\nSee how Lasso zeroed out some features completely? That’s automatic feature selection. Larger α means more aggressive selection.\n\n\n7.3 Visualizing the Regularization Path\nThe “regularization path” shows how coefficients shrink as α increases. For Lasso, coefficients hit zero and stay there.\n\n# Track coefficients across alpha values for both Lasso and Ridge\nalphas_lasso = np.logspace(-3, 1, 100)  # From 0.001 to 10\ncoefs_lasso = []\ncoefs_ridge_comparison = []\n\nfor alpha in alphas_lasso:\n    # Lasso\n    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n    lasso_model.fit(X_ridge_train_scaled, y_ridge_train)\n    coefs_lasso.append(lasso_model.coef_)\n\n    # Ridge (for comparison)\n    ridge_model = Ridge(alpha=alpha)\n    ridge_model.fit(X_ridge_train_scaled, y_ridge_train)\n    coefs_ridge_comparison.append(ridge_model.coef_)\n\ncoefs_lasso = np.array(coefs_lasso)\ncoefs_ridge_comparison = np.array(coefs_ridge_comparison)\n\n# Plot Lasso regularization path\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lasso path\ncolors = ['blue', 'red', 'green']\nfor i, feature in enumerate(feature_names):\n    axes[0].plot(alphas_lasso, coefs_lasso[:, i], label=feature, linewidth=2, color=colors[i])\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Alpha (log scale)', fontsize=12)\naxes[0].set_ylabel('Coefficient Value', fontsize=12)\naxes[0].set_title('Lasso Regularization Path', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n\n# Ridge path (for comparison)\nfor i, feature in enumerate(feature_names):\n    axes[1].plot(alphas_lasso, coefs_ridge_comparison[:, i], label=feature, linewidth=2, color=colors[i])\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Alpha (log scale)', fontsize=12)\naxes[1].set_ylabel('Coefficient Value', fontsize=12)\naxes[1].set_title('Ridge Regularization Path', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSee the difference? Lasso coefficients hit zero abruptly and stay there (left plot). Ridge coefficients smoothly approach zero but never reach it (right plot).\n\n\n\n\n\n\nNote\n\n\n\nLasso is more aggressive than Ridge. It makes hard decisions: “this feature matters” or “this feature doesn’t.” Ridge says “this feature matters a little less.” Choose based on whether you want sparse models (Lasso) or stable coefficients (Ridge).\n\n\n\n\n7.4 When to Use Lasso vs. Ridge\nUse Lasso when:\n\nYou suspect many features are irrelevant\nYou want a sparse model (fewer features)\nYou need to explain which features matter most\nInterpretability is critical\n\nUse Ridge when:\n\nYou think most features contribute something\nYou have multicollinearity and want stable coefficients\nYou’re okay with keeping all features\nYou prioritize prediction over interpretation\n\nThe truth? Try both and use cross-validation to decide. Sometimes Ridge wins. Sometimes Lasso wins. Sometimes they’re tied.\n\n# Compare Ridge vs Lasso performance\nalphas_compare = np.logspace(-3, 2, 50)\nridge_scores = []\nlasso_scores = []\n\nfor alpha in alphas_compare:\n    # Ridge\n    ridge = Ridge(alpha=alpha)\n    ridge_cv = cross_val_score(ridge, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    ridge_scores.append(ridge_cv)\n\n    # Lasso\n    lasso = Lasso(alpha=alpha, max_iter=10000)\n    lasso_cv = cross_val_score(lasso, X_ridge_train_scaled, y_ridge_train, cv=5, scoring='r2').mean()\n    lasso_scores.append(lasso_cv)\n\n# Plot comparison\nplt.figure(figsize=(10, 6))\nplt.plot(alphas_compare, ridge_scores, 'o-', label='Ridge', linewidth=2)\nplt.plot(alphas_compare, lasso_scores, 's-', label='Lasso', linewidth=2)\nplt.xscale('log')\nplt.xlabel('Alpha (log scale)', fontsize=12)\nplt.ylabel('Cross-Validation R²', fontsize=12)\nplt.title('Ridge vs Lasso: Cross-Validation Performance', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Best performance\nbest_ridge_score = max(ridge_scores)\nbest_lasso_score = max(lasso_scores)\nprint(f\"Best Ridge CV R²: {best_ridge_score:.4f}\")\nprint(f\"Best Lasso CV R²: {best_lasso_score:.4f}\")\n\n\n\n\n\n\n\n\nBest Ridge CV R²: 0.9692\nBest Lasso CV R²: 0.9692\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere we see that as \\(\\alpha\\) increased, ridge was relatively unaffectd, but lasso quickly had poor prediction. Why do you think that is? Compare this with the regularization path plots above, can you explain what’s going on here?"
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#elastic-net-best-of-both-worlds",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#elastic-net-best-of-both-worlds",
    "title": "Chapter 2: Regression Models",
    "section": "9. Elastic Net: Best of Both Worlds",
    "text": "9. Elastic Net: Best of Both Worlds\nCan’t decide between Ridge and Lasso? Why not both? Elastic Net combines L1 and L2 regularization to get the best of both worlds.\n\n9.1 Combining L1 and L2\nElastic Net uses a mix of Ridge (L2) and Lasso (L1) penalties: \\[\n\\text{Loss} = \\text{MSE} + \\alpha \\left[\n    \\omega \\sum |\\beta_j| + (1 - \\omega) \\sum \\beta_j^2 \\right]\n\\]\nTwo hyperparameters:\n\nα (alpha): Overall regularization strength (like Ridge and Lasso)\n\\(\\omega\\) = l1_ratio: Mix between L1 and L2\n\n\\(\\omega\\) = 0: Pure Ridge\n\\(\\omega\\) = 1: Pure Lasso\n\\(\\omega\\) = 0.5: Equal mix of both\n\n\nWhy combine them? Lasso can be unstable when features are highly correlated—it randomly picks one and zeros the others. Ridge keeps all correlated features but doesn’t select. Elastic Net does feature selection (like Lasso) but more stably (like Ridge).\n\nfrom sklearn.linear_model import ElasticNet\n\n# Fit Elastic Net with different l1_ratios\nl1_ratios = [0.2, 0.5, 0.8]\nalpha_en = 0.01\n\n# Also include Ridge and Lasso for comparison\nmodels = {\n    'Ridge (l1=0)': Ridge(alpha=alpha_en),\n    'ElasticNet (l1=0.2)': ElasticNet(alpha=alpha_en, l1_ratio=0.2, max_iter=10000),\n    'ElasticNet (l1=0.5)': ElasticNet(alpha=alpha_en, l1_ratio=0.5, max_iter=10000),\n    'ElasticNet (l1=0.8)': ElasticNet(alpha=alpha_en, l1_ratio=0.8, max_iter=10000),\n    'Lasso (l1=1)': Lasso(alpha=alpha_en, max_iter=10000)\n}\n\nfor idx, (name, model) in enumerate(models.items()):\n    model.fit(X_ridge_train_scaled, y_ridge_train)\n    r2 = model.score(X_ridge_test_scaled, y_ridge_test)\n    print(f\"{name}: Test R² = {r2:.4f}\")\n    print(f\"  Coefficients: {model.coef_}\")\n    print(f\"  Non-zero coefficients: {np.sum(np.abs(model.coef_) &gt; 0.0001)}\")\n    print()\n\n# Visualize one Elastic Net model\nmodel_en = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)\nmodel_en.fit(X_ridge_train_scaled, y_ridge_train)\n\n# Compare all three\nfig = plt.figure(figsize=(10, 6))\nx = np.arange(len(feature_names))\nwidth = 0.25\n\n# Get a Ridge and Lasso with same alpha for fair comparison\nmodel_ridge_comp = Ridge(alpha=0.01)\nmodel_ridge_comp.fit(X_ridge_train_scaled, y_ridge_train)\n\nmodel_lasso_comp = Lasso(alpha=0.01, max_iter=10000)\nmodel_lasso_comp.fit(X_ridge_train_scaled, y_ridge_train)\n\nplt.bar(x - width, model_ridge_comp.coef_, width, label='Ridge', alpha=0.8)\nplt.bar(x, model_en.coef_, width, label='Elastic Net (l1_ratio=0.5)', alpha=0.8)\nplt.bar(x + width, model_lasso_comp.coef_, width, label='Lasso', alpha=0.8)\n\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Coefficient Value', fontsize=12)\nplt.title('Ridge vs Elastic Net vs Lasso', fontsize=14)\nplt.xticks(x, feature_names)\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\nRidge (l1=0): Test R² = 0.9665\n  Coefficients: [ 5.70711611 -0.81673551  3.00184572]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.2): Test R² = 0.9673\n  Coefficients: [3.73186865 1.1378942  2.97734663]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.5): Test R² = 0.9673\n  Coefficients: [4.118586   0.75533971 2.98299579]\n  Non-zero coefficients: 3\n\nElasticNet (l1=0.8): Test R² = 0.9672\n  Coefficients: [4.82987896 0.04824256 2.98856424]\n  Non-zero coefficients: 3\n\nLasso (l1=1): Test R² = 0.9671\n  Coefficients: [4.88494701 0.         2.99232692]\n  Non-zero coefficients: 2\n\n\n\n\n\n\n\n\n\n\nElastic Net sits between Ridge and Lasso. It zeros out some features (like Lasso) but keeps coefficients more stable (like Ridge).\n\n\n9.2 When to Use Elastic Net\nUse Elastic Net when:\n\nYou have groups of correlated features\nYou want feature selection but Lasso is too unstable\nYou’re not sure if Ridge or Lasso is better (hedge your bets)\nYou have more features than observations (p &gt; n)\n\nReal-world scenario: You have 50 features measuring similar things (different weather stations, different survey questions, etc.). Lasso might randomly pick one from each group. Ridge keeps all 50. Elastic Net picks a few from each group—the best compromise.\nPractical advice: If you’re unsure, use Elastic Net with l1_ratio=0.5 and tune it with cross-validation. It’s a safe default that adapts to your data.\n\n# Tune both alpha and l1_ratio with grid search\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'alpha': np.logspace(-3, 1, 20),\n    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n}\n\nelastic_net = ElasticNet(max_iter=10000)\ngrid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='r2', n_jobs=-1)\ngrid_search.fit(X_ridge_train_scaled, y_ridge_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV R²: {grid_search.best_score_:.4f}\")\nprint(f\"Test R²: {grid_search.score(X_ridge_test_scaled, y_ridge_test):.4f}\")\n\n# Best model coefficients\nbest_model = grid_search.best_estimator_\nprint(f\"\\nBest model coefficients:\")\nfor feature, coef in zip(feature_names, best_model.coef_):\n    if abs(coef) &gt; 0.0001:\n        print(f\"  {feature}: {coef:.6f}\")\n    else:\n        print(f\"  {feature}: {coef:.6f} → ELIMINATED\")\n\nBest parameters: {'alpha': np.float64(0.001), 'l1_ratio': 0.9}\nBest CV R²: 0.9691\nTest R²: 0.9667\n\nBest model coefficients:\n  X1: 5.474694\n  X2: -0.584535\n  X3: 3.000771\n\n\nElastic Net automatically found the best combination of Ridge and Lasso for your data. That’s the power of combining regularization techniques."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#putting-it-all-together-a-complete-regression-analysis",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#putting-it-all-together-a-complete-regression-analysis",
    "title": "Chapter 2: Regression Models",
    "section": "10. Putting It All Together: A Complete Regression Analysis",
    "text": "10. Putting It All Together: A Complete Regression Analysis\nYou’ve learned all the pieces. Now let’s put them together into a complete regression workflow that you can follow for any project.\n\n9.1 The Diagnostic Workflow\nHere’s the process every regression analysis should follow:\n\nStep 1: Fit a baseline linear model\nStep 2: Check assumptions with diagnostic plots\nStep 3: Identify problems (non-linearity, heteroscedasticity, multicollinearity)\nStep 4: Fix problems (transformations, polynomial features, regularization)\nStep 5: Validate with cross-validation\nStep 6: Interpret and communicate results\n\nLet’s walk through this with a complete example:\n\nfrom scipy import stats\n\n# Step 1: Fit baseline linear model\nprint(\"=\"*60)\nprint(\"STEP 1: Baseline Linear Regression\")\nprint(\"=\"*60)\n\nfeatures_complete = ['TotalPop', 'Professional', 'Poverty', 'Unemployment', 'IncomePerCap', 'ChildPoverty']\nnyc_complete = nyc_census[features_complete + ['Income']].dropna()\nX_complete = nyc_complete[features_complete].values\ny_complete = nyc_complete['Income'].values\n\nX_train_complete, X_test_complete, y_train_complete, y_test_complete = train_test_split(\n    X_complete, y_complete, test_size=0.2, random_state=42\n)\n\n# Baseline model\nmodel_baseline = LinearRegression()\nmodel_baseline.fit(X_train_complete, y_train_complete)\ny_pred_baseline = model_baseline.predict(X_test_complete)\n\nprint(f\"Train R²: {model_baseline.score(X_train_complete, y_train_complete):.4f}\")\nprint(f\"Test R²: {model_baseline.score(X_test_complete, y_test_complete):.4f}\")\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test_complete, y_pred_baseline)):.4f}\")\n\n# Step 2: Check assumptions with diagnostic plots\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: Diagnostic Plots\")\nprint(\"=\"*60)\n\nresiduals_complete = y_test_complete - y_pred_baseline\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Residuals vs Fitted\naxes[0].scatter(y_pred_baseline, residuals_complete, alpha=0.5, s=10)\naxes[0].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Fitted Values')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('Residuals vs Fitted')\naxes[0].grid(True, alpha=0.3)\n\n# Histogram of residuals\naxes[1].hist(residuals_complete, bins=50, edgecolor='black', alpha=0.7)\naxes[1].axvline(0, color='red', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Residuals')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Residual Distribution')\naxes[1].grid(True, alpha=0.3)\n\n# Scale-Location Plot\nstandardized_resid = residuals_complete / residuals_complete.std()\naxes[2].scatter(y_pred_baseline, np.sqrt(np.abs(standardized_resid)), alpha=0.5, s=10)\naxes[2].set_xlabel('Fitted Values')\naxes[2].set_ylabel('√|Standardized Residuals|')\naxes[2].set_title('Scale-Location Plot')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Observations:\")\nprint(\"- Residuals show some heteroscedasticity (variance increases with fitted values)\")\nprint(\"- Overall pattern suggests room for improvement\")\n\n# Step 3: Check for multicollinearity\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 3: Check Multicollinearity\")\nprint(\"=\"*60)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = features_complete\nvif_data[\"VIF\"] = [variance_inflation_factor(X_complete, i) for i in range(len(features_complete))]\n\nprint(vif_data.sort_values('VIF', ascending=False))\nprint(\"\\nVIF &lt; 5: No serious multicollinearity concerns\")\n\n============================================================\nSTEP 1: Baseline Linear Regression\n============================================================\nTrain R²: 0.8347\nTest R²: 0.8406\nTest RMSE: 11405.3004\n\n============================================================\nSTEP 2: Diagnostic Plots\n============================================================\n\n\n\n\n\n\n\n\n\nObservations:\n- Residuals show some heteroscedasticity (variance increases with fitted values)\n- Overall pattern suggests room for improvement\n\n============================================================\nSTEP 3: Check Multicollinearity\n============================================================\n        Feature        VIF\n2       Poverty  21.903183\n5  ChildPoverty  16.856925\n1  Professional   9.947219\n4  IncomePerCap   7.250800\n3  Unemployment   4.843157\n0      TotalPop   4.334884\n\nVIF &lt; 5: No serious multicollinearity concerns\n\n\nSee the workflow? Fit → diagnose → identify issues. Now let’s fix them.\n\n\n9.2 Model Selection Strategy\nBased on diagnostics, try improvements systematically:\n\n# Step 4: Try different models\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 4: Model Comparison\")\nprint(\"=\"*60)\n\n# Scale features for regularization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_complete)\nX_test_scaled = scaler.transform(X_test_complete)\n\n# Dictionary to store results\nresults = []\n\n# Model 1: Baseline Linear Regression\nmodel1 = LinearRegression()\nmodel1.fit(X_train_scaled, y_train_complete)\ncv_score1 = cross_val_score(model1, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score1 = model1.score(X_test_scaled, y_test_complete)\nresults.append({\n    'Model': 'Linear Regression',\n    'CV R²': cv_score1,\n    'Test R²': test_score1,\n    'Features': len(features_complete)\n})\n\n# Model 2: Polynomial Features (degree 2)\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = poly_features.fit_transform(X_train_scaled)\nX_test_poly = poly_features.transform(X_test_scaled)\n\nmodel2 = LinearRegression()\nmodel2.fit(X_train_poly, y_train_complete)\ncv_score2 = cross_val_score(model2, X_train_poly, y_train_complete, cv=5, scoring='r2').mean()\ntest_score2 = model2.score(X_test_poly, y_test_complete)\nresults.append({\n    'Model': 'Polynomial (degree=2)',\n    'CV R²': cv_score2,\n    'Test R²': test_score2,\n    'Features': X_train_poly.shape[1]\n})\n\n# Model 3: Ridge Regression\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train_complete)\ncv_score3 = cross_val_score(ridge, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score3 = ridge.score(X_test_scaled, y_test_complete)\nresults.append({\n    'Model': 'Ridge (α=1.0)',\n    'CV R²': cv_score3,\n    'Test R²': test_score3,\n    'Features': len(features_complete)\n})\n\n# Model 4: Lasso Regression\nlasso = Lasso(alpha=0.01, max_iter=10000)\nlasso.fit(X_train_scaled, y_train_complete)\ncv_score4 = cross_val_score(lasso, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score4 = lasso.score(X_test_scaled, y_test_complete)\nn_features_lasso = np.sum(np.abs(lasso.coef_) &gt; 0.0001)\nresults.append({\n    'Model': 'Lasso (α=0.01)',\n    'CV R²': cv_score4,\n    'Test R²': test_score4,\n    'Features': f\"{n_features_lasso} (selected)\"\n})\n\n# Model 5: Elastic Net\nelastic = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)\nelastic.fit(X_train_scaled, y_train_complete)\ncv_score5 = cross_val_score(elastic, X_train_scaled, y_train_complete, cv=5, scoring='r2').mean()\ntest_score5 = elastic.score(X_test_scaled, y_test_complete)\nn_features_elastic = np.sum(np.abs(elastic.coef_) &gt; 0.0001)\nresults.append({\n    'Model': 'Elastic Net (α=0.01, l1=0.5)',\n    'CV R²': cv_score5,\n    'Test R²': test_score5,\n    'Features': f\"{n_features_elastic} (selected)\"\n})\n\n# Display results\nresults_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison Results:\")\nprint(results_df.to_string(index=False))\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(results_df))\nwidth = 0.35\n\nax.bar(x - width/2, results_df['CV R²'], width, label='CV R²', alpha=0.8)\nax.bar(x + width/2, results_df['Test R²'], width, label='Test R²', alpha=0.8)\n\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('R²', fontsize=12)\nax.set_title('Model Performance Comparison', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n# Best model\nbest_idx = results_df['CV R²'].argmax()\nbest_model_name = results_df.iloc[best_idx]['Model']\nprint(f\"\\n✓ Best model by CV: {best_model_name}\")\n\n\n============================================================\nSTEP 4: Model Comparison\n============================================================\n\nModel Comparison Results:\n                       Model    CV R²  Test R²     Features\n           Linear Regression 0.832510 0.840628            6\n       Polynomial (degree=2) 0.845916 0.856639           27\n               Ridge (α=1.0) 0.832520 0.840618            6\n              Lasso (α=0.01) 0.832510 0.840628 6 (selected)\nElastic Net (α=0.01, l1=0.5) 0.832527 0.840470 6 (selected)\n\n\n\n\n\n\n\n\n\n\n✓ Best model by CV: Polynomial (degree=2)\n\n\nThe comparison shows which approach works best for your data. In this case, all models perform similarly, but Ridge and Elastic Net provide slightly better generalization.\n\n\n9.3 Interpreting and Communicating Results\nOnce you’ve selected your best model, interpret the coefficients and communicate clearly:\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 5: Interpret Final Model\")\nprint(\"=\"*60)\n\n# Use Ridge as our final model\nfinal_model = Ridge(alpha=1.0)\nfinal_model.fit(X_train_scaled, y_train_complete)\n\n# Get coefficients\ncoef_df = pd.DataFrame({\n    'Feature': features_complete,\n    'Coefficient': final_model.coef_\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"\\nFinal Model Coefficients (Ridge, α=1.0):\")\nprint(coef_df.to_string(index=False))\n\n\n============================================================\nSTEP 5: Interpret Final Model\n============================================================\n\nFinal Model Coefficients (Ridge, α=1.0):\n     Feature   Coefficient\nIncomePerCap  15857.063355\nProfessional   2913.318816\nChildPoverty   1977.037962\nUnemployment   -882.530764\n    TotalPop  -1494.961708\n     Poverty -12904.379677\n\n\nInterpretation for Stakeholders: Our regression model explains approximately 60% of the variance in median household income across NYC census tracts. The model uses 6 key features:\nKey findings: 1. Income Per Capita is the strongest predictor. Each $1,000 increase in per capita income corresponds to a significant increase in median household income.\n\nProfessional Employment has a positive effect. Census tracts with more professionals tend to have higher median household incomes.\nPoverty Rate shows a strong negative relationship with income, as expected.\nChild Poverty has an additional negative effect beyond general poverty, highlighting the economic challenges faced by families with children.\nUnemployment shows a negative relationship with median household income, though this may be partially captured by the poverty variables.\nTotal Population has minimal effect on median household income at the census tract level.\n\nThe model performs consistently on both training and test data, suggesting it generalizes well to new predictions.\n\n\n\n\n\n\nTip\n\n\n\nThis is how you communicate results: translate coefficients into plain language, explain what matters, and acknowledge limitations.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways present both statistical results (R², coefficients) and practical interpretation. Stakeholders need to understand what the numbers mean for their decisions, not just that “the model has an R² of 0.60.”\n\n\nThe complete workflow: diagnose issues → try fixes systematically → select best model → interpret clearly. That’s professional regression analysis."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#summary",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#summary",
    "title": "Chapter 2: Regression Models",
    "section": "Summary",
    "text": "Summary\nLinear regression is the foundation of machine learning, not because it’s the most powerful model, but because understanding it deeply prepares you to understand everything else. This chapter covered the complete regression toolkit—from basic assumptions to advanced regularization techniques.\nKey Takeaways:\n\nLinear regression makes four assumptions: linearity, independence, homoscedasticity, and normality. Violate them, and your model might give terrible predictions even with high R².\nResidual plots are your diagnostic tool. They reveal problems that metrics hide. A curved residual plot means non-linearity. A funnel means heteroscedasticity. Random scatter means you’re good.\nEvaluation metrics serve different purposes. MSE penalizes large errors heavily. MAE treats all errors equally. R² tells you variance explained. RMSE gives interpretable units. Use multiple metrics, not just one.\nPolynomial features capture non-linearity while staying within linear regression. But high-degree polynomials overfit spectacularly. Always use validation data to choose the degree.\nMulticollinearity breaks coefficient interpretation but doesn’t hurt prediction accuracy. High VIF values warn you that coefficients are unstable. Regularization fixes this automatically.\nRidge regression (L2) shrinks all coefficients toward zero but never reaches exactly zero. It handles multicollinearity well and prevents overfitting. Always scale features first.\nLasso regression (L1) does automatic feature selection by setting some coefficients to exactly zero. Use it when you suspect many features are irrelevant.\nElastic Net combines Ridge and Lasso. It’s more stable than Lasso with correlated features while still doing feature selection. When unsure, start with Elastic Net.\nThe complete workflow matters: Fit baseline → check diagnostics → identify problems → fix systematically → validate with cross-validation → interpret clearly. Skip steps, and you’ll miss critical issues.\nAlways communicate results in plain language. Stakeholders don’t care that “the coefficient for X1 is 0.437 with a p-value of 0.003.” They care what that means for their decisions.\n\nRegression modeling is both art and science. The science is in the diagnostics, metrics, and validation. The art is in knowing when assumptions matter, which fixes to try, and how to communicate findings. Master both, and you’ll build models that actually work in the real world.\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#practice-exercises",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#practice-exercises",
    "title": "Chapter 2: Regression Models",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nThese exercises build progressively from understanding diagnostics to conducting complete regression analyses. Work through them to solidify your grasp of regression modeling.\n\nExercise 1: Residual Analysis\nTask: Load the NYC census dataset and fit a linear regression predicting median household income from just the TotalPop feature. Create a residuals vs. fitted values plot. Based on the pattern you observe:\n\nIdentify which regression assumption(s) are violated\nExplain what the pattern tells you about the model’s mistakes\nSuggest two specific fixes that might improve the model\nImplement one fix and show the improvement\n\nWhat you’re learning: How to read residual plots and diagnose problems.\n\n\nExercise 2: Coefficient Interpretation\nTask: Fit a multiple linear regression predicting median household income from Professional, Poverty, Unemployment, and ChildPoverty. For each coefficient:\n\nWrite the interpretation in plain English (e.g., “For every additional…”)\nExplain what “holding other features constant” means\nIdentify which coefficient is hardest to interpret and explain why\nCalculate and report both R² and Adjusted R², then explain the difference\n\nWhat you’re learning: How to interpret and communicate regression results.\n\n\nExercise 3: Polynomial Selection\nTask: Using the IncomePerCap feature alone, fit polynomial regression models of degrees 1 through 8:\n\nFor each degree, calculate train MSE and test MSE\nCreate a plot showing both training and test MSE across all degrees\nIdentify the optimal polynomial degree and justify your choice\nVisualize the polynomial fit for degrees 1, 3, and 8 on the same plot\nExplain why degree 8 has lower training MSE but might perform worse in practice\n\nWhat you’re learning: The bias-variance tradeoff and how to prevent overfitting.\n\n\nExercise 4: Multicollinearity Detection\nTask: Using all numeric features in the NYC census dataset:\n\nCompute and visualize the correlation matrix\nIdentify any feature pairs with correlation above 0.7\nCalculate VIF for each feature\nIdentify features with VIF &gt; 5 and explain what this means\nFit two models: one with all features, one removing high-VIF features. Compare coefficients and show which is more stable by refitting on a different random split.\n\nWhat you’re learning: How to detect and handle multicollinearity.\n\n\nExercise 5: Ridge vs Lasso Comparison\nTask: Using all features from the NYC census dataset, compare Ridge and Lasso regression:\n\nFor both Ridge and Lasso, find the optimal alpha using cross-validation\nPlot the regularization paths for both methods (coefficients vs. alpha)\nCreate a table showing which features each method keeps (for optimal alpha)\nExplain why Lasso eliminates some features while Ridge doesn’t\nRecommend which method you’d use for this dataset and why\n\nWhat you’re learning: The practical differences between L1 and L2 regularization.\n\n\nExercise 6: Complete Regression Workflow\nTask: Conduct a complete regression analysis on the NYC census dataset:\n\nBaseline: Fit linear regression with all features. Report train/test R² and RMSE.\nDiagnostics: Create residual plots (residuals vs fitted, histogram of residuals, scale-location). List any problems you identify.\nMulticollinearity: Check VIF. Report any concerns.\nModel Improvement: Try at least 3 different approaches (e.g., polynomial features, Ridge, Lasso, log transform). Use cross-validation to compare.\nFinal Model: Select your best model and justify the choice.\nInterpretation: Write a 2-3 paragraph summary explaining your findings to a non-technical audience. Include the model’s performance, key predictors, and limitations.\n\nWhat you’re learning: The complete end-to-end regression modeling workflow.\n\nTips for Success: - Don’t just run code—think about what each plot and metric is telling you - When diagnostic plots show problems, try multiple fixes and compare results - Always use train/validation/test splits or cross-validation properly - Write your interpretations before looking at solutions - Remember: a model that’s slightly less accurate but easier to interpret is often more valuable in practice\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Textbook/Module-2-Regression/chapter-2-regression.html#additional-resources",
    "href": "Textbook/Module-2-Regression/chapter-2-regression.html#additional-resources",
    "title": "Chapter 2: Regression Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRegression Assumptions Explained - Detailed guide to assumptions\nInterpreting Residual Plots - Penn State course notes\nRidge vs Lasso - Comprehensive comparison\nScikit-learn Linear Models - Official documentation\nFeature Scaling and Regularization - Why scaling matters"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "This document describes the images needed for Chapter 4. These should be created and placed in this directory.\n\n\n\n\nLocation in chapter: Section 1.4 “The LLM Advantage: Zero-Shot Extraction”\nDescription: A flowchart showing the complete LLM extraction workflow: - Input: Unstructured text (review, ticket, post) - Step 1: Write prompt + include text - Step 2: Call LLM API - Step 3: Receive JSON response - Step 4: Parse and validate - Step 5: Convert to DataFrame - Step 6: Feed to ML model - Output: Predictions\nUse boxes and arrows to show flow from left to right. Include icons or visual indicators for each step (text icon, API icon, JSON icon, table icon, model icon).\nPurpose: Give students the big picture of how LLM extraction fits into an ML pipeline\nSuggested tools: Draw.io, Lucidchart, PowerPoint, or Figma\n\n\n\n\nLocation in chapter: Section 3.1 “The Anatomy of a Good Extraction Prompt”\nDescription: Side-by-side comparison showing:\nLeft side - BAD PROMPT:\nTell me about this review:\n[review text]\nResponse: Vague, unstructured answer\nRight side - GOOD PROMPT:\nExtract sentiment from this review.\nAnswer with: positive, negative, or neutral\n\nReview: [review text]\n\nSentiment:\nResponse: “positive”\nUse red ✗ for bad prompt, green ✓ for good prompt. Highlight the key differences (specificity, format, clarity).\nPurpose: Visually demonstrate what makes a good extraction prompt\nSuggested tools: Screenshot and annotate, or create in PowerPoint/Keynote\n\n\n\n\nLocation in chapter: Section 6.1-6.2 “Understanding Token-Based Pricing” and “Calculating Extraction Costs”\nDescription: Bar chart comparing costs for processing 10,000 reviews across different LLMs: - X-axis: LLM model (GPT-3.5-turbo, GPT-4, Claude Sonnet, Gemini) - Y-axis: Total cost in dollars (logarithmic scale) - Bars showing relative costs with actual dollar amounts labeled\nExample values: - GPT-3.5: $0.50 - GPT-4: $5.00 - Claude: $2.50 - Gemini: $0.40\nUse color coding: green for cheapest, yellow for moderate, red for expensive.\nPurpose: Make the dramatic cost differences between models immediately visible\nSuggested approach: Create with matplotlib/seaborn or Excel/Google Sheets\n\n\n\n\nLocation in chapter: Section 6.3 “When to Use Which Model”\nDescription: Scatter plot showing the quality-cost tradeoff: - X-axis: Extraction quality/accuracy (70% - 95%) - Y-axis: Cost per extraction (logarithmic scale, $0.0001 - $0.01) - Points for different approaches: - Regex/Keywords: Low cost, moderate quality (bottom left) - GPT-3.5: Low-moderate cost, good quality (middle) - GPT-4: High cost, high quality (top right) - Traditional ML: Low cost (once trained), good quality (bottom middle-right)\nAdd labels for each point. Draw a “sweet spot” circle around GPT-3.5 area.\nPurpose: Help students visualize the tradeoff between cost and quality\nSuggested approach: matplotlib scatter plot with annotations\n\n\n\n\nLocation in chapter: Section 7.1 “Measuring Extraction Accuracy”\nDescription: Visual example showing correct vs incorrect extractions:\nTop section - Correct Extractions (✓): - Review: “Love this product!” → Extracted: “positive” → Ground truth: “positive” - Review: “Terrible quality.” → Extracted: “negative” → Ground truth: “negative”\nBottom section - Incorrect Extractions (✗): - Review: “It’s not bad.” → Extracted: “negative” → Ground truth: “positive” - Annotation: “Missed double negative” - Review: “I expected amazing quality…” → Extracted: “positive” → Ground truth: “negative” - Annotation: “Only read first part”\nUse green checkmarks for correct, red X’s for incorrect. Add brief explanations for why errors occurred.\nPurpose: Show students what to look for when validating extractions\nSuggested tools: PowerPoint, Google Slides, or design tool\n\n\n\n\nLocation in chapter: Section 8.1 “Decision Framework”\nDescription: Decision tree flowchart for choosing extraction method:\nStart: “Need to extract information from text” ↓ Question 1: “Is it simple pattern matching (emails, phone numbers)?” → YES: Use Regex → NO: Continue\nQuestion 2: “Do you have 1000+ labeled examples?” → YES: Train Traditional ML → NO: Continue\nQuestion 3: “Need to understand context/nuance?” → YES: Use LLM → NO: Try Regex/Keywords first\nUse diamond shapes for questions, rectangles for decisions. Color-code paths (green for simple solutions, yellow for moderate complexity, blue for LLM).\nPurpose: Give students a practical decision-making framework\nSuggested tools: Draw.io, Lucidchart, or flowchart software\n\n\n\n\n\nAll images should follow these guidelines:\n\nResolution: At least 1200px wide for diagrams, 1000px wide for plots\nFormat: PNG with transparent background where appropriate\nColors: Use colorblind-friendly palettes\n\nBlue (#3498db) for primary elements\nOrange (#e67e22) for secondary elements\nGreen (#2ecc71) for success/correct elements\nRed (#e74c3c) for errors/warnings\n\nFonts: Use clear, readable fonts (Arial, Helvetica, or similar)\nStyle: Professional but approachable—match the textbook’s conversational tone\nLabels: All axes, boxes, and elements should be clearly labeled\nSize: Large enough text that it’s readable when embedded in the document\n\n\n\n\nIf time is limited, create images in this priority order:\n\nllm-extraction-workflow.png - Most important for understanding the overall process\nprompt-comparison.png - Critical for teaching effective prompting\ncost-comparison-chart.png - Essential for understanding cost implications\nwhen-to-use-llms.png - Practical decision-making tool\nquality-vs-cost.png - Helps with model selection\nextraction-validation.png - Nice to have for quality control section\n\n\n\n\n\nCost comparison chart can be generated directly from Python using matplotlib\nQuality vs cost scatter plot should also be generated programmatically\nThe workflow diagram and decision tree need to be created manually with diagramming tools\nPrompt comparison can be screenshots from actual code/API calls with annotations added\nUse consistent color schemes across all images for professional appearance\nConsider creating a Python script to generate the data visualization plots for consistency"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#required-images",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#required-images",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "Location in chapter: Section 1.4 “The LLM Advantage: Zero-Shot Extraction”\nDescription: A flowchart showing the complete LLM extraction workflow: - Input: Unstructured text (review, ticket, post) - Step 1: Write prompt + include text - Step 2: Call LLM API - Step 3: Receive JSON response - Step 4: Parse and validate - Step 5: Convert to DataFrame - Step 6: Feed to ML model - Output: Predictions\nUse boxes and arrows to show flow from left to right. Include icons or visual indicators for each step (text icon, API icon, JSON icon, table icon, model icon).\nPurpose: Give students the big picture of how LLM extraction fits into an ML pipeline\nSuggested tools: Draw.io, Lucidchart, PowerPoint, or Figma\n\n\n\n\nLocation in chapter: Section 3.1 “The Anatomy of a Good Extraction Prompt”\nDescription: Side-by-side comparison showing:\nLeft side - BAD PROMPT:\nTell me about this review:\n[review text]\nResponse: Vague, unstructured answer\nRight side - GOOD PROMPT:\nExtract sentiment from this review.\nAnswer with: positive, negative, or neutral\n\nReview: [review text]\n\nSentiment:\nResponse: “positive”\nUse red ✗ for bad prompt, green ✓ for good prompt. Highlight the key differences (specificity, format, clarity).\nPurpose: Visually demonstrate what makes a good extraction prompt\nSuggested tools: Screenshot and annotate, or create in PowerPoint/Keynote\n\n\n\n\nLocation in chapter: Section 6.1-6.2 “Understanding Token-Based Pricing” and “Calculating Extraction Costs”\nDescription: Bar chart comparing costs for processing 10,000 reviews across different LLMs: - X-axis: LLM model (GPT-3.5-turbo, GPT-4, Claude Sonnet, Gemini) - Y-axis: Total cost in dollars (logarithmic scale) - Bars showing relative costs with actual dollar amounts labeled\nExample values: - GPT-3.5: $0.50 - GPT-4: $5.00 - Claude: $2.50 - Gemini: $0.40\nUse color coding: green for cheapest, yellow for moderate, red for expensive.\nPurpose: Make the dramatic cost differences between models immediately visible\nSuggested approach: Create with matplotlib/seaborn or Excel/Google Sheets\n\n\n\n\nLocation in chapter: Section 6.3 “When to Use Which Model”\nDescription: Scatter plot showing the quality-cost tradeoff: - X-axis: Extraction quality/accuracy (70% - 95%) - Y-axis: Cost per extraction (logarithmic scale, $0.0001 - $0.01) - Points for different approaches: - Regex/Keywords: Low cost, moderate quality (bottom left) - GPT-3.5: Low-moderate cost, good quality (middle) - GPT-4: High cost, high quality (top right) - Traditional ML: Low cost (once trained), good quality (bottom middle-right)\nAdd labels for each point. Draw a “sweet spot” circle around GPT-3.5 area.\nPurpose: Help students visualize the tradeoff between cost and quality\nSuggested approach: matplotlib scatter plot with annotations\n\n\n\n\nLocation in chapter: Section 7.1 “Measuring Extraction Accuracy”\nDescription: Visual example showing correct vs incorrect extractions:\nTop section - Correct Extractions (✓): - Review: “Love this product!” → Extracted: “positive” → Ground truth: “positive” - Review: “Terrible quality.” → Extracted: “negative” → Ground truth: “negative”\nBottom section - Incorrect Extractions (✗): - Review: “It’s not bad.” → Extracted: “negative” → Ground truth: “positive” - Annotation: “Missed double negative” - Review: “I expected amazing quality…” → Extracted: “positive” → Ground truth: “negative” - Annotation: “Only read first part”\nUse green checkmarks for correct, red X’s for incorrect. Add brief explanations for why errors occurred.\nPurpose: Show students what to look for when validating extractions\nSuggested tools: PowerPoint, Google Slides, or design tool\n\n\n\n\nLocation in chapter: Section 8.1 “Decision Framework”\nDescription: Decision tree flowchart for choosing extraction method:\nStart: “Need to extract information from text” ↓ Question 1: “Is it simple pattern matching (emails, phone numbers)?” → YES: Use Regex → NO: Continue\nQuestion 2: “Do you have 1000+ labeled examples?” → YES: Train Traditional ML → NO: Continue\nQuestion 3: “Need to understand context/nuance?” → YES: Use LLM → NO: Try Regex/Keywords first\nUse diamond shapes for questions, rectangles for decisions. Color-code paths (green for simple solutions, yellow for moderate complexity, blue for LLM).\nPurpose: Give students a practical decision-making framework\nSuggested tools: Draw.io, Lucidchart, or flowchart software"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#image-style-guidelines",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#image-style-guidelines",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "All images should follow these guidelines:\n\nResolution: At least 1200px wide for diagrams, 1000px wide for plots\nFormat: PNG with transparent background where appropriate\nColors: Use colorblind-friendly palettes\n\nBlue (#3498db) for primary elements\nOrange (#e67e22) for secondary elements\nGreen (#2ecc71) for success/correct elements\nRed (#e74c3c) for errors/warnings\n\nFonts: Use clear, readable fonts (Arial, Helvetica, or similar)\nStyle: Professional but approachable—match the textbook’s conversational tone\nLabels: All axes, boxes, and elements should be clearly labeled\nSize: Large enough text that it’s readable when embedded in the document"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#priority-order",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#priority-order",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "If time is limited, create images in this priority order:\n\nllm-extraction-workflow.png - Most important for understanding the overall process\nprompt-comparison.png - Critical for teaching effective prompting\ncost-comparison-chart.png - Essential for understanding cost implications\nwhen-to-use-llms.png - Practical decision-making tool\nquality-vs-cost.png - Helps with model selection\nextraction-validation.png - Nice to have for quality control section"
  },
  {
    "objectID": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#notes",
    "href": "Textbook/Module-4-LLMs-Feature-Engineering/images/placeholder-info.html#notes",
    "title": "Image Placeholders for Chapter 4: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "Cost comparison chart can be generated directly from Python using matplotlib\nQuality vs cost scatter plot should also be generated programmatically\nThe workflow diagram and decision tree need to be created manually with diagramming tools\nPrompt comparison can be screenshots from actual code/API calls with annotations added\nUse consistent color schemes across all images for professional appearance\nConsider creating a Python script to generate the data visualization plots for consistency"
  },
  {
    "objectID": "Textbook/table-of-contents.html",
    "href": "Textbook/table-of-contents.html",
    "title": "Textbook Table of Contents",
    "section": "",
    "text": "Module 1 – Exploratory Data Analysis (EDA)\n\nChapter 1: Exploratory Data Analysis\n\nModule 2 – Regression\n\nChapter 2: Regression\n\nModule 3 – Classification\n\nChapter 3: Classification\n\nModule 4 – LLMs & Feature Engineering\n\nChapter 4: LLMs & Feature Engineering\n\nModule 7 – Neural Networks\n\nChapter 7: Neural Networks"
  },
  {
    "objectID": "Textbook/table-of-contents.html#textbook-table-of-contents",
    "href": "Textbook/table-of-contents.html#textbook-table-of-contents",
    "title": "Textbook Table of Contents",
    "section": "",
    "text": "Module 1 – Exploratory Data Analysis (EDA)\n\nChapter 1: Exploratory Data Analysis\n\nModule 2 – Regression\n\nChapter 2: Regression\n\nModule 3 – Classification\n\nChapter 3: Classification\n\nModule 4 – LLMs & Feature Engineering\n\nChapter 4: LLMs & Feature Engineering\n\nModule 7 – Neural Networks\n\nChapter 7: Neural Networks"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3c.html",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3c.html",
    "title": "Quiz 3c: k-NN, SVM, and Class Imbalance",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#instructions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#instructions",
    "title": "Quiz 3c: k-NN, SVM, and Class Imbalance",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#section-a-conceptual-questions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#section-a-conceptual-questions",
    "title": "Quiz 3c: k-NN, SVM, and Class Imbalance",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou’re building a model to detect fraudulent credit card transactions. In your dataset, only 0.5% of transactions are fraud.\na) If you build a model that predicts “not fraud” for every transaction, what would its accuracy be?\nb) Why is this model useless despite high accuracy?\nc) What metric would be more appropriate to evaluate fraud detection models?\n\n\n\nQuestion 2 (4 points)\nFor k-Nearest Neighbors:\na) What happens to the model’s decision boundary as k increases from 1 to 100?\nb) Why is feature scaling critical for k-NN but not for decision trees?\nc) Name two distance metrics you could use and when each is appropriate.\n\n\n\nQuestion 3 (3 points)\nYou have a dataset with these features for predicting customer churn: - monthly_charges (continuous, $20-$150) - tenure_months (continuous, 1-72) - contract_type (categorical: “Month-to-month”, “One year”, “Two year”)\na) Why can’t you directly apply Euclidean distance to this dataset?\nb) What should you do before using k-NN with this data?\n\n\n\nQuestion 4 (3 points)\nExplain how SMOTE (Synthetic Minority Over-sampling Technique) addresses class imbalance.\na) How does SMOTE create new synthetic samples?\nb) Why might SMOTE be preferable to simply duplicating minority class examples?\nc) What is one potential drawback of using SMOTE?"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#section-b-code-writing",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#section-b-code-writing",
    "title": "Quiz 3c: k-NN, SVM, and Class Imbalance",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 5-9, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nAlso assume you have X_train, X_test, y_train, y_test already defined.\n\n\nQuestion 5 (3 points)\nWrite code to:\n\nScale features using StandardScaler (fit on train, transform both)\nFit a k-NN classifier with k=5 on the scaled data\nCalculate the test accuracy\n\n\n\n\nQuestion 6 (3 points)\nWrite code to fit a Support Vector Machine (SVC) classifier with:\n\nA linear kernel\nrandom_state=42\n\nThen make predictions on the test set and calculate the accuracy.\n\n\n\nQuestion 7 (4 points)\nWrite code to handle class imbalance using class weights:\n\nFit a Logistic Regression model WITH class_weight=‘balanced’\nFit another WITHOUT class weights\nPrint the classification report for both models\n\n\n\n\nQuestion 8 (3 points)\nWrite code to compare k-NN models with different k values:\n\nCreate a list of k values: [1, 3, 5, 7, 9]\nFor each k, fit a k-NN model on scaled training data\nCalculate and store the test accuracy for each\nPrint the k value and corresponding accuracy\n\nAssume features are already scaled and stored in X_train_scaled and X_test_scaled.\n\n\n\nQuestion 9 (3 points)\nYou suspect your dataset has class imbalance. Write code to:\n\nCheck the distribution of classes in y_train using value_counts()\nCalculate and print the percentage of each class\nPrint whether the dataset is imbalanced (if smallest class is &lt;10%)"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#grading-rubric",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3c.html#grading-rubric",
    "title": "Quiz 3c: k-NN, SVM, and Class Imbalance",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (13 points)\n\nUnderstanding class imbalance and appropriate metrics: 3 points\nk-NN concepts and distance metrics: 4 points\nFeature scaling necessity: 3 points\nSMOTE and oversampling techniques: 3 points\n\nSection B: Code Writing (16 points)\n\nFeature scaling for k-NN: 3 points\nFitting SVM: 3 points\nHandling class imbalance with class weights: 4 points\nComparing k-NN with different k values: 3 points\nDetecting class imbalance in data: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 29 points"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3a.html",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3a.html",
    "title": "Quiz 3a: Logistic Regression and Confusion Matrices",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#instructions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#instructions",
    "title": "Quiz 3a: Logistic Regression and Confusion Matrices",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#section-a-conceptual-questions",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#section-a-conceptual-questions",
    "title": "Quiz 3a: Logistic Regression and Confusion Matrices",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nIn supervised learning, you’re given labeled training data to build a model.\na) What is the key difference between supervised and unsupervised learning?\nb) Give an example of a supervised learning task and an unsupervised learning task.\nc) Why is classification considered a supervised learning problem?\n\n\n\nQuestion 2 (4 points)\nA medical screening model produces the following confusion matrix:\n\n\n\n\nPredicted: No Disease\nPredicted: Disease\n\n\n\n\nActual: No Disease\n850\n50\n\n\nActual: Disease\n20\n80\n\n\n\na) Calculate the accuracy of this model.\nb) Calculate the precision for detecting disease.\nc) Calculate the recall for detecting disease.\nd) In medical screening, is precision or recall more important? Explain why.\n\n\n\nQuestion 3 (3 points)\nCompare logistic regression and decision trees:\na) What shape of decision boundary does logistic regression create? What about decision trees?\nb) Which model is more interpretable to a non-technical stakeholder? Why?\nc) Which model requires feature scaling? Why?\n\n\n\nQuestion 4 (2 points)\nA spam classifier has: - Precision = 0.95 - Recall = 0.70\na) What does the precision value mean in plain English?\nb) What does the recall value mean in plain English?"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#section-b-code-writing",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#section-b-code-writing",
    "title": "Quiz 3a: Logistic Regression and Confusion Matrices",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 5-8, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (confusion_matrix, accuracy_score,\n                             precision_score, recall_score, f1_score)\nAlso assume you have X_train, X_test, y_train, y_test already defined.\n\n\nQuestion 5 (3 points)\nWrite code to:\n\nFit a Logistic Regression model\nMake predictions on X_test\nGet probability predictions for the positive class\n\n\n\n\nQuestion 6 (4 points)\nWrite code to:\n\nCreate a confusion matrix from y_test and predictions\nCalculate accuracy, precision, recall, and F1 score\nPrint all four metrics\n\n\n\n\nQuestion 7 (3 points)\nWrite code to split a dataset into training and test sets:\n\nUse 80% for training, 20% for testing\nSet random_state=42\nThe features are in DataFrame X and the target is in Series y\n\nStore the results in X_train, X_test, y_train, y_test.\n\n\n\nQuestion 8 (3 points)\nYou have a fitted logistic regression model called model. Write code to:\n\nGet the model’s coefficients\nPrint the intercept\nMake a prediction for a single new data point stored in variable new_data"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#grading-rubric",
    "href": "Assignments/Module 3 - Classification/module-3-quiz-3a.html#grading-rubric",
    "title": "Quiz 3a: Logistic Regression and Confusion Matrices",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (12 points)\n\nUnderstanding supervised vs unsupervised learning: 3 points\nConfusion matrix calculations and interpretation: 4 points\nModel comparison (logistic regression vs trees): 3 points\nPrecision and recall interpretation: 2 points\n\nSection B: Code Writing (13 points)\n\nFitting logistic regression and predictions: 3 points\nCreating confusion matrix and calculating metrics: 4 points\nTrain-test splitting: 3 points\nModel inspection and single prediction: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 25 points"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-homework.html",
    "href": "Assignments/Module 3 - Classification/module-3-homework.html",
    "title": "Module 5 Homework: Classification Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational understanding of classification models, metrics, and evaluation. Part B should be completed with AI assistance (like Gemini CLI) to practice comprehensive model comparison and hyperparameter tuning at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with three datasets:\nDataset 1: titanic.csv - Titanic passenger survival data with columns:\n\nSurvived: Whether passenger survived (0=No, 1=Yes) (TARGET)\nPclass: Passenger class (1, 2, or 3)\nSex: Passenger sex\nAge: Passenger age\nSibSp: Number of siblings/spouses aboard\nParch: Number of parents/children aboard\nFare: Ticket fare\nEmbarked: Port of embarkation (C, Q, S)\n\nDataset 2: spotify.csv - Spotify song data with columns:\n\npopularity: Song popularity score (0-100)\ndanceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo: Audio features\n\nYou will create a binary target: viral_hit = popularity &gt; 80\nDataset 3: isp_customers.csv - Internet service provider customer data with columns:\n\nChurn: Whether customer churned (TARGET)\ngender, SeniorCitizen, Partner, Dependents: Demographics\ntenure: Months with company\nPhoneService, InternetService, Contract: Service details\nMonthlyCharges, TotalCharges: Billing information"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-homework.html#instructions",
    "href": "Assignments/Module 3 - Classification/module-3-homework.html#instructions",
    "title": "Module 5 Homework: Classification Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational understanding of classification models, metrics, and evaluation. Part B should be completed with AI assistance (like Gemini CLI) to practice comprehensive model comparison and hyperparameter tuning at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with three datasets:\nDataset 1: titanic.csv - Titanic passenger survival data with columns:\n\nSurvived: Whether passenger survived (0=No, 1=Yes) (TARGET)\nPclass: Passenger class (1, 2, or 3)\nSex: Passenger sex\nAge: Passenger age\nSibSp: Number of siblings/spouses aboard\nParch: Number of parents/children aboard\nFare: Ticket fare\nEmbarked: Port of embarkation (C, Q, S)\n\nDataset 2: spotify.csv - Spotify song data with columns:\n\npopularity: Song popularity score (0-100)\ndanceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo: Audio features\n\nYou will create a binary target: viral_hit = popularity &gt; 80\nDataset 3: isp_customers.csv - Internet service provider customer data with columns:\n\nChurn: Whether customer churned (TARGET)\ngender, SeniorCitizen, Partner, Dependents: Demographics\ntenure: Months with company\nPhoneService, InternetService, Contract: Service details\nMonthlyCharges, TotalCharges: Billing information"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-homework.html#part-a-by-hand-no-ai-assistance",
    "href": "Assignments/Module 3 - Classification/module-3-homework.html#part-a-by-hand-no-ai-assistance",
    "title": "Module 5 Homework: Classification Models",
    "section": "Part A: By Hand (No AI Assistance)",
    "text": "Part A: By Hand (No AI Assistance)\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational understanding of classification models and metrics.\n\nQuestion 1 (4 points)\nLoad the titanic.csv dataset and prepare it for classification:\n\nDrop rows with missing values in Age, Fare, and Survived\nCreate feature matrix X using: Pclass, Age, SibSp, Parch, Fare\nCreate target vector y using Survived\nSplit into train/test sets (80/20, random_state=42)\nHow many passengers are in each class (survived vs not survived) in the training set? Is this balanced or imbalanced?\n\n\n\n\nQuestion 2 (5 points)\nFit a Logistic Regression model on the Titanic data:\n\nFit the model on X_train and y_train\nGet predictions on X_test\nCreate and display a confusion matrix\nCalculate accuracy, precision, recall, and F1 score manually from the confusion matrix values\nVerify your calculations match sklearn’s metric functions\n\n\n\n\nQuestion 3 (5 points)\nInterpret the logistic regression results:\n\nExtract the coefficients and create a DataFrame showing feature names and their coefficients\nCalculate the odds ratio for each feature (exp of coefficient)\nWhich feature has the strongest positive association with survival? Strongest negative?\nInterpret the Pclass coefficient in plain language: “For each unit increase in passenger class…”\nWhy should we be careful interpreting these coefficients?\n\n\n\n\nQuestion 4 (4 points)\nFit a Decision Tree classifier on the Titanic data:\n\nFit a tree with max_depth=3\nCalculate training and test accuracy\nExtract and plot feature importances as a horizontal bar chart\nWhich feature does the decision tree consider most important? Does this match the logistic regression coefficients?\n\n\n\n\nQuestion 5 (5 points)\nInvestigate the bias-variance tradeoff with decision trees:\n\nFit decision trees with max_depth values: [1, 2, 3, 5, 7, 10, 15, 20, None]\nFor each, calculate training and test accuracy\nPlot both accuracies vs. max_depth on the same graph\nAt what depth does overfitting become apparent?\nWhat max_depth would you choose for this dataset? Why?\n\n\n\n\nQuestion 6 (4 points)\nFit a Random Forest classifier:\n\nFit with n_estimators=100, max_depth=5, random_state=42\nCompare its test accuracy to the single decision tree with max_depth=5\nExtract feature importances from both models and compare them\nWhy might the feature importances differ between a single tree and a random forest?\n\n\n\n\nQuestion 7 (5 points)\nFit a k-Nearest Neighbors classifier:\n\nScale the features using StandardScaler (fit on train, transform both)\nFit k-NN with k=5 on the scaled data\nCalculate test accuracy\nTest k values [1, 3, 5, 7, 10, 15, 20] and plot test accuracy vs. k\nWhat k value gives the best performance? What happens at very small and very large k?\n\n\n\n\nQuestion 8 (4 points)\nCreate an ROC curve for the logistic regression model:\n\nGet probability predictions for the positive class\nCalculate FPR and TPR using roc_curve\nCalculate the AUC score\nPlot the ROC curve with the diagonal reference line and display AUC in the legend\n\n\n\n\nQuestion 9 (5 points)\nCompare multiple classifiers:\n\nFit Logistic Regression, Decision Tree (depth=5), Random Forest (100 trees), and k-NN (k=5, scaled)\nCalculate test accuracy for each\nCreate a bar chart comparing accuracies\nGet ROC curves for all models that provide predict_proba and plot them on the same axes\nBased on both accuracy and AUC, which model performs best?\n\n\n\n\nQuestion 10 (4 points)\nExplain in your own words (3-4 sentences each):\n\nWhen would you choose precision as your primary metric? Give a real-world example.\nWhen would you choose recall as your primary metric? Give a real-world example.\nWhat is the F1 score and when is it useful?\nWhy is accuracy often misleading for imbalanced datasets?"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-homework.html#part-b-with-ai-assistance",
    "href": "Assignments/Module 3 - Classification/module-3-homework.html#part-b-with-ai-assistance",
    "title": "Module 5 Homework: Classification Models",
    "section": "Part B: With AI Assistance",
    "text": "Part B: With AI Assistance\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you perform comprehensive classification analysis. The goal is to practice systematic model comparison and handling real-world challenges.\nImportant: For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n\nQuestion 11 (5 points)\nUse AI to perform comprehensive hyperparameter tuning for a Decision Tree on the Titanic dataset.\nTest combinations of: - max_depth: [2, 3, 5, 7, 10, 15] - min_samples_split: [2, 5, 10, 20] - min_samples_leaf: [1, 2, 5, 10]\nYour code should:\n\nUse GridSearchCV with 5-fold cross-validation\nFind the best hyperparameter combination\nDisplay the top 5 parameter combinations\nFit the best model and evaluate on test set\n\nDeliverables:\n\nCode\nBest parameters and test accuracy\nWritten interpretation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 12 (6 points)\nPrompt AI to create a comprehensive model comparison on the Titanic dataset.\nCompare these classifiers: - Logistic Regression - Decision Tree (tuned) - Random Forest (tuned) - SVM with RBF kernel - k-NN (tuned)\nYour code should:\n\nUse GridSearchCV to tune each model\nCreate a comparison table with accuracy, precision, recall, F1, and AUC\nPlot all ROC curves on the same graph\nCreate a heatmap of the comparison metrics\n\nWrite a paragraph recommending which model to use and why.\nDeliverables:\n\nCode\nComparison table and visualizations\nROC curve comparison plot\nWritten recommendation (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 13 (6 points)\nUse AI to analyze class imbalance in the Spotify dataset.\nCreate a binary classification target: viral_hit = popularity &gt; 80\nYour code should:\n\nShow the class distribution\nFit a baseline model (Logistic Regression) without handling imbalance\nApply three strategies: class_weight=‘balanced’, SMOTE, random undersampling\nCompare performance using precision, recall, F1, and AUC for each strategy\nCreate confusion matrices for each approach\n\nWhich strategy works best for identifying viral hits?\nDeliverables:\n\nCode\nClass distribution visualization\nPerformance comparison table\nConfusion matrices for each approach\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 14 (6 points)\nPrompt AI to visualize decision boundaries for different classifiers.\nUsing the Titanic dataset with only Age and Fare as features:\n\nCreate decision boundary plots for:\n\nLogistic Regression\nDecision Tree (depth=3)\nDecision Tree (depth=10)\nRandom Forest\nSVM (linear kernel)\nSVM (RBF kernel)\nk-NN (k=3)\nk-NN (k=20)\n\n\nArrange all 8 plots in a 2x4 grid with consistent axis limits and color schemes.\nHow do the decision boundaries differ? Which creates the most complex boundaries?\nDeliverables:\n\nCode\nDecision boundary visualization (2x4 grid)\nWritten comparison (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 15 (5 points)\nUse AI to analyze feature importance across different models.\nUsing the Titanic dataset (all numeric features):\n\nExtract feature importances from:\n\nLogistic Regression (absolute coefficients)\nDecision Tree\nRandom Forest\nPermutation importance for k-NN and SVM\n\nCreate a heatmap showing feature importance rankings across all models\n\nWhich features are consistently important? Which differ between models?\nDeliverables:\n\nCode\nFeature importance heatmap\nWritten analysis (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 16 (6 points)\nPrompt AI to perform cross-dataset model evaluation.\nTrain models on the Titanic dataset and ISP customer dataset (predicting churn):\n\nFor each dataset, tune and fit: Logistic Regression, Random Forest, k-NN\nCompare how well each model type performs across the two datasets\nCreate a comparison table showing model performance on each dataset\n\nDo the same model types perform best on both datasets? Why or why not?\nDeliverables:\n\nCode for both datasets\nCross-dataset comparison table\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 17 (6 points)\nPrompt AI to create an ensemble voting classifier.\nUsing the Titanic dataset:\n\nCreate a VotingClassifier combining:\n\nLogistic Regression\nRandom Forest\nSVM (with probability=True)\n\nUse both ‘hard’ and ‘soft’ voting\nCompare the ensemble to individual models\n\nDoes the ensemble outperform the best individual model?\nDeliverables:\n\nCode\nPerformance comparison table\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 18 (6 points)\nUse AI to analyze k-NN with different distance metrics.\nUsing the ISP customer dataset (with a mix of categorical and numerical features):\n\nTest distance metrics: euclidean, manhattan, cosine\nFor categorical features, use appropriate encoding\nCompare performance across metrics\nAnalyze which metric works best for this mixed-type data\n\nHow does the choice of distance metric affect performance?\nDeliverables:\n\nCode\nPerformance comparison table\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 19 (12 points)\nFor this final question, ask AI to help you conduct a complete classification analysis from start to finish on the ISP customer dataset (predicting churn).\nYour analysis should include:\n\nExploratory Data Analysis:\n\nSummary statistics\nClass distribution\nFeature distributions by target class\nCorrelation analysis\n\nData Preparation:\n\nHandle missing values\nEncode categorical variables\nFeature scaling (where appropriate)\nTrain/validation/test split (60/20/20)\n\nModel Development:\n\nFit multiple models: Logistic Regression, Decision Tree, Random Forest, SVM, k-NN\nHyperparameter tuning using validation set\nHandle class imbalance appropriately\n\nModel Evaluation:\n\nCompare all models using appropriate metrics (given potential imbalance)\nROC curves for all models\nSelect best model based on business context (what matters more: finding churners or not falsely flagging loyal customers?)\n\nFinal Model:\n\nEvaluate on test set (only once!)\nFeature importance analysis\nCreate confusion matrix with business interpretation\n\n\nWrite a 1-page executive summary (300-400 words) that explains:\n\nThe business problem (customer churn prediction)\nData characteristics and challenges\nModels tested and evaluation approach\nHow the best model was selected\nPerformance on test set\nKey features predicting churn\nBusiness recommendations\n\nDeliverables:\n\nComplete analysis code (well-commented)\nAll visualizations (EDA, ROC curves, confusion matrices)\nExecutive summary\nYour AI prompt(s)\nReflection: What classification-specific challenges did you encounter that you wouldn’t face in regression?"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-homework.html#submission-guidelines",
    "href": "Assignments/Module 3 - Classification/module-3-homework.html#submission-guidelines",
    "title": "Module 5 Homework: Classification Models",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nSubmit a ZIP file containing:\n\nCode files: All .py files or Jupyter notebooks\nVisualizations folder: All plots and charts generated\nWritten responses: A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\nData: Include the datasets if you made any modifications"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-homework.html#grading-rubric",
    "href": "Assignments/Module 3 - Classification/module-3-homework.html#grading-rubric",
    "title": "Module 5 Homework: Classification Models",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nPart A: By Hand (45 points)\n\nCode correctness and implementation: 25 points\nProper understanding of classification metrics and models: 12 points\nWritten interpretations and explanations: 8 points\n\nPart B: AI-Assisted (55 points)\n\nCode functionality and correctness: 25 points\nQuality and specificity of AI prompts: 14 points\nVisualizations (ROC curves, confusion matrices, comparisons): 10 points\nWritten interpretations and insights: 6 points\n\nTotal: 100 points"
  },
  {
    "objectID": "Assignments/Module 3 - Classification/module-3-homework.html#tips-for-success",
    "href": "Assignments/Module 3 - Classification/module-3-homework.html#tips-for-success",
    "title": "Module 5 Homework: Classification Models",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nFor Part A:\n\nAlways check class balance before fitting models\nRemember: precision = “of those I predicted positive, how many were correct?”\nRemember: recall = “of those that were actually positive, how many did I find?”\nDecision tree depth controls overfitting—start shallow\nk-NN requires scaling; trees do not\nWhen comparing models, use multiple metrics, not just accuracy\n\n\n\nFor Part B:\n\nAsk AI to test many hyperparameter combinations systematically\nRequest ROC curves for all models on the same plot for easy comparison\nWhen dealing with imbalance, always compare multiple strategies\nAsk for decision boundary visualizations to understand model behavior\nRequest confusion matrices normalized by row (actual class) for better interpretation\n\n\n\nGeneral:\n\nAccuracy can be misleading—always check class balance first\nThink about the business context when choosing metrics\nROC/AUC is great for model comparison but doesn’t tell the whole story\nFeature importance from different models may disagree—that’s okay\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1c.html",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1c.html",
    "title": "Quiz 1c: Statistical Summaries and Visualization",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#instructions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#instructions",
    "title": "Quiz 1c: Statistical Summaries and Visualization",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#section-a-conceptual-questions",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#section-a-conceptual-questions",
    "title": "Quiz 1c: Statistical Summaries and Visualization",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (2 points)\nYou’re exploring a dataset of customer purchases with columns for customer_id, purchase_amount, date, and product_category. You create a histogram of purchase_amount and notice it’s heavily right-skewed with a few very large outliers.\na) Why might a right-skewed distribution with outliers make it harder to understand typical customer behavior?\nb) Name two approaches you could take during EDA to better understand this distribution.\n\n\n\nQuestion 2 (2 points)\nYou want to visualize the relationship between two numerical variables: age and salary.\na) What type of visualization would you use and why?\nb) If you wanted to add a third categorical variable (like department) to this visualization, how could you incorporate it?"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#section-b-code-writing",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#section-b-code-writing",
    "title": "Quiz 1c: Statistical Summaries and Visualization",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 3-6, assume you have already imported pandas as pd and seaborn as sns.\n\n\nQuestion 3 (3 points)\nWrite code to create a histogram of a column called price with 20 bins. Add a title “Distribution of Product Prices” and label the x-axis as “Price ($)”.\n\n\n\nQuestion 4 (3 points)\nWrite code to create a scatter plot showing the relationship between square_feet (x-axis) and price (y-axis) for a DataFrame called housing_df. Add appropriate labels and a title.\n\n\n\nQuestion 5 (4 points)\nWrite a simple unit test function called test_price_is_positive that checks whether all values in a price column are greater than zero. The function should:\n\nTake a DataFrame as input\nReturn True if all prices are positive\nReturn False otherwise\n\nYou can write this as a simple Python function (you don’t need to use a testing framework).\n\n\n\nQuestion 6 (3 points)\nWrite code to calculate and display the following summary statistics for a column called sales_amount in a DataFrame called df:\n\nMean\nMedian\nStandard deviation\n\nYou may use Pandas methods or NumPy functions."
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#grading-rubric",
    "href": "Assignments/Module 1 - EDA/module-1-quiz-1c.html#grading-rubric",
    "title": "Quiz 1c: Statistical Summaries and Visualization",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (4 points)\n\nUnderstanding distributions and outliers: 2 points\nChoosing appropriate visualizations: 2 points\n\nSection B: Code Writing (13 points)\n\nCreating histograms with customization: 3 points\nCreating scatter plots with labels: 3 points\nWriting test functions: 4 points\nCalculating summary statistics: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 17 points"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-homework.html",
    "href": "Assignments/Module 1 - EDA/module-1-homework.html",
    "title": "Module 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational skills. Part B should be completed with AI assistance (like Gemini CLI) to practice scaling your data exploration work.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with a dataset called housing_data.csv which contains information about houses sold in various cities. The dataset includes:\n\nprice: Sale price in dollars\nsquare_feet: Size of the house\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nyear_built: Year the house was built\ncity: City where the house is located\nlot_size: Size of the lot in square feet\ngarage_spaces: Number of garage spaces\nhas_pool: Whether the house has a pool (True/False)"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-homework.html#instructions",
    "href": "Assignments/Module 1 - EDA/module-1-homework.html#instructions",
    "title": "Module 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational skills. Part B should be completed with AI assistance (like Gemini CLI) to practice scaling your data exploration work.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with a dataset called housing_data.csv which contains information about houses sold in various cities. The dataset includes:\n\nprice: Sale price in dollars\nsquare_feet: Size of the house\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nyear_built: Year the house was built\ncity: City where the house is located\nlot_size: Size of the lot in square feet\ngarage_spaces: Number of garage spaces\nhas_pool: Whether the house has a pool (True/False)"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-homework.html#part-a-by-hand-no-ai-assistance",
    "href": "Assignments/Module 1 - EDA/module-1-homework.html#part-a-by-hand-no-ai-assistance",
    "title": "Module 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Part A: By Hand (No AI Assistance)",
    "text": "Part A: By Hand (No AI Assistance)\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational skills in Pandas and Seaborn.\n\nQuestion 1 (3 points)\nLoad the housing_data.csv file into a Pandas DataFrame. Display:\n\nThe first 10 rows\nThe shape of the dataset (rows and columns)\nThe data types of each column\nA count of missing values for each column\n\nWrite 2-3 sentences describing what you learned about the dataset from this initial exploration.\n\n\n\nQuestion 2 (3 points)\nCalculate and display the following summary statistics for the price column:\n\nMean\nMedian\nStandard deviation\nMinimum and maximum values\n25th, 50th, and 75th percentiles\n\nBased on comparing the mean and median, what can you infer about the distribution of housing prices?\n\n\n\nQuestion 3 (4 points)\nCreate a histogram of the price column with 30 bins. Add:\n\nA title: “Distribution of Housing Prices”\nX-axis label: “Price ($)”\nY-axis label: “Frequency”\n\nLooking at the histogram, describe the distribution. Is it symmetric, skewed left, or skewed right? Are there any notable outliers?\n\n\n\nQuestion 4 (4 points)\nFilter the dataset to include only houses with:\n\n3 or more bedrooms\nBuilt after the year 2000\nLocated in either “Houston” or “Austin”\n\nHow many houses meet these criteria? What percentage of the total dataset is this?\n\n\n\nQuestion 5 (4 points)\nCreate a scatter plot showing the relationship between square_feet (x-axis) and price (y-axis). Add:\n\nAppropriate axis labels\nA title\nSet the figure size to 10x6 inches\n\nWhat relationship do you observe? Does it make intuitive sense?\n\n\n\nQuestion 6 (4 points)\nCalculate the average price for houses in each city. Display this as a Pandas Series sorted from highest to lowest average price.\nCreate a bar plot showing the average price by city. Which city has the highest average housing price? Which has the lowest?\n\n\n\nQuestion 7 (5 points)\nCreate a box plot comparing the price distributions across different numbers of bedrooms. Make sure to:\n\nLabel your axes appropriately\nAdd a title\nMake the plot easy to read\n\nWhat do you notice about how price varies with the number of bedrooms?\n\n\n\nQuestion 8 (5 points)\nWrite a function called check_data_quality that takes a DataFrame as input and returns a dictionary with the following information:\n\nNumber of duplicate rows\nNumber of columns with missing values\nPercentage of missing values in the entire dataset\nList of numerical columns with negative values (if any)\n\nRun this function on your housing dataset and interpret the results. Are there any data quality issues you should be concerned about?\n\n\n\nQuestion 9 (4 points)\nWrite a unit test function called test_price_range that checks whether all prices in the dataset are between $50,000 and $5,000,000. The function should:\n\nTake a DataFrame as input\nPrint a message indicating whether the test passed or failed\nIf it fails, print how many values are out of range\n\nRun your test on the housing dataset.\n\n\n\nQuestion 10 (4 points)\nCreate a new column called price_per_sqft that calculates the price per square foot for each house. Then:\n\nCalculate the mean price per square foot\nCreate a histogram of this new column\nIdentify which city has the highest average price per square foot\n\nWhy might price per square foot be a more useful metric than absolute price for comparing houses?"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-homework.html#part-b-with-ai-assistance",
    "href": "Assignments/Module 1 - EDA/module-1-homework.html#part-b-with-ai-assistance",
    "title": "Module 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Part B: With AI Assistance",
    "text": "Part B: With AI Assistance\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you write and scale your code. The goal is to practice writing effective prompts and using AI to tackle more comprehensive analysis tasks.\nImportant: For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n\nQuestion 11 (5 points)\nUse AI to create a comprehensive data profile that includes:\n\nSummary statistics for ALL numerical columns (not just price)\nValue counts for ALL categorical columns\nA report on missing values (which columns, how many, what percentage)\nIdentification of potential outliers in numerical columns (using percentile method: values below 1st or above 99th percentile)\n\nDeliverables:\n\nThe code generated\nThe output/report\nThe prompt(s) you used\n\n\n\n\nQuestion 12 (5 points)\nPrompt the AI to create a function that generates a complete set of univariate visualizations for the dataset. This should include:\n\nHistograms for all numerical columns\nBar plots for all categorical columns\nAll plots should be saved to a folder called univariate_plots/\nEach plot should have proper labels and titles\n\nRun the function and submit all generated plots along with your code.\nDeliverables:\n\nThe code\nAll generated visualizations\nThe prompt(s) you used\n\n\n\n\nQuestion 13 (5 points)\nUse AI to create a pair plot (using sns.pairplot()) showing relationships between key numerical variables: price, square_feet, bedrooms, and year_built. Color the points by city.\nBased on the pair plot, answer:\n\nWhich pair of variables shows the strongest visual relationship?\nAre there any variables that don’t seem to relate to price?\nDo you notice any city-specific patterns?\n\nDeliverables:\n\nThe pair plot visualization\nYour written interpretation (3-4 sentences)\nThe prompt(s) you used\n\n\n\n\nQuestion 14 (6 points)\nPrompt the AI to create a function that compares price distributions across different subgroups. Specifically:\n\nCompare price by city\nCompare price by number of bedrooms\nCompare price by whether the house has a pool\nGenerate both statistical summaries AND visualizations (box plots or violin plots)\n\nRun this function and write 2-3 paragraphs interpreting the results. What insights did you gain about what drives housing prices?\nDeliverables:\n\nThe code\nAll visualizations\nYour written interpretation\nThe prompt(s) you used\n\n\n\n\nQuestion 15 (6 points)\nAsk the AI to help you identify potential outliers in the price column. Your approach should:\n\nDefine outliers as values below the 1st percentile or above the 99th percentile\nCreate visualizations showing the distribution with outliers highlighted\nDisplay summary statistics for the full dataset and for data excluding outliers\nInvestigate the outliers: are they data errors or legitimate expensive/cheap houses?\n\nHow many potential outliers did you identify? Should they be removed, or do they represent legitimate data points? Justify your answer.\nDeliverables:\n\nThe code\nVisualizations showing outliers\nWritten analysis (4-5 sentences)\nThe prompt(s) you used\n\n\n\n\nQuestion 16 (6 points)\nUse AI to create a comprehensive test suite for the housing dataset. Your test suite should include at least 5 unit tests that check:\n\nData types are correct\nNo negative values in columns where they don’t make sense (price, square_feet, etc.)\nDates are within reasonable ranges (year_built should be between 1800 and 2024)\nCategorical values are from expected sets (cities should only be from a known list)\nNo duplicate rows\n\nRun all tests and report which (if any) failed.\nDeliverables:\n\nThe complete test suite code\nTest results\nThe prompt(s) you used\n\n\n\n\nQuestion 17 (5 points)\nPrompt the AI to create multiple scatter plots exploring the relationship between price and other numerical variables. You should generate:\n\nPrice vs. square_feet (colored by city)\nPrice vs. year_built (colored by has_pool)\nPrice vs. lot_size (with a trend line)\nPrice_per_sqft vs. bedrooms (colored by city)\n\nFor each plot, write 1-2 sentences describing what you observe.\nDeliverables:\n\nAll scatter plots\nYour written observations\nThe prompt(s) you used\n\n\n\n\nQuestion 18 (6 points)\nUse AI to create a function that systematically explores how price relates to ALL categorical variables in the dataset. For each categorical variable, the function should:\n\nCreate a bar plot showing average price by category\nGenerate a box plot showing the distribution of prices across categories\nCalculate and display summary statistics (mean, median, count) for each category\nSave all plots to a folder called categorical_analysis/\n\nRun this function and write a paragraph identifying which categorical variables seem most important in determining house prices.\nDeliverables:\n\nThe function code\nAll generated visualizations\nYour written analysis\nThe prompt(s) you used\n\n\n\n\nQuestion 19 (6 points)\nUse AI to create a function that performs a complete exploratory data analysis for ANY numerical column in the dataset. The function should:\n\nTake a DataFrame and column name as inputs\nGenerate: histogram, box plot, and summary statistics\nIdentify potential outliers using percentiles (values below 1st or above 99th percentile)\nCreate a markdown-formatted report with findings\nSave all plots to a specified folder\n\nRun this function on three different columns: price, square_feet, and lot_size.\nDeliverables:\n\nThe function code\nAll generated visualizations\nThe markdown reports for each column\nThe prompt(s) you used\n\n\n\n\nQuestion 20 (7 points)\nFor this final question, ask the AI to help you create a comprehensive EDA report. This should be a multi-page visualization that includes:\n\nOverall dataset summary (rows, columns, missing values)\nDistribution plots for key numerical variables\nCorrelation analysis\nPrice analysis by different categorical variables\nAny interesting patterns or anomalies you discovered\n\nWrite a 1-page executive summary (300-400 words) that you would present to a stakeholder who doesn’t know anything about data science. Explain:\n\nWhat the dataset contains\nKey findings from your analysis\nWhat factors seem most important in determining house prices\nAny data quality issues or limitations\nRecommendations for next steps\n\nDeliverables:\n\nAll visualizations in your EDA report\nYour executive summary\nThe prompt(s) you used with AI\nReflection: How did using AI change your approach to this comprehensive analysis compared to doing it by hand?"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-homework.html#submission-guidelines",
    "href": "Assignments/Module 1 - EDA/module-1-homework.html#submission-guidelines",
    "title": "Module 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nSubmit a ZIP file containing:\n\nCode files: All .py files or Jupyter notebooks\nVisualizations folder: All plots and charts generated\nWritten responses: A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\nData: Include your cleaned/modified datasets if you created any"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-homework.html#grading-rubric",
    "href": "Assignments/Module 1 - EDA/module-1-homework.html#grading-rubric",
    "title": "Module 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nPart A: By Hand (40 points)\n\nCode correctness and style: 20 points\nVisualizations: 10 points\nWritten interpretations: 10 points\n\nPart B: AI-Assisted (60 points)\n\nCode functionality: 25 points\nQuality of AI prompts: 15 points\nVisualizations and outputs: 10 points\nWritten interpretations and insights: 10 points\n\nTotal: 100 points"
  },
  {
    "objectID": "Assignments/Module 1 - EDA/module-1-homework.html#tips-for-success",
    "href": "Assignments/Module 1 - EDA/module-1-homework.html#tips-for-success",
    "title": "Module 1 Homework: AI-Assisted Coding and Exploratory Data Analysis",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nFor Part A:\n\nTest your code on small subsets first\nRemember to import necessary libraries\nUse meaningful variable names\nComment your code to explain your reasoning\n\n\n\nFor Part B:\n\nWrite specific, detailed prompts\nIf the AI’s first attempt isn’t quite right, refine your prompt\nAlways review and test the AI-generated code\nDon’t just accept AI output—understand what it’s doing\nInclude context in your prompts (e.g., “I’m working with a housing dataset with columns…”)\n\n\n\nGeneral:\n\nStart early—EDA often reveals unexpected issues that take time to address\nYour interpretations are just as important as your code\nIf you find something interesting in the data, explore it further!\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html",
    "title": "Module 4 Quiz: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#instructions",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#instructions",
    "title": "Module 4 Quiz: LLMs for Feature Engineering and Data Extraction",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#section-a-conceptual-questions",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#section-a-conceptual-questions",
    "title": "Module 4 Quiz: LLMs for Feature Engineering and Data Extraction",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou have three text extraction tasks:\nTask A: Extract email addresses from customer support tickets (format: name@company.com)\nTask B: Extract sentiment (positive/negative/neutral) from product reviews with complex, nuanced language\nTask C: Extract dates from legal documents (format: MM/DD/YYYY or Month DD, YYYY)\na) For which task(s) would using an LLM API be most appropriate? For which would regex or traditional methods be better?\nb) Explain your reasoning for each choice.\n\n\n\nQuestion 2 (3 points)\nCompare these two prompts for extracting product categories from reviews:\nPrompt A: “What category is this product?”\nPrompt B: “Read this product review and extract the product category. Return your answer as JSON with a single field ‘category’. Valid categories are: Electronics, Clothing, Home & Garden, Books, Toys. Review: {review_text}”\na) Which prompt will produce more reliable, consistent extractions?\nb) Explain two specific improvements Prompt B makes over Prompt A.\n\n\n\nQuestion 3 (4 points)\nYou need to extract sentiment from 1,000 customer reviews.\nGiven:\n\nGPT-4: $0.03 per 1,000 input tokens, $0.06 per 1,000 output tokens\nGPT-3.5-turbo: $0.001 per 1,000 input tokens, $0.002 per 1,000 output tokens\nAverage review: 150 tokens\nAverage response: 20 tokens\n\na) Calculate the total cost to process all 1,000 reviews using GPT-4.\nb) Calculate the total cost using GPT-3.5-turbo.\nc) If GPT-4 achieves 95% accuracy and GPT-3.5-turbo achieves 88% accuracy, which would you choose and why?\n\n\n\nQuestion 4 (3 points)\nYou receive these three JSON responses from an LLM extraction task:\nResponse 1: {\"sentiment\": \"positive\", \"rating\": 4}\nResponse 2: {\"sentiment\": \"positive\" \"rating\": 4}\nResponse 3: {\"sentiment\": \"positive\", \"rating\": \"four\"}\na) Which response(s) are valid JSON? Which are invalid?\nb) For the problematic responses, explain what’s wrong and how you would handle them in your code.\n\n\n\nQuestion 5 (3 points)\nYou’re using an LLM API to extract information from text. Explain the effect of each parameter:\na) What happens when you set temperature=0 vs temperature=1.0? When would you use each setting for extraction tasks?\nb) What does max_tokens control? Why might you set it to a low value like 50 for extraction tasks?\n\n\n\nQuestion 6 (3 points)\nYou extract sentiment from 100 product reviews and manually check the results:\n\n92 extractions match human judgment\n5 extractions are close but slightly off\n3 extractions are completely wrong\n\na) Is this extraction quality acceptable for use in a machine learning pipeline? Explain your reasoning.\nb) Describe two approaches you could take to improve the extraction quality.\n\n\n\nQuestion 7 (3 points)\nYou’re building a spam classifier. You have a dataset with email text and labels (spam/not spam). You want to use an LLM to extract additional features like “urgency level” and “contains financial terms.”\na) At what point in your workflow should you extract these LLM features to avoid data leakage?\nb) After extracting the features, how would you add them to your existing DataFrame before training a model?\n\n\n\nQuestion 8 (2 points)\na) What is few-shot prompting?\nb) When would you use few-shot prompting instead of zero-shot prompting for extraction tasks?"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#section-b-code-writing",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#section-b-code-writing",
    "title": "Module 4 Quiz: LLMs for Feature Engineering and Data Extraction",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 9-15, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nimport json\nfrom openai import OpenAI\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nAlso assume you have an OpenAI client initialized as client = OpenAI().\n\n\nQuestion 9 (3 points)\nWrite a prompt (as a Python string) to extract sentiment from a product review. The prompt should:\n\nAsk the LLM to analyze the review text\nRequest the response in JSON format with a field called “sentiment”\nSpecify that sentiment should be one of: “positive”, “negative”, or “neutral”\n\nStore the prompt in a variable called prompt.\n\n\n\nQuestion 10 (3 points)\nYou receive this JSON string response from an LLM:\nresponse_text = '{\"sentiment\": \"positive\", \"confidence\": 0.95}'\nWrite code to:\n\nParse the JSON string into a Python dictionary\nExtract the “sentiment” value\nStore it in a variable called sentiment\n\n\n\n\nQuestion 11 (3 points)\nYou have a list of extraction results from processing multiple reviews:\nresults = [\n    {\"sentiment\": \"positive\", \"rating\": 5},\n    {\"sentiment\": \"negative\", \"rating\": 2},\n    {\"sentiment\": \"neutral\", \"rating\": 3}\n]\nWrite code to convert this list into a pandas DataFrame called df with columns “sentiment” and “rating”.\n\n\n\nQuestion 12 (4 points)\nYou have two DataFrames:\nreviews_df  # columns: review_id, review_text\nextracted_df  # columns: review_id, sentiment, urgency\nWrite code to merge these DataFrames so that the sentiment and urgency features are added to the original reviews DataFrame. Use review_id as the key for merging. Store the result in combined_df.\n\n\n\nQuestion 13 (4 points)\nWrite code to make an API call to extract sentiment from a review. Your code should:\n\nCreate a message with the system role: “You extract sentiment from reviews”\nUse the user role with: “Extract sentiment from this review: {review_text}. Return JSON.”\nCall client.chat.completions.create() with model=“gpt-3.5-turbo”\nSet temperature=0 and max_tokens=50\n\nStore the response in a variable called response.\n(Note: You can use simplified syntax; exact parameter names don’t need to be perfect)\n\n\n\nQuestion 14 (4 points)\nWrite a function called calculate_extraction_cost that:\n\nTakes three parameters: num_texts, tokens_per_text, and price_per_1k_tokens\nCalculates the total tokens as num_texts * tokens_per_text\nCalculates the cost as (total_tokens / 1000) * price_per_1k_tokens\nReturns the total cost\n\n\n\n\nQuestion 15 (4 points)\nYou have a DataFrame df with columns: review_text, llm_sentiment, and label (the true sentiment).\nWrite code to:\n\nSplit the data into train and test sets (80/20 split, random_state=42)\nUse only the llm_sentiment feature (needs to be 2D: use double brackets)\nTrain a Logistic Regression model on the training data\nCalculate and print the accuracy on the test set"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#grading-rubric",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz.html#grading-rubric",
    "title": "Module 4 Quiz: LLMs for Feature Engineering and Data Extraction",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (24 points)\n\nUnderstanding when to use LLMs vs alternatives: 6 points (Q1, Q2)\nCost calculation and API knowledge: 7 points (Q3, Q5)\nQuality assessment and validation: 6 points (Q4, Q6)\nPrompt engineering and integration: 5 points (Q7, Q8)\n\nSection B: Code Writing (24 points)\n\nPrompt writing and JSON handling: 9 points (Q9, Q10, Q11)\nDataFrame operations and merging: 4 points (Q12)\nAPI usage: 4 points (Q13)\nCost calculation function: 4 points (Q14)\nML integration: 3 points (Q15)\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 48 points"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html",
    "title": "Quiz 4b: JSON Parsing, Quality Control, and ML Integration",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#instructions",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#instructions",
    "title": "Quiz 4b: JSON Parsing, Quality Control, and ML Integration",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#section-a-conceptual-questions",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#section-a-conceptual-questions",
    "title": "Quiz 4b: JSON Parsing, Quality Control, and ML Integration",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nYou receive these three JSON responses from an LLM extraction task:\nResponse 1: {\"sentiment\": \"positive\", \"rating\": 4}\nResponse 2: {\"sentiment\": \"positive\" \"rating\": 4}\nResponse 3: {\"sentiment\": \"positive\", \"rating\": \"four\"}\na) Which response(s) are valid JSON? Which are invalid?\nb) For the problematic responses, explain what’s wrong and how you would handle them in your code.\n\n\n\nQuestion 2 (3 points)\nYou extract sentiment from 100 product reviews and manually check the results:\n\n92 extractions match human judgment\n5 extractions are close but slightly off\n3 extractions are completely wrong\n\na) Is this extraction quality acceptable for use in a machine learning pipeline? Explain your reasoning.\nb) Describe two approaches you could take to improve the extraction quality.\n\n\n\nQuestion 3 (3 points)\nYou’re building a spam classifier. You have a dataset with email text and labels (spam/not spam). You want to use an LLM to extract additional features like “urgency level” and “contains financial terms.”\na) At what point in your workflow should you extract these LLM features to avoid data leakage?\nb) After extracting the features, how would you add them to your existing DataFrame before training a model?\n\n\n\nQuestion 4 (2 points)\na) What is few-shot prompting?\nb) When would you use few-shot prompting instead of zero-shot prompting for extraction tasks?\n\n\n\nQuestion 5 (3 points)\nWhen integrating LLM-extracted features into a machine learning pipeline:\na) Why is it important to extract features from the test set AFTER splitting the data?\nb) What problem could arise if you extract features before splitting?\nc) How do you ensure the LLM extraction process is the same for training and test data?"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#section-b-code-writing",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#section-b-code-writing",
    "title": "Quiz 4b: JSON Parsing, Quality Control, and ML Integration",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 6-11, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\nQuestion 6 (3 points)\nYou receive this JSON string response from an LLM:\nresponse_text = '{\"sentiment\": \"positive\", \"confidence\": 0.95}'\nWrite code to:\n\nParse the JSON string into a Python dictionary\nExtract the “sentiment” value\nStore it in a variable called sentiment\n\n\n\n\nQuestion 7 (3 points)\nYou have a list of extraction results from processing multiple reviews:\nresults = [\n    {\"sentiment\": \"positive\", \"rating\": 5},\n    {\"sentiment\": \"negative\", \"rating\": 2},\n    {\"sentiment\": \"neutral\", \"rating\": 3}\n]\nWrite code to convert this list into a pandas DataFrame called df with columns “sentiment” and “rating”.\n\n\n\nQuestion 8 (4 points)\nYou have two DataFrames:\nreviews_df  # columns: review_id, review_text\nextracted_df  # columns: review_id, sentiment, urgency\nWrite code to merge these DataFrames so that the sentiment and urgency features are added to the original reviews DataFrame. Use review_id as the key for merging. Store the result in combined_df.\n\n\n\nQuestion 9 (4 points)\nWrite code to handle a JSON parsing error gracefully:\n\nTry to parse a JSON string stored in variable response_text\nIf successful, extract the “sentiment” field\nIf parsing fails, set sentiment to “unknown”\nStore the final sentiment value in variable sentiment\n\nUse a try-except block.\n\n\n\nQuestion 10 (3 points)\nYou have a DataFrame df with a column llm_response containing JSON strings like '{\"category\": \"Electronics\"}'.\nWrite code to:\n\nParse each JSON string in the column\nExtract the “category” value from each\nCreate a new column called category with the extracted values\n\nYou can use a loop or .apply().\n\n\n\nQuestion 11 (4 points)\nYou have a DataFrame df with columns: review_text, llm_sentiment, and label (the true sentiment).\nWrite code to:\n\nSplit the data into train and test sets (80/20 split, random_state=42)\nUse only the llm_sentiment feature (needs to be 2D: use double brackets)\nTrain a Logistic Regression model on the training data\nCalculate and print the accuracy on the test set"
  },
  {
    "objectID": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#grading-rubric",
    "href": "Assignments/Module 4 - LLMs Feature Engineering/module-4-quiz-4b.html#grading-rubric",
    "title": "Quiz 4b: JSON Parsing, Quality Control, and ML Integration",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (14 points)\n\nJSON validation and error handling: 3 points\nQuality assessment strategies: 3 points\nData leakage prevention: 3 points\nFew-shot prompting: 2 points\nML pipeline integration best practices: 3 points\n\nSection B: Code Writing (21 points)\n\nJSON parsing: 3 points\nCreating DataFrames from extracted data: 3 points\nMerging DataFrames: 4 points\nError handling with try-except: 4 points\nBatch JSON processing: 3 points\nTraining ML model with LLM features: 4 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 35 points"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-homework.html",
    "href": "Assignments/Module 2 - Regression/module-2-homework.html",
    "title": "Module 4 Homework: Regression Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational understanding of regression models, diagnostics, and regularization. Part B should be completed with AI assistance (like Gemini CLI) to practice comprehensive model selection and diagnostic analysis at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with three datasets:\nDataset 1: real_estate.csv - Real estate pricing data with columns:\n\nprice: Sale price in dollars (TARGET)\nsqft: Square footage\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nage: Age of property in years\ndistance_downtown: Distance to downtown in miles\ncrime_rate: Neighborhood crime rate\nschool_rating: Local school rating (1-10)\n\nDataset 2: advertising.csv - Advertising budget and sales data with columns:\n\nsales: Product sales in thousands (TARGET)\ntv: TV advertising budget in thousands\nradio: Radio advertising budget in thousands\nnewspaper: Newspaper advertising budget in thousands\n\nDataset 3: employee_performance.csv - Employee metrics with multicollinearity with columns:\n\nperformance_score: Performance rating (TARGET)\nyears_experience: Years of experience\ntraining_hours: Hours of training completed\nprojects_completed: Number of projects completed\nmonths_employed: Months with company (highly correlated with years_experience)\ncertifications: Number of certifications"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-homework.html#instructions",
    "href": "Assignments/Module 2 - Regression/module-2-homework.html#instructions",
    "title": "Module 4 Homework: Regression Models",
    "section": "",
    "text": "Due Date: [To be assigned by instructor]\nThis homework is divided into two parts. Part A should be completed without AI assistance to build your foundational understanding of regression models, diagnostics, and regularization. Part B should be completed with AI assistance (like Gemini CLI) to practice comprehensive model selection and diagnostic analysis at scale.\nFor all questions, submit:\n\nYour code (in a .py file or Jupyter notebook)\nAll visualizations generated\nWritten answers to interpretation questions (can be in markdown or comments)\nFor Part B: Include the prompts you used with the AI assistant\n\n\n\nYou’ll be working with three datasets:\nDataset 1: real_estate.csv - Real estate pricing data with columns:\n\nprice: Sale price in dollars (TARGET)\nsqft: Square footage\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nage: Age of property in years\ndistance_downtown: Distance to downtown in miles\ncrime_rate: Neighborhood crime rate\nschool_rating: Local school rating (1-10)\n\nDataset 2: advertising.csv - Advertising budget and sales data with columns:\n\nsales: Product sales in thousands (TARGET)\ntv: TV advertising budget in thousands\nradio: Radio advertising budget in thousands\nnewspaper: Newspaper advertising budget in thousands\n\nDataset 3: employee_performance.csv - Employee metrics with multicollinearity with columns:\n\nperformance_score: Performance rating (TARGET)\nyears_experience: Years of experience\ntraining_hours: Hours of training completed\nprojects_completed: Number of projects completed\nmonths_employed: Months with company (highly correlated with years_experience)\ncertifications: Number of certifications"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-homework.html#part-a-by-hand-no-ai-assistance",
    "href": "Assignments/Module 2 - Regression/module-2-homework.html#part-a-by-hand-no-ai-assistance",
    "title": "Module 4 Homework: Regression Models",
    "section": "Part A: By Hand (No AI Assistance)",
    "text": "Part A: By Hand (No AI Assistance)\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational understanding of regression diagnostics and regularization.\n\nQuestion 1 (4 points)\nLoad the advertising.csv dataset and fit a Linear Regression model to predict sales using all three advertising channels.\n\nExtract and display the coefficients for each feature\nInterpret each coefficient in plain language (e.g., “For every additional thousand dollars spent on TV advertising, sales increase by…”)\nWhich advertising channel has the strongest impact on sales?\nCalculate R² for this model. What does this value tell you about the model’s performance?\n\n\n\n\nQuestion 2 (5 points)\nContinuing with the advertising dataset, manually calculate the following for your linear regression model:\n\nCalculate the residuals (actual - predicted) for the first 5 data points\nCompute the Mean Squared Error (MSE) manually from all residuals\nCompute the Mean Absolute Error (MAE) manually from all residuals\nVerify your calculations match sklearn’s mean_squared_error and mean_absolute_error\nWhich metric (MSE or MAE) is more sensitive to outliers? Why?\n\n\n\n\nQuestion 3 (5 points)\nCreate diagnostic plots for the linear regression model from Question 1:\n\nCreate a scatter plot of residuals vs. fitted values (predictions)\nAdd a horizontal line at y=0\nWhat pattern would you expect to see if the model assumptions are satisfied?\nDo you observe any concerning patterns in your plot? What might they indicate?\nCreate a histogram of the residuals. Do they appear approximately normally distributed?\n\n\n\n\nQuestion 4 (5 points)\nLoad the real_estate.csv dataset. Calculate the correlation matrix for all numerical features.\n\nDisplay the correlation matrix\nWhich pair of features has the highest correlation (excluding correlations with the target)?\nWhat is multicollinearity and why is it problematic for linear regression?\nBased on the correlation matrix, do you see evidence of multicollinearity in this dataset?\n\n\n\n\nQuestion 5 (5 points)\nUsing the real_estate.csv dataset, fit three models predicting price:\n\nModel 1: Linear Regression (no regularization)\nModel 2: Ridge Regression with alpha=1.0\nModel 3: Lasso Regression with alpha=1.0\n\nFor each model:\n\nFit the model using all features\nDisplay the coefficients\nCalculate R² on the test set (use 80/20 train/test split, random_state=42)\nHow do the coefficients differ between the three models? Which model has the smallest coefficient magnitudes?\n\n\n\n\nQuestion 6 (4 points)\nExplain in your own words (3-4 sentences each):\n\nWhat is the purpose of regularization in regression? When would you choose to use it?\nWhat is the difference between Ridge (L2) and Lasso (L1) regularization?\nWhy might Lasso set some coefficients to exactly zero while Ridge does not?\nIn what situation would you prefer Lasso over Ridge?\n\n\n\n\nQuestion 7 (5 points)\nUsing the advertising.csv dataset, create polynomial features of degree 2 for the tv feature only.\n\nUse PolynomialFeatures(degree=2, include_bias=False) to create polynomial features\nFit a Linear Regression model using the original features plus the polynomial features of TV\nCompare R² with the original linear model (without polynomial features)\nDid adding polynomial features improve performance? Why might this be the case?\nPlot sales vs. TV budget, showing both the linear and polynomial model predictions\n\n\n\n\nQuestion 8 (4 points)\nYou fit a linear regression model and observe the following residual plot:\n\nFor low fitted values: residuals are mostly positive\nFor medium fitted values: residuals are mostly negative\nFor high fitted values: residuals are mostly positive\n\n\nWhat assumption of linear regression might be violated?\nWhat does this pattern suggest about the relationship between features and target?\nSuggest two approaches to address this issue.\n\n\n\n\nQuestion 9 (4 points)\nUsing the employee_performance.csv dataset, fit a Ridge regression model with alpha=10.0.\n\nSplit data into train/test (80/20, random_state=42)\nFit the model and calculate train R² and test R²\nNow fit Ridge models with alpha values: [0.1, 1.0, 10.0, 100.0]\nFor each alpha, record the test R². Which alpha gives the best test performance?\nWhat happens to the coefficients as alpha increases?\n\n\n\n\nQuestion 10 (4 points)\nWrite a function called plot_residual_diagnostics that:\n\nTakes a fitted model, X_test, and y_test as inputs\nCalculates residuals\nCreates two plots side by side: (1) residuals vs fitted values, (2) histogram of residuals\nReturns the residuals array\n\nTest your function on a linear regression model fitted to the advertising dataset.\nBased on the plots, do the residuals appear randomly distributed? Are they centered around zero?"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-homework.html#part-b-with-ai-assistance",
    "href": "Assignments/Module 2 - Regression/module-2-homework.html#part-b-with-ai-assistance",
    "title": "Module 4 Homework: Regression Models",
    "section": "Part B: With AI Assistance",
    "text": "Part B: With AI Assistance\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you perform comprehensive regression analysis. The goal is to practice systematic model selection and diagnostic analysis.\nImportant: For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n\nQuestion 11 (5 points)\nUse AI to perform a comprehensive grid search for Ridge regression on the real_estate.csv dataset.\nTest alpha values: [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nYour code should:\n\nPerform grid search with cross-validation\nPlot test R² vs. alpha (use log scale for x-axis)\nIdentify the optimal alpha value\nShow the coefficients for the best model\n\nWhat happens to model performance as alpha increases? Is there a point where it gets worse?\nDeliverables:\n\nCode\nPerformance vs. alpha plot\nBest alpha and corresponding R²\nWritten interpretation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 12 (6 points)\nPrompt AI to create a comprehensive comparison of regularization techniques on the employee_performance.csv dataset (which has multicollinearity).\nCompare:\n\nLinear Regression (no regularization)\nRidge with optimal alpha\nLasso with optimal alpha\nElastic Net with optimal alpha\n\nYour code should:\n\nUse GridSearchCV to find optimal alpha for each regularized method\nCreate a bar plot comparing test R² across all methods\nCreate a heatmap showing coefficient values for all methods\nIdentify which features Lasso set to zero\n\nWrite a paragraph discussing how regularization helped with the multicollinearity problem.\nDeliverables:\n\nCode\nPerformance comparison plot\nCoefficient heatmap\nWritten analysis (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 13 (6 points)\nUse AI to test polynomial regression of different degrees on the advertising.csv dataset using the tv feature.\nTest polynomial degrees: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nYour code should:\n\nFit polynomial models for each degree\nCalculate train and test MSE for each\nPlot both MSE curves vs. polynomial degree\nCreate a visualization showing the fitted curves for degrees [1, 3, 5, 8] overlaid on the actual data\n\nAt what degree does overfitting begin to occur? How can you tell?\nDeliverables:\n\nCode\nMSE vs. degree plot\nFitted curves visualization\nWritten interpretation (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 14 (6 points)\nPrompt AI to generate a complete regression diagnostics report for a linear regression model on real_estate.csv.\nThe report should include:\n\nResiduals vs. Fitted values plot\nResiduals vs. each feature plot (to detect non-linearity with specific features)\nHistogram of residuals\nScale-location plot (sqrt of standardized residuals vs. fitted)\nSummary statistics of residuals (mean, median, std, min, max)\n\nBased on these diagnostics, are the linear regression assumptions satisfied? Which assumptions (if any) are violated?\nDeliverables:\n\nCode for complete diagnostics\nAll diagnostic plots\nWritten assessment of assumptions (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 15 (5 points)\nUse AI to create a “coefficient path” visualization for Lasso regression.\nUsing the real_estate.csv dataset:\n\nFit Lasso models with alpha values ranging from 0.001 to 1000 (log scale, 50 points)\nTrack the coefficient values for each feature at each alpha\nCreate a line plot showing how each coefficient changes with alpha\nMark the point where each coefficient becomes zero\n\nWhich features are most important (last to be zeroed out)? Which are least important?\nDeliverables:\n\nCode\nCoefficient path visualization\nWritten interpretation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 16 (6 points)\nPrompt AI to compare polynomial regression with and without regularization.\nUsing the advertising.csv dataset with TV feature:\n\nFit polynomial models of degree 10 with:\n\nNo regularization (Linear Regression)\nRidge regularization (optimal alpha)\nLasso regularization (optimal alpha)\n\nCompare test MSE for all three\nVisualize the fitted curves for all three\nShow the coefficient values for all three\n\nDoes regularization help control overfitting in high-degree polynomials? How?\nDeliverables:\n\nCode\nPerformance comparison\nFitted curves visualization\nCoefficient comparison\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 17 (5 points)\nUse AI to investigate the impact of feature scaling on Ridge and Lasso regression.\nUsing the real_estate.csv dataset:\n\nFit Ridge and Lasso models (alpha=1.0) with:\n\nNo scaling (raw features)\nStandardScaler (z-score normalization)\nMinMaxScaler (0-1 scaling)\n\nCompare test R² for all combinations\nShow how coefficients differ with different scaling approaches\n\nWhy does scaling matter for regularized regression but not for standard linear regression?\nDeliverables:\n\nCode\nPerformance comparison table\nCoefficient comparison\nWritten explanation (3-4 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 18 (6 points)\nPrompt AI to create an automated feature selection pipeline using Lasso.\nThe pipeline should:\n\nTake a dataset and alpha value as input\nFit Lasso regression\nIdentify features with non-zero coefficients\nRefit a Linear Regression model using only selected features\nCompare performance with using all features\nGenerate a report showing selected features and performance comparison\n\nTest this on the real_estate.csv dataset with alpha=0.1.\nHow many features were selected? Did feature selection improve or hurt performance?\nDeliverables:\n\nPipeline code\nFeature selection results\nPerformance comparison\nWritten analysis (4-5 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 19 (6 points)\nUse AI to create a comprehensive model selection workflow that combines polynomial features with regularization.\nUsing the advertising.csv dataset:\n\nTest polynomial degrees [1, 2, 3, 4, 5]\nFor each degree, test Ridge, Lasso, and Linear Regression\nUse cross-validation to evaluate each combination\nCreate a heatmap showing performance (columns=method, rows=polynomial degree)\nIdentify the best combination\n\nYour code should:\n\nAutomatically test all combinations\nGenerate comprehensive comparison visualizations\nSelect the best model based on cross-validation score\nEvaluate the best model on a held-out test set\n\nDeliverables:\n\nCode for complete workflow\nPerformance heatmap\nBest model identification\nFinal test set results\nWritten summary (5-6 sentences)\nYour AI prompt(s)\n\n\n\n\nQuestion 20 (7 points)\nFor this final question, ask AI to help you conduct a complete regression analysis from start to finish on the real_estate.csv dataset.\nYour analysis should include:\n\nExploratory Data Analysis:\n\nSummary statistics\nCorrelation analysis\nMulticollinearity detection (VIF scores)\n\nModel Development:\n\nTrain/validation/test split (60/20/20)\nFit multiple models: Linear, Ridge, Lasso, Polynomial (with regularization)\nHyperparameter tuning using validation set\n\nModel Diagnostics:\n\nComplete diagnostic plots for best model\nResidual analysis\nAssumption checking\n\nModel Selection:\n\nCompare all models on validation set\nSelect best model\nEvaluate on test set (only once!)\n\nInterpretation:\n\nFeature importance\nCoefficient interpretation\nBusiness insights\n\n\nWrite a 1-page executive summary (300-400 words) that explains:\n\nData characteristics and challenges (multicollinearity, etc.)\nModels tested and why\nHow the best model was selected\nPerformance on test set and what it means\nKey features driving predictions\nRecommendations and limitations\n\nDeliverables:\n\nComplete analysis code (well-commented)\nAll visualizations (EDA, diagnostics, comparisons)\nExecutive summary\nYour AI prompt(s)\nReflection: How did understanding diagnostics and regularization improve your modeling approach compared to just trying different models blindly?"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-homework.html#submission-guidelines",
    "href": "Assignments/Module 2 - Regression/module-2-homework.html#submission-guidelines",
    "title": "Module 4 Homework: Regression Models",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nSubmit a ZIP file containing:\n\nCode files: All .py files or Jupyter notebooks\nVisualizations folder: All plots and charts generated\nWritten responses: A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\nData: Include the datasets if you made any modifications"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-homework.html#grading-rubric",
    "href": "Assignments/Module 2 - Regression/module-2-homework.html#grading-rubric",
    "title": "Module 4 Homework: Regression Models",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nPart A: By Hand (45 points)\n\nCode correctness and implementation: 25 points\nProper understanding of diagnostics and regularization: 12 points\nWritten interpretations and explanations: 8 points\n\nPart B: AI-Assisted (55 points)\n\nCode functionality and correctness: 25 points\nQuality and specificity of AI prompts: 14 points\nVisualizations and diagnostic quality: 10 points\nWritten interpretations and insights: 6 points\n\nTotal: 100 points"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-homework.html#tips-for-success",
    "href": "Assignments/Module 2 - Regression/module-2-homework.html#tips-for-success",
    "title": "Module 4 Homework: Regression Models",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nFor Part A:\n\nDraw residual plots carefully and look for patterns\nWhen computing metrics by hand, work with small subsets first to verify\nRegularization coefficients shrink toward zero—compare magnitudes across models\nUnderstanding assumptions is crucial—violating them can make predictions unreliable\nPolynomial features can capture non-linear relationships but risk overfitting\n\n\n\nFor Part B:\n\nRequest diagnostic plots that check all assumptions systematically\nAsk for log-scale plots when testing many alpha values\nRequest coefficient path plots to understand feature selection\nAsk for side-by-side comparisons to see regularization effects clearly\nWhen testing polynomials, visualize the fitted curves to see overfitting\n\n\n\nGeneral:\n\nStart early—comprehensive diagnostics take time to interpret\nFocus on understanding why regularization helps, not just applying it\nUse residual plots as your primary diagnostic tool\nConnect regularization back to bias-variance tradeoff from Module 3\nUse your brain. That’s what it’s there for."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2b.html",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2b.html",
    "title": "Quiz 2b: Regression Metrics and Residual Analysis",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#instructions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#instructions",
    "title": "Quiz 2b: Regression Metrics and Residual Analysis",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#section-a-conceptual-questions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#section-a-conceptual-questions",
    "title": "Quiz 2b: Regression Metrics and Residual Analysis",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (3 points)\nExplain in 2-3 sentences:\na) What does a histogram of residuals tell you about your model, and what should you look for?\nb) Why is homoscedasticity (constant variance of residuals) important for linear regression?\n\n\n\nQuestion 2 (2 points)\nWhat is the difference between R² and adjusted R²? When would adjusted R² be more useful?\n\n\n\nQuestion 3 (3 points)\nYou fit three regression models to predict house prices:\nModel A: MSE = 10,000, MAE = 75 Model B: MSE = 8,000, MAE = 85 Model C: MSE = 12,000, MAE = 65\na) Which model has the largest errors on average?\nb) Which model is most affected by outliers? How can you tell?\nc) Which model would you choose if you want to avoid being heavily influenced by extreme values?\n\n\n\nQuestion 4 (2 points)\nYou examine a residuals vs. fitted values plot and notice that the residuals form a funnel shape - the spread of residuals increases as the fitted values increase.\na) What does this pattern indicate?\nb) Is this a problem for linear regression? Why?"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#section-b-code-writing",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#section-b-code-writing",
    "title": "Quiz 2b: Regression Metrics and Residual Analysis",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 5-9, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\nAlso assume you have X_train, X_test, y_train, y_test already defined and a fitted model called model.\n\n\nQuestion 5 (3 points)\nWrite code to:\n\nMake predictions on X_test using the fitted model\nCalculate MSE, MAE, and R² on the test set\nPrint all three metrics with labels\n\n\n\n\nQuestion 6 (3 points)\nWrite code to create a histogram of residuals:\n\nCalculate residuals as (y_test - predictions)\nCreate a histogram with 30 bins\nAdd a title “Distribution of Residuals”\nAdd xlabel “Residuals”\n\n\n\n\nQuestion 7 (4 points)\nWrite a function called evaluate_model that:\n\nTakes four parameters: model, X_test, y_test, and model_name\nMakes predictions using the model\nCalculates MSE, MAE, and R²\nPrints all three metrics with the model name\nReturns a dictionary with keys ‘MSE’, ‘MAE’, ‘R2’ and their values\n\n\n\n\nQuestion 8 (4 points)\nWrite code to compare the performance of a model on training vs. test data:\n\nCalculate R² on the training set\nCalculate R² on the test set\nPrint both values\nPrint whether the model appears to be overfitting (if train R² &gt;&gt; test R²)\n\nAssume you have access to the fitted model and all train/test data.\n\n\n\nQuestion 9 (3 points)\nWrite code to create a scatter plot comparing actual vs. predicted values:\n\nPlot y_test (x-axis) vs. predictions (y-axis)\nAdd a diagonal line representing perfect predictions\nAdd labels “Actual Values” and “Predicted Values”\nAdd title “Actual vs. Predicted”"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#grading-rubric",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2b.html#grading-rubric",
    "title": "Quiz 2b: Regression Metrics and Residual Analysis",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (10 points)\n\nUnderstanding residual diagnostics: 6 points\nUnderstanding evaluation metrics: 4 points\n\nSection B: Code Writing (17 points)\n\nMetric calculation: 6 points\nVisualization (histogram and scatter): 6 points\nFunction writing: 5 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 27 points"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2d.html",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2d.html",
    "title": "Quiz 2d: Regularization (Ridge, Lasso, Elastic Net)",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#instructions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#instructions",
    "title": "Quiz 2d: Regularization (Ridge, Lasso, Elastic Net)",
    "section": "",
    "text": "Time: 30-45 minutes Format: In-class, by-hand (no computer)\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn’t perfect."
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#section-a-conceptual-questions",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#section-a-conceptual-questions",
    "title": "Quiz 2d: Regularization (Ridge, Lasso, Elastic Net)",
    "section": "Section A: Conceptual Questions",
    "text": "Section A: Conceptual Questions\n\nQuestion 1 (4 points)\nConsider these two linear regression models predicting house prices:\nModel A: Uses features: sqft, bedrooms, bathrooms\n\nCoefficients: sqft=150, bedrooms=10000, bathrooms=5000\nR² = 0.75\n\nModel B: Same features after Ridge regularization (alpha=10)\n\nCoefficients: sqft=120, bedrooms=8000, bathrooms=4000\nR² = 0.73\n\na) Why are the coefficients smaller in Model B?\nb) Which model is likely to generalize better to new data? Why?\nc) What is the tradeoff we’re making by using regularization?\n\n\n\nQuestion 2 (4 points)\nCompare Ridge and Lasso regularization:\na) What is the key difference in how they shrink coefficients?\nb) Which one can perform automatic feature selection? Why?\nc) If you have 100 features and suspect only 10 are truly important, which regularization method would you prefer?\n\n\n\nQuestion 3 (3 points)\nYou fit a Lasso model with different alpha values:\n\nAlpha = 0.1: 8 features have non-zero coefficients\nAlpha = 1.0: 5 features have non-zero coefficients\nAlpha = 10.0: 2 features have non-zero coefficients\n\na) As alpha increases, what happens to the number of selected features?\nb) What happens to the magnitude of the remaining non-zero coefficients as alpha increases?\nc) How would you choose the optimal alpha value?"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#section-b-code-writing",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#section-b-code-writing",
    "title": "Quiz 2d: Regularization (Ridge, Lasso, Elastic Net)",
    "section": "Section B: Code Writing",
    "text": "Section B: Code Writing\nFor questions 4-9, assume you have already imported:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nAlso assume you have X_train, X_test, y_train, y_test already defined.\n\n\nQuestion 4 (4 points)\nWrite code to fit a Ridge regression model with alpha=10.0:\n\nCreate the Ridge model with alpha=10.0\nFit it on X_train and y_train\nExtract the coefficients\nCalculate the R² score on X_test\n\n\n\n\nQuestion 5 (3 points)\nWrite code to fit Lasso regression with alpha=1.0 and identify which features were selected:\n\nFit Lasso with alpha=1.0\nGet the coefficients\nPrint which features have non-zero coefficients\n\nAssume features are named in a list called feature_names.\n\n\n\nQuestion 6 (4 points)\nWrite code to compare Ridge models with different alpha values:\n\nCreate a list of alpha values: [0.1, 1.0, 10.0, 100.0]\nFor each alpha, fit a Ridge model\nCalculate and store the test R² for each\nPrint the alpha and corresponding R²\n\n\n\n\nQuestion 7 (4 points)\nWrite code to properly scale features before applying Ridge regression:\n\nCreate a StandardScaler\nFit the scaler on X_train and transform both X_train and X_test\nFit a Ridge model (alpha=5.0) on the scaled training data\nCalculate R² on the scaled test data\n\n\n\n\nQuestion 8 (4 points)\nWrite a function called compare_regularization that:\n\nTakes X_train, X_test, y_train, y_test, and alpha as parameters\nFits both Ridge and Lasso with the given alpha\nCalculates test R² for both models\nReturns a dictionary with keys ‘Ridge’ and ‘Lasso’ and their R² scores\n\n\n\n\nQuestion 9 (3 points)\nWrite code to fit an Elastic Net model with alpha=1.0 and l1_ratio=0.5:\n\nCreate and fit the Elastic Net model\nMake predictions on X_test\nCalculate and print the MSE"
  },
  {
    "objectID": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#grading-rubric",
    "href": "Assignments/Module 2 - Regression/module-2-quiz-2d.html#grading-rubric",
    "title": "Quiz 2d: Regularization (Ridge, Lasso, Elastic Net)",
    "section": "Grading Rubric",
    "text": "Grading Rubric\nSection A: Conceptual Questions (11 points)\n\nUnderstanding Ridge regularization: 4 points\nUnderstanding Lasso and feature selection: 4 points\nUnderstanding alpha hyperparameter: 3 points\n\nSection B: Code Writing (22 points)\n\nFitting regularized models: 11 points\nFeature scaling: 4 points\nModel comparison: 4 points\nFunction writing: 3 points\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: 33 points"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html",
    "href": "Claude-planning/module-3-classification.html",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Duration: 3-4 weeks Focus: Understanding classification approaches, selecting appropriate models, interpreting classification metrics, and handling real-world classification challenges like class imbalance\n\n\n\n\nLogistic Regression\n\nHow logistic regression differs from linear regression\nThe sigmoid function and probability outputs\nDecision boundaries and thresholds\nInterpreting coefficients in classification\nAssumptions and when to use logistic regression\n\nDecision Trees\n\nHow trees make decisions (splitting criteria)\nGini impurity vs entropy\nTree depth and overfitting\nPruning techniques\nInterpretability advantages\nFeature importance from trees\n\nRandom Forests and Ensemble Methods\n\nBootstrap aggregating (bagging)\nHow random forests reduce overfitting\nFeature randomness and tree diversity\nOut-of-bag error estimation\nFeature importance in random forests\nWhen ensembles are worth the complexity\n\nSupport Vector Machines (SVM)\n\nMargin concept and maximum margin classifier\nSupport vectors and why they matter\nKernel trick for non-linear boundaries\nDifferent kernel types (linear, RBF, polynomial)\nWhen SVMs are appropriate\nComputational considerations\n\nk-Nearest Neighbors (k-NN)\n\nDistance-based classification\nChoosing k (bias-variance tradeoff again)\nDistance metrics (Euclidean, Manhattan, etc.)\nCurse of dimensionality\nFeature scaling importance for k-NN\nWhen k-NN is appropriate\n\nClassification Metrics\n\nConfusion matrix interpretation\nAccuracy and when it’s misleading\nPrecision vs recall tradeoff\nF1 score as a balance\nROC curves and AUC\nChoosing metrics based on business context\nCost-sensitive classification\n\nClass Imbalance\n\nWhy imbalance is a problem\nDetecting class imbalance\nResampling techniques (SMOTE, undersampling, oversampling)\nClass weights\nChoosing appropriate metrics for imbalanced data\nWhen to worry about imbalance and when not to\n\n\n\n\n\nStudents will manually code and understand:\n\nImplementing Each Classifier\n\nFit LogisticRegression in scikit-learn\nFit DecisionTreeClassifier with different depths\nFit RandomForestClassifier\nFit SVC with different kernels\nFit KNeighborsClassifier with different k values\nMake predictions using .predict() and .predict_proba()\n\nCreating and Interpreting Confusion Matrices\n\nUse confusion_matrix() from scikit-learn\nManually calculate TP, FP, TN, FN\nInterpret what each quadrant means\nIdentify when model is making specific types of errors\n\nComputing Classification Metrics\n\nCalculate accuracy, precision, recall from confusion matrix\nUnderstand when each metric is important\nCompute F1 score\nInterpret classification_report() output\n\nVisualizing Decision Boundaries\n\nCreate 2D visualizations of decision boundaries\nUnderstand how different models create different boundaries\nSee the difference between linear and non-linear boundaries\nVisualize how k-NN creates complex boundaries\n\nROC Curves and AUC\n\nPlot ROC curves using sklearn.metrics\nUnderstand true positive rate vs false positive rate\nInterpret AUC values\nCompare models using ROC curves\n\nUnderstanding Feature Importance\n\nExtract feature_importances_ from tree-based models\nInterpret which features are most predictive\nUnderstand the difference between importance in trees vs coefficients in logistic regression\n\nDetecting Class Imbalance\n\nUse value_counts() to check class distribution\nCalculate class proportions\nRecognize when imbalance is problematic\n\n\n\n\n\nStudents will use AI coding assistants to:\n\nHyperparameter Tuning at Scale\n\nGrid search over many hyperparameters simultaneously\nTune multiple classifiers (LogisticRegression, DecisionTree, RandomForest, SVC, KNN)\nTest different tree depths, number of estimators, k values, kernels\nCross-validation for all hyperparameter combinations\n\nHandling Class Imbalance\n\nImplement SMOTE for synthetic oversampling\nTest different class weights\nCompare undersampling strategies\nEvaluate impact of different balancing techniques\n\nComprehensive Model Comparison\n\nTrain 5+ classifiers on same dataset\nGenerate comparison tables across metrics (accuracy, precision, recall, F1, AUC)\nCreate ROC curves for all models on same plot\nSystematic evaluation of which model performs best\n\nComplete Classification Pipeline\n\nBuild end-to-end pipeline: load data → preprocess → train → evaluate → visualize\nAutomated generation of confusion matrices for multiple models\nCreating comprehensive classification reports\nGenerating decision boundary plots for multiple models\n\nCross-Dataset Testing\n\nTest same models on multiple datasets\nCompare performance across different data characteristics\nIdentify which models are robust vs dataset-specific\n\n\n\n\n\n\n\nSession 1 - Foundational Session: - Introduce classification vs regression - Explain logistic regression: sigmoid function, probability outputs - Live-code: Fit LogisticRegression, make predictions - Interpret predict() vs predict_proba() - Create and interpret confusion matrix - Calculate accuracy, precision, recall by hand - Introduce decision trees conceptually - Live-code: Fit DecisionTreeClassifier with different max_depth\nSession 2 - AI-Assisted Session: - Use AI to tune logistic regression hyperparameters - Use AI to tune decision tree depth - Use AI to generate decision boundary visualizations - Practice comparing logistic regression vs decision trees - Explore when each is appropriate\n\n\n\nSession 3 - Foundational Session: - Introduce Random Forests: bagging, feature randomness - Live-code: Fit RandomForestClassifier - Extract and interpret feature importances - Introduce ROC curves and AUC - Live-code: Create ROC curve for binary classifier - Discuss precision-recall tradeoff - Practice selecting metrics based on business context\nSession 4 - AI-Assisted Session: - Use AI to tune Random Forest hyperparameters (n_estimators, max_depth, min_samples_split) - Use AI to compare multiple models with ROC curves - Use AI to generate comprehensive classification reports - Practice interpreting trade-offs between models\n\n\n\nSession 5 - Foundational Session: - Introduce SVMs: margin concept, kernels - Live-code: Fit SVC with linear and RBF kernels - Visualize decision boundaries with different kernels - Introduce k-NN: distance-based classification - Live-code: Fit KNeighborsClassifier with different k - Discuss curse of dimensionality - Emphasize feature scaling for k-NN - Introduce class imbalance problem\nSession 6 - AI-Assisted Session: - Use AI to tune SVM kernels and hyperparameters - Use AI to test many k values for k-NN - Use AI to detect and handle class imbalance (SMOTE, class weights) - Compare models on imbalanced datasets - Practice complete model selection workflow\n\n\n\nSession 7 - Foundational Session: - Review all classification models - Discuss when to use each model type - Practice model selection justification - Work through complex classification scenarios - Understand computational trade-offs\nSession 8 - AI-Assisted Session: - Use AI to build complete classification pipelines - Test multiple models on real-world datasets - Practice interpreting results and making recommendations - Prepare for capstone project work\n\n\n\n\nStructure: - Part A (By-Hand): 10 questions covering classification fundamentals - Fitting each classifier type (Logistic, Tree, Forest, SVM, k-NN) - Creating and interpreting confusion matrices - Computing precision, recall, F1 from confusion matrix - Creating ROC curves - Extracting feature importance - Detecting class imbalance - Visualizing decision boundaries for 2D data - Choosing appropriate metrics for scenarios\n\nPart B (AI-Assisted): 10 questions focusing on scaling analysis\n\nHyperparameter tuning for all 5 classifier types\nComparing 5+ models systematically with all metrics\nHandling class imbalance with SMOTE and class weights\nCreating comprehensive ROC curve comparisons\nBuilding complete classification pipeline\nTesting models across multiple datasets\nGenerating classification reports\nFinal analysis with executive summary and model recommendation\n\n\nDatasets: - Balanced binary classification (clean baseline) - Imbalanced binary classification (fraud detection, rare disease) - Multi-class classification - High-dimensional data (to see k-NN curse of dimensionality)\n\n\n\nFormat: In-class, by-hand, 30-45 minutes\nStructure: - Section A - Conceptual (50%): - Identifying when to use classification vs regression - Understanding confusion matrix elements (TP, FP, TN, FN) - Interpreting precision vs recall tradeoff scenarios - Selecting appropriate metrics based on business context - Understanding when different classifiers are appropriate - Recognizing class imbalance problems - Understanding decision boundaries - Explaining how ensemble methods reduce overfitting\n\nSection B - Code Writing (50%):\n\nFitting LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\nMaking predictions with .predict()\nCreating confusion matrix\nComputing accuracy, precision, recall\nPlotting ROC curve\nExtracting feature importances\nChecking class balance with value_counts()\nImplementing simple classification workflow\n\n\n\n\n\nBy the end of Module 5, students should be able to:\n✅ Fit all major classifiers (Logistic Regression, Decision Trees, Random Forests, SVMs, k-NN) ✅ Create and interpret confusion matrices ✅ Calculate and understand classification metrics (accuracy, precision, recall, F1, AUC) ✅ Plot and interpret ROC curves ✅ Extract and interpret feature importances from tree-based models ✅ Visualize decision boundaries for different classifiers ✅ Recognize class imbalance and understand its impact ✅ Select appropriate classifiers based on data characteristics ✅ Choose evaluation metrics based on business context ✅ Use AI to tune hyperparameters and compare many models ✅ Justify model selection with evidence from metrics and visualizations\n\n\n\nBuilds on: - Module 2: Basic model fitting workflow (instantiate, fit, predict) - Module 3: Bias-variance tradeoff (tree depth, k in k-NN, regularization in logistic regression) - Module 3: Train/validation/test methodology - Module 3: Cross-validation for hyperparameter tuning - Module 4: Regularization concepts (apply to logistic regression)\nPrepares for: - Module 6: Neural networks (logistic regression is the building block) - Module 7: Using pretrained models for classification - Capstone projects: Real-world classification problems\n\n\n\n\nEmphasize metric selection - accuracy is often misleading, especially with imbalance\nUse real datasets with class imbalance to make problems concrete\nVisualize decision boundaries extensively - makes models intuitive\nConnect ensemble methods to bias-variance from Module 3\nShow ROC curves for multiple models on same plot for comparison\nPractice interpreting confusion matrices in business context (what does FP mean for fraud detection?)\nDistinguish between model types clearly:\n\nLogistic regression: linear boundaries, probabilistic, interpretable coefficients\nDecision trees: non-linear boundaries, very interpretable, prone to overfitting\nRandom forests: non-linear, less interpretable, robust, slower\nSVMs: flexible boundaries with kernels, hard to interpret, good for high dimensions\nk-NN: very flexible boundaries, no training, slow prediction, curse of dimensionality\n\nFeature scaling matters for some models (k-NN, SVM) but not others (trees)\nComputational trade-offs are real (random forests are slower than logistic regression)\nImbalanced data is the norm in many real applications - practice handling it\nConnect logistic regression to neural networks - it’s a single neuron with sigmoid activation"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#overview",
    "href": "Claude-planning/module-3-classification.html#overview",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Duration: 3-4 weeks Focus: Understanding classification approaches, selecting appropriate models, interpreting classification metrics, and handling real-world classification challenges like class imbalance"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#core-topics-covered",
    "href": "Claude-planning/module-3-classification.html#core-topics-covered",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Logistic Regression\n\nHow logistic regression differs from linear regression\nThe sigmoid function and probability outputs\nDecision boundaries and thresholds\nInterpreting coefficients in classification\nAssumptions and when to use logistic regression\n\nDecision Trees\n\nHow trees make decisions (splitting criteria)\nGini impurity vs entropy\nTree depth and overfitting\nPruning techniques\nInterpretability advantages\nFeature importance from trees\n\nRandom Forests and Ensemble Methods\n\nBootstrap aggregating (bagging)\nHow random forests reduce overfitting\nFeature randomness and tree diversity\nOut-of-bag error estimation\nFeature importance in random forests\nWhen ensembles are worth the complexity\n\nSupport Vector Machines (SVM)\n\nMargin concept and maximum margin classifier\nSupport vectors and why they matter\nKernel trick for non-linear boundaries\nDifferent kernel types (linear, RBF, polynomial)\nWhen SVMs are appropriate\nComputational considerations\n\nk-Nearest Neighbors (k-NN)\n\nDistance-based classification\nChoosing k (bias-variance tradeoff again)\nDistance metrics (Euclidean, Manhattan, etc.)\nCurse of dimensionality\nFeature scaling importance for k-NN\nWhen k-NN is appropriate\n\nClassification Metrics\n\nConfusion matrix interpretation\nAccuracy and when it’s misleading\nPrecision vs recall tradeoff\nF1 score as a balance\nROC curves and AUC\nChoosing metrics based on business context\nCost-sensitive classification\n\nClass Imbalance\n\nWhy imbalance is a problem\nDetecting class imbalance\nResampling techniques (SMOTE, undersampling, oversampling)\nClass weights\nChoosing appropriate metrics for imbalanced data\nWhen to worry about imbalance and when not to"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#topics-done-by-hand",
    "href": "Claude-planning/module-3-classification.html#topics-done-by-hand",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Students will manually code and understand:\n\nImplementing Each Classifier\n\nFit LogisticRegression in scikit-learn\nFit DecisionTreeClassifier with different depths\nFit RandomForestClassifier\nFit SVC with different kernels\nFit KNeighborsClassifier with different k values\nMake predictions using .predict() and .predict_proba()\n\nCreating and Interpreting Confusion Matrices\n\nUse confusion_matrix() from scikit-learn\nManually calculate TP, FP, TN, FN\nInterpret what each quadrant means\nIdentify when model is making specific types of errors\n\nComputing Classification Metrics\n\nCalculate accuracy, precision, recall from confusion matrix\nUnderstand when each metric is important\nCompute F1 score\nInterpret classification_report() output\n\nVisualizing Decision Boundaries\n\nCreate 2D visualizations of decision boundaries\nUnderstand how different models create different boundaries\nSee the difference between linear and non-linear boundaries\nVisualize how k-NN creates complex boundaries\n\nROC Curves and AUC\n\nPlot ROC curves using sklearn.metrics\nUnderstand true positive rate vs false positive rate\nInterpret AUC values\nCompare models using ROC curves\n\nUnderstanding Feature Importance\n\nExtract feature_importances_ from tree-based models\nInterpret which features are most predictive\nUnderstand the difference between importance in trees vs coefficients in logistic regression\n\nDetecting Class Imbalance\n\nUse value_counts() to check class distribution\nCalculate class proportions\nRecognize when imbalance is problematic"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#topics-done-with-ai-assistance",
    "href": "Claude-planning/module-3-classification.html#topics-done-with-ai-assistance",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Students will use AI coding assistants to:\n\nHyperparameter Tuning at Scale\n\nGrid search over many hyperparameters simultaneously\nTune multiple classifiers (LogisticRegression, DecisionTree, RandomForest, SVC, KNN)\nTest different tree depths, number of estimators, k values, kernels\nCross-validation for all hyperparameter combinations\n\nHandling Class Imbalance\n\nImplement SMOTE for synthetic oversampling\nTest different class weights\nCompare undersampling strategies\nEvaluate impact of different balancing techniques\n\nComprehensive Model Comparison\n\nTrain 5+ classifiers on same dataset\nGenerate comparison tables across metrics (accuracy, precision, recall, F1, AUC)\nCreate ROC curves for all models on same plot\nSystematic evaluation of which model performs best\n\nComplete Classification Pipeline\n\nBuild end-to-end pipeline: load data → preprocess → train → evaluate → visualize\nAutomated generation of confusion matrices for multiple models\nCreating comprehensive classification reports\nGenerating decision boundary plots for multiple models\n\nCross-Dataset Testing\n\nTest same models on multiple datasets\nCompare performance across different data characteristics\nIdentify which models are robust vs dataset-specific"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#in-class-activities",
    "href": "Claude-planning/module-3-classification.html#in-class-activities",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Session 1 - Foundational Session: - Introduce classification vs regression - Explain logistic regression: sigmoid function, probability outputs - Live-code: Fit LogisticRegression, make predictions - Interpret predict() vs predict_proba() - Create and interpret confusion matrix - Calculate accuracy, precision, recall by hand - Introduce decision trees conceptually - Live-code: Fit DecisionTreeClassifier with different max_depth\nSession 2 - AI-Assisted Session: - Use AI to tune logistic regression hyperparameters - Use AI to tune decision tree depth - Use AI to generate decision boundary visualizations - Practice comparing logistic regression vs decision trees - Explore when each is appropriate\n\n\n\nSession 3 - Foundational Session: - Introduce Random Forests: bagging, feature randomness - Live-code: Fit RandomForestClassifier - Extract and interpret feature importances - Introduce ROC curves and AUC - Live-code: Create ROC curve for binary classifier - Discuss precision-recall tradeoff - Practice selecting metrics based on business context\nSession 4 - AI-Assisted Session: - Use AI to tune Random Forest hyperparameters (n_estimators, max_depth, min_samples_split) - Use AI to compare multiple models with ROC curves - Use AI to generate comprehensive classification reports - Practice interpreting trade-offs between models\n\n\n\nSession 5 - Foundational Session: - Introduce SVMs: margin concept, kernels - Live-code: Fit SVC with linear and RBF kernels - Visualize decision boundaries with different kernels - Introduce k-NN: distance-based classification - Live-code: Fit KNeighborsClassifier with different k - Discuss curse of dimensionality - Emphasize feature scaling for k-NN - Introduce class imbalance problem\nSession 6 - AI-Assisted Session: - Use AI to tune SVM kernels and hyperparameters - Use AI to test many k values for k-NN - Use AI to detect and handle class imbalance (SMOTE, class weights) - Compare models on imbalanced datasets - Practice complete model selection workflow\n\n\n\nSession 7 - Foundational Session: - Review all classification models - Discuss when to use each model type - Practice model selection justification - Work through complex classification scenarios - Understand computational trade-offs\nSession 8 - AI-Assisted Session: - Use AI to build complete classification pipelines - Test multiple models on real-world datasets - Practice interpreting results and making recommendations - Prepare for capstone project work"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#homework-assignment",
    "href": "Claude-planning/module-3-classification.html#homework-assignment",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Structure: - Part A (By-Hand): 10 questions covering classification fundamentals - Fitting each classifier type (Logistic, Tree, Forest, SVM, k-NN) - Creating and interpreting confusion matrices - Computing precision, recall, F1 from confusion matrix - Creating ROC curves - Extracting feature importance - Detecting class imbalance - Visualizing decision boundaries for 2D data - Choosing appropriate metrics for scenarios\n\nPart B (AI-Assisted): 10 questions focusing on scaling analysis\n\nHyperparameter tuning for all 5 classifier types\nComparing 5+ models systematically with all metrics\nHandling class imbalance with SMOTE and class weights\nCreating comprehensive ROC curve comparisons\nBuilding complete classification pipeline\nTesting models across multiple datasets\nGenerating classification reports\nFinal analysis with executive summary and model recommendation\n\n\nDatasets: - Balanced binary classification (clean baseline) - Imbalanced binary classification (fraud detection, rare disease) - Multi-class classification - High-dimensional data (to see k-NN curse of dimensionality)"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#quiz",
    "href": "Claude-planning/module-3-classification.html#quiz",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Format: In-class, by-hand, 30-45 minutes\nStructure: - Section A - Conceptual (50%): - Identifying when to use classification vs regression - Understanding confusion matrix elements (TP, FP, TN, FN) - Interpreting precision vs recall tradeoff scenarios - Selecting appropriate metrics based on business context - Understanding when different classifiers are appropriate - Recognizing class imbalance problems - Understanding decision boundaries - Explaining how ensemble methods reduce overfitting\n\nSection B - Code Writing (50%):\n\nFitting LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\nMaking predictions with .predict()\nCreating confusion matrix\nComputing accuracy, precision, recall\nPlotting ROC curve\nExtracting feature importances\nChecking class balance with value_counts()\nImplementing simple classification workflow"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#learning-outcomes-assessment",
    "href": "Claude-planning/module-3-classification.html#learning-outcomes-assessment",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "By the end of Module 5, students should be able to:\n✅ Fit all major classifiers (Logistic Regression, Decision Trees, Random Forests, SVMs, k-NN) ✅ Create and interpret confusion matrices ✅ Calculate and understand classification metrics (accuracy, precision, recall, F1, AUC) ✅ Plot and interpret ROC curves ✅ Extract and interpret feature importances from tree-based models ✅ Visualize decision boundaries for different classifiers ✅ Recognize class imbalance and understand its impact ✅ Select appropriate classifiers based on data characteristics ✅ Choose evaluation metrics based on business context ✅ Use AI to tune hyperparameters and compare many models ✅ Justify model selection with evidence from metrics and visualizations"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#connection-to-other-modules",
    "href": "Claude-planning/module-3-classification.html#connection-to-other-modules",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Builds on: - Module 2: Basic model fitting workflow (instantiate, fit, predict) - Module 3: Bias-variance tradeoff (tree depth, k in k-NN, regularization in logistic regression) - Module 3: Train/validation/test methodology - Module 3: Cross-validation for hyperparameter tuning - Module 4: Regularization concepts (apply to logistic regression)\nPrepares for: - Module 6: Neural networks (logistic regression is the building block) - Module 7: Using pretrained models for classification - Capstone projects: Real-world classification problems"
  },
  {
    "objectID": "Claude-planning/module-3-classification.html#notes-for-instruction",
    "href": "Claude-planning/module-3-classification.html#notes-for-instruction",
    "title": "Module 5: Classification Models",
    "section": "",
    "text": "Emphasize metric selection - accuracy is often misleading, especially with imbalance\nUse real datasets with class imbalance to make problems concrete\nVisualize decision boundaries extensively - makes models intuitive\nConnect ensemble methods to bias-variance from Module 3\nShow ROC curves for multiple models on same plot for comparison\nPractice interpreting confusion matrices in business context (what does FP mean for fraud detection?)\nDistinguish between model types clearly:\n\nLogistic regression: linear boundaries, probabilistic, interpretable coefficients\nDecision trees: non-linear boundaries, very interpretable, prone to overfitting\nRandom forests: non-linear, less interpretable, robust, slower\nSVMs: flexible boundaries with kernels, hard to interpret, good for high dimensions\nk-NN: very flexible boundaries, no training, slow prediction, curse of dimensionality\n\nFeature scaling matters for some models (k-NN, SVM) but not others (trees)\nComputational trade-offs are real (random forests are slower than logistic regression)\nImbalanced data is the norm in many real applications - practice handling it\nConnect logistic regression to neural networks - it’s a single neuron with sigmoid activation"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html",
    "href": "Claude-planning/module-2-regression.html",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Duration: 2-3 weeks Focus: Deep dive into linear regression, understanding assumptions, diagnostics, regularization techniques, and when/why to use different regression approaches\n\n\n\n\nLinear Regression Deep Dive\n\nMathematical formulation and interpretation\nAssumptions (linearity, independence, homoscedasticity, normality)\nCoefficient interpretation\nR² and adjusted R²\nWhen linear models are appropriate\n\nPolynomial Regression and Feature Expansion\n\nCreating polynomial features\nUnderstanding feature interactions\nPolynomial degree selection\nOverfitting with high-degree polynomials\n\nRegularization Techniques\n\nRidge regression (L2 regularization)\nLasso regression (L1 regularization)\nElastic Net (combination of L1 and L2)\nHow regularization prevents overfitting\nFeature selection with Lasso\n\nMulticollinearity\n\nWhat is multicollinearity and why it’s problematic\nDetecting multicollinearity (correlation matrices, VIF)\nHow regularization addresses multicollinearity\nImpact on coefficient interpretation\n\nResidual Analysis and Diagnostics\n\nResiduals vs fitted values plots\nQ-Q plots for normality\nResiduals vs feature plots\nIdentifying patterns in residuals\nDetecting heteroscedasticity\nIdentifying influential points and outliers\n\nEvaluation Metrics\n\nMSE, MAE, RMSE\nR² and adjusted R²\nChoosing metrics based on problem context\nUnderstanding metric limitations\n\n\n\n\n\nStudents will manually code and understand:\n\nComputing Metrics Manually\n\nCalculate residuals by hand\nCompute MSE, MAE, R² from residuals on small datasets\nUnderstand what each metric measures\nInterpret metric values in context\n\nFitting Regression Models\n\nFit Linear Regression using scikit-learn\nFit Ridge, Lasso, and Elastic Net\nExtract and interpret coefficients\nMake predictions and evaluate\n\nCreating Diagnostic Plots\n\nPlot residuals vs fitted values\nCreate Q-Q plots\nPlot residuals vs individual features\nIdentify violations of assumptions from plots\n\nUnderstanding Regularization\n\nFit models with different alpha values\nCompare coefficients with/without regularization\nObserve coefficient shrinkage\nUnderstand impact on predictions\n\nPolynomial Regression\n\nCreate polynomial features manually\nFit polynomial models of different degrees\nCompare performance across degrees\nIdentify when overfitting occurs\n\nInterpreting Results\n\nRead and explain coefficient values\nUnderstand when assumptions are violated\nDiagnose problems from residual plots\nJustify model selection decisions\n\n\n\n\n\nStudents will use AI coding assistants to:\n\nHyperparameter Tuning for Regularization\n\nGrid search over many alpha values\nTest Ridge, Lasso, and Elastic Net simultaneously\nVisualize coefficient paths vs alpha\nSelect optimal regularization strength\n\nComprehensive Polynomial Testing\n\nTest polynomial degrees 1-15 systematically\nGenerate performance comparisons\nCreate visualizations of fitted curves\nIdentify optimal polynomial degree\n\nComplete Diagnostic Reports\n\nAutomatically generate all diagnostic plots\nCheck all assumptions systematically\nCreate comprehensive residual analysis\nGenerate statistical test results\n\nLarge-Scale Model Comparison\n\nCompare linear, polynomial, and regularized models\nTest across multiple datasets\nGenerate comparison tables and visualizations\nSystematic evaluation of many model variants\n\nFeature Engineering at Scale\n\nCreate interaction terms\nTest many feature combinations\nPolynomial feature generation for multiple features\nAutomated feature selection with Lasso\n\n\n\n\n\n\n\nSession 1 - Foundational Session: - Review linear regression from Module 2 - Discuss linear regression assumptions in detail - Live-code: Fit linear regression and extract coefficients - Calculate residuals manually - Create residuals vs fitted plot - Interpret what violations look like in plots - Discuss when linear regression is appropriate\nSession 2 - AI-Assisted Session: - Use AI to generate complete diagnostic reports - Use AI to create comprehensive assumption checking - Explore datasets that violate different assumptions - Practice identifying problems from diagnostic plots - Use AI to test transformations (log, sqrt) to fix violations\n\n\n\nSession 3 - Foundational Session: - Introduce multicollinearity with examples - Show how it affects coefficient estimates - Introduce Ridge and Lasso regularization - Live-code: Fit Ridge and Lasso with different alphas - Compare coefficients with/without regularization - Observe coefficient shrinkage - Discuss when to use Ridge vs Lasso\nSession 4 - AI-Assisted Session: - Use AI to perform grid search over alpha values - Use AI to visualize coefficient paths - Use AI to compare Ridge, Lasso, and Elastic Net - Practice feature selection with Lasso - Test regularization on datasets with multicollinearity\n\n\n\nSession 5 - Foundational Session: - Introduce polynomial regression - Create polynomial features manually - Fit polynomials of different degrees - Observe overfitting with high degrees - Discuss bias-variance tradeoff in polynomial context\nSession 6 - AI-Assisted Session: - Use AI to test many polynomial degrees - Use AI to combine polynomial features with regularization - Compare performance systematically - Practice complete model selection workflow - Integrate all concepts: polynomials + regularization + diagnostics\n\n\n\n\nStructure: - Part A (By-Hand): 10 questions covering regression fundamentals - Computing metrics manually (residuals, MSE, R²) - Fitting linear, Ridge, Lasso models - Creating diagnostic plots - Interpreting coefficients and residual plots - Understanding regularization impact - Polynomial regression basics - Detecting assumption violations\n\nPart B (AI-Assisted): 10 questions focusing on scaling analysis\n\nGrid search over alpha values for regularization\nTesting many polynomial degrees\nComprehensive diagnostic report generation\nComparing regularized vs non-regularized at scale\nFeature selection with Lasso\nComplete model selection pipeline\nCross-dataset comparisons\n\n\nDatasets: - Clean dataset where assumptions hold - Dataset with multicollinearity - Dataset with heteroscedasticity - Dataset requiring polynomial features\n\n\n\nFormat: In-class, by-hand, 30-45 minutes\nStructure: - Section A - Conceptual (50%): - Understanding linear regression assumptions - Interpreting residual plots (identifying violations) - Understanding regularization (Ridge vs Lasso) - Recognizing multicollinearity - Choosing appropriate evaluation metrics - Understanding coefficient interpretation - Knowing when to use polynomial regression\n\nSection B - Code Writing (50%):\n\nFitting linear, Ridge, Lasso models\nComputing residuals and metrics\nCreating diagnostic plots\nImplementing polynomial features\nWriting functions to check assumptions\nComparing regularized models\n\n\n\n\n\nBy the end of Module 4, students should be able to:\n✅ Fit linear regression models and interpret coefficients ✅ Compute regression metrics (MSE, MAE, R²) and understand what they measure ✅ Create diagnostic plots (residuals vs fitted, Q-Q plots) ✅ Identify violations of linear regression assumptions from plots ✅ Explain what multicollinearity is and how it affects models ✅ Apply Ridge, Lasso, and Elastic Net regularization ✅ Understand when and why regularization helps ✅ Implement polynomial regression for non-linear relationships ✅ Use AI to perform comprehensive model selection and diagnostics ✅ Select appropriate regression models based on data characteristics ✅ Justify model choices using diagnostic evidence\n\n\n\nBuilds on: - Module 2: Basic model fitting and evaluation - Module 3: Bias-variance tradeoff (regularization manages this) - Module 3: Loss functions (MSE is the loss for linear regression) - Module 3: Train/validation/test methodology\nPrepares for: - Module 5: Classification (logistic regression uses similar concepts) - Module 6: Neural networks (regularization techniques apply there too) - All future work: Residual analysis and diagnostics apply to all models\n\n\n\n\nEmphasize diagnostic plots as the primary tool for assessing model quality\nUse real datasets that violate assumptions to make concepts concrete\nShow how regularization connects to bias-variance tradeoff from Module 3\nStress that assumptions matter - violating them can make predictions unreliable\nMake clear that regularization is not just for preventing overfitting - it also handles multicollinearity\nUse visualizations extensively to show coefficient shrinkage, regularization paths\nConnect polynomial regression to overfitting - concrete example of bias-variance tradeoff\nEmphasize that R² alone is insufficient - need diagnostic plots\nPractice interpreting coefficients in context (e.g., “for every additional year of experience, salary increases by $5000”)\nDistinguish between Ridge and Lasso clearly - Lasso does feature selection"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#overview",
    "href": "Claude-planning/module-2-regression.html#overview",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Duration: 2-3 weeks Focus: Deep dive into linear regression, understanding assumptions, diagnostics, regularization techniques, and when/why to use different regression approaches"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#core-topics-covered",
    "href": "Claude-planning/module-2-regression.html#core-topics-covered",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Linear Regression Deep Dive\n\nMathematical formulation and interpretation\nAssumptions (linearity, independence, homoscedasticity, normality)\nCoefficient interpretation\nR² and adjusted R²\nWhen linear models are appropriate\n\nPolynomial Regression and Feature Expansion\n\nCreating polynomial features\nUnderstanding feature interactions\nPolynomial degree selection\nOverfitting with high-degree polynomials\n\nRegularization Techniques\n\nRidge regression (L2 regularization)\nLasso regression (L1 regularization)\nElastic Net (combination of L1 and L2)\nHow regularization prevents overfitting\nFeature selection with Lasso\n\nMulticollinearity\n\nWhat is multicollinearity and why it’s problematic\nDetecting multicollinearity (correlation matrices, VIF)\nHow regularization addresses multicollinearity\nImpact on coefficient interpretation\n\nResidual Analysis and Diagnostics\n\nResiduals vs fitted values plots\nQ-Q plots for normality\nResiduals vs feature plots\nIdentifying patterns in residuals\nDetecting heteroscedasticity\nIdentifying influential points and outliers\n\nEvaluation Metrics\n\nMSE, MAE, RMSE\nR² and adjusted R²\nChoosing metrics based on problem context\nUnderstanding metric limitations"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#topics-done-by-hand",
    "href": "Claude-planning/module-2-regression.html#topics-done-by-hand",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Students will manually code and understand:\n\nComputing Metrics Manually\n\nCalculate residuals by hand\nCompute MSE, MAE, R² from residuals on small datasets\nUnderstand what each metric measures\nInterpret metric values in context\n\nFitting Regression Models\n\nFit Linear Regression using scikit-learn\nFit Ridge, Lasso, and Elastic Net\nExtract and interpret coefficients\nMake predictions and evaluate\n\nCreating Diagnostic Plots\n\nPlot residuals vs fitted values\nCreate Q-Q plots\nPlot residuals vs individual features\nIdentify violations of assumptions from plots\n\nUnderstanding Regularization\n\nFit models with different alpha values\nCompare coefficients with/without regularization\nObserve coefficient shrinkage\nUnderstand impact on predictions\n\nPolynomial Regression\n\nCreate polynomial features manually\nFit polynomial models of different degrees\nCompare performance across degrees\nIdentify when overfitting occurs\n\nInterpreting Results\n\nRead and explain coefficient values\nUnderstand when assumptions are violated\nDiagnose problems from residual plots\nJustify model selection decisions"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#topics-done-with-ai-assistance",
    "href": "Claude-planning/module-2-regression.html#topics-done-with-ai-assistance",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Students will use AI coding assistants to:\n\nHyperparameter Tuning for Regularization\n\nGrid search over many alpha values\nTest Ridge, Lasso, and Elastic Net simultaneously\nVisualize coefficient paths vs alpha\nSelect optimal regularization strength\n\nComprehensive Polynomial Testing\n\nTest polynomial degrees 1-15 systematically\nGenerate performance comparisons\nCreate visualizations of fitted curves\nIdentify optimal polynomial degree\n\nComplete Diagnostic Reports\n\nAutomatically generate all diagnostic plots\nCheck all assumptions systematically\nCreate comprehensive residual analysis\nGenerate statistical test results\n\nLarge-Scale Model Comparison\n\nCompare linear, polynomial, and regularized models\nTest across multiple datasets\nGenerate comparison tables and visualizations\nSystematic evaluation of many model variants\n\nFeature Engineering at Scale\n\nCreate interaction terms\nTest many feature combinations\nPolynomial feature generation for multiple features\nAutomated feature selection with Lasso"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#in-class-activities",
    "href": "Claude-planning/module-2-regression.html#in-class-activities",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Session 1 - Foundational Session: - Review linear regression from Module 2 - Discuss linear regression assumptions in detail - Live-code: Fit linear regression and extract coefficients - Calculate residuals manually - Create residuals vs fitted plot - Interpret what violations look like in plots - Discuss when linear regression is appropriate\nSession 2 - AI-Assisted Session: - Use AI to generate complete diagnostic reports - Use AI to create comprehensive assumption checking - Explore datasets that violate different assumptions - Practice identifying problems from diagnostic plots - Use AI to test transformations (log, sqrt) to fix violations\n\n\n\nSession 3 - Foundational Session: - Introduce multicollinearity with examples - Show how it affects coefficient estimates - Introduce Ridge and Lasso regularization - Live-code: Fit Ridge and Lasso with different alphas - Compare coefficients with/without regularization - Observe coefficient shrinkage - Discuss when to use Ridge vs Lasso\nSession 4 - AI-Assisted Session: - Use AI to perform grid search over alpha values - Use AI to visualize coefficient paths - Use AI to compare Ridge, Lasso, and Elastic Net - Practice feature selection with Lasso - Test regularization on datasets with multicollinearity\n\n\n\nSession 5 - Foundational Session: - Introduce polynomial regression - Create polynomial features manually - Fit polynomials of different degrees - Observe overfitting with high degrees - Discuss bias-variance tradeoff in polynomial context\nSession 6 - AI-Assisted Session: - Use AI to test many polynomial degrees - Use AI to combine polynomial features with regularization - Compare performance systematically - Practice complete model selection workflow - Integrate all concepts: polynomials + regularization + diagnostics"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#homework-assignment",
    "href": "Claude-planning/module-2-regression.html#homework-assignment",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Structure: - Part A (By-Hand): 10 questions covering regression fundamentals - Computing metrics manually (residuals, MSE, R²) - Fitting linear, Ridge, Lasso models - Creating diagnostic plots - Interpreting coefficients and residual plots - Understanding regularization impact - Polynomial regression basics - Detecting assumption violations\n\nPart B (AI-Assisted): 10 questions focusing on scaling analysis\n\nGrid search over alpha values for regularization\nTesting many polynomial degrees\nComprehensive diagnostic report generation\nComparing regularized vs non-regularized at scale\nFeature selection with Lasso\nComplete model selection pipeline\nCross-dataset comparisons\n\n\nDatasets: - Clean dataset where assumptions hold - Dataset with multicollinearity - Dataset with heteroscedasticity - Dataset requiring polynomial features"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#quiz",
    "href": "Claude-planning/module-2-regression.html#quiz",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Format: In-class, by-hand, 30-45 minutes\nStructure: - Section A - Conceptual (50%): - Understanding linear regression assumptions - Interpreting residual plots (identifying violations) - Understanding regularization (Ridge vs Lasso) - Recognizing multicollinearity - Choosing appropriate evaluation metrics - Understanding coefficient interpretation - Knowing when to use polynomial regression\n\nSection B - Code Writing (50%):\n\nFitting linear, Ridge, Lasso models\nComputing residuals and metrics\nCreating diagnostic plots\nImplementing polynomial features\nWriting functions to check assumptions\nComparing regularized models"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#learning-outcomes-assessment",
    "href": "Claude-planning/module-2-regression.html#learning-outcomes-assessment",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "By the end of Module 4, students should be able to:\n✅ Fit linear regression models and interpret coefficients ✅ Compute regression metrics (MSE, MAE, R²) and understand what they measure ✅ Create diagnostic plots (residuals vs fitted, Q-Q plots) ✅ Identify violations of linear regression assumptions from plots ✅ Explain what multicollinearity is and how it affects models ✅ Apply Ridge, Lasso, and Elastic Net regularization ✅ Understand when and why regularization helps ✅ Implement polynomial regression for non-linear relationships ✅ Use AI to perform comprehensive model selection and diagnostics ✅ Select appropriate regression models based on data characteristics ✅ Justify model choices using diagnostic evidence"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#connection-to-other-modules",
    "href": "Claude-planning/module-2-regression.html#connection-to-other-modules",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Builds on: - Module 2: Basic model fitting and evaluation - Module 3: Bias-variance tradeoff (regularization manages this) - Module 3: Loss functions (MSE is the loss for linear regression) - Module 3: Train/validation/test methodology\nPrepares for: - Module 5: Classification (logistic regression uses similar concepts) - Module 6: Neural networks (regularization techniques apply there too) - All future work: Residual analysis and diagnostics apply to all models"
  },
  {
    "objectID": "Claude-planning/module-2-regression.html#notes-for-instruction",
    "href": "Claude-planning/module-2-regression.html#notes-for-instruction",
    "title": "Module 4: Regression Models",
    "section": "",
    "text": "Emphasize diagnostic plots as the primary tool for assessing model quality\nUse real datasets that violate assumptions to make concepts concrete\nShow how regularization connects to bias-variance tradeoff from Module 3\nStress that assumptions matter - violating them can make predictions unreliable\nMake clear that regularization is not just for preventing overfitting - it also handles multicollinearity\nUse visualizations extensively to show coefficient shrinkage, regularization paths\nConnect polynomial regression to overfitting - concrete example of bias-variance tradeoff\nEmphasize that R² alone is insufficient - need diagnostic plots\nPractice interpreting coefficients in context (e.g., “for every additional year of experience, salary increases by $5000”)\nDistinguish between Ridge and Lasso clearly - Lasso does feature selection"
  },
  {
    "objectID": "Templates/quiz-template.html",
    "href": "Templates/quiz-template.html",
    "title": "Quiz Template",
    "section": "",
    "text": "All quizzes should follow this two-section structure: - Section A: Conceptual Questions (approximately 50% of points) - Testing understanding of concepts, trade-offs, and decision-making - Section B: Code Writing (approximately 50% of points) - Testing ability to write code by hand\nTotal: Typically 25-50 points (adjustable based on module complexity)\nTime: 30-45 minutes\n\n\n\nPoints should reflect: - Complexity of the concept or code required - Number of parts in the question - Level of reasoning needed\nTypical ranges: - Simple recall/identification: 2 points - Moderate explanation/interpretation: 3 points - Complex reasoning/multi-part questions: 3-4 points - Code writing tasks: 2-4 points depending on complexity\n\n\n\nPurpose: - Test conceptual understanding without a computer - Assess ability to reason about trade-offs and make decisions - Evaluate recognition of when techniques are appropriate/inappropriate - Check understanding of metrics and their interpretations\nQuestion Types:\n\nScenario identification (2-3 points)\n\nGiven scenarios, identify problem type/appropriate technique\nExample: “Is this classification or regression?”\nMultiple scenarios in one question\n\nMetric interpretation (3 points)\n\nGiven metric values, interpret meaning\nCompare models based on different metrics\nExplain which metric to prioritize in a business context\n\nTrade-off reasoning (3-4 points)\n\nUnderstand precision vs recall, bias vs variance, etc.\nExplain when to choose one approach over another\nJustify decisions based on context\n\nConceptual explanation (2-3 points)\n\nDefine terms in own words\nExplain why something works or fails\nDescribe relationships between concepts\n\nProblem diagnosis (3-4 points)\n\nGiven symptoms (overfitting, poor performance, etc.), identify cause\nSuggest solutions to problems\nRecognize when assumptions are violated\n\nMatching questions (2-3 points)\n\nMatch models to their characteristics\nMatch metrics to their uses\nMatch problems to solutions\n\nScenario-based reasoning (3-4 points)\n\nMulti-part questions about a specific scenario\nRequires applying multiple concepts\nTests understanding of workflow and decision-making\n\n\nTypical distribution: 6-8 questions totaling 45-55% of quiz points\n\n\n\nPurpose: - Test ability to write basic code without computer/IDE - Ensure students know fundamental syntax and workflows - Assess understanding of what code does, not just copying - Check recall of essential library functions\nImportant Notes: - Syntax perfection is NOT required - Focus on logic and correct approach - Minor syntax errors minimally penalized - Students should demonstrate understanding of the workflow\nQuestion Types:\n\nData manipulation (2-3 points)\n\nLoad data, select columns, filter rows\nCreate train/test splits\nBasic Pandas operations\n\nModel instantiation (2-3 points)\n\nCreate model objects with correct parameters\nExample: LinearRegression(), DecisionTreeClassifier(max_depth=5)\n\nModel workflow (3-4 points)\n\nComplete fit/predict workflow\nMay include model creation, fitting, and prediction in sequence\n\nMetric calculation (2-3 points)\n\nCalculate metrics using sklearn functions\nExample: accuracy_score(y_test, predictions)\n\nVisualization code (3-4 points)\n\nCreate basic plots (scatter, histogram, bar)\nAdd labels and titles\nMay be more challenging due to syntax details\n\nFunction writing (4-5 points)\n\nWrite simple functions that take inputs and return outputs\nDemonstrate logic and workflow understanding\nExample: comparison function, evaluation function\n\n\nTypical distribution: 5-7 questions totaling 45-55% of quiz points\n\n\n\nSection A (Conceptual): - Full credit: Demonstrates clear understanding - Partial credit: Shows correct reasoning even if incomplete - Focus on understanding, not perfect wording\nSection B (Code): - Correct logic and approach: 70-80% of points - Syntax and details: 20-30% of points - Minor syntax errors minimally penalized - Must show understanding of workflow\nInclude statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\n\n\nEvery quiz should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions with:\n\nTime limit (30-45 minutes)\nFormat: In-class, by-hand (no computer)\nGeneral instruction about writing neatly\nNote about partial credit\n\nSection A: Conceptual Questions (~50% of points, 6-8 questions)\nSection B: Code Writing (~50% of points, 5-7 questions)\nGrading Rubric (breakdown by section with note about syntax)\n\n\n\n\nAlways include for Section B:\nFor questions X-Y, assume you have already imported:\n```python\n[relevant imports for the module]\nAlso assume you have [description of available data/variables].\n\n**Example for ML module:**\n```markdown\nFor questions 6-12, assume you have already imported:\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nAlso assume you have a DataFrame called df with features and a target column.\n\n### Writing Style\n\n**Instructions:**\n- Clear and direct\n- Specify \"by hand\" and \"no computer\" explicitly\n- Set expectations about syntax\n\n**Questions:**\n- Start with clear action verbs\n- Provide sufficient context\n- Multi-part questions should use a), b), c)\n- Ask \"why\" and \"explain your reasoning\" frequently\n\n**Conceptual questions:**\n- Present realistic scenarios\n- Require application of knowledge, not just recall\n- Include business/practical context when relevant\n\n**Code questions:**\n- Be specific about what to create\n- Specify variable names if relevant\n- Break complex tasks into steps\n\n### Formatting Guidelines for Quarto Markdown\n\n**IMPORTANT - List Rendering:**\n- Lists preceded by bold text, headings, or other content **MUST** have a blank line before the list starts\n- Without the blank line, Quarto will not render the list properly\n\n**Correct formatting:**\n```markdown\n**Models:** Logistic Regression, Decision Tree, k-Nearest Neighbors\n\n**Advantages:**\n\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\n3. Provides probability estimates and handles binary classification well\nIncorrect formatting (will not render as list):\n**Advantages:**\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\nApply this rule to: - Multi-part questions (a, b, c) - Matching question options - Scenario lists - Any list following text, bold text, or headings\n\n\n\n\n\n---\ntitle: \"Module [X] Quiz: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    code-fold: false\n    theme: cosmo\n---\n\n## Instructions\n\n**Time:** 30-45 minutes\n**Format:** In-class, by-hand (no computer)\n\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn't perfect.\n\n---\n\n## Section A: Conceptual Questions\n\n### Question 1 ([2-4] points)\n\n[Conceptual question with clear scenario/context]\n\n[If multi-part: **a)**, **b)**, **c)**]\n\n---\n\n### Question 2 ([2-4] points)\n\n[Continue pattern for 6-8 questions total]\n\n---\n\n[Questions 3-8...]\n\n---\n\n## Section B: Code Writing\n\nFor questions [X]-[Y], assume you have already imported:\n```python\n[relevant imports]\nAlso assume you have [description of available data/variables].\n\n\n\nWrite code to [specific task].\n\n\n\n\n[Continue pattern for 5-7 questions total]\n\n[Remaining code questions…]\n\n\n\n\n\nSection A: Conceptual Questions ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[Category 3]: [points]\n[etc.]\n\nSection B: Code Writing ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[etc.]\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: [sum] points\n\n---\n\n## Module-Specific Adaptations\n\n### Module 1 (EDA):\n**Conceptual focus:**\n- When to use different visualizations\n- Data quality issues and solutions\n- Interpretation of distributions and patterns\n- AI prompting strategies\n- Purpose of different types of tests\n\n**Code focus:**\n- Loading and filtering data (Pandas)\n- Creating visualizations (Seaborn)\n- Basic statistics\n- Writing simple test functions\n\n**Imports:**\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nConceptual focus: - Classification vs regression identification - Model selection justification - Metric interpretation (R², MSE, accuracy, precision, recall, F1) - Understanding overfitting - Hyperparameter concepts - Scaling importance\nCode focus: - Train/test split - Model instantiation with parameters - Fit/predict workflow - Metric calculation - Simple comparison functions\nImports:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n\n\n\nConceptual focus: - Loss function selection and interpretation - Bias-variance tradeoff - Optimization concepts (gradient descent) - Overfitting vs underfitting diagnosis - Training/validation/test set methodology\nCode focus: - Implementing simple loss functions - Creating train/val/test splits - Comparing models on different metrics - Plotting learning curves\n\n\n\nConceptual focus: - Linear regression assumptions - When regularization helps - Interpreting coefficients - Residual analysis - Multicollinearity problems - Choosing between Ridge/Lasso/Elastic Net\nCode focus: - Fitting linear regression variants - Computing residuals - Polynomial feature creation - Regularized regression with alpha parameter - Diagnostic plotting\n\n\n\nConceptual focus: - Choosing appropriate evaluation metrics - Precision-recall tradeoffs - Class imbalance handling - Decision boundary interpretation - Feature importance understanding - When to use different classifiers\nCode focus: - Multiple classifier implementation - Confusion matrix creation - ROC curve plotting - Handling class weights - Feature importance extraction\n\n\n\nConceptual focus: - When neural networks are appropriate - Activation function selection - Architecture design reasoning - Interpreting training curves - Overfitting diagnosis - Regularization techniques (dropout, batch norm)\nCode focus: - Reading PyTorch model definitions - Understanding layer structures - Creating DataLoaders - Interpreting training loop code - Plotting loss curves\n\n\n\nConceptual focus: - When to use pretrained vs train from scratch - Transfer learning concepts - Fine-tuning vs feature extraction - Model selection from Hugging Face - Understanding model cards - Trade-offs between model size and accuracy\nCode focus: - Loading pretrained models - Basic inference code - Understanding tokenization - Data format preparation for pretrained models - Interpreting model outputs\n\n\n\n\n\nBefore finalizing a quiz, verify:\n\nTotal points add up correctly (typically 25-50)\nConceptual and code sections are roughly balanced (±5 points)\nTime estimate is reasonable (30-45 min)\nAll necessary imports are listed for code section\nQuestions progress from simpler to more complex\nMulti-part questions use consistent formatting (a, b, c)\nConceptual questions require reasoning, not just recall\nCode questions are realistic to write by hand\nGrading rubric is clear and complete\nNote about syntax errors is included\nQuestions align with module learning objectives\nScenarios are realistic and relatable"
  },
  {
    "objectID": "Templates/quiz-template.html#template-guidelines",
    "href": "Templates/quiz-template.html#template-guidelines",
    "title": "Quiz Template",
    "section": "",
    "text": "All quizzes should follow this two-section structure: - Section A: Conceptual Questions (approximately 50% of points) - Testing understanding of concepts, trade-offs, and decision-making - Section B: Code Writing (approximately 50% of points) - Testing ability to write code by hand\nTotal: Typically 25-50 points (adjustable based on module complexity)\nTime: 30-45 minutes\n\n\n\nPoints should reflect: - Complexity of the concept or code required - Number of parts in the question - Level of reasoning needed\nTypical ranges: - Simple recall/identification: 2 points - Moderate explanation/interpretation: 3 points - Complex reasoning/multi-part questions: 3-4 points - Code writing tasks: 2-4 points depending on complexity\n\n\n\nPurpose: - Test conceptual understanding without a computer - Assess ability to reason about trade-offs and make decisions - Evaluate recognition of when techniques are appropriate/inappropriate - Check understanding of metrics and their interpretations\nQuestion Types:\n\nScenario identification (2-3 points)\n\nGiven scenarios, identify problem type/appropriate technique\nExample: “Is this classification or regression?”\nMultiple scenarios in one question\n\nMetric interpretation (3 points)\n\nGiven metric values, interpret meaning\nCompare models based on different metrics\nExplain which metric to prioritize in a business context\n\nTrade-off reasoning (3-4 points)\n\nUnderstand precision vs recall, bias vs variance, etc.\nExplain when to choose one approach over another\nJustify decisions based on context\n\nConceptual explanation (2-3 points)\n\nDefine terms in own words\nExplain why something works or fails\nDescribe relationships between concepts\n\nProblem diagnosis (3-4 points)\n\nGiven symptoms (overfitting, poor performance, etc.), identify cause\nSuggest solutions to problems\nRecognize when assumptions are violated\n\nMatching questions (2-3 points)\n\nMatch models to their characteristics\nMatch metrics to their uses\nMatch problems to solutions\n\nScenario-based reasoning (3-4 points)\n\nMulti-part questions about a specific scenario\nRequires applying multiple concepts\nTests understanding of workflow and decision-making\n\n\nTypical distribution: 6-8 questions totaling 45-55% of quiz points\n\n\n\nPurpose: - Test ability to write basic code without computer/IDE - Ensure students know fundamental syntax and workflows - Assess understanding of what code does, not just copying - Check recall of essential library functions\nImportant Notes: - Syntax perfection is NOT required - Focus on logic and correct approach - Minor syntax errors minimally penalized - Students should demonstrate understanding of the workflow\nQuestion Types:\n\nData manipulation (2-3 points)\n\nLoad data, select columns, filter rows\nCreate train/test splits\nBasic Pandas operations\n\nModel instantiation (2-3 points)\n\nCreate model objects with correct parameters\nExample: LinearRegression(), DecisionTreeClassifier(max_depth=5)\n\nModel workflow (3-4 points)\n\nComplete fit/predict workflow\nMay include model creation, fitting, and prediction in sequence\n\nMetric calculation (2-3 points)\n\nCalculate metrics using sklearn functions\nExample: accuracy_score(y_test, predictions)\n\nVisualization code (3-4 points)\n\nCreate basic plots (scatter, histogram, bar)\nAdd labels and titles\nMay be more challenging due to syntax details\n\nFunction writing (4-5 points)\n\nWrite simple functions that take inputs and return outputs\nDemonstrate logic and workflow understanding\nExample: comparison function, evaluation function\n\n\nTypical distribution: 5-7 questions totaling 45-55% of quiz points\n\n\n\nSection A (Conceptual): - Full credit: Demonstrates clear understanding - Partial credit: Shows correct reasoning even if incomplete - Focus on understanding, not perfect wording\nSection B (Code): - Correct logic and approach: 70-80% of points - Syntax and details: 20-30% of points - Minor syntax errors minimally penalized - Must show understanding of workflow\nInclude statement: “Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.”\n\n\n\nEvery quiz should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions with:\n\nTime limit (30-45 minutes)\nFormat: In-class, by-hand (no computer)\nGeneral instruction about writing neatly\nNote about partial credit\n\nSection A: Conceptual Questions (~50% of points, 6-8 questions)\nSection B: Code Writing (~50% of points, 5-7 questions)\nGrading Rubric (breakdown by section with note about syntax)\n\n\n\n\nAlways include for Section B:\nFor questions X-Y, assume you have already imported:\n```python\n[relevant imports for the module]\nAlso assume you have [description of available data/variables].\n\n**Example for ML module:**\n```markdown\nFor questions 6-12, assume you have already imported:\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nAlso assume you have a DataFrame called df with features and a target column.\n\n### Writing Style\n\n**Instructions:**\n- Clear and direct\n- Specify \"by hand\" and \"no computer\" explicitly\n- Set expectations about syntax\n\n**Questions:**\n- Start with clear action verbs\n- Provide sufficient context\n- Multi-part questions should use a), b), c)\n- Ask \"why\" and \"explain your reasoning\" frequently\n\n**Conceptual questions:**\n- Present realistic scenarios\n- Require application of knowledge, not just recall\n- Include business/practical context when relevant\n\n**Code questions:**\n- Be specific about what to create\n- Specify variable names if relevant\n- Break complex tasks into steps\n\n### Formatting Guidelines for Quarto Markdown\n\n**IMPORTANT - List Rendering:**\n- Lists preceded by bold text, headings, or other content **MUST** have a blank line before the list starts\n- Without the blank line, Quarto will not render the list properly\n\n**Correct formatting:**\n```markdown\n**Models:** Logistic Regression, Decision Tree, k-Nearest Neighbors\n\n**Advantages:**\n\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\n3. Provides probability estimates and handles binary classification well\nIncorrect formatting (will not render as list):\n**Advantages:**\n1. Easy to interpret and visualize the decision-making process\n2. Makes no assumptions about the underlying data distribution\nApply this rule to: - Multi-part questions (a, b, c) - Matching question options - Scenario lists - Any list following text, bold text, or headings"
  },
  {
    "objectID": "Templates/quiz-template.html#quiz-template-structure",
    "href": "Templates/quiz-template.html#quiz-template-structure",
    "title": "Quiz Template",
    "section": "",
    "text": "---\ntitle: \"Module [X] Quiz: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    code-fold: false\n    theme: cosmo\n---\n\n## Instructions\n\n**Time:** 30-45 minutes\n**Format:** In-class, by-hand (no computer)\n\nAnswer all questions. Write code by hand as neatly as possible. Partial credit will be given for correct reasoning even if syntax isn't perfect.\n\n---\n\n## Section A: Conceptual Questions\n\n### Question 1 ([2-4] points)\n\n[Conceptual question with clear scenario/context]\n\n[If multi-part: **a)**, **b)**, **c)**]\n\n---\n\n### Question 2 ([2-4] points)\n\n[Continue pattern for 6-8 questions total]\n\n---\n\n[Questions 3-8...]\n\n---\n\n## Section B: Code Writing\n\nFor questions [X]-[Y], assume you have already imported:\n```python\n[relevant imports]\nAlso assume you have [description of available data/variables].\n\n\n\nWrite code to [specific task].\n\n\n\n\n[Continue pattern for 5-7 questions total]\n\n[Remaining code questions…]"
  },
  {
    "objectID": "Templates/quiz-template.html#grading-rubric",
    "href": "Templates/quiz-template.html#grading-rubric",
    "title": "Quiz Template",
    "section": "",
    "text": "Section A: Conceptual Questions ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[Category 3]: [points]\n[etc.]\n\nSection B: Code Writing ([total] points)\n\n[Category 1]: [points]\n[Category 2]: [points]\n[etc.]\n\nNote: Minor syntax errors will not be heavily penalized. Focus is on correct logic and understanding of the workflow.\nTotal: [sum] points\n\n---\n\n## Module-Specific Adaptations\n\n### Module 1 (EDA):\n**Conceptual focus:**\n- When to use different visualizations\n- Data quality issues and solutions\n- Interpretation of distributions and patterns\n- AI prompting strategies\n- Purpose of different types of tests\n\n**Code focus:**\n- Loading and filtering data (Pandas)\n- Creating visualizations (Seaborn)\n- Basic statistics\n- Writing simple test functions\n\n**Imports:**\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nConceptual focus: - Classification vs regression identification - Model selection justification - Metric interpretation (R², MSE, accuracy, precision, recall, F1) - Understanding overfitting - Hyperparameter concepts - Scaling importance\nCode focus: - Train/test split - Model instantiation with parameters - Fit/predict workflow - Metric calculation - Simple comparison functions\nImports:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n\n\n\nConceptual focus: - Loss function selection and interpretation - Bias-variance tradeoff - Optimization concepts (gradient descent) - Overfitting vs underfitting diagnosis - Training/validation/test set methodology\nCode focus: - Implementing simple loss functions - Creating train/val/test splits - Comparing models on different metrics - Plotting learning curves\n\n\n\nConceptual focus: - Linear regression assumptions - When regularization helps - Interpreting coefficients - Residual analysis - Multicollinearity problems - Choosing between Ridge/Lasso/Elastic Net\nCode focus: - Fitting linear regression variants - Computing residuals - Polynomial feature creation - Regularized regression with alpha parameter - Diagnostic plotting\n\n\n\nConceptual focus: - Choosing appropriate evaluation metrics - Precision-recall tradeoffs - Class imbalance handling - Decision boundary interpretation - Feature importance understanding - When to use different classifiers\nCode focus: - Multiple classifier implementation - Confusion matrix creation - ROC curve plotting - Handling class weights - Feature importance extraction\n\n\n\nConceptual focus: - When neural networks are appropriate - Activation function selection - Architecture design reasoning - Interpreting training curves - Overfitting diagnosis - Regularization techniques (dropout, batch norm)\nCode focus: - Reading PyTorch model definitions - Understanding layer structures - Creating DataLoaders - Interpreting training loop code - Plotting loss curves\n\n\n\nConceptual focus: - When to use pretrained vs train from scratch - Transfer learning concepts - Fine-tuning vs feature extraction - Model selection from Hugging Face - Understanding model cards - Trade-offs between model size and accuracy\nCode focus: - Loading pretrained models - Basic inference code - Understanding tokenization - Data format preparation for pretrained models - Interpreting model outputs"
  },
  {
    "objectID": "Templates/quiz-template.html#quality-checklist",
    "href": "Templates/quiz-template.html#quality-checklist",
    "title": "Quiz Template",
    "section": "",
    "text": "Before finalizing a quiz, verify:\n\nTotal points add up correctly (typically 25-50)\nConceptual and code sections are roughly balanced (±5 points)\nTime estimate is reasonable (30-45 min)\nAll necessary imports are listed for code section\nQuestions progress from simpler to more complex\nMulti-part questions use consistent formatting (a, b, c)\nConceptual questions require reasoning, not just recall\nCode questions are realistic to write by hand\nGrading rubric is clear and complete\nNote about syntax errors is included\nQuestions align with module learning objectives\nScenarios are realistic and relatable"
  },
  {
    "objectID": "Templates/homework-template.html",
    "href": "Templates/homework-template.html",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "All homework assignments should follow this two-part structure: - Part A: By Hand (40-45 points) - No AI assistance, 10 questions - Part B: With AI Assistance (55-60 points) - Using AI coding assistants, 10 questions\nTotal: 100 points across 20 questions\n\n\n\nPart A (40-45 points): - Questions should range from 3-5 points each - Simpler questions (identifying concepts, basic code): 3-4 points - More complex questions (writing functions, multi-step analysis): 4-5 points\nPart B (55-60 points): - Questions should range from 5-7 points each - Standard AI-assisted tasks: 5-6 points - Comprehensive final question with executive summary: 7 points\n\n\n\nPart A - By Hand: 1. Conceptual identification (3 points): Identify problem types, match concepts 2. Basic implementation (3-4 points): Load data, implement single model/technique 3. Evaluation and interpretation (4-5 points): Calculate metrics, create visualizations, interpret results 4. Function writing (4-5 points): Write reusable functions that demonstrate understanding 5. Explanation questions (4-5 points): Explain concepts in own words, justify decisions\nPart B - AI Assisted: 1. Hyperparameter tuning (5-6 points): Test multiple parameter combinations systematically 2. Large-scale comparison (5-6 points): Compare many models/approaches simultaneously 3. Automation (5-6 points): Create functions that automate repetitive tasks at scale 4. Feature engineering/exploration (5-6 points): Test multiple feature combinations or data transformations 5. Cross-validation/robustness (5-6 points): Evaluate across multiple splits or conditions 6. Comprehensive pipeline (6-7 points): End-to-end workflow combining multiple steps 7. Final integrated project (7 points): Complete analysis with executive summary\n\n\n\nGoals: - Build foundational skills that cannot be outsourced to AI - Ensure students understand the underlying concepts - Practice the basic workflow that AI will scale up in Part B - Develop intuition for when something is wrong\nTopics covered by-hand: - Basic library usage and syntax - Single-model implementation - Metric calculation and interpretation - Simple visualizations - Understanding what code does and why - Explaining concepts in own words - Recognizing patterns and problems\nTypical progression: 1. Conceptual warm-up (identify problem types, match definitions) 2. Basic data loading and exploration 3. Implement 2-4 different techniques/models individually 4. Evaluate and compare using appropriate metrics 5. Create basic visualizations 6. Write helper functions demonstrating understanding 7. Explain concepts and justify decisions\n\n\n\nGoals: - Scale up the techniques learned in Part A - Practice effective AI prompting - Learn to verify and understand AI-generated code - Tackle problems too large/tedious to do by hand - Build comprehensive pipelines and workflows\nAI assistance is used for: - Testing many hyperparameters systematically - Comparing 5+ models simultaneously - Creating comprehensive visualization suites - Automating repetitive tasks - Building end-to-end pipelines - Generating formatted reports - Cross-validation and robustness testing\nRequired deliverables for each question: - The code (AI-generated but verified by student) - Outputs (visualizations, tables, results) - Written interpretation (student’s own analysis) - The prompt(s) used with AI assistant\nTypical progression: 1. Hyperparameter tuning for single model type 2. Compare multiple models at scale 3. Explore impact of different choices (features, scaling, etc.) 4. Build automation functions/pipelines 5. Perform robust evaluation (cross-validation, multiple conditions) 6. Comprehensive integrated analysis with executive summary\n\n\n\nProvide 1-2 datasets per assignment: - Should be realistic and somewhat messy (matching real-world data) - Include both numerical and categorical features - Should align with module topics - Clearly specify which dataset to use for which questions - Include complete column descriptions\nExample dataset formats: - Module 1 (EDA): housing_data.csv - Module 2 (ML Models): employee_salaries.csv (regression) + customer_churn.csv (classification) - Module 4 (Regression): Include features with multicollinearity, outliers - Module 5 (Classification): Include class imbalance, multiple classes\n\n\n\nInstructions: - Clear, direct, and specific - Use numbered lists for multi-part questions - Bold key terms and concepts - Specify exact deliverables expected\nQuestions: - Start with action verbs: “Calculate”, “Create”, “Explain”, “Compare”, “Write” - Include context and reasoning prompts: “Why?”, “How does X affect Y?”, “What do you notice?” - For Part B: explicitly request AI prompts as deliverables\nTips sections: - Casual but helpful tone - Practical, actionable advice - Separate tips for Part A, Part B, and General - End with: “Use your brain. That’s what it’s there for.”\n\n\n\nIMPORTANT - List Rendering: - Lists preceded by bold text, headings, or other content MUST have a blank line before the list starts - Without the blank line, Quarto will not render the list properly\nCorrect formatting:\n**Deliverables:**\n\n- Code\n- Visualizations\n- Written interpretation\nIncorrect formatting (will not render as list):\n**Deliverables:**\n- Code\n- Visualizations\n- Written interpretation\nApply this rule to: - Deliverables sections in Part B questions - Dataset column descriptions - Multi-part question items (a, b, c) - Submission guidelines - Tips sections - Any list following text, bold text, or headings\n\n\n\nPart A (40-45 points): - Code correctness and functionality: ~55% - Proper implementation of techniques: ~25% - Written interpretations and explanations: ~20%\nPart B (55-60 points): - Code functionality and correctness: ~45% - Quality and specificity of AI prompts: ~25% - Visualizations and comparative analysis: ~20% - Written interpretations and insights: ~10%\n\n\n\nEvery homework should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions section with:\n\nDue date (to be assigned)\nTwo-part structure explanation\nSubmission requirements (code, visualizations, written answers, AI prompts)\nDataset descriptions\n\nPart A: By Hand (10 questions, 40-45 points)\nPart B: With AI Assistance (10 questions, 55-60 points)\nSubmission Guidelines (ZIP file contents)\nGrading Rubric (detailed breakdown)\nTips for Success (Part A, Part B, General)\n\n\n\n\n\n\n---\ntitle: \"Module [X] Homework: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Instructions\n\n**Due Date:** [To be assigned by instructor]\n\nThis homework is divided into two parts. Part A should be completed **without AI assistance** to build your foundational [skills/understanding of topic]. Part B should be completed **with AI assistance** (like Gemini CLI) to practice [scaling your work/tackling more comprehensive tasks].\n\nFor all questions, submit:\n\n1. Your code (in a `.py` file or Jupyter notebook)\n2. All visualizations generated\n3. Written answers to interpretation questions (can be in markdown or comments)\n4. For Part B: Include the prompts you used with the AI assistant\n\n### Dataset(s)\n\n[Provide 1-2 dataset descriptions with column explanations]\n\n**[Dataset Name 1]:** `filename.csv` contains [description] with columns:\n\n- `column1`: Description (note if TARGET variable)\n- `column2`: Description\n- [etc.]\n\n---\n\n## Part A: By Hand (No AI Assistance)\n\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational [skills in X].\n\n### Question 1 ([3-5] points)\n\n[Question text with clear instructions]\n\n[Optional: Multi-part with a), b), c)]\n\n---\n\n### Question 2 ([3-5] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 3-10...]\n\n---\n\n## Part B: With AI Assistance\n\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you write and scale your [analysis/code/experiments]. The goal is to practice [specific AI-assisted skills].\n\n**Important:** For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n---\n\n### Question 11 ([5-7] points)\n\n[Question text]\n\n**Deliverables:**\n\n- Code\n- [Visualizations/Tables/Results]\n- Written interpretation ([X] sentences)\n- Your AI prompt(s)\n\n---\n\n### Question 12 ([5-7] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 12-19...]\n\n---\n\n### Question 20 (7 points)\n\n[Comprehensive final question requiring integration of multiple concepts]\n\nWrite a [length] executive summary ([word count] words) that explains:\n\n- [Point 1]\n- [Point 2]\n- [Point 3]\n- [etc.]\n\n**Deliverables:**\n\n- Complete [workflow/analysis] code (well-commented)\n- All visualizations\n- Executive summary\n- Your AI prompt(s)\n- Reflection: [Specific reflection question about AI-assisted vs by-hand work]\n\n---\n\n## Submission Guidelines\n\nSubmit a ZIP file containing:\n\n1. **Code files:** All `.py` files or Jupyter notebooks\n2. **Visualizations folder:** All plots and charts generated\n3. **Written responses:** A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\n4. **Data:** Include your processed datasets if you created any modifications\n\n---\n\n## Grading Rubric\n\n**Part A: By Hand ([40-45] points)**\n\n- Code correctness and functionality: [~22-25] points\n- Proper implementation of [techniques/models]: [~10-11] points\n- Written interpretations and explanations: [~8-10] points\n\n**Part B: AI-Assisted ([55-60] points)**\n\n- Code functionality and correctness: [~25-27] points\n- Quality and specificity of AI prompts: [~14-15] points\n- Visualizations and comparative analysis: [~11-12] points\n- Written interpretations and insights: [~5-6] points\n\n**Total: 100 points**\n\n---\n\n## Tips for Success\n\n### For Part A:\n\n- [3-5 specific, actionable tips relevant to the module]\n- Test your code on small examples first\n- Use meaningful variable names\n- Comment your code to explain your reasoning\n\n### For Part B:\n\n- Write specific, detailed prompts\n- If the AI's first attempt isn't quite right, refine your prompt\n- Always review and test the AI-generated code\n- Don't just accept AI output—understand what it's doing\n- Include context in your prompts (e.g., \"I'm working with a [dataset] with columns...\")\n\n### General:\n\n- Start early—[module-specific reason]\n- Your interpretations are just as important as your code\n- If you find something interesting in the data, explore it further!\n- Use your brain. That's what it's there for.\n\n\n\n\n\n\n\nFocus: Data exploration, visualization, testing\nBy-hand: Basic Pandas, Seaborn, simple statistics\nAI-assisted: Comprehensive profiling, many visualizations, complete test suites\n\n\n\n\n\nFocus: Model implementation, comparison, evaluation\nBy-hand: Instantiate/fit/predict workflow, basic metrics\nAI-assisted: Hyperparameter tuning, comparing 5+ models, cross-validation\n\n\n\n\n\nFocus: Loss functions, optimization, bias-variance tradeoff\nBy-hand: Implement simple loss functions, understand concepts\nAI-assisted: Compare many loss functions, visualize optimization landscapes\n\n\n\n\n\nFocus: Linear regression variants, regularization, diagnostics\nBy-hand: Fit linear/ridge/lasso, residual analysis, interpret coefficients\nAI-assisted: Grid search over many alphas, polynomial feature testing, comprehensive diagnostics\n\n\n\n\n\nFocus: Multiple classifiers, evaluation metrics, class imbalance\nBy-hand: Implement classifiers, confusion matrices, metric calculation\nAI-assisted: Hyperparameter tuning, handling imbalance strategies, comprehensive metric comparison\n\n\n\n\n\nFocus: Network architecture, PyTorch, training loops\nBy-hand: Read PyTorch code, understand architecture, plot training curves\nAI-assisted: Write training loops, architecture variations, extensive hyperparameter tuning\n\n\n\n\n\nFocus: Hugging Face, fine-tuning, transfer learning\nBy-hand: Load and use pretrained models, basic inference\nAI-assisted: Fine-tuning pipelines, comparing many pretrained models, end-to-end applications"
  },
  {
    "objectID": "Templates/homework-template.html#template-guidelines",
    "href": "Templates/homework-template.html#template-guidelines",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "All homework assignments should follow this two-part structure: - Part A: By Hand (40-45 points) - No AI assistance, 10 questions - Part B: With AI Assistance (55-60 points) - Using AI coding assistants, 10 questions\nTotal: 100 points across 20 questions\n\n\n\nPart A (40-45 points): - Questions should range from 3-5 points each - Simpler questions (identifying concepts, basic code): 3-4 points - More complex questions (writing functions, multi-step analysis): 4-5 points\nPart B (55-60 points): - Questions should range from 5-7 points each - Standard AI-assisted tasks: 5-6 points - Comprehensive final question with executive summary: 7 points\n\n\n\nPart A - By Hand: 1. Conceptual identification (3 points): Identify problem types, match concepts 2. Basic implementation (3-4 points): Load data, implement single model/technique 3. Evaluation and interpretation (4-5 points): Calculate metrics, create visualizations, interpret results 4. Function writing (4-5 points): Write reusable functions that demonstrate understanding 5. Explanation questions (4-5 points): Explain concepts in own words, justify decisions\nPart B - AI Assisted: 1. Hyperparameter tuning (5-6 points): Test multiple parameter combinations systematically 2. Large-scale comparison (5-6 points): Compare many models/approaches simultaneously 3. Automation (5-6 points): Create functions that automate repetitive tasks at scale 4. Feature engineering/exploration (5-6 points): Test multiple feature combinations or data transformations 5. Cross-validation/robustness (5-6 points): Evaluate across multiple splits or conditions 6. Comprehensive pipeline (6-7 points): End-to-end workflow combining multiple steps 7. Final integrated project (7 points): Complete analysis with executive summary\n\n\n\nGoals: - Build foundational skills that cannot be outsourced to AI - Ensure students understand the underlying concepts - Practice the basic workflow that AI will scale up in Part B - Develop intuition for when something is wrong\nTopics covered by-hand: - Basic library usage and syntax - Single-model implementation - Metric calculation and interpretation - Simple visualizations - Understanding what code does and why - Explaining concepts in own words - Recognizing patterns and problems\nTypical progression: 1. Conceptual warm-up (identify problem types, match definitions) 2. Basic data loading and exploration 3. Implement 2-4 different techniques/models individually 4. Evaluate and compare using appropriate metrics 5. Create basic visualizations 6. Write helper functions demonstrating understanding 7. Explain concepts and justify decisions\n\n\n\nGoals: - Scale up the techniques learned in Part A - Practice effective AI prompting - Learn to verify and understand AI-generated code - Tackle problems too large/tedious to do by hand - Build comprehensive pipelines and workflows\nAI assistance is used for: - Testing many hyperparameters systematically - Comparing 5+ models simultaneously - Creating comprehensive visualization suites - Automating repetitive tasks - Building end-to-end pipelines - Generating formatted reports - Cross-validation and robustness testing\nRequired deliverables for each question: - The code (AI-generated but verified by student) - Outputs (visualizations, tables, results) - Written interpretation (student’s own analysis) - The prompt(s) used with AI assistant\nTypical progression: 1. Hyperparameter tuning for single model type 2. Compare multiple models at scale 3. Explore impact of different choices (features, scaling, etc.) 4. Build automation functions/pipelines 5. Perform robust evaluation (cross-validation, multiple conditions) 6. Comprehensive integrated analysis with executive summary\n\n\n\nProvide 1-2 datasets per assignment: - Should be realistic and somewhat messy (matching real-world data) - Include both numerical and categorical features - Should align with module topics - Clearly specify which dataset to use for which questions - Include complete column descriptions\nExample dataset formats: - Module 1 (EDA): housing_data.csv - Module 2 (ML Models): employee_salaries.csv (regression) + customer_churn.csv (classification) - Module 4 (Regression): Include features with multicollinearity, outliers - Module 5 (Classification): Include class imbalance, multiple classes\n\n\n\nInstructions: - Clear, direct, and specific - Use numbered lists for multi-part questions - Bold key terms and concepts - Specify exact deliverables expected\nQuestions: - Start with action verbs: “Calculate”, “Create”, “Explain”, “Compare”, “Write” - Include context and reasoning prompts: “Why?”, “How does X affect Y?”, “What do you notice?” - For Part B: explicitly request AI prompts as deliverables\nTips sections: - Casual but helpful tone - Practical, actionable advice - Separate tips for Part A, Part B, and General - End with: “Use your brain. That’s what it’s there for.”\n\n\n\nIMPORTANT - List Rendering: - Lists preceded by bold text, headings, or other content MUST have a blank line before the list starts - Without the blank line, Quarto will not render the list properly\nCorrect formatting:\n**Deliverables:**\n\n- Code\n- Visualizations\n- Written interpretation\nIncorrect formatting (will not render as list):\n**Deliverables:**\n- Code\n- Visualizations\n- Written interpretation\nApply this rule to: - Deliverables sections in Part B questions - Dataset column descriptions - Multi-part question items (a, b, c) - Submission guidelines - Tips sections - Any list following text, bold text, or headings\n\n\n\nPart A (40-45 points): - Code correctness and functionality: ~55% - Proper implementation of techniques: ~25% - Written interpretations and explanations: ~20%\nPart B (55-60 points): - Code functionality and correctness: ~45% - Quality and specificity of AI prompts: ~25% - Visualizations and comparative analysis: ~20% - Written interpretations and insights: ~10%\n\n\n\nEvery homework should include:\n\nTitle and metadata (Quarto YAML header)\nInstructions section with:\n\nDue date (to be assigned)\nTwo-part structure explanation\nSubmission requirements (code, visualizations, written answers, AI prompts)\nDataset descriptions\n\nPart A: By Hand (10 questions, 40-45 points)\nPart B: With AI Assistance (10 questions, 55-60 points)\nSubmission Guidelines (ZIP file contents)\nGrading Rubric (detailed breakdown)\nTips for Success (Part A, Part B, General)"
  },
  {
    "objectID": "Templates/homework-template.html#homework-template-structure",
    "href": "Templates/homework-template.html#homework-template-structure",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "---\ntitle: \"Module [X] Homework: [Topic Name]\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\njupyter: python3\n---\n\n## Instructions\n\n**Due Date:** [To be assigned by instructor]\n\nThis homework is divided into two parts. Part A should be completed **without AI assistance** to build your foundational [skills/understanding of topic]. Part B should be completed **with AI assistance** (like Gemini CLI) to practice [scaling your work/tackling more comprehensive tasks].\n\nFor all questions, submit:\n\n1. Your code (in a `.py` file or Jupyter notebook)\n2. All visualizations generated\n3. Written answers to interpretation questions (can be in markdown or comments)\n4. For Part B: Include the prompts you used with the AI assistant\n\n### Dataset(s)\n\n[Provide 1-2 dataset descriptions with column explanations]\n\n**[Dataset Name 1]:** `filename.csv` contains [description] with columns:\n\n- `column1`: Description (note if TARGET variable)\n- `column2`: Description\n- [etc.]\n\n---\n\n## Part A: By Hand (No AI Assistance)\n\nComplete questions 1-10 without using AI coding assistants. The goal is to build your foundational [skills in X].\n\n### Question 1 ([3-5] points)\n\n[Question text with clear instructions]\n\n[Optional: Multi-part with a), b), c)]\n\n---\n\n### Question 2 ([3-5] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 3-10...]\n\n---\n\n## Part B: With AI Assistance\n\nFor questions 11-20, you should use an AI coding assistant (like Gemini CLI) to help you write and scale your [analysis/code/experiments]. The goal is to practice [specific AI-assisted skills].\n\n**Important:** For each question, save the prompt(s) you used with the AI assistant. Part of your grade will be based on the quality of your prompts.\n\n---\n\n### Question 11 ([5-7] points)\n\n[Question text]\n\n**Deliverables:**\n\n- Code\n- [Visualizations/Tables/Results]\n- Written interpretation ([X] sentences)\n- Your AI prompt(s)\n\n---\n\n### Question 12 ([5-7] points)\n\n[Continue pattern for 10 questions total]\n\n---\n\n[Questions 12-19...]\n\n---\n\n### Question 20 (7 points)\n\n[Comprehensive final question requiring integration of multiple concepts]\n\nWrite a [length] executive summary ([word count] words) that explains:\n\n- [Point 1]\n- [Point 2]\n- [Point 3]\n- [etc.]\n\n**Deliverables:**\n\n- Complete [workflow/analysis] code (well-commented)\n- All visualizations\n- Executive summary\n- Your AI prompt(s)\n- Reflection: [Specific reflection question about AI-assisted vs by-hand work]\n\n---\n\n## Submission Guidelines\n\nSubmit a ZIP file containing:\n\n1. **Code files:** All `.py` files or Jupyter notebooks\n2. **Visualizations folder:** All plots and charts generated\n3. **Written responses:** A single document (PDF or Markdown) with all your written answers, interpretations, and AI prompts used\n4. **Data:** Include your processed datasets if you created any modifications\n\n---\n\n## Grading Rubric\n\n**Part A: By Hand ([40-45] points)**\n\n- Code correctness and functionality: [~22-25] points\n- Proper implementation of [techniques/models]: [~10-11] points\n- Written interpretations and explanations: [~8-10] points\n\n**Part B: AI-Assisted ([55-60] points)**\n\n- Code functionality and correctness: [~25-27] points\n- Quality and specificity of AI prompts: [~14-15] points\n- Visualizations and comparative analysis: [~11-12] points\n- Written interpretations and insights: [~5-6] points\n\n**Total: 100 points**\n\n---\n\n## Tips for Success\n\n### For Part A:\n\n- [3-5 specific, actionable tips relevant to the module]\n- Test your code on small examples first\n- Use meaningful variable names\n- Comment your code to explain your reasoning\n\n### For Part B:\n\n- Write specific, detailed prompts\n- If the AI's first attempt isn't quite right, refine your prompt\n- Always review and test the AI-generated code\n- Don't just accept AI output—understand what it's doing\n- Include context in your prompts (e.g., \"I'm working with a [dataset] with columns...\")\n\n### General:\n\n- Start early—[module-specific reason]\n- Your interpretations are just as important as your code\n- If you find something interesting in the data, explore it further!\n- Use your brain. That's what it's there for."
  },
  {
    "objectID": "Templates/homework-template.html#module-specific-adaptations",
    "href": "Templates/homework-template.html#module-specific-adaptations",
    "title": "Homework Assignment Template",
    "section": "",
    "text": "Focus: Data exploration, visualization, testing\nBy-hand: Basic Pandas, Seaborn, simple statistics\nAI-assisted: Comprehensive profiling, many visualizations, complete test suites\n\n\n\n\n\nFocus: Model implementation, comparison, evaluation\nBy-hand: Instantiate/fit/predict workflow, basic metrics\nAI-assisted: Hyperparameter tuning, comparing 5+ models, cross-validation\n\n\n\n\n\nFocus: Loss functions, optimization, bias-variance tradeoff\nBy-hand: Implement simple loss functions, understand concepts\nAI-assisted: Compare many loss functions, visualize optimization landscapes\n\n\n\n\n\nFocus: Linear regression variants, regularization, diagnostics\nBy-hand: Fit linear/ridge/lasso, residual analysis, interpret coefficients\nAI-assisted: Grid search over many alphas, polynomial feature testing, comprehensive diagnostics\n\n\n\n\n\nFocus: Multiple classifiers, evaluation metrics, class imbalance\nBy-hand: Implement classifiers, confusion matrices, metric calculation\nAI-assisted: Hyperparameter tuning, handling imbalance strategies, comprehensive metric comparison\n\n\n\n\n\nFocus: Network architecture, PyTorch, training loops\nBy-hand: Read PyTorch code, understand architecture, plot training curves\nAI-assisted: Write training loops, architecture variations, extensive hyperparameter tuning\n\n\n\n\n\nFocus: Hugging Face, fine-tuning, transfer learning\nBy-hand: Load and use pretrained models, basic inference\nAI-assisted: Fine-tuning pipelines, comparing many pretrained models, end-to-end applications"
  }
]