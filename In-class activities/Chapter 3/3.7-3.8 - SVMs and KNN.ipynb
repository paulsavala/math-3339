{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 3.7-3.8: SVMs and k-Nearest Neighbors\n",
    "\n",
    "Goal: Fit SVMs and k-NN models, understand when feature scaling matters, and compare all Chapter 3 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### Topics:\n",
    "- SVMs: margin maximization, linear vs RBF kernels\n",
    "- k-NN: majority vote, choosing k\n",
    "- Why feature scaling matters for distance-based models\n",
    "- Comparing multiple classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Quick Recap\n",
    "\n",
    "- **SVMs** find the boundary that maximizes the margin between classes\n",
    "- The **kernel trick** lets SVMs handle non-linear boundaries (RBF kernel)\n",
    "- **Always scale features** before using SVMs — they're sensitive to feature magnitudes\n",
    "- **k-NN** classifies by majority vote of the k nearest neighbors — no explicit training step\n",
    "- Small k → overfitting (too sensitive to noise); large k → underfitting (too smooth)\n",
    "- k-NN is also sensitive to feature scale (distances depend on units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the classic **Iris** dataset — nice numeric features and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"Classes: {list(iris.target_names)}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 1. Use AI — Train/test split and scale features\n",
    "\n",
    "Split into train/test (80/20), then scale features with `StandardScaler`. Remember: fit the scaler on training data only, then transform both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train/test split (80/20, random_state=42)\n",
    "\n",
    "\n",
    "# Step 2: Fit StandardScaler on training data, transform both\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2. Use AI — Compare SVM kernels\n",
    "\n",
    "Fit `SVC(kernel='linear')` and `SVC(kernel='rbf')` on the scaled data. Compare their test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM with linear kernel\n",
    "\n",
    "\n",
    "# Fit SVM with RBF kernel\n",
    "\n",
    "\n",
    "# Compare test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 3. Interpretation — Why does scaling matter?\n",
    "\n",
    "Why do SVMs and k-NN require feature scaling, but decision trees don't? Think about how each algorithm uses the feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 4. Use AI — Find the best k for k-NN\n",
    "\n",
    "Fit `KNeighborsClassifier` with k = 1, 3, 7, 15, 30. Plot test accuracy vs k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit k-NN with different values of k and plot accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 5. Interpretation — Choosing k\n",
    "\n",
    "Looking at the plot:\n",
    "- What happens at k=1? (Think about overfitting)\n",
    "- What happens at very large k? (Think about underfitting)\n",
    "- What value of k looks best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 6. Use AI — Compare all Chapter 3 models\n",
    "\n",
    "Fit all five models on the Iris data: logistic regression, decision tree, random forest, SVM (RBF), and k-NN (using the best k from above). Display results in a DataFrame sorted by accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models and display results as a sorted DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 7. Interpretation — Model comparison\n",
    "\n",
    "Looking at the results:\n",
    "- Is there one clear \"winner\"?\n",
    "- Are the differences between models large or small?\n",
    "- What does this tell you about model selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "When would you reach for SVM instead of random forest? When would k-NN be a particularly bad choice (think about dataset size and number of features)?\n",
    "\n",
    "(Discuss with a neighbor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}