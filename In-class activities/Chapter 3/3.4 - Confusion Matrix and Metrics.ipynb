{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 3.4: Confusion Matrix and Metrics\n",
    "\n",
    "Goal: Build and interpret confusion matrices, and reason about when to prioritize precision vs recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### Topics:\n",
    "- True positives, true negatives, false positives, false negatives\n",
    "- Accuracy, precision, recall, and F1 score\n",
    "- Choosing the right metric for the problem\n",
    "- Normalized confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Quick Recap\n",
    "\n",
    "- **TP** (True Positive): Predicted positive, actually positive\n",
    "- **TN** (True Negative): Predicted negative, actually negative\n",
    "- **FP** (False Positive): Predicted positive, actually negative — \"false alarm\"\n",
    "- **FN** (False Negative): Predicted negative, actually positive — \"missed case\"\n",
    "- **Accuracy** = (TP + TN) / Total — overall correctness\n",
    "- **Precision** = TP / (TP + FP) — \"when I say positive, am I right?\"\n",
    "- **Recall** = TP / (TP + FN) — \"did I catch all the positives?\"\n",
    "- **F1 Score** = harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the **Titanic** dataset — survival prediction makes the TP/FP/FN concepts very concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic data\n",
    "titanic = pd.read_csv('../../Textbook/data/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 1. Use AI — Prepare data and fit a model\n",
    "\n",
    "Select numeric features (`Pclass`, `Age`, `SibSp`, `Parch`, `Fare`), fill missing `Age` with the median, split into train/test (80/20), and fit a `LogisticRegression` model. Get predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select features and target, fill missing Age\n",
    "\n",
    "\n",
    "# Step 2: Train/test split (80/20, random_state=42)\n",
    "\n",
    "\n",
    "# Step 3: Fit LogisticRegression\n",
    "\n",
    "\n",
    "# Step 4: Get predictions on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2. Use AI — Create and display a confusion matrix\n",
    "\n",
    "Use `confusion_matrix()` to create the confusion matrix, then display it as a heatmap with `sns.heatmap(annot=True, fmt='d')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create confusion matrix\n",
    "\n",
    "\n",
    "# Step 2: Display as heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 3. Interpretation — What do the errors mean?\n",
    "\n",
    "In the Titanic context (positive = survived):\n",
    "- What does a **false positive** mean? (Model predicted ___ but actually ___)\n",
    "- What does a **false negative** mean?\n",
    "- Which type of error feels \"worse\" to you in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 4. Use AI — Generate a classification report\n",
    "\n",
    "Use `classification_report()` to see precision, recall, and F1 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 5. By hand — Disease screening scenario\n",
    "\n",
    "You're building a model to screen patients for a disease.\n",
    "- A **false negative** means a sick patient is sent home untreated\n",
    "- A **false positive** means a healthy patient gets additional (unnecessary) tests\n",
    "\n",
    "Which metric should you prioritize — **precision** or **recall**? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 6. By hand — Spam filter scenario\n",
    "\n",
    "You're building an email spam filter.\n",
    "- A **false positive** means a legitimate email goes to the spam folder\n",
    "- A **false negative** means a spam email reaches the inbox\n",
    "\n",
    "Which metric matters more here — **precision** or **recall**? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 7. Use AI — Normalized confusion matrix\n",
    "\n",
    "Normalize the confusion matrix by row (use `normalize='true'` in `confusion_matrix()`) and display as a heatmap. This shows the **percentage** of each actual class that was correctly/incorrectly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create normalized confusion matrix\n",
    "\n",
    "\n",
    "# Step 2: Display as heatmap with percentage format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "**Your interpretation:** What percentage of actual survivors did the model correctly identify? What about non-survivors?\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 8. Interpretation — The \"95% accuracy\" question\n",
    "\n",
    "If someone told you \"my model is 95% accurate,\" what follow-up question would you ask before being impressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Can a model have high accuracy but terrible recall for one class? When would that happen?\n",
    "\n",
    "(Discuss with a neighbor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}