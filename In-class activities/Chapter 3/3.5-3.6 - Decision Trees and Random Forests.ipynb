{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 3.5-3.6: Decision Trees and Random Forests\n",
    "\n",
    "Goal: Visualize how decision trees make splits, observe overfitting, and see how random forests improve stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "### Topics:\n",
    "- How decision trees split on features (Gini impurity)\n",
    "- Controlling tree depth to prevent overfitting\n",
    "- Random forests: bagging + random feature subsets\n",
    "- Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Quick Recap\n",
    "\n",
    "- **Decision trees** split data on features that best separate the classes (using Gini impurity)\n",
    "- Deeper trees fit training data better but are prone to **overfitting**\n",
    "- `max_depth` controls how deep the tree can grow — smaller = simpler model\n",
    "- **Random forests** = many decision trees, each trained on a random subset of data and features\n",
    "- Random forests are more robust and stable than a single tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the **California Housing** dataset, converted to a binary classification problem: is the median house value above or below the overall median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing and convert to binary classification\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Create binary target: 1 if above median, 0 if below\n",
    "median_value = df['MedHouseVal'].median()\n",
    "df['high_value'] = (df['MedHouseVal'] > median_value).astype(int)\n",
    "\n",
    "print(f\"Median house value: ${median_value * 100000:,.0f}\")\n",
    "df['high_value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 1. Use AI — Prepare features and train/test split\n",
    "\n",
    "Select features (`MedInc`, `HouseAge`, `AveRooms`, `AveOccup`, `Latitude`, `Longitude`), define the target as `high_value`, and split into train/test (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select features and target\n",
    "\n",
    "\n",
    "# Step 2: Train/test split (80/20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2. Use AI — Fit and visualize a shallow decision tree\n",
    "\n",
    "Fit a `DecisionTreeClassifier(max_depth=3, random_state=42)` and visualize it with `plot_tree()`. Use `filled=True` and pass the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit DecisionTreeClassifier with max_depth=3\n",
    "\n",
    "\n",
    "# Step 2: Visualize with plot_tree (use figsize=(20, 10) for readability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 3. Interpretation — Reading the tree\n",
    "\n",
    "Look at the tree visualization:\n",
    "- What is the **first feature** the tree splits on? Why do you think it chose that feature?\n",
    "- Trace the path for a neighborhood with: median income = 3.5, house age = 20. What does the tree predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 4. Use AI — Compare trees with different depths\n",
    "\n",
    "Fit decision trees with `max_depth` = 2, 5, 10, and None (unlimited). Print the **train accuracy** and **test accuracy** for each. Which one overfits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit trees with different max_depth values and compare train/test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 5. Interpretation — Overfitting\n",
    "\n",
    "Explain in your own words why an unlimited-depth tree overfits. What is it \"memorizing\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 6. Use AI — Fit a random forest and compare\n",
    "\n",
    "Fit a `RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)`. Compare its test accuracy to the best single decision tree from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit RandomForestClassifier\n",
    "\n",
    "\n",
    "# Step 2: Print test accuracy and compare to best single tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 7. Use AI — Compare feature importances\n",
    "\n",
    "Plot the feature importances for both the single decision tree (max_depth=5) and the random forest side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances side by side\n",
    "# Hint: use plt.subplots(1, 2, figsize=(14, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### 8. Interpretation — Feature importance stability\n",
    "\n",
    "Are the feature importances the same between the single tree and the random forest? Why might a random forest give more stable and reliable feature importances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "\n",
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "If you had to explain your model to a home buyer, would you show them the decision tree or the random forest results? Why?\n",
    "\n",
    "(Discuss with a neighbor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}